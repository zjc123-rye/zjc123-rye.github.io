[{"title":"Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新）","date":"2019-12-30T16:00:00.000Z","path":"2019/12/31/Flink-resources/","text":"Flink 学习项目代码地址：https://github.com/zhisheng17/flink-learning 麻烦路过的各位亲给这个项目点个 star，太不易了，写了这么多，算是对我坚持下来的一种鼓励吧！ 本项目结构 2019/06/08 新增 Flink 四本电子书籍的 PDF，在 books 目录下： Introduction_to_Apache_Flink_book.pdf 这本书比较薄，处于介绍阶段，国内有这本的翻译书籍 Learning Apache Flink.pdf 这本书比较基础，初学的话可以多看看 Stream Processing with Apache Flink.pdf 这本书是 Flink PMC 写的 Streaming System.pdf 这本书评价不是一般的高 2019/06/09 新增流处理引擎相关的 Paper，在 paper 目录下： 流处理引擎相关的 Paper 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 Flink 源码项目结构 学习资料另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到，转载请联系本人获取授权，违者必究。 更多私密资料请加入知识星球！ 有人要问知识星球里面更新什么内容？值得加入吗？ 目前知识星球内已更新的系列文章： 1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalonesession 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 除了《从1到100深入学习Flink》源码学习这个系列文章，《从0到1学习Flink》的案例文章也会优先在知识星球更新，让大家先通过一些 demo 学习 Flink，再去深入源码学习！ 如果学习 Flink 的过程中，遇到什么问题，可以在里面提问，我会优先解答，这里做个抱歉，自己平时工作也挺忙，微信的问题不能做全部做一些解答，但肯定会优先回复给知识星球的付费用户的，庆幸的是现在星球里的活跃氛围还是可以的，有不少问题通过提问和解答的方式沉淀了下来。 1、为何我使用 ValueState 保存状态 Job 恢复是状态没恢复？ 2、flink中watermark究竟是如何生成的，生成的规则是什么，怎么用来处理乱序数据 3、消费kafka数据的时候，如果遇到了脏数据，或者是不符合规则的数据等等怎么处理呢？ 4、在Kafka 集群中怎么指定读取/写入数据到指定broker或从指定broker的offset开始消费？ 5、Flink能通过oozie或者azkaban提交吗？ 6、jobmanager挂掉后，提交的job怎么不经过手动重新提交执行？ 7、使用flink-web-ui提交作业并执行 但是/opt/flink/log目录下没有日志文件 请问关于flink的日志（包括jobmanager、taskmanager、每个job自己的日志默认分别存在哪个目录 ）需要怎么配置？ 8、通过flink 仪表盘提交的jar 是存储在哪个目录下？ 9、从Kafka消费数据进行etl清洗，把结果写入hdfs映射成hive表，压缩格式、hive直接能够读取flink写出的文件、按照文件大小或者时间滚动生成文件 10、flink jar包上传至集群上运行，挂掉后，挂掉期间kafka中未被消费的数据，在重新启动程序后，是自动从checkpoint获取挂掉之前的kafka offset位置，自动消费之前的数据进行处理，还是需要某些手动的操作呢？ 11、flink 启动时不自动创建 上传jar的路径，能指定一个创建好的目录吗 12、Flink sink to es 集群上报 slot 不够，单机跑是好的，为什么？ 13、Fllink to elasticsearch如何创建索引文档期时间戳？ 14、blink有没有api文档或者demo，是否建议blink用于生产环境。 15、flink的Python api怎样？bug多吗？ 16、Flink VS Spark Streaming VS Storm VS Kafka Stream 17、你们做实时大屏的技术架构是什么样子的？flume→kafka→flink→redis，然后后端去redis里面捞数据，酱紫可行吗？ 18、做一个统计指标的时候，需要在Flink的计算过程中多次读写redis，感觉好怪，星主有没有好的方案？ 19、Flink 使用场景大分析，列举了很多的常用场景，可以好好参考一下 20、将kafka中数据sink到mysql时，metadata的数据为空，导入mysql数据不成功？？？ 21、使用了ValueState来保存中间状态，在运行时中间状态保存正常，但是在手动停止后，再重新运行，发现中间状态值没有了，之前出现的键值是从0开始计数的，这是为什么？是需要实现CheckpointedFunction吗？ 22、flink on yarn jobmanager的HA需要怎么配置。还是说yarn给管理了 23、有两个数据流就行connect，其中一个是实时数据流（kafka 读取)，另一个是配置流。由于配置流是从关系型数据库中读取，速度较慢，导致实时数据流流入数据的时候，配置信息还未发送，这样会导致有些实时数据读取不到配置信息。目前采取的措施是在connect方法后的flatmap的实现的在open 方法中，提前加载一次配置信息，感觉这种实现方式不友好，请问还有其他的实现方式吗？ 24、Flink能通过oozie或者azkaban提交吗？ 25、不采用yarm部署flink，还有其他的方案吗？ 主要想解决服务器重启后，flink服务怎么自动拉起？ jobmanager挂掉后，提交的job怎么不经过手动重新提交执行？ 26、在一个 Job 里将同份数据昨晚清洗操作后，sink 到后端多个地方（看业务需求），如何保持一致性？（一个sink出错，另外的也保证不能插入） 27、flink sql任务在某个特定阶段会发生tm和jm丢失心跳，是不是由于gc时间过长呢， 28、有这样一个需求，统计用户近两周进入产品详情页的来源（1首页大搜索，2产品频道搜索，3其他），为php后端提供数据支持，该信息在端上报事件中，php直接获取有点困难。 我现在的解决方案 通过flink滚动窗口（半小时），统计用户半小时内3个来源pv，然后按照日期序列化，直接写mysql。php从数据库中解析出来，再去统计近两周占比。 问题1，这个需求适合用flink去做吗？ 问题2，我的方案总感觉怪怪的，有没有好的方案？ 29、一个task slot 只能同时运行一个任务还是多个任务呢？如果task slot运行的任务比较大，会出现OOM的情况吗？ 30、你们怎么对线上flink做监控的，如果整个程序失败了怎么自动重启等等 31、flink cep规则动态解析有接触吗？有没有成型的框架？ 32、每一个Window都有一个watermark吗？window是怎么根据watermark进行触发或者销毁的？ 33、 CheckPoint与SavePoint的区别是什么？ 34、flink可以在算子中共享状态吗？或者大佬你有什么方法可以共享状态的呢？ 35、运行几分钟就报了，看taskmager日志，报的是 failed elasticsearch bulk request null，可是我代码里面已经做过空值判断了呀 而且也过滤掉了，flink版本1.7.2 es版本6.3.1 36、这种情况，我们调并行度 还是配置参数好 37、大家都用jdbc写，各种数据库增删查改拼sql有没有觉得很累，ps.set代码一大堆，还要计算每个参数的位置 38、关于datasource的配置，每个taskmanager对应一个datasource?还是每个slot? 实际运行下来，每个slot中datasorce线程池只要设置1就行了，多了也用不到? 39、kafka现在每天出现数据丢失，现在小批量数据，一天200W左右, kafka版本为 1.0.0，集群总共7个节点，TOPIC有十六个分区，单条报文1.5k左右 40、根据key.hash的绝对值 对并发度求模，进行分组，假设10各并发度，实际只有8个分区有处理数据，有2个始终不处理，还有一个分区处理的数据是其他的三倍，如截图 41、flink每7小时不知道在处理什么， CPU 负载 每7小时，有一次高峰，5分钟内平均负载超过0.8，如截图 42、有没有Flink写的项目推荐？我想看到用Flink写的整体项目是怎么组织的，不单单是一个单例子 43、Flink 源码的结构图 44、我想根据不同业务表（case when）进行不同的redis sink（hash ，set），我要如何操作？ 45、这个需要清理什么数据呀，我把hdfs里面的已经清理了 启动还是报这个 46、 在流处理系统，在机器发生故障恢复之后，什么情况消息最多会被处理一次？什么情况消息最少会被处理一次呢？ 47、我检查点都调到5分钟了，这是什么问题 48、reduce方法后 那个交易时间 怎么不是最新的，是第一次进入的那个时间， 49、Flink on Yarn 模式，用yarn session脚本启动的时候，我在后台没有看到到Jobmanager，TaskManager，ApplicationMaster这几个进程，想请问一下这是什么原因呢？因为之前看官网的时候，说Jobmanager就是一个jvm进程，Taskmanage也是一个JVM进程 50、Flink on Yarn的时候得指定 多少个TaskManager和每个TaskManager slot去运行任务，这样做感觉不太合理，因为用户也不知道需要多少个TaskManager适合，Flink 有动态启动TaskManager的机制吗。 51、参考这个例子，Flink 零基础实战教程：如何计算实时热门商品 | Jark’s Blog， 窗口聚合的时候，用keywindow，用的是timeWindowAll，然后在aggregate的时候用aggregate(new CustomAggregateFunction(), new CustomWindowFunction())，打印结果后，发现窗口中一直使用的重复的数据，统计的结果也不变，去掉CustomWindowFunction()就正常了 ？ 非常奇怪 52、用户进入产品预定页面（端埋点上报），并填写了一些信息（端埋点上报），但半小时内并没有产生任何订单，然后给该类用户发送一个push。 1. 这种需求适合用flink去做吗？2. 如果适合，说下大概的思路 53、业务场景是实时获取数据存redis，请问我要如何按天、按周、按月分别存入redis里？（比方说过了一天自动换一个位置存redis） 54、有人 AggregatingState 的例子吗, 感觉官方的例子和 官网的不太一样? 55、flink-jdbc这个jar有吗？怎么没找到啊？1.8.0的没找到，1.6.2的有 56、现有个关于savepoint的问题，操作流程为，取消任务时设置保存点，更新任务，从保存点启动任务；现在遇到个问题，假设我中间某个算子重写，原先通过state编写，有用定时器，现在更改后，采用窗口，反正就是实现方式完全不一样；从保存点启动就会一直报错，重启，原先的保存点不能还原，此时就会有很多数据重复等各种问题，如何才能保证数据不丢失，不重复等，恢复到停止的时候，现在想到的是记下kafka的偏移量，再做处理，貌似也不是很好弄，有什么解决办法吗 57、需要在flink计算app页面访问时长，消费Kafka计算后输出到Kafka。第一条log需要等待第二条log的时间戳计算访问时长。我想问的是，flink是分布式的，那么它能否保证执行的顺序性？后来的数据有没有可能先被执行？ 58、我公司想做实时大屏，现有技术是将业务所需指标实时用spark拉到redis里存着，然后再用一条spark streaming流计算简单乘除运算，指标包含了各月份的比较。请问我该如何用flink简化上述流程？ 59、flink on yarn 方式，这样理解不知道对不对，yarn-session这个脚本其实就是准备yarn环境的，执行run任务的时候，根据yarn-session初始化的yarnDescription 把 flink 任务的jobGraph提交到yarn上去执行 60、同样的代码逻辑写在单独的main函数中就可以成功的消费kafka ，写在一个spring boot的程序中，接受外部请求，然后执行相同的逻辑就不能消费kafka。你遇到过吗？能给一些查问题的建议，或者在哪里打个断点，能看到为什么消费不到kafka的消息呢？ 61、请问下flink可以实现一个流中同时存在订单表和订单商品表的数据 两者是一对多的关系 能实现得到 以订单表为主 一个订单多个商品 这种需求嘛 62、在用中间状态的时候，如果中间一些信息保存在state中，有没有必要在redis中再保存一份，来做第三方的存储。 63、能否出一期flink state的文章。什么场景下用什么样的state？如，最简单的，实时累加update到state。 64、flink的双流join博主有使用的经验吗？会有什么常见的问题吗 65、窗口触发的条件问题 66、flink 定时任务怎么做？有相关的demo么？ 67、流式处理过程中数据的一致性如何保证或者如何检测 68、重启flink单机集群，还报job not found 异常。 69、kafka的数据是用 org.apache.kafka.common.serialization.ByteArraySerialize序列化的，flink这边消费的时候怎么通过FlinkKafkaConsumer创建DataStream？ 70、现在公司有一个需求，一些用户的支付日志，通过sls收集，要把这些日志处理后，结果写入到MySQL，关键这些日志可能连着来好几条才是一个用户的，因为发起请求，响应等每个环节都有相应的日志，这几条日志综合处理才能得到最终的结果，请问博主有什么好的方法没有？ 71、flink 支持hadoop 主备么？ hadoop主节点挂了 flink 会切换到hadoop 备用节点？ 72、请教大家: 实际 flink 开发中用 scala 多还是 java多些？ 刚入手 flink 大数据 scala 需要深入学习么？ 73、我使用的是flink是1.7.2最近用了split的方式分流，但是底层的SplitStream上却标注为Deprecated，请问是官方不推荐使用分流的方式吗？ 74、KeyBy 的正确理解，和数据倾斜问题的解释 75、用flink时，遇到个问题 checkpoint大概有2G左右， 有背压时，flink会重启有遇到过这个问题吗 76、flink使用yarn-session方式部署，如何保证yarn-session的稳定性，如果yarn-session挂了，需要重新部署一个yarn-session，如何恢复之前yarn-session上的job呢，之前的checkpoint还能使用吗？ 77、我想请教一下关于sink的问题。我现在的需求是从Kafka消费Json数据，这个Json数据字段可能会增加，然后将拿到的json数据以parquet的格式存入hdfs。现在我可以拿到json数据的schema，但是在保存parquet文件的时候不知道怎么处理。一是flink没有专门的format parquet，二是对于可变字段的Json怎么处理成parquet比较合适？ 78、flink如何在较大的数据量中做去重计算。 79、flink能在没有数据的时候也定时执行算子吗？ 80、使用rocksdb状态后端，自定义pojo怎么实现序列化和反序列化的，有相关demo么？ 81、check point 老是失败，是不是自定义的pojo问题？到本地可以，到hdfs就不行，网上也有很多类似的问题 都没有一个很好的解释和解决方案 82、cep规则如图，当start事件进入时，时间00:00:15，而后进入end事件，时间00:00:40。我发现规则无法命中。请问within 是从start事件开始计时？还是跟window一样根据系统时间划分的？如果是后者，请问怎么配置才能从start开始计时？ 83、Flink聚合结果直接写Mysql的幂等性设计问题 84、Flink job打开了checkpoint，用的rocksdb，通过观察hdfs上checkpoint目录，为啥算副本总量会暴增爆减 85、Flink 提交任务的 jar包可以指定路径为 HDFS 上的吗 86、在flink web Ui上提交的任务，设置的并行度为2，flink是stand alone部署的。两个任务都正常的运行了几天了，今天有个地方逻辑需要修改，于是将任务cancel掉(在命令行cancel也试了)，结果taskmanger挂掉了一个节点。后来用其他任务试了，也同样会导致节点挂掉 87、一个配置动态更新的问题折腾好久（配置用个静态的map变量存着，有个线程定时去数据库捞数据然后存在这个map里面更新一把），本地 idea 调试没问题，集群部署就一直报 空指针异常。下游的算子使用这个静态变量map去get key在集群模式下会出现这个空指针异常，估计就是拿不到 map 88、批量写入MySQL，完成HBase批量写入 89、用flink清洗数据，其中要访问redis，根据redis的结果来决定是否把数据传递到下流，这有可能实现吗？ 90、监控页面流处理的时候这个发送和接收字节为0。 91、sink到MySQL，如果直接用idea的话可以运行，并且成功，大大的代码上面用的FlinkKafkaConsumer010，而我的Flink版本为1.7，kafka版本为2.12，所以当我用FlinkKafkaConsumer010就有问题，于是改为 FlinkKafkaConsumer就可以直接在idea完成sink到MySQL，但是为何当我把该程序打成Jar包，去运行的时候，就是报FlinkKafkaConsumer找不到呢 92、SocketTextStreamWordCount中输入中文统计不出来，请问这个怎么解决，我猜测应该是需要修改一下代码，应该是这个例子默认统计英文 93、 Flink 应用程序本地 ide 里面运行的时候并行度是怎么算的？ 94、 请问下flink中对于窗口的全量聚合有apply和process两种 他们有啥区别呢 95、不知道大大熟悉Hbase不，我想直接在Hbase中查询某一列数据，因为有重复数据，所以想使用distinct统计实际数据量，请问Hbase中有没有类似于sql的distinct关键字。如果没有，想实现这种可以不？ 96、 来分析一下现在Flink,Kafka方面的就业形势，以及准备就业该如何准备的这方面内容呢？ 97、 大佬知道flink的dataStream可以转换为dataSet吗？因为数据需要11分钟一个批次计算五六个指标，并且涉及好几步reduce，计算的指标之间有联系，用Stream卡住了。 98、1.如何在同一窗口内实现多次的聚合，比如像spark中的这样2.多个实时流的jion可以用window来处理一批次的数据吗？ 99、写的批处理的功能，现在本机跑是没问题的，就是在linux集群上出现了问题，就是不知道如果通过本地调用远程jar包然后传参数和拿到结果参数返回本机 100、我用standalone开启一个flink集群，上传flink官方用例Socket Window WordCount做测试，开启两个parallelism能正常运行，但是开启4个parallelism后出现错误 101、 有使用AssignerWithPunctuatedWatermarks 的案例Demo吗？网上找了都是AssignerWithPeriodicWatermarks的，不知道具体怎么使用？ 102、 有一个datastream(从文件读取的)，然后我用flink sql进行计算，这个sql是一个加总的运算，然后通过retractStreamTableSink可以把文件做sql的结果输出到文件吗？这个输出到文件的接口是用什么呢？ 103、 为啥split这个流设置为过期的 104、 需要使用flink table的水印机制控制时间的乱序问题，这种场景下我就使用水印+窗口了，我现在写的demo遇到了问题，就是在把触发计算的窗口table（WindowedTable）转换成table进行sql操作时发现窗口中的数据还是乱序的，是不是flink table的WindowedTable不支持水印窗口转table-sql的功能 105、 Flink 对 SQL 的重视性 106、 flink job打开了checkpoint，任务跑了几个小时后就出现下面的错，截图是打出来的日志，有个OOM，又遇到过的没？ 107、 本地测试是有数据的，之前该任务放在集群也是有数据的，可能提交过多次，现在读不到数据了 group id 也换过了， 只能重启集群解决么？ 108、使用flink清洗数据存到es中，直接在flatmap中对处理出来的数据用es自己的ClientInterface类直接将数据存入es当中，不走sink，这样的处理逻辑是不是会有问题。 108、 flink从kafka拿数据（即增量数据）与存量数据进行内存聚合的需求，现在有一个方案就是程序启动的时候先用flink table将存量数据加载到内存中创建table中，然后将stream的增量数据与table的数据进行关联聚合后输出结束，不知道这种方案可行么。目前个人认为有两个主要问题：1是增量数据stream转化成append table后不知道能与存量的table关联聚合不，2是聚合后输出的结果数据是否过于频繁造成网络传输压力过大 109、 设置时间时间特性有什么区别呢, 分别在什么场景下使用呢?两种设置时间延迟有什么区别呢 , 分别在什么场景下使用 110、 flink从rabbitmq中读取数据，设置了rabbitmq的CorrelationDataId和checkpoint为EXACTLY_ONCE；如果flink完成一次checkpoint后，在这次checkpoint之前消费的数据都会从mq中删除。如果某次flink停机更新，那就会出现mq中的一些数据消费但是处于Unacked状态。在flink又重新开启后这批数据又会重新消费。那这样是不是就不能保证EXACTLY_ONCE了 111、1. 在Flink checkpoint 中, 像 operator的状态信息 是在设置了checkpoint 之后自动的进行快照吗 ?2. 上面这个和我们手动存储的 Keyed State 进行快照(这个应该是增量快照) 112、现在有个实时商品数，交易额这种统计需求，打算用 flink从kafka读取binglog日志进行计算，但binglog涉及到insert和update这种操作时 怎么处理才能统计准确，避免那种重复计算的问题？ 113、我这边用flink做实时监控，功能很简单，就是每条消息做keyby然后三分钟窗口，然后做些去重操作，触发阈值则报警，现在问题是同一个时间窗口同一个人的告警会触发两次，集群是三台机器，standalone cluster，初步结果是三个算子里有两个收到了同样的数据 114、在使用WaterMark的时候，默认是每200ms去设置一次watermark，那么每个taskmanager之间，由于得到的数据不同，所以往往产生的最大的watermark不同。 那么这个时候，是各个taskmanager广播这个watermark，得到全局的最大的watermark，还是说各个taskmanager都各自用自己的watermark。主要没看到广播watermark的源码。不知道是自己观察不仔细还是就是没有广播这个变量。 115、现在遇到一个需求，需要在job内部定时去读取redis的信息，想请教flink能实现像普通程序那样的定时任务吗？ 116、有个触发事件开始聚合，等到数量足够，或者超时则sink推mq 环境 flink 1.6 用了mapState 记录触发事件 1 数据足够这个OK 2 超时state ttl 1.6支持，但是问题来了，如何在超时时候增加自定义处理？ 117、请问impala这种mpp架构的sql引擎，为什么稳定性比较差呢？ 118、watermark跟并行度相关不是，过于全局了，期望是keyby之后再针对每个keyed stream 打watermark，这个有什么好的实践呢？ 119、请问如果把一个文件的内容读取成datastream和dataset，有什么区别吗？？他们都是一条数据一条数据的被读取吗？ 120、有没有kylin相关的资料，或者调优的经验？ 121、flink先从jdbc读取配置表到流中，另外从kafka中新增或者修改这个配置，这个场景怎么把两个流一份配置流？我用的connect,接着发不成广播变量，再和实体流合并，但在合并时报Exception in thread “main” java.lang.IllegalArgumentException 122、Flink exactly-once，kafka版本为0.11.0 ，sink基于FlinkKafkaProducer011 每五分钟一次checkpoint，但是checkpoint开始后系统直接卡死，at-lease-once 一分钟能完成的checkpoint， 现在十分钟无法完成没进度还是0， 不知道哪里卡住了 123、flink的状态是默认存在于内存的(也可以设置为rocksdb或hdfs)，而checkpoint里面是定时存放某个时刻的状态信息，可以设置hdfs或rocksdb是这样理解的吗？ 124、Flink异步IO中，下图这两种有什么区别？为啥要加 CompletableFuture.supplyAsync，不太明白？ 125、flink的状态是默认存在于内存的(也可以设置为rocksdb或hdfs)，而checkpoint里面是定时存放某个时刻的状态信息，可以设置hdfs或rocksdb是这样理解的吗？ 126、有个计算场景，从kafka消费两个数据源，两个数据结构都有时间段概念，计算需要做的是匹配两个时间段，匹配到了，就生成一条新的记录。请问使用哪个工具更合适，flink table还是cep？请大神指点一下 我这边之前的做法，将两个数据流转为table.两个table over window后join成新的表。结果job跑一会就oom. 127、一个互联网公司，或者一个业务系统，如果想做一个全面的监控要怎么做？有什么成熟的方案可以参考交流吗？有什么有什么度量指标吗？ 128、怎么深入学习flink,或者其他大数据组件，能为未来秋招找一份大数据相关（计算方向）的工作增加自己的竞争力？ 129、oppo的实时数仓，其中明细层和汇总层都在kafka中，他们的关系库的实时数据也抽取到kafka的ods，那么在构建数仓的，需要join 三四个大业务表，业务表会变化，那么是大的业务表是从kafka的ods读取吗？实时数仓，多个大表join可以吗 130、Tuple类型有什么方法转换成json字符串吗？现在的场景是，结果在存储到sink中时希望存的是json字符串，这样应用程序获取数据比较好转换一点。如果Tuple不好转换json字符串，那么应该以什么数据格式存储到sink中 140、端到端的数据保证，是否意味着中间处理程序中断，也不会造成该批次处理失败的消息丢失，处理程序重新启动之后，会再次处理上次未处理的消息 141、关于flink datastream window相关的。比如我现在使用滚动窗口，统计一周内去重用户指标，按照正常watermark触发计算，需要等到当前周的window到达window的endtime时，才会触发，这样指标一周后才能产出结果。我能不能实现一小时触发一次计算，每次统计截止到当前时间，window中所有到达元素的去重数量。 142、FLIP-16 Loop Fault Tolerance 是讲现在的checkpoint机制无法在stream loop的时候容错吗？现在这个问题解决了没有呀？ 143、现在的需求是，统计各个key的今日累计值，一分钟输出一次。如，各个用户今日累计点击次数。这种需求用datastream还是table API方便点？ 144、本地idea可以跑的工程，放在standalone集群上，总报错，报错截图如下，大佬请问这是啥原因 145、比如现在用k8s起了一个flink集群，这时候数据源kafka或者hdfs会在同一个集群上吗，还是会单独再起一个hdfs/kafka集群 146、flink kafka sink 的FlinkFixedPartitioner 分配策略，在并行度小于topic的partitions时，一个并行实例固定的写消息到固定的一个partition，那么就有一些partition没数据写进去？ 147、基于事件时间，每五分钟一个窗口，五秒钟滑动一次，同时watermark的时间同样是基于事件事件时间的，延迟设为1分钟，假如数据流从12：00开始，如果12：07-12：09期间没有产生任何一条数据，即在12：07-12：09这段间的数据流情况为···· （12：07:00，xxx）,(12:09:00,xxx)······，那么窗口[12:02:05-12:07:05]，[12:02:10-12:07:10]等几个窗口的计算是否意味着只有等到，12：09：00的数据到达之后才会触发 148、使用flink1.7，当消费到某条消息(protobuf格式)，报Caused by: org.apache.kafka.common.KafkaException: Record batch for partition Notify-18 at offset 1803009 is invalid, cause: Record is corrupt 这个异常。 如何设置跳过已损坏的消息继续消费下一条来保证业务不终断？ 我看了官网kafka connectors那里，说在DeserializationSchema.deserialize(…)方法中返回null，flink就会跳过这条消息，然而依旧报这个异常 149、是否可以抽空总结一篇Flink 的 watermark 的原理案例？一直没搞明白基于事件时间处理时的数据乱序和数据迟到底咋回事 150、flink中rpc通信的原理，与几个类的讲解，有没有系统详细的文章样，如有求分享，谢谢 151、Flink中如何使用基于事件时间处理，但是又不使用Watermarks? 我在会话窗口中使用遇到一些问题，图一是基于处理时间的，测试结果session是基于keyby(用户)的，图二是基于事件时间的，不知道是我用法不对还是怎么的，测试结果发现并不是基于keyby(用户的)，而是全局的session。不知道怎么修改？ 152、flink实时计算平台，yarn模式日志收集怎么做，为什么会checkpoint失败，报警处理，后需要做什么吗？job监控怎么做 153、有flink与jstorm的在不同应用场景下, 性能比较的数据吗? 从网络上能找大部分都是flink与storm的比较. 在jstorm官网上有一份比较的图表, 感觉参考意义不大, 应该是比较早的flink版本. 154、为什么使用SessionWindows.withGap窗口的话，State存不了东西呀，每次加1 ，拿出来都是null, 我换成 TimeWindow就没问题。 155、请问一下，flink datastream流处理怎么统计去重指标？ 官方文档中只看到批处理有distinct概念。 156、好全的一篇文章，对比分析 Flink，Spark Streaming，Storm 框架 157、关于 structured_streaming 的 paper 158、zookeeper集群切换领导了，flink集群项目重启了就没有数据的输入和输出了，这个该从哪方面入手解决？ 159、我想请教下datastream怎么和静态数据join呢 160、时钟问题导致收到了明天的数据，这时候有什么比较好的处理方法？看到有人设置一个最大的跳跃阈值，如果当前数据时间 - 历史最大时间 超过阈值就不更新。如何合理的设计水印，有没有一些经验呢？ 161、大佬们flink怎么定时查询数据库？ 162、现在我们公司有个想法，就是提供一个页面，在页面上选择source sink 填写上sql语句，然后后台生成一个flink的作业，然后提交到集群。功能有点类似于华为的数据中台，就是页面傻瓜式操作。后台能自动根据相应配置得到结果。请问拘你的了解，可以实现吗？如何实现？有什么好的思路。现在我无从下手 163、请教一下 flink on yarn 的 ha机制 164、在一般的流处理以及cep, 都可以对于eventtime设置watermark, 有时可能需要设置相对大一点的值, 这内存压力就比较大, 有没有办法不应用jvm中的内存, 而用堆外内存, 或者其他缓存, 最好有cache机制, 这样可以应对大流量的峰值. 165、请教一个flink sql的问题。我有两个聚合后的流表A和B，A和Bjoin得到C表。在设置state TTL 的时候是直接对C表设置还是，对A表和B表设置比较好？ 166、spark改写为flink，会不会很复杂，还有这两者在SQL方面的支持差别大吗？ 167、请问flink allowedLateness导致窗口被多次fire，最终数据重复消费，这种问题怎么处理，数据是写到es中 168、设置taskmanager.numberOfTaskSlots: 4的时候没有问题，但是cpu没有压上去，只用了30%左右，于是设置了taskmanager.numberOfTaskSlots: 8，但是就报错误找不到其中一个自定义的类，然后kafka数据就不消费了。为什么？cpu到多少合适？slot是不是和cpu数量一致是最佳配置？kafka分区数多少合适，是不是和slot,parallesim一致最佳？ 169、需求是根据每条日志切分出需要9个字段，有五个指标再根据9个字段的不同组合去做计算。 第一个方法是：我目前做法是切分的9个字段开5分钟大小1分钟计算一次的滑动窗口窗口，进行一次reduce去重，然后再map取出需要的字段，然后过滤再开5分钟大小1分钟计算一次的滑动窗口窗口进行计算保存结果，这个思路遇到的问题是上一个滑动窗口会每一分钟会计算5分钟数据，到第二个窗口划定的5分钟范围的数据会有好多重复，这个思路会造成数据重复。 第二个方法是：切分的9个字段开5分钟大小1分钟计算一次的滑动窗口窗口，再pross方法里完成所有的过滤，聚合计算，但是再高峰期每分钟400万条数据，这个思路担心在高峰期flink计算不过来 170、a,b,c三个表，a和c有eventtime，a和c直接join可以，a和b join后再和c join 就会报错，这是怎么回事呢 171、自定义的source是这样的（图一所示） 使用的时候是这样的（图二所示），为什么无论 sum.print().setParallelism(2)（图2所示）的并行度设置成几最后结果都是这样的 172、刚接触flink，如有问的不合适的地方，请见谅。 1、为什么说flink是有状态的计算？ 2、这个状态是什么？3、状态存在哪里 173、这边用flink 1.8.1的版本，采用flink on yarn，hadoop版本2.6.0。代码是一个简单的滚动窗口统计函数，但启动的时候报错，如下图片。 （2）然后我把flink版本换成1.7.1，重新提交到2.6.0的yarn平台，就能正常运行了。 （3）我们测试集群hadoop版本是3.0，我用flink 1.8.1版本将这个程序再次打包，提交到3.0版本的yarn平台，也能正常运行。 貌似是flink 1.8.1版本与yarn 2.6.0版本不兼容造成的这个问题 174、StateBackend我使用的是MemoryStateBackend， State是怎么释放内存的，例如我在函数中用ValueState存储了历史状态信息。但是历史状态数据我没有手动释放，那么程序会自动释放么？还是一直驻留在内存中 175、请问老师是否可以提供一些Apachebeam的学习资料 谢谢 176、flink 的 DataSet或者DataStream支持索引查询以及删除吗，像spark rdd，如果不支持的话，该转换成什么 177、关于flink的状态，能否把它当做数据库使用，类似于内存数据库，在处理过程中存业务数据。如果是数据库可以算是分布式数据库吗?是不是使用rocksdb这种存储方式才算是?支持的单库大小是不是只是跟本地机器的磁盘大小相关?如果使用硬盘存储会不会效率性能有影响 178、我这边做了个http sink，想要批量发送数据，不过现在只能用数量控制发送，但最后的几个记录没法触发发送动作，想问下有没有什么办法 179、请问下如何做定时去重计数，就是根据时间分窗口，窗口内根据id去重计数得出结果，多谢。试了不少办法，没有简单直接办法 180、我有个job使用了elastic search sink. 设置了批量5000一写入，但是看es监控显示每秒只能插入500条。是不是bulkprocessor的currentrequest为0有关 181、有docker部署flink的资料吗 182、在说明KeyBy的StreamGraph执行过程时，keyBy的ID为啥是6？ 根据前面说，ID是一个静态变量，每取一次就递增1，我觉得应该是3啊，是我理解错了吗 183、有没计划出Execution Graph的远码解析 184、可以分享下物理执行图怎样划分task，以及task如何执行，还有他们之间数据如何传递这块代码嘛？ 185、Flink源码和这个学习项目的结构图 186、请问flink1.8，如何做到动态加载外部udf-jar包呢？ 187、同一个Task Manager中不同的Slot是怎么交互的，比如：source处理完要传递给map的时候，如果在不同的Slot中，他们的内存是相互隔离，是怎么交互的呢？ 我猜是通过序列化和反序列化对象，并且通过网络来进行交互的 188、你们有没有这种业务场景。flink从kafka里面取数据，每一条数据里面有mongdb表A的id,这时我会在map的时候采用flink的异步IO连接A表，然后查询出A表的字段1，再根据该字段1又需要异步IO去B表查询字段2，然后又根据字段2去C表查询字段3…..像这样的业务场景，如果多来几种逻辑，我应该用什么方案最好呢 189、今天本地运行flink程序，消费socket中的数据，连续只能消费两条，第三条flink就消费不了了 190、源数据经过过滤后分成了两条流，然后再分别提取事件时间和水印，做时间窗口，我测试时一条流没有数据，另一条的数据看日志到了窗口操作那边就没走下去，貌似窗口一直没有等到触发 191、有做flink cep的吗，有资料没？ 192、麻烦问一下 BucketingSink跨集群写，如果任务运行在hadoop A集群，从kafka读取数据处理后写到Hadoo B集群，即使把core-site.xml和hdfs-site.xml拷贝到代码resources下，路径使用hdfs://hadoopB/xxx，会提示ava.lang.RuntimeException: Error while creating FileSystem when initializing the state of the BucketingSink.，跨集群写这个问题 flink不支持吗？ 193、想咨询下，如何对flink中的datastream和dataset进行数据采样 194、一个flink作业经常发生oom，可能是什么原因导致的。 处理流程只有15+字段的解析，redis数据读取等操作，TM配置10g。 业务会在夜间刷数据，qps能打到2500左右~ 195、我看到flink 1.8的状态过期仅支持Processing Time，那么如果我使用的是Event time那么状态就不会过期吗 196、请问我想每隔一小时统计一个属性从当天零点到当前时间的平均值，这样的时间窗该如何定义？ 197、flink任务里面反序列化一个类，报ClassNotFoundException，可是包里面是有这个类的，有遇到这种情况吗？ 198、在构造StreamGraph，类似PartitionTransformmation 这种类型的 transform，为什么要添加成一个虚拟节点，而不是一个实际的物理节点呢？ 199、flink消费kafka的数据写入到hdfs中，我采用了BucketingSink 这个sink将operator出来的数据写入到hdfs文件上，并通过在hive中建外部表来查询这个。但现在有个问题，处于in-progress的文件，hive是无法识别出来该文件中的数据，可我想能在hive中实时查询进来的数据，且不想产生很多的小文件，这个该如何处理呢 200、采用Flink单机集群模式一个jobmanager和两个taskmanager，机器是单机是24核，现在做个简单的功能从kafka的一个topic转满足条件的消息到另一个topic，topic的分区是30，我设置了程序默认并发为30，现在每秒消费2w多数据，不够快，请问可以怎么提高job的性能呢？ 201、Flink Metric 源码分析 等等等，还有很多，复制粘贴的我手累啊 😂 另外里面还会及时分享 Flink 的一些最新的资料（包括数据、视频、PPT、优秀博客，持续更新，保证全网最全，因为我知道 Flink 目前的资料还不多） 关于自己对 Flink 学习的一些想法和建议 Flink 全网最全资料获取，持续更新，点击可以获取 再就是星球用户给我提的一点要求：不定期分享一些自己遇到的 Flink 项目的实战，生产项目遇到的问题，是如何解决的等经验之谈！ 1、如何查看自己的 Job 执行计划并获取执行计划图 2、当实时告警遇到 Kafka 千万数据量堆积该咋办？ 3、如何在流数据中比两个数据的大小？多种解决方法 4、kafka 系列文章 5、Flink环境部署、应用配置及运行应用程序 6、监控平台该有架构是长这样子的 7、《大数据“重磅炸弹”——实时计算框架 Flink》专栏系列文章目录大纲 8、《大数据“重磅炸弹”——实时计算框架 Flink》Chat 付费文章 9、Apache Flink 是如何管理好内存的？ 10、Flink On K8s 11、Flink-metrics-core 12、Flink-metrics-datadog 13、Flink-metrics-dropwizard 14、Flink-metrics-graphite 15、Flink-metrics-influxdb 16、Flink-metrics-jmx 17、Flink-metrics-slf4j 18、Flink-metrics-statsd 19、Flink-metrics-prometheus 当然，除了更新 Flink 相关的东西外，我还会更新一些大数据相关的东西，因为我个人之前不是大数据开发，所以现在也要狂补些知识！总之，希望进来的童鞋们一起共同进步！ 1、Java 核心知识点整理.pdf 2、假如我是面试官，我会问你这些问题 3、Kafka 系列文章和学习视频 4、重新定义 Flink 第二期 pdf 5、GitChat Flink 文章答疑记录 6、Java 并发课程要掌握的知识点 7、Lightweight Asynchronous Snapshots for Distributed Dataflows 8、Apache Flink™- Stream and Batch Processing in a Single Engine 9、Flink状态管理与容错机制 10、Flink 流批一体的技术架构以及在阿里 的实践 11、Flink Checkpoint-\u0007\b轻量级分布式快照 12、Flink 流批一体的技术架构以及在阿里 的实践 13、Stream Processing with Apache Flink pdf 14、Flink 结合机器学习算法的监控平台实践 15、《大数据重磅炸弹-实时计算Flink》预备篇——大数据实时计算介绍及其常用使用场景 pdf 和 视频 16、《大数据重磅炸弹-实时计算Flink》开篇词 pdf 和 视频 17、四本 Flink 书 18、流处理系统 的相关 paper 19、Apache Flink 1.9 特性解读 20、打造基于Flink Table API的机器学习生态 21、基于Flink on Kubernetes的大数据平台 22、基于Apache Flink的高性能机器学习算法库 23、Apache Flink在快手的应用与实践 24、Apache Flink-1.9与Hive的兼容性 25、打造基于Flink Table API的机器学习生态 26、流处理系统 的相关 paper","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"一文让你彻底了解大数据实时计算引擎 Flink","date":"2019-08-18T16:00:00.000Z","path":"2019/08/19/flink/","text":"前言在上一篇文章 你公司到底需不需要引入实时计算引擎？ 中我讲解了日常中常见的实时需求，然后分析了这些需求的实现方式，接着对比了实时计算和离线计算。随着这些年大数据的飞速发展，也出现了不少计算的框架（Hadoop、Storm、Spark、Flink）。在网上有人将大数据计算引擎的发展分为四个阶段。 第一代：Hadoop 承载的 MapReduce 第二代：支持 DAG（有向无环图）框架的计算引擎 Tez 和 Oozie，主要还是批处理任务 第三代：支持 Job 内部的 DAG（有向无环图），以 Spark 为代表 第四代：大数据统一计算引擎，包括流处理、批处理、AI、Machine Learning、图计算等，以 Flink 为代表 或许会有人不同意以上的分类，我觉得其实这并不重要的，重要的是体会各个框架的差异，以及更适合的场景。并进行理解，没有哪一个框架可以完美的支持所有的场景，也就不可能有任何一个框架能完全取代另一个。 本文将对 Flink 的整体架构和 Flink 的多种特性做个详细的介绍！在讲 Flink 之前的话，我们先来看看 数据集类型 和 数据运算模型 的种类。 数据集类型 无穷数据集：无穷的持续集成的数据集合 有界数据集：有限不会改变的数据集合 那么那些常见的无穷数据集有哪些呢？ 用户与客户端的实时交互数据 应用实时产生的日志 金融市场的实时交易记录 … 数据运算模型 流式：只要数据一直在产生，计算就持续地进行 批处理：在预先定义的时间内运行计算，当计算完成时释放计算机资源 那么我们再来看看 Flink 它是什么呢？ Flink 是什么？ Flink 是一个针对流数据和批数据的分布式处理引擎，代码主要是由 Java 实现，部分代码是 Scala。它可以处理有界的批量数据集、也可以处理无界的实时数据集。对 Flink 而言，其所要处理的主要场景就是流数据，批数据只是流数据的一个极限特例而已，所以 Flink 也是一款真正的流批统一的计算引擎。 Flink 提供了 State、Checkpoint、Time、Window 等，它们为 Flink 提供了基石，本篇文章下面会稍作讲解，具体深度分析后面会有专门的文章来讲解。 Flink 整体结构 从下至上： 1、部署：Flink 支持本地运行（IDE 中直接运行程序）、能在独立集群（Standalone 模式）或者在被 YARN、Mesos、K8s 管理的集群上运行，也能部署在云上。 2、运行：Flink 的核心是分布式流式数据引擎，意味着数据以一次一个事件的形式被处理。 3、API：DataStream、DataSet、Table、SQL API。 4、扩展库：Flink 还包括用于 CEP（复杂事件处理）、机器学习、图形处理等场景。 Flink 支持多种方式部署 Flink 支持多种模式下的运行。 Local：直接在 IDE 中运行 Flink Job 时则会在本地启动一个 mini Flink 集群 Standalone：在 Flink 目录下执行 bin/start-cluster.sh 脚本则会启动一个 Standalone 模式的集群 YARN：YARN 是 Hadoop 集群的资源管理系统，它可以在群集上运行各种分布式应用程序，Flink 可与其他应用并行于 YARN 中，Flink on YARN 的架构如下： Kubernetes：Kubernetes 是 Google 开源的容器集群管理系统，在 Docker 技术的基础上，为容器化的应用提供部署运行、资源调度、服务发现和动态伸缩等一系列完整功能，提高了大规模容器集群管理的便捷性，Flink 也支持部署在 Kubernetes 上，在 GitHub 看到有下面这种运行架构的。 通常上面四种居多，另外还支持 AWS、MapR、Aliyun OSS 等。 Flink 分布式运行Flink 作业提交架构流程可见下图： 1、Program Code：我们编写的 Flink 应用程序代码 2、Job Client：Job Client 不是 Flink 程序执行的内部部分，但它是任务执行的起点。 Job Client 负责接受用户的程序代码，然后创建数据流，将数据流提交给 Job Manager 以便进一步执行。 执行完成后，Job Client 将结果返回给用户 3、Job Manager：主进程（也称为作业管理器）协调和管理程序的执行。 它的主要职责包括安排任务，管理 checkpoint ，故障恢复等。机器集群中至少要有一个 master，master 负责调度 task，协调 checkpoints 和容灾，高可用设置的话可以有多个 master，但要保证一个是 leader, 其他是 standby; Job Manager 包含 Actor system、Scheduler、Check pointing 三个重要的组件 4、Task Manager：从 Job Manager 处接收需要部署的 Task。Task Manager 是在 JVM 中的一个或多个线程中执行任务的工作节点。 任务执行的并行性由每个 Task Manager 上可用的任务槽（Slot 个数）决定。 每个任务代表分配给任务槽的一组资源。 例如，如果 Task Manager 有四个插槽，那么它将为每个插槽分配 25％ 的内存。 可以在任务槽中运行一个或多个线程。 同一插槽中的线程共享相同的 JVM。同一 JVM 中的任务共享 TCP 连接和心跳消息。Task Manager 的一个 Slot 代表一个可用线程，该线程具有固定的内存，注意 Slot 只对内存隔离，没有对 CPU 隔离。默认情况下，Flink 允许子任务共享 Slot，即使它们是不同 task 的 subtask，只要它们来自相同的 job。这种共享可以有更好的资源利用率。 Flink API Flink 提供了不同的抽象级别的 API 以开发流式或批处理应用。 最底层提供了有状态流。它将通过 Process Function 嵌入到 DataStream API 中。它允许用户可以自由地处理来自一个或多个流数据的事件，并使用一致性、容错的状态。除此之外，用户可以注册事件时间和处理事件回调，从而使程序可以实现复杂的计算。 DataStream / DataSet API 是 Flink 提供的核心 API ，DataSet 处理有界的数据集，DataStream 处理有界或者无界的数据流。用户可以通过各种方法（map / flatmap / window / keyby / sum / max / min / avg / join 等）将数据进行转换或者计算。 Table API 是以表为中心的声明式 DSL，其中表可能会动态变化（在表达流数据时）。Table API 提供了例如 select、project、join、group-by、aggregate 等操作，使用起来却更加简洁（代码量更少）。你可以在表与 DataStream/DataSet 之间无缝切换，也允许程序将 Table API 与 DataStream 以及 DataSet 混合使用。 Flink 提供的最高层级的抽象是 SQL 。这一层抽象在语法与表达能力上与 Table API 类似，但是是以 SQL查询表达式的形式表现程序。SQL 抽象与 Table API 交互密切，同时 SQL 查询可以直接在 Table API 定义的表上执行。 Flink 程序与数据流结构 一个完整的 Flink 应用程序结构就是如上两图所示： 1、Source：数据输入，Flink 在流处理和批处理上的 source 大概有 4 类：基于本地集合的 source、基于文件的 source、基于网络套接字的 source、自定义的 source。自定义的 source 常见的有 Apache kafka、Amazon Kinesis Streams、RabbitMQ、Twitter Streaming API、Apache NiFi 等，当然你也可以定义自己的 source。 2、Transformation：数据转换的各种操作，有 Map / FlatMap / Filter / KeyBy / Reduce / Fold / Aggregations / Window / WindowAll / Union / Window join / Split / Select / Project 等，操作很多，可以将数据转换计算成你想要的数据。 3、Sink：数据输出，Flink 将转换计算后的数据发送的地点 ，你可能需要存储下来，Flink 常见的 Sink 大概有如下几类：写入文件、打印出来、写入 socket 、自定义的 sink 。自定义的 sink 常见的有 Apache kafka、RabbitMQ、MySQL、ElasticSearch、Apache Cassandra、Hadoop FileSystem 等，同理你也可以定义自己的 sink。 Flink 支持多种扩展库Flink 拥有丰富的库来进行机器学习，图形处理，关系数据处理等。由于其架构，很容易执行复杂的事件处理和警报。 Flink 提供多种 Time 语义Flink 支持多种 Time，比如 Event time、Ingestion Time、Processing Time，后面的文章 Flink 中 Processing Time、Event Time、Ingestion Time 对比及其使用场景分析 中会很详细的讲解 Flink 中 Time 的概念。 Flink 提供灵活的窗口机制Flink 支持多种 Window，比如 Time Window、Count Window、Session Window，还支持自定义 Window。后面的文章 如何使用 Flink Window 及 Window 基本概念与实现原理 中会很详细的讲解 Flink 中 Window 的概念。 Flink 并行的执行任务Flink 的程序内在是并行和分布式的，数据流可以被分区成 stream partitions，operators 被划分为 operator subtasks; 这些 subtasks 在不同的机器或容器中分不同的线程独立运行；operator subtasks 的数量在具体的 operator 就是并行计算数，程序不同的 operator 阶段可能有不同的并行数；如下图所示，source operator 的并行数为 2，但最后的 sink operator 为 1： Flink 支持状态存储Flink 是一款有状态的流处理框架，它提供了丰富的状态访问接口，按照数据的划分方式，可以分为 Keyed State 和 Operator State，在 Keyed State 中又提供了多种数据结构： ValueState MapState ListState ReducingState AggregatingState 另外状态存储也支持多种方式： MemoryStateBackend：存储在内存中 FsStateBackend：存储在文件中 RocksDBStateBackend：存储在 RocksDB 中 Flink 支持容错机制Flink 中支持使用 Checkpoint 来提高程序的可靠性，开启了 Checkpoint 之后，Flink 会按照一定的时间间隔对程序的运行状态进行备份，当发生故障时，Flink 会将所有任务的状态恢复至最后一次发生 Checkpoint 中的状态，并从那里开始重新开始执行。 另外 Flink 还支持根据 Savepoint 从已停止作业的运行状态进行恢复，这种方式需要通过命令进行触发。 Flink 实现了自己的内存管理机制Flink 在 JVM 中提供了自己的内存管理，使其独立于 Java 的默认垃圾收集器。 它通过使用散列，索引，缓存和排序有效地进行内存管理。我们在后面的文章 深入探索 Flink 内存管理机制 会深入讲解 Flink 里面的内存管理机制。 总结本篇文章对 Flink 做了一个详细的介绍，将 Flink 的特点一一做了描述，后面文章中我们也会进一步地对这里面的特点进行原理解析。本文的地址是 http://www.54tianzhisheng.cn/2019/08/19/flink/ ，未经允许禁止任何形式的转载，违者必究。 最后GitHub Flink 学习代码地址：https://github.com/zhisheng17/flink-learning 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从0到1学习 —— 如何使用 Side Output 来分流？","date":"2019-08-17T16:00:00.000Z","path":"2019/08/18/flink-side-output/","text":"前言之前在 Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 讲过 Flink 使用连续的 Split 会有问题，当时提供了几种解决方法，有一种方法就是使用 Side Output 来进行，当时留了个余念，那么就在这篇文章详细的讲一波，教大家如何使用 Side Output 来分流。 Side Output通常我们在处理数据的时候，有时候想对不同情况的数据进行不同的处理，那么就需要把数据流进行分流。比如我们在那篇文章里面的例子：需要将从 Kafka 过来的告警和恢复数据进行分类拆分，然后在对每种数据再分为告警数据和恢复数据。 如果是使用 filter 来进行拆分，也能满足我们的需求，但每次筛选过滤都要保留整个流，然后通过遍历整个流来获取相应的数据，显然很浪费性能。假如能够在一个流里面就进行多次输出就好了，恰好 Flink 的 Side Output 则提供了这样的功能。 如何使用？要使用 Side Output 的话，你首先需要做的是定义一个 OutputTag 来标识 Side Output，代表这个 Tag 是要收集哪种类型的数据，如果是要收集多种不一样类型的数据，那么你就需要定义多种 OutputTag。例如：如果我要将告警/恢复的数据分为机器、容器、中间件等的数据，那么我们起码就得定义三个 OutputTag，如下： 123456private static final OutputTag&lt;AlertEvent&gt; middleware = new OutputTag&lt;AlertEvent&gt;(\"MIDDLEWARE\") &#123;&#125;;private static final OutputTag&lt;AlertEvent&gt; machine = new OutputTag&lt;AlertEvent&gt;(\"MACHINE\") &#123;&#125;;private static final OutputTag&lt;AlertEvent&gt; docker = new OutputTag&lt;AlertEvent&gt;(\"DOCKER\") &#123;&#125;; 然后呢，你可以使用下面几种函数来处理数据，在处理数据的过程中，进行判断将不同种类型的数据存到不同的 OutputTag 中去。 ProcessFunction KeyedProcessFunction CoProcessFunction ProcessWindowFunction ProcessAllWindowFunction 比如： 12345678910111213141516//dataStream 是总的数据流SingleOutputStreamOperator&lt;AlertEvent, AlertEvent&gt; outputStream = dataStream.process(new ProcessFunction&lt;AlertEvent, AlertEvent&gt;() &#123; @Override public void processElement(AlertEvent value, Context ctx, Collector&lt;AlertEvent&gt; out) throws Exception &#123; if (\"MACHINE\".equals(value.type)) &#123; ctx.output(machine, value); &#125; else if (\"DOCKER\".equals(value.type)) &#123; ctx.output(docker, value); &#125; else if (\"MIDDLEWARE\".equals(value.type)) &#123; ctx.output(middleware, value); &#125; else &#123; //其他的业务逻辑 out.collect(value); &#125; &#125;&#125;) 好了，既然上面我们已经将不同类型的数据进行放到不同的 OutputTag 里面了，那么我们该如何去获取呢？你可以使用 getSideOutput 方法来获取不同 OutputTag 的数据，比如： 12345678//机器相关的告警&amp;恢复数据outputStream.getSideOutput(machine).print();//容器相关的告警&amp;恢复数据outputStream.getSideOutput(docker).print();//中间件相关的告警&amp;恢复数据outputStream.getSideOutput(middleware).print(); 这样你就可以获取到 Side Output 数据了。 另外你还可以看下我在 Github 放的一个完整 demo 代码: https://github.com/zhisheng17/flink-learning/blob/master/flink-learning-examples/src/main/java/com/zhisheng/examples/streaming/sideoutput/Main.java 总结本文讲了如何使用 Side Output 来进行分流，比较简单，大家可以稍微阅读一下 demo 代码就可以很清楚了解。 本文地址是：http://www.54tianzhisheng.cn/2019/08/18/flink-side-output/ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"你公司到底需不需要引入实时计算引擎？","date":"2019-08-05T16:00:00.000Z","path":"2019/08/06/flink-streaming-system/","text":"合理的需求选择恰当的技术栈 前言 先广而告之，本文摘自本人《大数据重磅炸弹——实时计算框架 Flink》课程第二篇，内容首发自我的知识星球，后面持续在星球里更新。 自己之前发布过一篇 Chat 《大数据“重磅炸弹”：实时计算框架 Flink》，里面介绍了多种需求： 1234567891011121314151617小田，你看能不能做个监控大屏实时查看促销活动销售额（GMV）？小朱，搞促销活动的时候能不能实时统计下网站的 PV/UV 啊？小鹏，我们现在搞促销活动能不能实时统计销量 Top5 啊？小李，怎么回事啊？现在搞促销活动结果服务器宕机了都没告警，能不能加一个？小刘，服务器这会好卡，是不是出了什么问题啊，你看能不能做个监控大屏实时查看机器的运行情况？小赵，我们线上的应用频繁出现 Error 日志，但是只有靠人肉上机器查看才知道情况，能不能在出现错误的时候及时告警通知？小夏，我们 1 元秒杀促销活动中有件商品被某个用户薅了 100 件，怎么都没有风控啊？小宋，你看我们搞促销活动能不能根据每个顾客的浏览记录实时推荐不同的商品啊？…… 大数据发展至今，数据呈指数倍的增长，对实效性的要求也越来越高，于是像上面这种需求也变得越来越多了。 那这些场景对应着什么业务需求呢？我们来总结下，大概如下： 初看这些需求，是不是感觉很难？ 那么我们接下来来分析一下该怎么去实现？ 从这些需求来看，最根本的业务都是需要实时查看数据信息，那么首先我们得想想如何去采集这些实时数据，然后将采集的实时数据进行实时的计算，最后将计算后的结果下发到第三方。 数据实时采集就上面这些需求，我们需要采集些什么数据呢？ 买家搜索记录信息 买家浏览的商品信息 买家下单订单信息 网站的所有浏览记录 机器 CPU/MEM/IO 信息 应用日志信息 数据实时计算采集后的数据实时上报后，需要做实时的计算，那我们怎么实现计算呢？ 计算所有商品的总销售额 统计单个商品的销量，最后求 Top5 关联用户信息和浏览信息、下单信息 统计网站所有的请求 IP 并统计每个 IP 的请求数量 计算一分钟内机器 CPU/MEM/IO 的平均值、75 分位数值 过滤出 Error 级别的日志信息 数据实时下发实时计算后的数据，需要及时的下发到下游，这里说的下游代表可能是： 告警方式（邮件、短信、钉钉、微信） 在计算层会将计算结果与阈值进行比较，超过阈值触发告警，让运维提前收到通知，及时做好应对措施，减少故障的损失大小。 存储（消息队列、DB、文件系统等） 数据存储后，监控大盘（Dashboard）从存储（ElasticSearch、HBase 等）里面查询对应指标的数据就可以查看实时的监控信息，做到对促销活动的商品销量、销售额，机器 CPU、MEM 等有实时监控，运营、运维、开发、领导都可以实时查看并作出对应的措施。 让运营知道哪些商品是爆款，哪些店铺成交额最多，哪些商品成交额最高，哪些商品浏览量最多； 让运维可以时刻了解机器的运行状况，出现宕机或者其他不稳定情况可以及时处理； 让开发知道自己项目运行的情况，从 Error 日志知道出现了哪些 Bug； 让领导知道这次促销赚了多少 money。 从数据采集到数据计算再到数据下发，整个流程在上面的场景对实时性要求还是很高的，任何一个地方出现问题都将影响最后的效果！ 实时计算场景前面说了这么多场景，这里我们总结一下实时计算常用的场景有哪些呢？ 交通信号灯数据 道路上车流量统计（拥堵状况） 公安视频监控 服务器运行状态监控 金融证券公司实时跟踪股市波动，计算风险价值 数据实时 ETL 银行或者支付公司涉及金融盗窃的预警 …… 另外我自己在我的群里也有做过调研（不完全统计），他们在公司 Flink（一个实时计算框架）使用场景有这些： 总结一下大概有下面这四类： 实时数据存储 实时数据存储的时候做一些微聚合、过滤某些字段、数据脱敏，组建数据仓库，实时 ETL。 实时数据分析 实时数据接入机器学习框架（TensorFlow）或者一些算法进行数据建模、分析，然后动态的给出商品推荐、广告推荐 实时监控告警 金融相关涉及交易、实时风控、车流量预警、服务器监控告警、应用日志告警 实时数据报表 活动营销时销售额/销售量大屏，TopN 商品 说到实时计算，这里不得不讲一下和传统的离线计算的区别！ 实时计算 VS 离线计算再讲这两个区别之前，我们先来看看流处理和批处理的区别： 流处理与批处理 看完流处理与批处理这两者的区别之后，我们来抽象一下前面文章的场景需求（实时计算）： 实时计算需要不断的从 MQ 中读取采集的数据，然后处理计算后往 DB 里存储，在计算这层你无法感知到会有多少数据量过来、要做一些简单的操作（过滤、聚合等）、及时将数据下发。 相比传统的离线计算，它却是这样的： 在计算这层，它从 DB（不限 MySQL，还有其他的存储介质）里面读取数据，该数据一般就是固定的（前一天、前一星期、前一个月），然后再做一些复杂的计算或者统计分析，最后生成可供直观查看的报表（dashboard）。 离线计算的特点 数据量大且时间周期长（一天、一星期、一个月、半年、一年） 在大量数据上进行复杂的批量运算 数据在计算之前已经固定，不再会发生变化 能够方便的查询批量计算的结果 实时计算的特点在大数据中与离线计算对应的则是实时计算，那么实时计算有什么特点呢？由于应用场景的各不相同，所以这两种计算引擎接收数据的方式也不太一样：离线计算的数据是固定的（不再会发生变化），通常离线计算的任务都是定时的，如：每天晚上 0 点的时候定时计算前一天的数据，生成报表；然而实时计算的数据源却是流式的。 这里我不得不讲讲什么是流式数据呢？我的理解是比如你在淘宝上下单了某个商品或者点击浏览了某件商品，你就会发现你的页面立马就会给你推荐这种商品的广告和类似商品的店铺，这种就是属于实时数据处理然后作出相关推荐，这类数据需要不断的从你在网页上的点击动作中获取数据，之后进行实时分析然后给出推荐。 流式数据的特点 数据实时到达 数据到达次序独立，不受应用系统所控制 数据规模大且无法预知容量 原始数据一经处理，除非特意保存，否则不能被再次取出处理，或者再次提取数据代价昂贵 实时计算的优势实时计算一时爽，一直实时计算一直爽，对于持续生成最新数据的场景，采用流数据处理是非常有利的。例如，再监控服务器的一些运行指标的时候，能根据采集上来的实时数据进行判断，当超出一定阈值的时候发出警报，进行提醒作用。再如通过处理流数据生成简单的报告，如五分钟的窗口聚合数据平均值。复杂的事情还有在流数据中进行数据多维度关联、聚合、塞选，从而找到复杂事件中的根因。更为复杂的是做一些复杂的数据分析操作，如应用机器学习算法，然后根据算法处理后的数据结果提取出有效的信息，作出、给出不一样的推荐内容，让不同的人可以看见不同的网页（千人千面）。 使用实时数据流面临的挑战 数据处理唯一性（如何保证数据只处理一次？至少一次？最多一次？） 数据处理的及时性（采集的实时数据量太大的话可能会导致短时间内处理不过来，如何保证数据能够及时的处理，不出现数据堆积？） 数据处理层和存储层的可扩展性（如何根据采集的实时数据量的大小提供动态扩缩容？） 数据处理层和存储层的容错性（如何保证数据处理层和存储层高可用，出现故障时数据处理层和存储层服务依旧可用？） 总结本文从日常需求来分析该如何去实现这类需求，需要实时采集、实时计算、实时下发，并用图片把需求完成后的效果图展示了出来，接着我们分析了对实时性要求高的计算这块，然后将离线计算与实时计算进行了对比、批处理与流处理进行对比、离线计算的特点与实时计算的特点进行了对比，再加上我自己的调研结果，归纳了实时计算的四种使用场景，提出了使用实时计算时要面临的挑战。因为各种需求，也就造就了现在不断出现实时计算框架，而下文我们将重磅介绍我们推荐的实时计算框架 —— Flink。 Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink Clients 源码解析","date":"2019-07-03T16:00:00.000Z","path":"2019/07/04/Flink-code-clients/","text":"Flink-Client 模块中的类结构如下： https://t.zsxq.com/IMzNZjY 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink Annotations 源码解析","date":"2019-07-02T16:00:00.000Z","path":"2019/07/03/Flink-code-Annotations/","text":"Flink-Annotations 模块中的类结构如下： https://t.zsxq.com/f6eAu3J 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink Metrics 源码解析","date":"2019-07-01T16:00:00.000Z","path":"2019/07/02/Flink-code-metrics/","text":"Flink Metrics 有如下模块： Flink Metrics 源码解析 —— Flink-metrics-core Flink Metrics 源码解析 —— Flink-metrics-datadog Flink Metrics 源码解析 —— Flink-metrics-dropwizard Flink Metrics 源码解析 —— Flink-metrics-graphite Flink Metrics 源码解析 —— Flink-metrics-influxdb Flink Metrics 源码解析 —— Flink-metrics-jmx Flink Metrics 源码解析 —— Flink-metrics-slf4j Flink Metrics 源码解析 —— Flink-metrics-statsd Flink Metrics 源码解析 —— Flink-metrics-prometheus 使用 InflubDB 和 Grafana 监控 Flink JobManager TaskManager 和 Job 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Apache Flink 1.9 重大特性提前解读","date":"2019-06-30T16:00:00.000Z","path":"2019/07/01/flink-1.9-preview/","text":"今天在 Apache Flink meetup ·北京站进行 Flink 1.9 重大新特性进行了讲解，两位讲师分别是 戴资力/杨克特，zhisheng 我也从看完了整个 1.9 特性解读的直播，预计 Flink 1.9 版本正式发布时间大概是 7 月底 8 月初左右正式发布，下面一起来看看直播内容： 架构改动 Table/SQL API Runtime 生态 最后GitHub Flink 学习代码地址：https://github.com/zhisheng17/flink-learning 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了","date":"2019-06-25T16:00:00.000Z","path":"2019/06/26/flink-TensorFlow/","text":"1．前言随着互联网的迅速发展，各个公司都建立了自己的监控体系，用于提前发现问题降低损失，携程亦是如此。然而携程的监控体系存在以下三个问题： 本文转自 AI 前线公众号，作者 | 潘国庆 编辑 | Natalie Flink 已经渐渐成为实时计算引擎的首选之一，从简单的实时 ETL 到复杂的 CEP 场景，Flink 都能够很好地驾驭。本文整理自携程实时计算负责人潘国庆在 QCon 全球软件开发大会（北京站）2019 的演讲，他介绍了携程如何基于 Flink 与 TensorFlow 构建实时智能异常检测平台，以解决规则告警系统准确率低、时效性低、规则配置复杂与耗费人力等诸多问题，实现了业务指标毫秒级延迟与智能化检测，同时依托 Flink 实现了强大的容错机制。 监控系统繁多 监控告警配置复杂 没有统一规范 首先携程目前光公司级别的监控系统就有三套，各个 BU 为了满足自己的业务监控需求也陆续开发了许多自己的监控系统。其次这些监控系统都是基于规则来判断是否存在异常，比如当满足同环比连续几个点上升或下降到用户配置的阈值时触发告警。最后是没有统一的规范，这里指的是两个规范，第一，没有统一的规则告警配置规范，不同的监控系统都带有不同的规则告警配置方式；第二，没有统一的异常判断规范，研发人员或 QA 人员都是根据自己对业务的理解，通过主观判断指标达到一定阀值时监控系统需要进行告警。 基于以上的三点问题给用户带来了诸多不便，首先是规则告警维护成本高，用户时常需要基于多个监控系统以不同的方式配置规则告警，而且还需要根据告警的情况持续调整阈值，导致一个规则告警从配置到最终能够产生较好的效果需要一个很长的周期。其次，基于规则告警往往表现不尽如人意，会导致准确率低、覆盖率低和时效性低的三低状况。用户很多情况下为了提高异常的覆盖率降低漏报的情况，不得不将规则告警的阀值设置的非常敏感，虽然这样能够覆盖更多的异常场景，却导致了大量的误报，规则告警的准确性也就大大折扣。 为了应对上述的诸多问题，携程打造了自己的实时智能异常检测平台 Prophet。简单概括，Prophet 是一个基于时序类型数据、以平台为接入对象、去规则化为目标的异常检测系统，基于深度学习算法实现异常的智能检测，基于实时计算引擎实现异常的实时检测，提供了统一的异常检测解决方案。接下来的文章会详细介绍我们是如何依次实现了异常的智能化、实时化检测以及平台的构建。 2． 智能化2.1 深度学习算法选择目前业界采用比较多的方式是引入统计分析的各种方法，框定一个滑动的样本集，对这个样本集进行一些数据处理和转化，经过归一化，去周期，去趋势，再将最新采集到的数据点经过同样的转换，和样本集的残差序列的统计量进行比较，比如距离、方差、移动平均、分位数等，超出一定的范围就判断为异常，或是综合各种离群点计算的方法来做个投票，多数算法认为异常则报异常。起初我们也借鉴了这种做法，却发现虽然可以不用维护告警规则了，但报警的质量并没有提升。 我们需要设计一套新的算法，降低报警总量到可以人工逐个处理的程度，同时不能以增加漏报真正的生产订单故障为代价，并且这套算法的设计还不能太复杂，影响到告警的实时性，最好还能做到算法即服务，有较强的可移植性，提供给其他的监控系统使用。自然而然的，基于神经网络的深度学习算法 成为我们进一步探索的工具。 RNN 算法比较适合处理序列变化的数据，符合我们时序特征的场景，但是存在梯度消失和过拟合的现象。而他的改进版 LSTM 算法，能够通过控制传输状态来选择性地记住较重要的长期数据，能在更长的序列上有良好的表现，业界也有很多成功的应用。LSTM 算法的异常检测方式是基于指标的历史数据训练出模型并基于现有数据预测指标未来的走势，基于预测数据与现实数据各种偏差来判断指标是否有异常。这样好处在于每个指标都会训练一个自己的模型，能够达到很高的精度，但是也带来了一定的弊端，需要消耗较多的训练与检测资源。 DNN 算法的检测方式与 LSTM 的方式不同，我们基于小波变换算法提取监控指标不同频域的特征喂给 DNN 模型，直接输出是否存在异常。这种的好处在于一个 DNN 模型就能够满足所有异常检测场景的需求，但是相对的特征工程也要复杂很多，我们需要大量的人工标记数据来提高模型的精度。 最后无论是基于 LSTM 算法还是 DNN 算法实现的异常检测需要根据各自所需的不同场景来决定使用哪个。在携程，对于最重要的订单、支付类指标，我们都是采取 LSTM 算法，单个指标训练单个模型，对于其他一些非重要的指标可以使用 DNN 算法。 2.2 模型训练选定好深度学习算法之后，我们也就开始尝试模型的训练。我们首先取得监控指标的历史数据对其进行清洗，其中需要对一些空值进行插补，节假日数据对于数据模型的影响很大，导致训练出来的数据有偏差，我们也选择性的剔除节假日期间的数据；如果历史数据中的某个区间数据是异常区间，我们也需要使用预测值替换异常区间的数值。 做完数据清洗之后，也就需要实现特征工程。我们使用了多尺度滑动窗口时序特征的方法，将一个滑动窗口内的数据和前 n 个周期做统计量上的对比，均值、方差、变化率等这些，这样基本上就可以把明显的周期性和平稳型数据给分离出来。剩下的时序中，有些是波动很大的随机序列，有的则是带有趋势的周期性序列，通过时序分析法把周期性去掉，再用频域分析尝试分解成频谱。对于带有明显频谱的，则归类为周期型时序，而频谱杂乱的，则归类为非周期性。 在做完特征提取与指标分类之后，我们也就根据指标的类型使用不同的算法进行模型训练。我们根据线上的人工标注数据持续性的优化我们的模型。我们经历过初期不停的调参和验证之后，我们将模型训练的频率设为了两周，我们每两周重新走下图中的整个流程，这个也是根据我们业务变更的频率所做的考虑。 3． 实时化3.1 Why Flink？在解决了智能化异常检测的问题后，我们开始考虑提高我们的时效性。以往的规则告警，从数据产生到落地到监控系统，再到触发规则判断，期间已经经历了一定延迟。并且很多规则告警往往需要连续 3 个点或则 5 个点触发下跌或上升规则判断才会告警，这样如果一个指标的采集粒度是一分钟，那么异常往往需要过好几分钟才会被发现。为了解决时效性的问题，我们尝试引入实时计算引擎。现在常见的实时计算引擎有 Storm、Spark Streaming 以及 Flink，那么为什么我们最终选择了 Flink？ 首先第一点就是 Flink 提供了强大的容错保障，所有的实时作业无论提供了多么繁多的功能，如果在作业的容错保障上做的不好，对于用户都是不可接受的。我们的数据源是 Kafka，基于 Flink 的 Checkpoint 与 Kafka 的 Offset 回溯功能能够实现数据源到执行引擎层面的 Exactly Once 的语义保证，基于幂等或事物保证最终输出的 Exactly Once 语义。 第二点，Flink 提供了高效的状态管理，我们在做异常检测的时候需要保存异常区间的预测数据用于下一轮的异常检测，这个后续会讲到。 第三点与第四点放在一起讲就是，Flink 提供了基于 Event Time 的丰富窗口函数，Spark Streaming 虽然也提供了对窗口的支持，但是其本质上还都是基于 Processing Time 的数据处理。终上所述，我们最终选择了 Flink 作为我们的实时计算引擎。 3.2 实时检测在选择好实时计算引擎后，我们也就开始尝试在 Flink 中加载 Tensorflow 的模型用来实时做异常检测。首先我们将所有训练好的 Tensorflow 模型以.pb 的格式上传到 HDFS 并将新增或更新的模型配置更新到配置中心 QConfig 上。Flink 作业在启动或运行中时，监听配置中心中需要监控的指标并尝试从 HDFS 上加载模型。由于后期模型较多，为了避免重复加载和负载均衡，所有指标会先根据 id keyBy 分发到不同的 TaskManager 上，每个 TaskManager 只加载属于自己那部分的模型。 模型加载完毕后，我们基于 Flink 滑动窗口与 Event Time 实现数据实时消费与预测。窗口滑动的时间为指标的时间粒度（下图中为 1 分钟），窗口长度为十个指标时间粒度（下图中为 10 分钟）。一个窗口中总计 10 条数据，我们采用前面 5 条数据预测第 6 个位置的数据，然后基于 2 到 4 的实际数值加上第 6 条的预测数据预测第 7 个数据。依此类推，最终我们获取到了窗口中后 5 位的预测值与实际值，基于 5 个预测值与实际值对比检测是否存在异常。 然而实际的消费过程中并不会像上面说的那么简单，首先一个窗口内可能存在缺失数据的情况，我们采用窗口内其余数据的均值与标准差补齐。其次，在上个时间段如果存在异常，我们无法直接使用原始的值去预测数值，因为这个原始值可能是一个异常值，我们需要使用上个时间段的预测值来替换这个异常值，这样能够保证我们的预测线不被带跑偏。上一个窗口的预测值我们采用 flink 中的 state 来存储。 在取得当前窗口后 5 个预测值与实际值之后，我们就开始进异常检测了。我们会根据异常的类型（比如上升或下降）与敏感度来做不同的判断，下图中的三个异常曲线分别对应了高中低三个敏感的场景，在使用高敏度时，可能只要有一个下跌的抖动，我们可能就认为其是一个潜在的异常，中敏感度需要连续两个下跌的情况，低敏感度则需在下降幅度非常大的情况下才会认定为潜在异常。 我们会基于预测值与实际数据的偏差来先做一个潜在判断，当认定它是一个潜在异常时，我们会在基于预测值与历史同期数据的均值与标准差做判断，这样最终得出当前的窗口是否存在异常。我们这边在异常判断的时候还是采用了统计学作为判断方式，如果在样本足够的情况下，完全可以使用机器学习，训练一个异常检测模型来判断是否存在异常。 4. Prophet4.1 Prophet 系统架构在讲述完如何实现智能化与实时化异常检测之后，相信大家对于 Prophet 已经有了一定的认知。下图展示了整个 Prophet 平台的系统架构，首先是最底层的 Hadoop 集群承担了分布式存储与资源调度的功能，HDFS 用来存储 Tensorflow 训练好的模型，所有 Flink 作业运行在 Yarn 集群上。中间层的消息队列承担了实时数据源的作用，所有指标的历史数据存储在时序数据库中，实时化与智能化检测依托于 Flink 与 Tensorflow 两套引擎实现。最上层的 Prophet 以平台的方式对外提供服务，Clog 用于日志存储与排障，Muise 是我们的实时计算平台，Qconfig 用于存储于监控指标相关的配置信息，最后 Hickwall 用于监控作业的各项指标。 4.2 Prophet 操作流程一个用户想要配置智能告警只需要做两件事，首先在我们的平台上配置智能告警，由于我们大部分对接的是监控平台，所以用户大多是在各个监控平台上配置智能告警，然后监控平台调用我们的服务注册监控指标。然后用户需要按照我们定义好的格式将原始数据发送到我们的 Kafka 消息队列，这一步在对接平台时，也由平台做了，所以直接在我们平台上配置监控指标的用户很少。当一个用户注册好监控指标后，我们平台会先检测该指标的历史数据是否足够，如果足够则触发模型训练的流程，训练好的模型会上传到 HDFS。如果历史数据不足，Prophet 会持续实时存储用户指标的数据，当满足数据量的需求时，重新触发模型训练。当模型训练完成后，我们会更新配置中心，告知 Flink 作业有新的或更新的指标模型已经就位。 实时这块的流程是 Flink 启动或运行中一旦监听到有新的或更新的模型，作业会重新加载模型。另外 Flink 会实时从 Kafka 中消费数据，实时的过模型做异常检测，最终将异常告警回吐到 Kafka，各个平台消费自己的异常告警数据并给相关的负责人发送告警通知。 4.3 平台现状目前 Prophet 已经覆盖了携程所有的业务线，接入了十余个监控平台，其中包含公司级的监控系统 Sitemon 与 Hickwall，监控了 7000+ 个业务指标，包含订单、支付、应用、服务等多种业务类型指标。 在平台运行的半年时间内，我们的算法能够达到 90% 的召回率（也就是异常覆盖率）；由于我们业务方需求是尽量覆盖更多的异常，不要漏报，所以我们的准确率保持在 75% 左右；在引入了 Flink 实时消费数据与检测，极大的降低了我们告警的延迟，达到了毫秒级的延迟；对比规则告警，我们帮助用户降低了 10 倍的告警数量，提升了 10 倍的用户效率。 下图展示了从 18 年 10 月 Prophet 上线以来至 19 年 4 月底，智能告警与规则告警对异常的覆盖率对比。总计发生 176 起异常，其中 Prophet 图表中显示的是覆盖了 90% 的异常，但其实真正的覆盖率要高于 90%，其中 18 个未覆盖异常有 15 个是由于初期算法一直处于调整阶段导致了漏报。在 19 年之后，我们的异常覆盖率能够达到接近 100%。相比较规则告警，我们的覆盖率上升了 22%，及时的帮助用户降低损失。 下图展示了智能告警与规则告警在告警数量上的对比，规则告警的数量基本是智能告警的 2 到 5 倍，但是这并非是站在同一层面上的对比，其中智能告警的数量是基于 800 监控指标，而规则告警是基于 200 个监控，如果规则告警的指标数量与智能告警的持平，那智能告警降低的告警数量会更为显著。告警数量对于用户的效率提升是十分明显的，以往用户每天需要花费大量的精力去排查每一个告警邮件，在使用了智能告警后，这部分帮助用户减少的时间是实实在在的效率提升。 5． 挑战与展望Prophet 在携程投入生产使用已有半年之久，在这期间我们也遇到过形形色色的挑战。 首先，基于 LSTM 算法的异常检测方式存在一个明显的弊端，我们需要对每一个指标训练一个模型，这样无论是模型训练所需的资源以及实时作业加载模型所需的资源都消耗比较大。 其次，LSTM 算法对于波动剧烈的非周期型指标表现不是十分良好，有一些业务会不定期的做一些活动导致业务指标的突增或突减，这种趋势是无法从历史数据中学习到。 然后，对于一些系统性能指标类型的数据也无需使用智能告警，规则告警可能更加方便，比如当服务器的 cpu 使用率达到 95% 的时候就告警。 最后，节假日对于智能告警的影响十分之大，业务指标通常会在节假日前呈倍数的增长，假日期间又曾倍数的下降，这样导致了大量漏报或误报。 针对以上的问题，我们也在持续的改进之中。首先，基于 DNN 算法的通用模型已经在线下陪跑了数月之久，虽然在精度上比 LSTM 算法的异常检测方式稍有逊色，但在我们持续优化之后已经基本能够 hold 住线上非重要指标的告警需求，实现单个模型监控数千个指标的功能，大大降低了资源损耗。我们在应对节假日对智能检测影响时引入了增长系数的概念，用来拉升或降低预测值，并且采用一定方式将增长系数持续衰减，防止增长系数导致预测值的跑偏。关于算法的细节以及各种场景下的应对方式由于篇幅关系无法在本篇文章中一一展开，如果对算法相关细节感兴趣的朋友可以在评论区留言，我们这边也会考虑让算法同事另起炉灶，详细的介绍算法、特征工程等相关话题。 Prophet 后续也会陆续的接入携程所有的监控系统，这也是我们一直努力在做的事。实时计算与人工智能不光在异常检测这个场景下有很好的发挥，在很多其他的场景下也能够有亮眼的表现，比如风控、个性化推荐、排序等，本篇文章也算是抛砖引玉，希望给大家能够带来一些其法，这样可以将这套方式更多的使用在其他的场景下。 最后GitHub Flink 学习代码地址：https://github.com/zhisheng17/flink-learning 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"360深度实践：Flink与Storm协议级对比","date":"2019-06-20T16:00:00.000Z","path":"2019/06/21/flink-in-360/","text":"本文从数据传输和数据可靠性的角度出发，对比测试了Storm与Flink在流处理上的性能，并对测试结果进行分析，给出在使用Flink时提高性能的建议。 作者 张馨予，360 大数据计算平台负责人。北京邮电大学硕士，2015年加入360系统部，一直致力于公司大数据计算平台的易用性、稳定性和性能优化的研发工作。目前主要负责Flink的研发，完成公司计算引擎的大一统。 Apache Storm、Apache Spark和Apache Flink都是开源社区中非常活跃的分布式计算平台，在很多公司可能同时使用着其中两种甚至三种。对于实时计算来说，Storm与Flink的底层计算引擎是基于流的，本质上是一条一条的数据进行处理，且处理的模式是流水线模式，即所有的处理进程同时存在，数据在这些进程之间流动处理。而Spark是基于批量数据的处理，即一小批一小批的数据进行处理，且处理的逻辑在一批数据准备好之后才会进行计算。在本文中，我们把同样基于流处理的Storm和Flink拿来做对比测试分析。 在我们做测试之前，调研了一些已有的大数据平台性能测试报告，比如，雅虎的Streaming-benchmarks，或者Intel的HiBench等等。除此之外，还有很多的论文也从不同的角度对分布式计算平台进行了测试。虽然这些测试case各有不同的侧重点，但他们都用到了同样的两个指标，即吞吐和延迟。吞吐表示单位时间内所能处理的数据量，是可以通过增大并发来提高的。延迟代表处理一条数据所需要的时间，与吞吐量成反比关系。 在我们设计计算逻辑时，首先考虑一下流处理的计算模型。上图是一个简单的流计算模型，在Source中将数据取出，发往下游Task，并在Task中进行处理，最后输出。对于这样的一个计算模型，延迟时间由三部分组成：数据传输时间、Task计算时间和数据排队时间。我们假设资源足够，数据不用排队。则延迟时间就只由数据传输时间和Task计算时间组成。而在Task中处理所需要的时间与用户的逻辑息息相关，所以对于一个计算平台来说，数据传输的时间才更能反映这个计算平台的能力。因此，我们在设计测试Case时，为了更好的体现出数据传输的能力，Task中没有设计任何计算逻辑。 在确定数据源时，我们主要考虑是在进程中直接生成数据，这种方法在很多之前的测试标准中也同样有使用。这样做是因为数据的产生不会受到外界数据源系统的性能限制。但由于在我们公司内部大部分的实时计算数据都来源于kafka，所以我们增加了从kafka中读取数据的测试。 对于数据传输方式，可以分为两种：进程间的数据传输和进程内的数据传输。 进程间的数据传输是指这条数据会经过序列化、网络传输和反序列化三个步骤。在Flink中，2个处理逻辑分布在不同的TaskManager上，这两个处理逻辑之间的数据传输就可以叫做进程间的数据传输。Flink网络传输是采用的Netty技术。在Storm中，进程间的数据传输是worker之间的数据传输。早版本的storm网络传输使用的ZeroMQ，现在也改成了Netty。 进程内的数据传输是指两个处理逻辑在同一个进程中。在Flink中，这两个处理逻辑被Chain在了一起，在一个线程中通过方法调用传参的形式进程数据传输。在Storm中，两个处理逻辑变成了两个线程，通过一个共享的队列进行数据传输。 Storm和Flink都有各自的可靠性机制。在Storm中，使用ACK机制来保证数据的可靠性。而在Flink中是通过checkpoint机制来保证的，这是来源于chandy-lamport算法。 事实上exactly-once可靠性的保证跟处理的逻辑和结果输出的设计有关。比如结果要输出到kafka中，而输出到kafka的数据无法回滚，这就无法保证exactly-once。我们在测试的时候选用的at-least-once语义的可靠性和不保证可靠性两种策略进行测试。 上图是我们测试的环境和各个平台的版本。 上图展示的是Flink在自产数据的情况下，不同的传输方式和可靠性的吞吐量：在进程内+不可靠、进程内+可靠、进程间+不可靠、进程间+可靠。可以看到进程内的数据传输是进程间的数据传输的3.8倍。是否开启checkpoint机制对Flink的吞吐影响并不大。因此我们在使用Flink时，进来使用进程内的传输，也就是尽可能的让算子可以Chain起来。 那么我们来看一下为什么Chain起来的性能好这么多，要如何在写Flink代码的过程中让Flink的算子Chain起来使用进程间的数据传输。 大家知道我们在Flink代码时一定会创建一个env，调用env的disableOperatorChainning()方法会使得所有的算子都无法chain起来。我们一般是在debug的时候回调用这个方法，方便调试问题。 如果允许Chain的情况下，上图中Source和mapFunction就会Chain起来，放在一个Task中计算。反之，如果不允许Chain，则会放到两个Task中。 对于没有Chain起来的两个算子，他们被放到了不同的两个Task中，那么他们之间的数据传输是这样的：SourceFunction取到数据序列化后放入内存，然后通过网络传输给MapFunction所在的进程，该进程将数据方序列化后使用。 对于Chain起来的两个算子，他们被放到同一个Task中，那么这两个算子之间的数据传输则是：SourceFunction取到数据后，进行一次深拷贝，然后MapFunction把深拷贝出来的这个对象作为输入数据。 虽然Flink在序列化上做了很多优化，跟不用序列化和不用网络传输的进程内数据传输对比，性能还是差很多。所以我们尽可能的把算子Chain起来。 不是任何两个算子都可以Chain起来的，要把算子Chain起来有很多条件：第一，下游算子只能接受一种上游数据流，比如Map接受的流不能是一条union后的流；其次上下游的并发数一定要一样；第三，算子要使用同一个资源Group，默认是一致的，都是default；第四，就是之前说的env中不能调用disableOperatorChainning()方法，最后，上游发送数据的方法是Forward的，比如，开发时没有调用rebalance()方法，没有keyby()，没有boardcast等。 对比一下自产数据时，使用进程内通信，且不保证数据可靠性的情况下，Flink与Storm的吞吐。在这种情况下，Flink的性能是Storm的15倍。Flink吞吐能达到2060万条/s。不仅如此，如果在开发时调用了env.getConfig().enableObjectReuse()方法，Flink的但并发吞吐能达到4090万条/s。 当调用了enableObjectReuse方法后，Flink会把中间深拷贝的步骤都省略掉，SourceFunction产生的数据直接作为MapFunction的输入。但需要特别注意的是，这个方法不能随便调用，必须要确保下游Function只有一种，或者下游的Function均不会改变对象内部的值。否则可能会有线程安全的问题。 当对比在不同可靠性策略的情况下，Flink与Storm的表现时，我们发现，保证可靠性对Flink的影响非常小，但对Storm的影响非常大。总的来说，在保证可靠的情况下，Flink单并发的吞吐是Storm的15倍，而不保证可靠的情况下，Flink的性能是Storm的66倍。会产生这样的结果，主要是因为Flink与Storm保证数据可靠性的机制不同。 而Storm的ACK机制为了保证数据的可靠性，开销更大。 左边的图展示的是Storm的Ack机制。Spout每发送一条数据到Bolt，就会产生一条ack的信息给acker，当Bolt处理完这条数据后也会发送ack信息给acker。当acker收到这条数据的所有ack信息时，会回复Spout一条ack信息。也就是说，对于一个只有两级（spout+bolt）的拓扑来说，每发送一条数据，就会传输3条ack信息。这3条ack信息则是为了保证可靠性所需要的开销。 右边的图展示的是Flink的Checkpoint机制。Flink中Checkpoint信息的发起者是JobManager。它不像Storm中那样，每条信息都会有ack信息的开销，而且按时间来计算花销。用户可以设置做checkpoint的频率，比如10秒钟做一次checkpoint。每做一次checkpoint，花销只有从Source发往map的1条checkpoint信息（JobManager发出来的checkpoint信息走的是控制流，与数据流无关）。与storm相比，Flink的可靠性机制开销要低得多。这也就是为什么保证可靠性对Flink的性能影响较小，而storm的影响确很大的原因。 最后一组自产数据的测试结果对比是Flink与Storm在进程间的数据传输的对比，可以看到进程间数据传输的情况下，Flink但并发吞吐是Storm的4.7倍。保证可靠性的情况下，是Storm的14倍。 上图展示的是消费kafka中数据时，Storm与Flink的但并发吞吐情况。因为消费的是kafka中的数据，所以吞吐量肯定会收到kafka的影响。我们发现性能的瓶颈是在SourceFunction上，于是增加了topic的partition数和SourceFunction取数据线程的并发数，但是MapFunction的并发数仍然是1.在这种情况下，我们发现flink的瓶颈转移到上游往下游发数据的地方。而Storm的瓶颈确是在下游收数据反序列化的地方。 之前的性能分析使我们基于数据传输和数据可靠性的角度出发，单纯的对Flink与Storm计算平台本身进行了性能分析。但实际使用时，task是肯定有计算逻辑的，这就势必更多的涉及到CPU，内存等资源问题。我们将来打算做一个智能分析平台，对用户的作业进行性能分析。通过收集到的指标信息，分析出作业的瓶颈在哪，并给出优化建议。 最后GitHub Flink 学习代码地址：https://github.com/zhisheng17/flink-learning 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理","date":"2019-06-19T16:00:00.000Z","path":"2019/06/20/flink-kafka-Exactly-Once/","text":"Apache Flink 自2017年12月发布的1.4.0版本开始，为流计算引入了一个重要的里程碑特性：TwoPhaseCommitSinkFunction（相关的 Jira）。它提取了两阶段提交协议的通用逻辑，使得通过 Flink 来构建端到端的 Exactly-Once 程序成为可能。同时支持一些数据源（source）和输出端（sink），包括 Apache Kafka 0.11及更高版本。它提供了一个抽象层，用户只需要实现少数方法就能实现端到端的 Exactly-Once 语义。 本文作者是 Piotr Nowojski，翻译自 周凯波原文地址：https://www.ververica.com/blog/end-to-end-exactly-once-processing-apache-flink-apache-kafka 有关 TwoPhaseCommitSinkFunction 的使用详见文档: TwoPhaseCommitSinkFunction。或者可以直接阅读 Kafka 0.11 sink 的文档: kafka。 接下来会详细分析这个新功能以及Flink的实现逻辑，分为如下几点。 描述 Flink checkpoint 机制是如何保证Flink程序结果的 Exactly-Once 的 显示 Flink 如何通过两阶段提交协议与数据源和数据输出端交互，以提供端到端的 Exactly-Once 保证 通过一个简单的示例，了解如何使用 TwoPhaseCommitSinkFunction 实现 Exactly-Once 的文件输出 Flink 应用程序中的 Exactly-Once 语义当我们说『Exactly-Once』时，指的是每个输入的事件只影响最终结果一次。即使机器或软件出现故障，既没有重复数据，也不会丢数据。 Flink 很久之前就提供了 Exactly-Once 语义。在过去几年中，我们对 Flink 的 checkpoint 机制有过深入的描述，这是 Flink 有能力提供 Exactly-Once 语义的核心。Flink 文档还提供了该功能的全面概述。 在继续之前，先看下对 checkpoint 机制的简要介绍，这对理解后面的主题至关重要。 一次 checkpoint 是以下内容的一致性快照： 应用程序的当前状态 输入流的位置 Flink 可以配置一个固定的时间点，定期产生 checkpoint，将 checkpoint 的数据写入持久存储系统，例如 S3 或 HDFS 。将 checkpoint 数据写入持久存储是异步发生的，这意味着 Flink 应用程序在 checkpoint 过程中可以继续处理数据。 如果发生机器或软件故障，重新启动后，Flink 应用程序将从最新的 checkpoint 点恢复处理； Flink 会恢复应用程序状态，将输入流回滚到上次 checkpoint 保存的位置，然后重新开始运行。这意味着 Flink 可以像从未发生过故障一样计算结果。 在 Flink 1.4.0 之前，Exactly-Once 语义仅限于 Flink 应用程序内部，并没有扩展到 Flink 数据处理完后发送的大多数外部系统。Flink 应用程序与各种数据输出端进行交互，开发人员需要有能力自己维护组件的上下文来保证 Exactly-Once 语义。 为了提供端到端的 Exactly-Once 语义 - 也就是说，除了 Flink 应用程序内部， Flink 写入的外部系统也需要能满足 Exactly-Once 语义 - 这些外部系统必须提供提交或回滚的方法，然后通过 Flink 的 checkpoint 机制来协调。 分布式系统中，协调提交和回滚的常用方法是两阶段提交协议。在下一节中，我们将讨论 Flink 的 TwoPhaseCommitSinkFunction 是如何利用两阶段提交协议来提供端到端的 Exactly-Once 语义。 Flink 应用程序端到端的 Exactly-Once 语义我们将介绍两阶段提交协议，以及它如何在一个读写 Kafka 的 Flink 程序中实现端到端的 Exactly-Once 语义。Kafka 是一个流行的消息中间件，经常与 Flink 一起使用。Kafka 在最近的 0.11 版本中添加了对事务的支持。这意味着现在通过 Flink 读写 Kafka ，并提供端到端的 Exactly-Once 语义有了必要的支持。 Flink 对端到端的 Exactly-Once 语义的支持不仅局限于 Kafka ，您可以将它与任何一个提供了必要的协调机制的源/输出端一起使用。例如 Pravega，来自 DELL/EMC 的开源流媒体存储系统，通过 Flink 的 TwoPhaseCommitSinkFunction 也能支持端到端的 Exactly-Once 语义。 在今天讨论的这个示例程序中，我们有： 从 Kafka 读取的数据源（ Flink 内置的 KafkaConsumer） 窗口聚合 将数据写回 Kafka 的数据输出端（ Flink 内置的 KafkaProducer ） 要使数据输出端提供 Exactly-Once 保证，它必须将所有数据通过一个事务提交给 Kafka。提交捆绑了两个 checkpoint 之间的所有要写入的数据。这可确保在发生故障时能回滚写入的数据。但是在分布式系统中，通常会有多个并发运行的写入任务的，简单的提交或回滚是不够的，因为所有组件必须在提交或回滚时“一致”才能确保一致的结果。Flink 使用两阶段提交协议及预提交阶段来解决这个问题。 在 checkpoint 开始的时候，即两阶段提交协议的“预提交”阶段。当 checkpoint 开始时，Flink 的 JobManager 会将 checkpoint barrier（将数据流中的记录分为进入当前 checkpoint 与进入下一个 checkpoint ）注入数据流。 brarrier 在 operator 之间传递。对于每一个 operator，它触发 operator 的状态快照写入到 state backend。 数据源保存了消费 Kafka 的偏移量(offset)，之后将 checkpoint barrier 传递给下一个 operator。 这种方式仅适用于 operator 具有『内部』状态。所谓内部状态，是指 Flink statebackend 保存和管理的 -例如，第二个 operator 中 window 聚合算出来的 sum 值。当一个进程有它的内部状态的时候，除了在 checkpoint 之前需要将数据变更写入到 state backend ，不需要在预提交阶段执行任何其他操作。Flink 负责在 checkpoint 成功的情况下正确提交这些写入，或者在出现故障时中止这些写入。 示例 Flink 应用程序启动预提交阶段但是，当进程具有『外部』状态时，需要作些额外的处理。外部状态通常以写入外部系统（如 Kafka）的形式出现。在这种情况下，为了提供 Exactly-Once 保证，外部系统必须支持事务，这样才能和两阶段提交协议集成。 在本文示例中的数据需要写入 Kafka，因此数据输出端（ Data Sink ）有外部状态。在这种情况下，在预提交阶段，除了将其状态写入 state backend 之外，数据输出端还必须预先提交其外部事务。 当 checkpoint barrier 在所有 operator 都传递了一遍，并且触发的 checkpoint 回调成功完成时，预提交阶段就结束了。所有触发的状态快照都被视为该 checkpoint 的一部分。checkpoint 是整个应用程序状态的快照，包括预先提交的外部状态。如果发生故障，我们可以回滚到上次成功完成快照的时间点。 下一步是通知所有 operator，checkpoint 已经成功了。这是两阶段提交协议的提交阶段，JobManager 为应用程序中的每个 operator 发出 checkpoint 已完成的回调。 数据源和 windnow operator 没有外部状态，因此在提交阶段，这些 operator 不必执行任何操作。但是，数据输出端（Data Sink）拥有外部状态，此时应该提交外部事务。 我们对上述知识点总结下： 一旦所有 operator 完成预提交，就提交一个 commit。 如果至少有一个预提交失败，则所有其他提交都将中止，我们将回滚到上一个成功完成的 checkpoint 。 在预提交成功之后，提交的 commit 需要保证最终成功 - operator 和外部系统都需要保障这点。如果 commit 失败（例如，由于间歇性网络问题），整个 Flink 应用程序将失败，应用程序将根据用户的重启策略重新启动，还会尝试再提交。这个过程至关重要，因为如果 commit 最终没有成功，将会导致数据丢失。 因此，我们可以确定所有 operator 都同意 checkpoint 的最终结果：所有 operator 都同意数据已提交，或提交被中止并回滚。 在 Flink 中实现两阶段提交 Operator完整的实现两阶段提交协议可能有点复杂，这就是为什么 Flink 将它的通用逻辑提取到抽象类 TwoPhaseCommitSinkFunction 中的原因。 接下来基于输出到文件的简单示例，说明如何使用 TwoPhaseCommitSinkFunction 。用户只需要实现四个函数，就能为数据输出端实现 Exactly-Once 语义： beginTransaction - 在事务开始前，我们在目标文件系统的临时目录中创建一个临时文件。随后，我们可以在处理数据时将数据写入此文件。 preCommit - 在预提交阶段，我们刷新文件到存储，关闭文件，不再重新写入。我们还将为属于下一个 checkpoint 的任何后续文件写入启动一个新的事务。 commit - 在提交阶段，我们将预提交阶段的文件原子地移动到真正的目标目录。需要注意的是，这会增加输出数据可见性的延迟。 abort - 在中止阶段，我们删除临时文件。 我们知道，如果发生任何故障，Flink 会将应用程序的状态恢复到最新的一次 checkpoint 点。一种极端的情况是，预提交成功了，但在这次 commit 的通知到达 operator 之前发生了故障。在这种情况下，Flink 会将 operator 的状态恢复到已经预提交，但尚未真正提交的状态。 我们需要在预提交阶段保存足够多的信息到 checkpoint 状态中，以便在重启后能正确的中止或提交事务。在这个例子中，这些信息是临时文件和目标目录的路径。 TwoPhaseCommitSinkFunction 已经把这种情况考虑在内了，并且在从 checkpoint 点恢复状态时，会优先发出一个 commit 。我们需要以幂等方式实现提交，一般来说，这并不难。在这个示例中，我们可以识别出这样的情况：临时文件不在临时目录中，但已经移动到目标目录了。 在 TwoPhaseCommitSinkFunction 中，还有一些其他边界情况也会考虑在内，请参考 Flink 文档了解更多信息。 总结总结下本文涉及的一些要点： Flink 的 checkpoint 机制是支持两阶段提交协议并提供端到端的 Exactly-Once 语义的基础。 这个方案的优点是: Flink 不像其他一些系统那样，通过网络传输存储数据 - 不需要像大多数批处理程序那样将计算的每个阶段写入磁盘。 Flink 的 TwoPhaseCommitSinkFunction 提取了两阶段提交协议的通用逻辑，基于此将 Flink 和支持事务的外部系统结合，构建端到端的 Exactly-Once 成为可能。 从 Flink 1.4.0 开始，Pravega 和 Kafka 0.11 producer 都提供了 Exactly-Once 语义；Kafka 在0.11版本首次引入了事务，为在 Flink 程序中使用 Kafka producer 提供 Exactly-Once 语义提供了可能性。 Kafka 0.11 producer的事务是在 TwoPhaseCommitSinkFunction 基础上实现的，和 at-least-once producer 相比只增加了非常低的开销。 这是个令人兴奋的功能，期待 Flink TwoPhaseCommitSinkFunction 在未来支持更多的数据接收端。 最后GitHub Flink 学习代码地址：https://github.com/zhisheng17/flink-learning 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink状态管理和容错机制介绍","date":"2019-06-17T16:00:00.000Z","path":"2019/06/18/flink-state/","text":"有状态的流数据处理 本文整理自去年8月11日在北京举行的 Flink Meetup 会议，分享嘉宾施晓罡，目前在阿里大数据团队部从事Blink方面的研发，现在主要负责Blink状态管理和容错相关技术的研发。 1.1. 什么是有状态的计算计算任务的结果不仅仅依赖于输入，还依赖于它的当前状态，其实大多数的计算都是有状态的计算。 比如wordcount,给一些word,其计算它的count,这是一个很常见的业务场景。count做为输出，在计算的过程中要不断的把输入累加到count上去，那么count就是一个state。 1.2.传统流计算缺少对于程序状态的有效支持状态数据的存储和访问； 状态数据的备份和恢复； 状态数据的划分和动态扩容； 在传统的批处理中，数据是划分为块分片去完成的，然后每一个Task去处理一个分片。当分片执行完成后，把输出聚合起来就是最终的结果。在这个过程当中，对于state的需求还是比较小的。 对于流计算而言，对State有非常高的要求，因为在流系统中输入是一个无限制的流，会运行很长一段时间，甚至运行几天或者几个月都不会停机。在这个过程当中，就需要将状态数据很好的管理起来。很不幸的是，在传统的流计算系统中，对状态管理支持并不是很完善。比如storm,没有任何程序状态的支持，一种可选的方案是storm+hbase这样的方式去实现，把这状态数据存放在Hbase中，计算的时候再次从Hbase读取状态数据，做更新在写入进去。这样就会有如下几个问题 1.3.Flink丰富的状态访问和高效的容错机制Flink在最早设计的时候就意识到了这个问题，并提供了丰富的状态访问和容错机制。如下图所示： Flink中的状态管理 2.1.按照数据的划分和扩张方式 Keyed States Operator States 2.1.1. Keyed States Keyed States的使用 Flink也提供了Keyed States多种数据结构类型 Keyed States的动态扩容 2.1.2.Operator State Operator States的使用 Operator States的数据结构不像Keyed States丰富，现在只支持List。 Operator States多种扩展方式 Operator States的动态扩展是非常灵活的，现提供了3种扩展，下面分别介绍： ListState:并发度在改变的时候，会将并发上的每个List都取出，然后把这些List合并到一个新的List,然后根据元素的个数在均匀分配给新的Task; UnionListState:相比于ListState更加灵活，把划分的方式交给用户去做，当改变并发的时候，会将原来的List拼接起来。然后不做划分，直接交给用户； BroadcastState:如大表和小表做Join时，小表可以直接广播给大表的分区，在每个并发上的数据都是完全一致的。做的更新也相同，当改变并发的时候，把这些数据COPY到新的Task即可； 以上是Flink Operator States提供的3种扩展方式，用户可以根据自己的需求做选择。 使用Checkpoint提高程序的可靠性用户可以根据的程序里面的配置将checkpoint打开，给定一个时间间隔后，框架会按照时间间隔给程序的状态进行备份。当发生故障时，Flink会将所有Task的状态一起恢复到Checkpoint的状态。从哪个位置开始重新执行。 Flink也提供了多种正确性的保障，包括： AT LEAST ONCE; Exactly once; 备份为保存在State中的程序状态数据 Flink也提供了一套机制，允许把这些状态放到内存当中。做Checkpoint的时候，由Flink去完成恢复。 从已停止作业的运行状态中恢复 当组件升级的时候，需要停止当前作业。这个时候需要从之前停止的作业当中恢复，Flink提供了2种机制恢复作业: Savepoint:是一种特殊的checkpoint，只不过不像checkpoint定期的从系统中去触发的，它是用户通过命令触发，存储格式和checkpoint也是不相同的，会将数据按照一个标准的格式存储，不管配置什么样，Flink都会从这个checkpoint恢复，是用来做版本升级一个非常好的工具； External Checkpoint：对已有checkpoint的一种扩展，就是说做完一次内部的一次Checkpoint后，还会在用户给定的一个目录中，多存储一份checkpoint的数据； 状态管理和容错机制实现下面介绍一下状态管理和容错机制实现方式，Flink提供了3种不同的StateBackend MemoryStateBackend FsStateBackend RockDBStateBackend 用户可以根据自己的需求选择，如果数据量较小，可以存放到MemoryStateBackend和FsStateBackend中，如果数据量较大，可以放到RockDB中。 HeapKeyedStateBackend RockDBKeyedStateBackend Checkpoint的执行流程 Checkpoint的执行流程是按照Chandy-Lamport算法实现的 Checkpoint Barrier的对齐 全量Checkpoint 全量Checkpoint会在每个节点做备份数据时，只需要将数据都便利一遍，然后写到外部存储中，这种情况会影响备份性能。在此基础上做了优化。 RockDB的增量Checkpoint RockDB的数据会更新到内存，当内存满时，会写入到磁盘中。增量的机制会将新产生的文件COPY持久化中，而之前产生的文件就不需要COPY到持久化中去了。通过这种方式减少COPY的数据量，并提高性能。 阿里相关工作介绍4.1.Flink在阿里的成长路线 阿里是从2015年开始调研Flink,2015年10月启动Blink项目，并完善Flink在大规模生产下的一些优化和改进。2016年双11采用了Blink系统，为搜索，推荐，广告业务提供服务。2017年5月Blink已成为阿里的实时计算引擎。 4.2.阿里在状态管理和容错相关的工作 正在做的工作，基于State重构Window方面的一些优化，阿里也正在将功能做完善。后续将包括asynchronous Checkpoint的功能完善，并和社区进一步沟通和合作。帮助Flink社区完善相关方面的工作。 最后GitHub Flink 学习代码地址：https://github.com/zhisheng17/flink-learning 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"流计算框架 Flink 与 Storm 的性能对比","date":"2019-06-16T16:00:00.000Z","path":"2019/06/17/flink-vs-storm/","text":"1. 背景Apache Flink 和 Apache Storm 是当前业界广泛使用的两个分布式实时计算框架。其中 Apache Storm（以下简称“Storm”）在美团点评实时计算业务中已有较为成熟的运用（可参考 Storm 的可靠性保证测试），有管理平台、常用 API 和相应的文档，大量实时作业基于 Storm 构建。而 Apache Flink（以下简称“Flink”）在近期倍受关注，具有高吞吐、低延迟、高可靠和精确计算等特性，对事件窗口有很好的支持，目前在美团点评实时计算业务中也已有一定应用。 本文转载自美团技术团队公众号，作者：梦瑶 为深入熟悉了解 Flink 框架，验证其稳定性和可靠性，评估其实时处理性能，识别该体系中的缺点，找到其性能瓶颈并进行优化，给用户提供最适合的实时计算引擎，我们以实践经验丰富的 Storm 框架作为对照，进行了一系列实验测试 Flink 框架的性能，计算 Flink 作为确保“至少一次”和“恰好一次”语义的实时计算框架时对资源的消耗，为实时计算平台资源规划、框架选择、性能调优等决策及 Flink 平台的建设提出建议并提供数据支持，为后续的 SLA 建设提供一定参考。 Flink 与 Storm 两个框架对比： 2. 测试目标评估不同场景、不同数据压力下 Flink 和 Storm 两个实时计算框架目前的性能表现，获取其详细性能数据并找到处理性能的极限；了解不同配置对 Flink 性能影响的程度，分析各种配置的适用场景，从而得出调优建议。 2.1 测试场景“输入-输出”简单处理场景 通过对“输入-输出”这样简单处理逻辑场景的测试，尽可能减少其它因素的干扰，反映两个框架本身的性能。 同时测算框架处理能力的极限，处理更加复杂的逻辑的性能不会比纯粹“输入-输出”更高。 用户作业耗时较长的场景 如果用户的处理逻辑较为复杂，或是访问了数据库等外部组件，其执行时间会增大，作业的性能会受到影响。因此，我们测试了用户作业耗时较长的场景下两个框架的调度性能。 窗口统计场景 实时计算中常有对时间窗口或计数窗口进行统计的需求，例如一天中每五分钟的访问量，每 100 个订单中有多少个使用了优惠等。Flink 在窗口支持上的功能比 Storm 更加强大，API 更加完善，但是我们同时也想了解在窗口统计这个常用场景下两个框架的性能。 精确计算场景（即消息投递语义为“恰好一次”） Storm 仅能保证“至多一次” (At Most Once) 和“至少一次” (At Least Once) 的消息投递语义，即可能存在重复发送的情况。有很多业务场景对数据的精确性要求较高，希望消息投递不重不漏。Flink 支持“恰好一次” (Exactly Once) 的语义，但是在限定的资源条件下，更加严格的精确度要求可能带来更高的代价，从而影响性能。因此，我们测试了在不同消息投递语义下两个框架的性能，希望为精确计算场景的资源规划提供数据参考。 2.2 性能指标吞吐量（Throughput） 单位时间内由计算框架成功地传送数据的数量，本次测试吞吐量的单位为：条/秒。 反映了系统的负载能力，在相应的资源条件下，单位时间内系统能处理多少数据。 * 吞吐量常用于资源规划，同时也用于协助分析系统性能瓶颈，从而进行相应的资源调整以保证系统能达到用户所要求的处理能力。假设商家每小时能做二十份午餐（吞吐量 20 份/小时），一个外卖小哥每小时只能送两份（吞吐量 2 份/小时），这个系统的瓶颈就在小哥配送这个环节，可以给该商家安排十个外卖小哥配送。 延迟（Latency） 数据从进入系统到流出系统所用的时间，本次测试延迟的单位为：毫秒。 反映了系统处理的实时性。 金融交易分析等大量实时计算业务对延迟有较高要求，延迟越低，数据实时性越强。 假设商家做一份午餐需要 5 分钟，小哥配送需要 25 分钟，这个流程中用户感受到了 30 分钟的延迟。如果更换配送方案后延迟变成了 60 分钟，等送到了饭菜都凉了，这个新的方案就是无法接受的。 3. 测试环境为 Storm 和 Flink 分别搭建由 1 台主节点和 2 台从节点构成的 Standalone 集群进行本次测试。其中为了观察 Flink 在实际生产环境中的性能，对于部分测内容也进行了 on Yarn 环境的测试。 3.1 集群参数 3.2 框架参数 4. 测试方法4.1 测试流程 数据生产 Data Generator 按特定速率生成数据，带上自增的 id 和 eventTime 时间戳写入 Kafka 的一个 Topic（Topic Data）。 数据处理 Storm Task 和 Flink Task （每个测试用例不同）从 Kafka Topic Data 相同的 Offset 开始消费，并将结果及相应 inTime、outTime 时间戳分别写入两个 Topic（Topic Storm 和 Topic Flink）中。 指标统计 Metrics Collector 按 outTime 的时间窗口从这两个 Topic 中统计测试指标，每五分钟将相应的指标写入 MySQL 表中。Metrics Collector 按 outTime 取五分钟的滚动时间窗口，计算五分钟的平均吞吐（输出数据的条数）、五分钟内的延迟（outTime - eventTime 或 outTime - inTime）的中位数及 99 线等指标，写入 MySQL 相应的数据表中。最后对 MySQL 表中的吞吐计算均值，延迟中位数及延迟 99 线选取中位数，绘制图像并分析。 4.2 默认参数 Storm 和 Flink 默认均为 At Least Once 语义。 Storm 开启 ACK，ACKer 数量为 1。 Flink 的 Checkpoint 时间间隔为 30 秒，默认 StateBackend 为 Memory。 保证 Kafka 不是性能瓶颈，尽可能排除 Kafka 对测试结果的影响。 测试延迟时数据生产速率小于数据处理能力，假设数据被写入 Kafka 后立刻被读取，即 eventTime 等于数据进入系统的时间。 测试吞吐量时从 Kafka Topic 的最旧开始读取，假设该 Topic 中的测试数据量充足。 4.3 测试用例Identity Identity 用例主要模拟“输入-输出”简单处理场景，反映两个框架本身的性能。 输入数据为“msgId, eventTime”，其中 eventTime 视为数据生成时间。单条输入数据约 20 B。 进入作业处理流程时记录 inTime，作业处理完成后（准备输出时）记录 outTime。 作业从 Kafka Topic Data 中读取数据后，在字符串末尾追加时间戳，然后直接输出到 Kafka。 输出数据为“msgId, eventTime, inTime, outTime”。单条输出数据约 50 B。 Sleep Sleep 用例主要模拟用户作业耗时较长的场景，反映复杂用户逻辑对框架差异的削弱，比较两个框架的调度性能。 输入数据和输出数据均与 Identity 相同。 读入数据后，等待一定时长（1 ms）后在字符串末尾追加时间戳后输出 Windowed Word Count Windowed Word Count 用例主要模拟窗口统计场景，反映两个框架在进行窗口统计时性能的差异。 此外，还用其进行了精确计算场景的测试，反映 Flink 恰好一次投递的性能。 输入为 JSON 格式，包含 msgId、eventTime 和一个由若干单词组成的句子，单词之间由空格分隔。单条输入数据约 150 B。 读入数据后解析 JSON，然后将句子分割为相应单词，带 eventTime 和 inTime 时间戳发给 CountWindow 进行单词计数，同时记录一个窗口中最大最小的 eventTime 和 inTime，最后带 outTime 时间戳输出到 Kafka 相应的 Topic。 Spout/Source 及 OutputBolt/Output/Sink 并发度恒为 1，增大并发度时仅增大 JSONParser、CountWindow 的并发度。 由于 Storm 对 window 的支持较弱，CountWindow 使用一个 HashMap 手动实现，Flink 用了原生的 CountWindow 和相应的 Reduce 函数。 5. 测试结果5.1 Identity 单线程吞吐量 上图中蓝色柱形为单线程 Storm 作业的吞吐，橙色柱形为单线程 Flink 作业的吞吐。 Identity 逻辑下，Storm 单线程吞吐为 8.7 万条/秒，Flink 单线程吞吐可达 35 万条/秒。 当 Kafka Data 的 Partition 数为 1 时，Flink 的吞吐约为 Storm 的 3.2 倍；当其 Partition 数为 8 时，Flink 的吞吐约为 Storm 的 4.6 倍。 由此可以看出，Flink 吞吐约为 Storm 的 3-5 倍。 5.2 Identity 单线程作业延迟 采用 outTime - eventTime 作为延迟，图中蓝色折线为 Storm，橙色折线为 Flink。虚线为 99 线，实线为中位数。 从图中可以看出随着数据量逐渐增大，Identity 的延迟逐渐增大。其中 99 线的增大速度比中位数快，Storm 的 增大速度比 Flink 快。 其中 QPS 在 80000 以上的测试数据超过了 Storm 单线程的吞吐能力，无法对 Storm 进行测试，只有 Flink 的曲线。 对比折线最右端的数据可以看出，Storm QPS 接近吞吐时延迟中位数约 100 毫秒，99 线约 700 毫秒，Flink 中位数约 50 毫秒，99 线约 300 毫秒。Flink 在满吞吐时的延迟约为 Storm 的一半。 5.3 Sleep 吞吐量 从图中可以看出，Sleep 1 毫秒时，Storm 和 Flink 单线程的吞吐均在 900 条/秒左右，且随着并发增大基本呈线性增大。 对比蓝色和橙色的柱形可以发现，此时两个框架的吞吐能力基本一致。 5.4 Sleep 单线程作业延迟（中位数） 依然采用 outTime - eventTime 作为延迟，从图中可以看出，Sleep 1 毫秒时，Flink 的延迟仍低于 Storm。 5.5 Windowed Word Count 单线程吞吐量 单线程执行大小为 10 的计数窗口，吞吐量统计如图。 从图中可以看出，Storm 吞吐约为 1.2 万条/秒，Flink Standalone 约为 4.3 万条/秒。Flink 吞吐依然为 Storm 的 3 倍以上。 5.6 Windowed Word Count Flink At Least Once 与 Exactly Once 吞吐量对比 由于同一算子的多个并行任务处理速度可能不同，在上游算子中不同快照里的内容，经过中间并行算子的处理，到达下游算子时可能被计入同一个快照中。这样一来，这部分数据会被重复处理。因此，Flink 在 Exactly Once 语义下需要进行对齐，即当前最早的快照中所有数据处理完之前，属于下一个快照的数据不进行处理，而是在缓存区等待。当前测试用例中，在 JSON Parser 和 CountWindow、CountWindow 和 Output 之间均需要进行对齐，有一定消耗。为体现出对齐场景，Source/Output/Sink 并发度的并发度仍为 1，提高了 JSONParser/CountWindow 的并发度。具体流程细节参见前文 Windowed Word Count 流程图。 上图中橙色柱形为 At Least Once 的吞吐量，黄色柱形为 Exactly Once 的吞吐量。对比两者可以看出，在当前并发条件下，Exactly Once 的吞吐较 At Least Once 而言下降了 6.3% 5.7 Windowed Word Count Storm At Least Once 与 At Most Once 吞吐量对比 Storm 将 ACKer 数量设置为零后，每条消息在发送时就自动 ACK，不再等待 Bolt 的 ACK，也不再重发消息，为 At Most Once 语义。 上图中蓝色柱形为 At Least Once 的吞吐量，浅蓝色柱形为 At Most Once 的吞吐量。对比两者可以看出，在当前并发条件下，At Most Once 语义下的吞吐较 At Least Once 而言提高了 16.8% 5.8 Windowed Word Count 单线程作业延迟 Identity 和 Sleep 观测的都是 outTime - eventTime，因为作业处理时间较短或 Thread.sleep() 精度不高，outTime - inTime 为零或没有比较意义；Windowed Word Count 中可以有效测得 outTime - inTime 的数值，将其与 outTime - eventTime 画在同一张图上，其中 outTime - eventTime 为虚线，outTime - InTime 为实线。 观察橙色的两条折线可以发现，Flink 用两种方式统计的延迟都维持在较低水平；观察两条蓝色的曲线可以发现，Storm 的 outTime - inTime 较低，outTime - eventTime 一直较高，即 inTime 和 eventTime 之间的差值一直较大，可能与 Storm 和 Flink 的数据读入方式有关。 蓝色折线表明 Storm 的延迟随数据量的增大而增大，而橙色折线表明 Flink 的延迟随着数据量的增大而减小（此处未测至 Flink 吞吐量，接近吞吐时 Flink 延迟依然会上升）。 即使仅关注 outTime - inTime（即图中实线部分），依然可以发现，当 QPS 逐渐增大的时候，Flink 在延迟上的优势开始体现出来。 5.9 Windowed Word Count Flink At Least Once 与 Exactly Once 延迟对比 图中黄色为 99 线，橙色为中位数，虚线为 At Least Once，实线为 Exactly Once。图中相应颜色的虚实曲线都基本重合，可以看出 Flink Exactly Once 的延迟中位数曲线与 At Least Once 基本贴合，在延迟上性能没有太大差异。 5.10 Windowed Word Count Storm At Least Once 与 At Most Once 延迟对比 图中蓝色为 99 线，浅蓝色为中位数，虚线为 At Least Once，实线为 At Most Once。QPS 在 4000 及以前的时候，虚线实线基本重合；QPS 在 6000 时两者已有差异，虚线略高；QPS 接近 8000 时，已超过 At Least Once 语义下 Storm 的吞吐，因此只有实线上的点。 可以看出，QPS 较低时 Storm At Most Once 与 At Least Once 的延迟观察不到差异，随着 QPS 增大差异开始增大，At Most Once 的延迟较低。 5.11 Windowed Word Count Flink 不同 StateBackends 吞吐量对比 Flink 支持 Standalone 和 on Yarn 的集群部署模式，同时支持 Memory、FileSystem、RocksDB 三种状态存储后端（StateBackends）。由于线上作业需要，测试了这三种 StateBackends 在两种集群部署模式上的性能差异。其中，Standalone 时的存储路径为 JobManager 上的一个文件目录，on Yarn 时存储路径为 HDFS 上一个文件目录。 对比三组柱形可以发现，使用 FileSystem 和 Memory 的吞吐差异不大，使用 RocksDB 的吞吐仅其余两者的十分之一左右。 对比两种颜色可以发现，Standalone 和 on Yarn 的总体差异不大，使用 FileSystem 和 Memory 时 on Yarn 模式下吞吐稍高，使用 RocksDB 时 Standalone 模式下的吞吐稍高。 5.12 Windowed Word Count Flink 不同 StateBackends 延迟对比 使用 FileSystem 和 Memory 作为 Backends 时，延迟基本一致且较低。 使用 RocksDB 作为 Backends 时，延迟稍高，且由于吞吐较低，在达到吞吐瓶颈前的延迟陡增。其中 on Yarn 模式下吞吐更低，接近吞吐时的延迟更高。 6. 结论及建议6.1 框架本身性能 由 5.1、5.5 的测试结果可以看出，Storm 单线程吞吐约为 8.7 万条/秒，Flink 单线程吞吐可达 35 万条/秒。Flink 吞吐约为 Storm 的 3-5 倍。 由 5.2、5.8 的测试结果可以看出，Storm QPS 接近吞吐时延迟（含 Kafka 读写时间）中位数约 100 毫秒，99 线约 700 毫秒，Flink 中位数约 50 毫秒，99 线约 300 毫秒。Flink 在满吞吐时的延迟约为 Storm 的一半，且随着 QPS 逐渐增大，Flink 在延迟上的优势开始体现出来。 综上可得，Flink 框架本身性能优于 Storm。 6.2 复杂用户逻辑对框架差异的削弱对比 5.1 和 5.3、5.2 和 5.4 的测试结果可以发现，单个 Bolt Sleep 时长达到 1 毫秒时，Flink 的延迟仍低于 Storm，但吞吐优势已基本无法体现。 因此，用户逻辑越复杂，本身耗时越长，针对该逻辑的测试体现出来的框架的差异越小。 6.3 不同消息投递语义的差异 由 5.6、5.7、5.9、5.10 的测试结果可以看出，Flink Exactly Once 的吞吐较 At Least Once 而言下降 6.3%，延迟差异不大；Storm At Most Once 语义下的吞吐较 At Least Once 提升 16.8%，延迟稍有下降。 由于 Storm 会对每条消息进行 ACK，Flink 是基于一批消息做的检查点，不同的实现原理导致两者在 At Least Once 语义的花费差异较大，从而影响了性能。而 Flink 实现 Exactly Once 语义仅增加了对齐操作，因此在算子并发量不大、没有出现慢节点的情况下对 Flink 性能的影响不大。Storm At Most Once 语义下的性能仍然低于 Flink。 6.4 Flink 状态存储后端选择Flink 提供了内存、文件系统、RocksDB 三种 StateBackends，结合 5.11、5.12 的测试结果，三者的对比如下： 6.5 推荐使用 Flink 的场景综合上述测试结果，以下实时计算场景建议考虑使用 Flink 框架进行计算： + 要求消息投递语义为 Exactly Once 的场景； + 数据量较大，要求高吞吐低延迟的场景； + 需要进行状态管理或窗口统计的场景。 7. 展望 本次测试中尚有一些内容没有进行更加深入的测试，有待后续测试补充。例如： Exactly Once 在并发量增大的时候是否吞吐会明显下降？ 用户耗时到 1ms 时框架的差异已经不再明显（Thread.sleep() 的精度只能到毫秒），用户耗时在什么范围内 Flink 的优势依然能体现出来？ 本次测试仅观察了吞吐量和延迟两项指标，对于系统的可靠性、可扩展性等重要的性能指标没有在统计数据层面进行关注，有待后续补充。 Flink 使用 RocksDBStateBackend 时的吞吐较低，有待进一步探索和优化。 关于 Flink 的更高级 API，如 Table API &amp; SQL 及 CEP 等，需要进一步了解和完善。 8. 参考内容 分布式流处理框架——功能对比和性能评估. intel-hadoop/HiBench: HiBench is a big data benchmark suite. Yahoo的流计算引擎基准测试. Extending the Yahoo! Streaming Benchmark. 最后GitHub Flink 学习代码地址：https://github.com/zhisheng17/flink-learning 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库","date":"2019-06-15T16:00:00.000Z","path":"2019/06/16/flink-sql-oppo/","text":"一.OPPO 实时数仓的演进思路 本文转载自 AI 前线公众号，作者张俊，编辑 | Vincent 本文整理自 2019 年 4 月 13 日在深圳举行的 Flink Meetup 会议，分享嘉宾张俊，目前担任 OPPO 大数据平台研发负责人，也是 Apache Flink contributor。本文主要内容如下： OPPO 实时数仓的演进思路； 基于 Flink SQL 的扩展工作； 构建实时数仓的应用案例； 未来工作的思考和展望。 1.1.OPPO 业务与数据规模大家都知道 OPPO 是做智能手机的，但并不知道 OPPO 与互联网以及大数据有什么关系，下图概要介绍了 OPPO 的业务与数据情况： OPPO 作为手机厂商，基于 Android 定制了自己的 ColorOS 系统，当前日活跃用户超过 2 亿。围绕 ColorOS，OPPO 构建了很多互联网应用，比如应用商店、浏览器、信息流等。在运营这些互联网应用的过程中，OPPO 积累了大量的数据，上图右边是整体数据规模的演进：从 2012 年开始每年都是 2~3 倍的增长速度，截至目前总数据量已经超过 100PB，日增数据量超过 200TB。 要支撑这么大的一个数据量，OPPO 研发出一整套的数据系统与服务，并逐渐形成了自己的数据中台体系。 1.2.OPPO 数据中台 今年大家都在谈数据中台，OPPO 是如何理解数据中台的呢？我们把它分成了 4 个层次： 最下层是统一工具体系，涵盖了”接入 - 治理 - 开发 - 消费”全数据链路； 基于工具体系之上构建了数据仓库，划分成”原始层 - 明细层 - 汇总层 - 应用层”，这也是经典的数仓架构； 再往上是全域的数据体系，什么是全域呢？就是把公司所有的业务数据都打通，形成统一的数据资产，比如 ID-Mapping、用户标签等； 最终，数据要能被业务用起来，需要场景驱动的数据产品与服务。 以上就是 OPPO 数据中台的整个体系，而数据仓库在其中处于非常基础与核心的位置。 1.3. 构建 OPPO 离线数仓 过往 2、3 年，我们的重点聚焦在离线数仓的构建。上图大致描述了整个构建过程：首先，数据来源基本是手机、日志文件以及 DB 数据库，我们基于 Apache NiFi 打造了高可用、高吞吐的接入系统，将数据统一落入 HDFS，形成原始层；紧接着，基于 Hive 的小时级 ETL 与天级汇总 Hive 任务，分别负责计算生成明细层与汇总层；最后，应用层是基于 OPPO 内部研发的数据产品，主要是报表分析、用户画像以及接口服务。此外，中间的明细层还支持基于 Presto 的即席查询与自助提数。 伴随着离线数仓的逐步完善，业务对实时数仓的诉求也愈发强烈。 1.4. 数仓实时化的诉求 对于数仓实时化的诉求，大家通常都是从业务视角来看，但其实站在平台的角度，实时化也能带来切实的好处。首先，从业务侧来看，报表、标签、接口等都会有实时的应用场景，分别参见上图左边的几个案例；其次，对平台侧来说，我们可以从三个案例来看：第一，OPPO 大量的批量任务都是从 0 点开始启动，都是通过 T+1 的方式去做数据处理，这会导致计算负载集中爆发，对集群的压力很大；第二，标签导入也属于一种 T+1 批量任务，每次全量导入都会耗费很长的时间；第三，数据质量的监控也必须是 T+1 的，导致没办法及时发现数据的一些问题。 既然业务侧和平台侧都有实时化的这个诉求，那 OPPO 是如何来构建自己的实时数仓呢？ 1.5. 离线到实时的平滑迁移 无论是一个平台还是一个系统，都离不开上下两个层次的构成：上层是 API，是面向用户的编程抽象与接口；下层是 Runtime，是面向内核的执行引擎。我们希望从离线到实时的迁移是平滑的，是什么意思呢？从 API 这层来看，数仓的抽象是 Table、编程接口是 SQL+UDF，离线数仓时代用户已经习惯了这样的 API，迁移到实时数仓后最好也能保持一致。而从 Runtime 这层来看，计算引擎从 Hive 演进到了 Flink，存储引擎从 HDFS 演进到了 Kafka。 基于以上的思路，只需要把之前提到的离线数仓 pipeline 改造下，就得到了实时数仓 pipeline。 1.6. 构建 OPPO 实时数仓 HDFS 替换为 Kafka。从总体流程来看，基本模型是不变的，还是由原始层、明细层、汇总层、应用层的级联计算来构成。 因此，这里的核心问题是如何基于 Flink 构建出这个 pipeline，下面就介绍下我们基于 Flink SQL 所做的一些工作。 二. 基于 Flink SQL 的扩展工作2.1.Why Flink SQL首先，为什么要用 Flink SQL? 下图展示了 Flink 框架的基本结构，最下面是 Runtime，这个执行引擎我们认为最核心的优势是四个：第一，低延迟，高吞吐；第二，端到端的 Exactly-once；第三，可容错的状态管理；第四，Window &amp; Event time 的支持。基于 Runtime 抽象出 3 个层次的 API，SQL 处于最上层。 Flink SQL API 有哪些优势呢？我们也从四个方面去看：第一，支持 ANSI SQL 的标准；第二，支持丰富的数据类型与内置函数，包括常见的算术运算与统计聚合；第三，可自定义 Source/Sink，基于此可以灵活地扩展上下游；第四，批流统一，同样的 SQL，既可以跑离线也可以跑实时。 那么，基于 Flink SQL API 如何编程呢？下面是一个简单的演示： 首先是定义与注册输入 / 输出表，这里创建了 2 张 Kakfa 的表，指定 kafka 版本是什么、对应哪个 topic；接下来是注册 UDF，篇幅原因这里没有列出 UDF 的定义；最后是才是执行真正的 SQL。可以看到，为了执行 SQL，需要做这么多的编码工作，这并不是我们希望暴露给用户的接口。 2.2. 基于 WEB 的开发 IDE 前面提到过，数仓的抽象是 Table，编程接口是 SQL+UDF。对于用户来说，平台提供的编程界面应该是类似上图的那种，有用过 HUE 做交互查询的应该很熟悉。左边的菜单是 Table 列表，右边是 SQL 编辑器，可以在上面直接写 SQL，然后提交执行。要实现这样一种交互方式，Flink SQL 默认是无法实现的，中间存在 gap，总结下来就 2 点：第一，元数据的管理，怎么去创建库表，怎么去上传 UDF，使得之后在 SQL 中可直接引用；第二，SQL 作业的管理，怎么去编译 SQL，怎么去提交作业。 在技术调研过程中，我们发现了 Uber 在 2017 年开源的 AthenaX 框架。 2.3.AthenaX：基于 REST 的 SQL 管理器 AthenaX 可以看作是一个基于 REST 的 SQL 管理器，它是怎么实现 SQL 作业与元数据管理的呢？ 对于 SQL 作业提交，AthenaX 中有一个 Job 的抽象，封装了要执行的 SQL 以及作业资源等信息。所有的 Job 由一个 JobStore 来托管，它定期跟 YARN 当中处于 Running 状态的 App 做一个匹配。如果不一致，就会向 YARN 提交对应的 Job。 对于元数据管理，核心的问题是如何将外部创建的库表注入 Flink，使得 SQL 中可以识别到。实际上，Flink 本身就预留了与外部元数据对接的能力，分别提供了 ExternalCatalog 和 ExternalCatalogTable 这两个抽象。AthenaX 在此基础上再封装出一个 TableCatalog，在接口层面做了一定的扩展。在提交 SQL 作业的阶段，AthenaX 会自动将 TableCatalog 注册到 Flink，再调用 Flink SQL 的接口将 SQL 编译为 Flink 的可执行单元 JobGraph，并最终提交到 YARN 生成新的 App。 AthenaX 虽然定义好了 TableCatalog 接口，但并没有提供可直接使用的实现。那么，我们怎么来实现，以便对接到我们已有的元数据系统呢？ 2.4.Flink SQL 注册库表的过程首先，我们得搞清楚 Flink SQL 内部是如何注册库表的。整个过程涉及到三个基本的抽象：TableDescriptor、TableFactory 以及 TableEnvironment。 TableDescriptor 顾名思义，是对表的描述，它由三个子描述符构成：第一是 Connector，描述数据的来源，比如 Kafka、ES 等；第二是 Format，描述数据的格式，比如 csv、json、avro 等；第三是 Schema，描述每个字段的名称与类型。TableDescriptor 有两个基本的实现——ConnectTableDescriptor 用于描述内部表，也就是编程方式创建的表；ExternalCatalogTable 用于描述外部表。 有了 TableDescriptor，接下来需要 TableFactory 根据描述信息来实例化 Table。不同的描述信息需要不同的 TableFactory 来处理，Flink 如何找到匹配的 TableFactory 实现呢？实际上，为了保证框架的可扩展性，Flink 采用了 Java SPI 机制来加载所有声明过的 TableFactory，通过遍历的方式去寻找哪个 TableFactory 是匹配该 TableDescriptor 的。TableDescriptor 在传递给 TableFactory 前，被转换成一个 map，所有的描述信息都用 key-value 形式来表达。TableFactory 定义了两个用于过滤匹配的方法——一个是 requiredContext()，用于检测某些特定 key 的 value 是否匹配，比如 connector.type 是否为 kakfa；另一个是 supportedProperties()，用于检测 key 是否能识别，如果出现不识别的 key，说明无法匹配。 匹配到了正确的 TableFactory，接下来就是创建真正的 Table，然后将其通过 TableEnvironment 注册。最终注册成功的 Table，才能在 SQL 中引用。 2.5.Flink SQL 对接外部数据源搞清楚了 Flink SQL 注册库表的过程，给我们带来这样一个思路：如果外部元数据创建的表也能被转换成 TableFactory 可识别的 map，那么就能被无缝地注册到 TableEnvironment。基于这个思路，我们实现了 Flink SQL 与已有元数据中心的对接，大致过程参见下图： 通过元数据中心创建的表，都会将元数据信息存储到 MySQL，我们用一张表来记录 Table 的基本信息，然后另外三张表分别记录 Connector、Format、Schema 转换成 key-value 后的描述信息。之所以拆开成三张表，是为了能够能独立的更新这三种描述信息。接下来是定制实现的 ExternalCatalog，能够读取 MySQL 这四张表，并转换成 map 结构。 2.6. 实时表 - 维表关联到目前为止，我们的平台已经具备了元数据管理与 SQL 作业管理的能力，但是要真正开放给用户使用，还有一点基本特性存在缺失。通过我们去构建数仓，星型模型是无法避免的。这里有一个比较简单的案例：中间的事实表记录了广告点击流，周边是关于用户、广告、产品、渠道的维度表。 假定我们有一个 SQL 分析，需要将点击流表与用户维表进行关联，这个目前在 Flink SQL 中应该怎么来实现？我们有两种实现方式，一个基于 UDF，一个基于 SQL 转换，下面分别展开来讲一下。 2.7. 基于 UDF 的维表关联首先是基于 UDF 的实现，需要用户将原始 SQL 改写为带 UDF 调用的 SQL，这里是 userDimFunc，上图右边是它的代码实现。UserDimFunc 继承了 Flink SQL 抽象的 TableFunction，它是其中一种 UDF 类型，可以将任意一行数据转换成一行或多行数据。为了实现维表关联，在 UDF 初始化时需要从 MySQL 全量加载维表的数据，缓存在内存 cache 中。后续对每行数据的处理，TableFunction 会调用 eval() 方法，在 eval() 中根据 user_id 去查找 cache，从而实现关联。当然，这里是假定维表数据比较小，如果数据量很大，不适合全量的加载与缓存，这里不做展开了。 基于 UDF 的实现，对用户和平台来说都不太友好：用户需要写奇怪的 SQL 语句，比如图中的 LATERAL TABLE；平台需要为每个关联场景定制特定的 UDF，维护成本太高。有没有更好的方式呢？下面我们来看看基于 SQL 转换的实现。 2.8. 基于 SQL 转换的维表关联我们希望解决基于 UDF 实现所带来的问题，用户不需要改写原始 SQL，平台不需要开发很多 UDF。有一种思路是，是否可以在 SQL 交给 Flink 编译之前，加一层 SQL 的解析与改写，自动实现维表的关联？经过一定的技术调研与 POC，我们发现是行得通的，所以称之为基于 SQL 转换的实现。下面将该思路展开解释下。 首先，增加的 SQL 解析是为了识别 SQL 中是否存在预先定义的维度表，比如上图中的 user_dim。一旦识别到维表，将触发 SQL 改写的流程，将红框标注的 join 语句改写成新的 Table，这个 Table 怎么得到呢？我们知道，流计算领域近年来发展出“流表二象性”的理念，Flink 也是该理念的践行者。这意味着，在 Flink 中 Stream 与 Table 之间是可以相互转换的。我们把 ad_clicks 对应的 Table 转换成 Stream，再调用 flatmap 形成另一个 Stream，最后再转换回 Table，就得到了 ad_clicks_user。最后的问题是，flatmap 是如何实现维表关联的？ Flink 中对于 Stream 的 flatmap 操作，实际上是执行一个 RichFlatmapFunciton，每来一行数据就调用其 flatmap() 方法做转换。那么，我们可以定制一个 RichFlatmapFunction，来实现维表数据的加载、缓存、查找以及关联，功能与基于 UDF 的 TableFunction 实现类似。 既然 RichFlatmapFunciton 的实现逻辑与 TableFunction 相似，那为什么相比基于 UDF 的方式，这种实现能更加通用呢？核心的点在于多了一层 SQL 解析，可以将维表的信息获取出来（比如维表名、关联字段、select 字段等），再封装成 JoinContext 传递给 RichFlatmapFunciton，使得的表达能力就具备通用性了。 三. 构建实时数仓的应用案例下面分享几个典型的应用案例，都是在我们的平台上用 Flink SQL 来实现的。 3.1. 实时 ETL 拆分这里是一个典型的实时 ETL 链路，从大表中拆分出各业务对应的小表： OPPO 的最大数据来源是手机端埋点，从手机 APP 过来的数据有一个特点，所有的数据是通过统一的几个通道上报过来。因为不可能每一次业务有新的埋点，都要去升级客户端，去增加新的通道。比如我们有个 sdk_log 通道，所有 APP 应用的埋点都往这个通道上报数据，导致这个通道对应的原始层表巨大，一天几十个 TB。但实际上，每个业务只关心它自身的那部分数据，这就要求我们在原始层进行 ETL 拆分。 这个 SQL 逻辑比较简单，无非是根据某些业务字段做筛选，插入到不同的业务表中去。它的特点是，多行 SQL 最终合并成一个 SQL 提交给 Flink 执行。大家担心的是，包含了 4 个 SQL，会不会对同一份数据重复读取 4 次？其实，在 Flink 编译 SQL 的阶段是会做一些优化的，因为最终指向的是同一个 kafka topic，所以只会读取 1 次数据。 另外，同样的 Flink SQL，我们同时用于离线与实时数仓的 ETL 拆分，分别落入 HDFS 与 Kafka。Flink 中本身支持写入 HDFS 的 Sink，比如 RollingFileSink。 3.2. 实时指标统计这里是一个典型的计算信息流 CTR 的这个案例，分别计算一定时间段内的曝光与点击次数，相除得到点击率导入 Mysql，然后通过我们内部的报表系统来可视化。这个 SQL 的特点是它用到了窗口 (Tumbling Window) 以及子查询。 3.3. 实时标签导入这里是一个实时标签导入的案例，手机端实时感知到当前用户的经纬度，转换成具体 POI 后导入 ES，最终在标签系统上做用户定向。 这个 SQL 的特点是用了 AggregateFunction，在 5 分钟的窗口内，我们只关心用户最新一次上报的经纬度。AggregateFunction 是一种 UDF 类型，通常是用于聚合指标的统计，比如计算 sum 或者 average。在这个示例中，由于我们只关心最新的经纬度，所以每次都替换老的数据即可。 四. 未来工作的思考和展望最后，给大家分享一下关于未来工作，我们的一些思考与规划，还不是太成熟，抛出来和大家探讨一下。 4.1. 端到端的实时流处理什么是端到端？一端是采集到的原始数据，另一端是报表 / 标签 / 接口这些对数据的呈现与应用，连接两端的是中间实时流。当前我们基于 SQL 的实时流处理，源表是 Kafka，目标表也是 Kafka，统一经过 Kafka 后再导入到 Druid/ES/HBase。这样设计的目的是提高整体流程的稳定性与可用性：首先，kafka 作为下游系统的缓冲，可以避免下游系统的异常影响实时流的计算（一个系统保持稳定，比起多个系统同时稳定，概率上更高点）；其次，kafka 到 kafka 的实时流，exactly-once 语义是比较成熟的，一致性上有保证。 然后，上述的端到端其实是由割裂的三个步骤来完成的，每一步可能需要由不同角色人去负责处理：数据处理需要数据开发人员，数据导入需要引擎开发人员，数据资产化需要产品开发人员。 我们的平台能否把端到端给自动化起来，只需要一次 SQL 提交就能打通处理、导入、资产化这三步？在这个思路下，数据开发中看到的不再是 Kafka Table，而应该是面向场景的展示表 / 标签表 / 接口表。比如对于展示表，创建表的时候只要指定维度、指标等字段，平台会将实时流结果数据从 Kafka 自动导入 Druid，再在报表系统自动导入 Druid 数据源，甚至自动生成报表模板。 4.2. 实时流的血缘分析关于血缘分析，做过离线数仓的朋友都很清楚它的重要性，它在数据治理中都起着不可或缺的关键作用。对于实时数仓来说也莫不如此。我们希望构建端到端的血缘关系，从采集系统的接入通道开始，到中间流经的实时表与实时作业，再到消费数据的产品，都能很清晰地展现出来。基于血缘关系的分析，我们才能评估数据的应用价值，核算数据的计算成本。 4.3. 离线 - 实时数仓一体化最后提一个方向是离线实时数仓的一体化。我们认为短期内，实时数仓无法替代离线数仓，两者并存是新常态。在离线数仓时代，我们积累的工具体系，如何去适配实时数仓，如何实现离线与实时数仓的一体化管理？理论上来讲，它们的数据来源是一致的，上层抽象也都是 Table 与 SQL，但本质上也有不同的点，比如时间粒度以及计算模式。对于数据工具与产品来说，需要做哪些改造来实现完全的一体化，这也是我们在探索和思考的。 最后GitHub Flink 学习代码地址：https://github.com/zhisheng17/flink-learning 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"为什么说流处理即未来？","date":"2019-06-14T16:00:00.000Z","path":"2019/06/15/Stream-processing/","text":"本文转自 AI 前线公众号，作者｜Stephan Ewen，策划编辑｜Natalie，编辑｜Debra，整理｜秦江杰 本文整理自 Flink 创始公司 dataArtisans（现在为Ververica） 联合创始人兼 CTO Stephan Ewen 在 Flink Forward China 2018 上的演讲《Stream Processing takes on Everything》。这个演讲主题看似比较激进：流处理解决所有问题。很多人对于 Flink 可能还停留在最初的认知，觉得 Flink 是一个流处理引擎，实际上 Flink 可以做很多其他的工作，比如批处理、应用程序。在这个演讲中，Stephan 首先会简单说明他对 Flink 功能的观点，然后深入介绍一个特定领域的应用和事件处理场景。这个场景乍看起来不是一个流处理的使用场景，但是在 Stephan 看来，它实际上就是一个很有趣的流处理使用场景。 上图对为什么流处理可以处理一切作出诠释，将数据看做流是一个自然而又十分强大的想法。大部分数据的产生过程都是随时间生成的流，比如一个 Petabyte 的数据不会凭空产生。这些数据通常都是一些事件的积累，比如支付、将商品放入购物车，网页浏览，传感器采样输出， 基于数据是流的想法，我们对数据处理可以有相应的理解。比如将过去的历史数据看做是一个截止到某一时刻的有限的流，或是将一个实时处理应用看成是从某一个时刻开始处理未来到达的数据。可能在未来某个时刻它会停止，那么它就变成了处理从开始时刻到停止时刻的有限数据的批处理。当然，它也有可能一直运行下去，不断处理新到达的数据。这个对数据的重要理解方式非常强大，基于这一理解，Flink 可以支持整个数据处理范畴内的所有场景。 最广为人知的 Flink 使用场景是流分析、连续处理（或者说渐进式处理），这些场景中 Flink 实时或者近实时的处理数据，或者采集之前提到的历史数据并且连续的对这些事件进行计算。晓伟在之前的演讲中提到一个非常好的例子来说明怎么样通过对 Flink 进行一些优化，进而可以针对有限数据集做一些特别的处理，这使得 Flink 能够很好的支持批处理的场景，从性能上来说能够与最先进的批处理引擎相媲美。而在这根轴的另一头，是我今天的演讲将要说明的场景 – 事件驱动的应用。这类应用普遍存在于任何服务或者微服务的架构中。这类应用接收各类事件（可能是 RPC 调用、HTTP 请求），并且对这些事件作出一些响应，比如把商品放进购物车，或者加入社交网络中的某个群组。 在我进一步展开今天的演讲之前，我想先对社区在 Flink 的传统领域（实时分析、连续处理）近期所做的工作做一个介绍。Flink 1.7 在 2018 年 11 月 30 日已经发布。在 Flink 1.7 中为典型的流处理场景加入了一些非常有趣的功能。比如我个人非常感兴趣的在流式 SQL 中带时间版本的 Join。一个基本想法是有两个不同的流，其中一个流被定义为随时间变化的参照表，另一个是与参照表进行 Join 的事件流。比如事件流是一个订单流，参照表是不断被更新的汇率，而每个订单需要使用最新的汇率来进行换算，并将换算的结果输出到结果表。这个例子在标准的 SQL 当中实际上并不容易表达，但在我们对 Streaming SQL 做了一点小的扩展以后，这个逻辑表达变得非常简单，我们发现这样的表达有非常多的应用场景。 另一个在流处理领域十分强大的新功能是将复杂事件处理（CEP）和 SQL 相结合。CEP 应用观察事件模式。比如某个 CEP 应用观察股市，当有两个上涨后紧跟一个下跌时，这个应用可能做些交易。再比如一个观察温度计的应用，当它发现有温度计在两个超过 90 摄氏度的读数之后的两分钟里没有任何操作，可能会进行一些操作。与 SQL 的结合使这类逻辑的表达也变得非常简单。 第三个 Flink 1.7 中做了很多工作的功能是 Schema 升级。这个功能和基于流的应用紧密相关。就像你可以对数据库进行数据 Schema 升级一样，你可以修改 Flink 表中列的类型或者重新写一个列， 另外我想简单介绍的是流处理技术不仅仅是简单对数据进行计算，这还包括了很多与外部系统进行事务交互。流处理引擎需要在采用不同协议的系统之间以事务的方式移动数据，并保证计算过程和数据的一致性。这一部分功能也是在 Flink 1.7 中得到了增强。 以上我对 Flink 1.7 的新功能向大家做了简单总结。下面让我们来看看今天我演讲的主要部分，也就是利用 Flink 来搭建应用和服务。我将说明为什么流处理是一个搭建应用和服务或者微服务的有趣技术。 我将从左边这个高度简化的图说起，我们一会儿将聊一些其中的细节。首先我们来看一个理解应用简单的视角。如左图所示，一个应用可以是一个 Container，一个 Spring 应用，或者 Java 应用、Ruby 应用，等等。这个应用从诸如 RPC，HTTP 等渠道接收请求，然后依据请求进行数据库变更。这个应用也可能调用另一个微服务并进行下一步的处理。我们可以非常自然的想到进入到应用的这些请求可以看做是个事件组成的序列，所以我们可以把它们看做是事件流。可能这些事件被缓存在消息队列中，而应用会从消息队列中消费这些事件进行处理，当应用需要响应一个请求时，它将结果输出到另一个消息队列，而请求发送方可以从这个消息队列中消费得到所发送请求的响应。在这张图中我们已经可以看到一些有趣的不同。 第一个不同是在这张图中应用和数据库不再是分开的两个实体，而是被一个有状态的流处理应用所代替。所以在流处理应用的架构中，不再有应用和数据库的连接了，它们被放到了一起。这个做法有利有弊，但其中有些好处是非常重要的。首先是性能上的好处是明显的，因为应用不再需要和数据库进行交互，处理可以基于内存中的变量进行。其次这种做法有很好并且很简单的一致性。 这张图被简化了很多，实际上我们通常会有很多个应用，而不是一个被隔离的应用，很多情况下你的应用会更符合这张图。系统中有个接收请求的接口，然后请求被发送到第一个应用，可能会再被发到另一个应用，然后得到相应。在图中有些应用会消费中间结果的流。这张图已经展示了为什么流处理是更适合比较复杂的微服务场景的技术。因为很多时候系统中不会有一个直接接收用户请求并直接响应的服务，通常来说一个微服务需要跟其他微服务通信。这正如在流处理的架构中不同应用在创建输出流，同时基于衍生出的流再创建并输出新的流。 到目前为止，我们看到的内容多少还比较直观。而对基于流处理技术的微服务架构而言，人们最常问的一个问题是如何保证事务性？如果系统中使用的是数据库，通常来说都会有非常成熟复杂的数据校验和事务模型。这也是数据库在过去许多年中十分成功的原因。开始一个事务，对数据做一些操作，提交或者撤销一个事务。这个机制使得数据完整性得到了保证（一致性，持久性等等）。 那么在流处理中我们怎么做到同样的事情呢？作为一个优秀的流处理引擎，Flink 支持了恰好一次语义，保证了每个事件只会被处理一遍。但是这依然对某些操作有限制，这也成为了使用流处理应用的一个障碍。我们通过一个非常简单流处理应用例子来看我们可以做一些什么扩展来解决这个问题。我们会看到，解决办法其实出奇的简单。 让我们以这个教科书式的事务为例子来看一下事务性应用的过程。这个系统维护了账户和其中存款余额的信息。这样的信息可能是银行或者在线支付系统的场景中用到的。假设我们想要处理类似下面的事务：如果账户 A 中的余额大于 100，那么从账户 A 中转账 50 元到账户 B。这是个非常简单的两个账户之间进行转账的例子。 数据库对于这样的事务已经有了一个核心的范式，也就是原子性，一致性，隔离性和持久性（ACID）。这是能够让用户放心使用事务的几个基本保证。有了他们，用户不用担心钱在转账过程中会丢失或者其他问题。让我们用这个例子来放到流处理应用中，来让流处理应用也能提供和数据相同的 ACID 支持： 原子性要求一个转账要不就完全完成，也就是说转账金额从一个账户减少，并增加到另一个账户，要不就两个账户的余额都没有变化。而不会只有一个账户余额改变。否则的话钱就会凭空减少或者凭空增加。 一致性和隔离性是说如果有很多用户同时想要进行转账，那么这些转账行为之间应该互不干扰，每个转账行为应该被独立的完成，并且完成后每个账户的余额应该是正确的。也就是说如果两个用户同时操作同一个账户，系统不应该出错。 持久性指的是如果一个操作已经完成，那么这个操作的结果会被妥善的保存而不会丢失。 我们假设持久性已经被满足。一个流处理器有状态，这个状态会被 checkpoint，所以流处理器的状态是可恢复的。也就是说只要我们完成了一个修改，并且这个修改被 checkpoint 了，那么这个修改就是持久化的。 让我们来看看另外三个例子。设想一下，如果我们用流处理应用来实现这样一个转账系统会发生什么。我们先把问题简化一些，假设转账不需要有条件，仅仅是将 50 元从账户 A 转到账户，也就是说账户 A 的余额减少 50 元而账户 B 的余额增加 50 元。我们的系统是一个分布式的并行系统，而不是一个单机系统。简单起见我们假设系统中只有两台机器，这两台机器可以是不同的物理机或者是在 YARN 或者 Kubernetes 上不同的容器。总之它们是两个不同的流处理器实例，数据分布在这两个流处理器上。我们假设账户 A 的数据由其中一台机器维护，而账户 B 的数据有另一台机器维护。 现在我们要做个转账，将 50 元从账户 A 转移到账户 B，我们把这个请求放进队列中，然后这个转账请求被分解为对账户 A 和 B 分别进行操作，并且根据键将这两个操作路由到维护账户 A 和维护账户 B 的这两台机器上，这两台机器分别根据要求对账户 A 和账户 B 的余额进行改动。这并不是事务操作，而只是两个独立无意义的改动。一旦我们将转账的请求改的稍微复杂一些就会发现问题。 下面我们假设转账是有条件的，我们只想在账户 A 的余额足够的情况下才进行转账，这样就已经有些不太对了。如果我们还是像之前那样操作，将这个转账请求分别发送给维护账户 A 和 B 的两台机器，如果 A 没有足够的余额，那么 A 的余额不会发生变化，而 B 的余额可能已经被改动了。我们就违反了一致性的要求。 我们看到我们需要首先以某种方式统一做出是否需要更改余额的决定，如果这个统一的决定中余额需要被修改，我们再进行修改余额的操作。所以我们先给维护 A 的余额的机器发送一个请求，让它查看 A 的余额。我们也可以对 B 做同样的事情，但是这个例子里面我们不关心 B 的余额。然后我们把所有这样的条件检查的请求汇总起来去检验条件是否满足。因为 Flink 这样的流处理器支持迭代，如果满足转账条件，我们可以把这个余额改动的操作放进迭代的反馈流当中来告诉对应的节点来进行余额修改。反之如果条件不满足，那么余额改动的操作将不会被放进反馈流。这个例子里面，通过这种方式我们可以正确的进行转账操作。从某种角度上来说我们实现了原子性，基于一个条件我们可以进行全部的余额修改，或者不进行任何余额修改。这部分依然还是比较直观的，更大的困难是在于如何做到并发请求的隔离性。 假设我们的系统没有变，但是系统中有多个并发的请求。我们在之前的演讲中已经知道，这样的并发可能达到每秒钟几十亿条。如图，我们的系统可能从两个流中同时接受请求。如果这两个请求同时到达，我们像之前那样将每个请求拆分成多个请求，首先检查余额条件，然后进行余额操作。然而我们发现这会带来问题。管理账户 A 的机器会首先检查 A 的余额是否大于 50，然后又会检查 A 的余额是否大于 100，因为两个条件都满足，所以两笔转账操作都会进行，但实际上账户 A 上的余额可能无法同时完成两笔转账，而只能完成 50 元或者 100 元的转账中的一笔。这里我们需要进一步思考怎么样来处理并发的请求，我们不能只是简单地并发处理请求，这会违反事务的保证。从某种角度来说，这是整个数据库事务的核心。数据库的专家们花了一些时间提供了不同解决方案，有的方案比较简单，有的则很复杂。但所有的方案都不是那么容易，尤其是在分布式系统当中。 在流处理中怎么解决这个问题呢？直觉上讲，如果我们能够让所有的事务都按照顺序依次发生，那么问题就解决了，这也被成为可序列化的特性。但是我们当然不希望所有的请求都被依次顺序处理，这与我们使用分布式系统的初衷相违背。所以我们需要保证这些请求最后的产生的影响看起来是按照顺序发生的，也就是一个请求产生的影响是基于前一个请求产生影响的基础之上的。换句话说也就是一个事务的修改需要在前一个事务的所有修改都完成后才能进行。这种希望一件事在另一件事之后发生的要求看起来很熟悉，这似乎是我们以前在流处理中曾经遇到过的问题。是的，这听上去像是事件时间。用高度简化的方式来解释，如果所有的请求都在不同的事件时间产生，即使由于种种原因他们到达处理器的时间是乱序的，流处理器依然会根据他们的事件时间来对他们进行处理。流处理器会使得所有的事件的影响看上去都是按顺序发生的。按事件时间处理是 Flink 已经支持的功能。 那么详细说来，我们到底怎么解决这个一致性问题呢？假设我们有并行的请求输入并行的事务请求，这些请求读取某些表中的记录，然后修改某些表中的记录。我们首先需要做的是把这些事务请求根据事件时间顺序摆放。这些请求的事务时间不能够相同，但是他们之间的时间也需要足够接近，这是因为在事件时间的处理过程中会引入一定的延迟，我们需要保证所处理的事件时间在向前推进。因此第一步是定义事务执行的顺序，也就是说需要有一个聪明的算法来为每个事务制定事件时间。 在图上，假设这三个事务的事件时间分别是 T+2, T 和 T+1。那么第二个事务的影响需要在第一和第三个事务之前。不同的事务所做的修改是不同的，每个事务都会产生不同的操作请求来修改状态。我们现在需要将对访问每个行和状态的事件进行排序，保证他们的访问是符合事件时间顺序的。这也意味着那些相互之间没有关系的事务之间自然也没有了任何影响。比如这里的第三个事务请求，它与前两个事务之间没有访问共同的状态，所以它的事件时间排序与前两个事务也相互独立。而当前两个事务之间的操作的到达顺序与事件时间不符时，Flink 则会依据它们的事件时间进行排序后再处理。 必须承认，这样说还是进行了一些简化，我们还需要做一些事情来保证高效执行，但是总体原则上来说，这就是全部的设计。除此之外我们并不需要更多其他东西。 为了实现这个设计，我们引入了一种聪明的分布式事件时间分配机制。这里的事件时间是逻辑时间，它并不需要有什么现实意义，比如它不需要是真实的时钟。使用 Flink 的乱序处理能力，并且使用 Flink 迭代计算的功能来进行某些前提条件的检查。这些就是我们构建一个支持事务的流处理器的要素。 我们实际上已经完成了这个工作，称之为流式账簿（Streaming Ledger），这是个在 Apache Flink 上很小的库。它基于流处理器做到了满足 ACID 的多键事务性操作。我相信这是个非常有趣的进化。流处理器一开始基本上没有任何保障，然后类似 Storm 的系统增加了至少一次的保证。但显然至少一次依然不够好。然后我们看到了恰好一次的语义，这是一个大的进步，但这只是对于单行操作的恰好一次语义，这与键值库很类似。而支持多行恰好一次或者多行事务操作将流处理器提升到了一个可以解决传统意义上关系型数据库所应用场景的阶段。 Streaming Ledger 的实现方式是允许用户定义一些表和对这些表进行修改的函数。 Streaming Ledger 会运行这些函数和表，所有的这些一起编译成一个 Apache Flink 的有向无环图（DAG）。Streaming Ledger 会注入所有事务时间分配的逻辑，以此来保证所有事务的一致性。 搭建这样一个库并不难，难的是让它高性能的运行。让我们来看看它的性能。这些性能测试是几个月之前的，我们并没有做什么特别的优化，我们只是想看看一些最简单的方法能够有什么样的性能表现。而实际性能表现看起来相当不错。如果你看这些性能条形成的阶梯跨度，随着流处理器数量的增长，性能的增长相当线性。 在事务设计中，没有任何协同或者锁参与其中。这只是流处理，将事件流推入系统，缓存一小段时间来做一些乱序处理，然后做一些本地状态更新。在这个方案中，没有什么特别代价高昂的操作。在图中性能增长似乎超过了线性，我想这主要是因为 JAVA 的 JVM 当中 GC 的工作原因导致的。在 32 个节点的情况下我们每秒可以处理大约两百万个事务。为了与数据库性能测试进行对比，通常当你看数据库的性能测试时，你会看到类似读写操作比的说明，比如 10% 的更新操作。而我们的测试使用的是 100% 的更新操作，而每个写操作至少更新在不同分区上的 4 行数据，我们的表的大小大约是两亿行。即便没有任何优化，这个方案的性能也非常不错。 另一个在事务性能中有趣的问题是当更新的操作对象是一个比较小的集合时的性能。如果事务之间没有冲突，并发的事务处理是一个容易的事情。如果所有的事务都独立进行而互不干扰，那这个不是什么难题，任何系统应该都能很好的解决这样的问题。 当所有的事务都开始操作同一些行时，事情开始变得更有趣了，你需要隔离不同的修改来保证一致性。所以我们开始比较一个只读的程序、一个又读又写但是没有写冲突的程序和一个又读又写并有中等程度写冲突的程序这三者之间的性能。你可以看到性能表现相当稳定。这就像是一个乐观的并发冲突控制，表现很不错。那如果我们真的想要针对这类系统的阿喀琉斯之踵进行考验，也就是反复的更新同一个小集合中的键。 在传统数据库中，这种情况下可能会出现反复重试，反复失败再重试，这是一种我们总想避免的糟糕情况。是的，我们的确需要付出性能代价，这很自然，因为如果你的表中有几行数据每个人都想更新，那么你的系统就失去了并发性，这本身就是个问题。但是这种情况下，系统并没崩溃，它仍然在稳定的处理请求，虽然失去了一些并发性，但是请求依然能够被处理。这是因为我们没有冲突重试的机制，你可以认为我们有一个基于乱序处理天然的冲突避免的机制，这是一种非常稳定和强大的技术。 我们还尝试了在跨地域分布的情况下的性能表现。比如我们在美国、巴西，欧洲，日本和澳大利亚各设置了一个 Flink 集群。也就是说我们有个全球分布的系统。如果你在使用一个关系型数据库，那么你会付出相当高昂的性能代价，因为通信的延迟变得相当高。跨大洲的信息交互比在同一个数据中心甚至同一个机架上的信息交互要产生大得多的延迟。 但是有趣的是，流处理的方式对延迟并不是十分敏感，延迟对性能有所影响，但是相比其它很多方案，延迟对流处理的影响要小得多。所以，在这样的全球分布式环境中执行分布式程序，的确会有更差的性能，部分原因也是因为跨大洲的通信带宽不如统一数据中心里的带宽，但是性能表现依然不差。 实际上，你可以拿它当做一个跨地域的数据库，同时仍然能够在一个大概 10 个节点的集群上获得每秒几十万条事务的处理能力。在这个测试中我们只用了 10 个节点，每个大洲两个节点。所以 10 个节点可以带来全球分布的每秒 20 万事务的处理能力。我认为这是很有趣的结果，这是因为这个方案对延迟并不敏感。 我已经说了很多利用流处理来实现事务性的应用。可能听起来这是个很自然的想法，从某种角度上来说的确是这样。但是它的确需要一些很复杂的机制来作为支撑。它需要一个连续处理而非微批处理的能力，需要能够做迭代，需要复杂的基于事件时间处理乱序处理。为了更好地性能，它需要灵活的状态抽象和异步 checkpoint 机制。这些是真正困难的事情。这些不是由 Ledger Streaming 库实现的，而是 Apache Flink 实现的，所以即使对这类事务性的应用而言，Apache Flink 也是真正的中流砥柱。 至此，我们可以说流处理不仅仅支持连续处理、流式分析、批处理或者事件驱动的处理，你也可以用它做事务性的处理。当然，前提是你有一个足够强大的流处理引擎。这就是我演讲的全部内容。 最后GitHub Flink 学习代码地址：https://github.com/zhisheng17/flink-learning 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ Flink 实战1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍Flink中的Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 写入数据到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 写入数据到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了? 19、Flink 从0到1学习 —— Flink 中如何管理配置？ Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink 源码解析 —— 如何获取 ExecutionGraph ？","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 架构、原理与部署测试","date":"2019-06-13T16:00:00.000Z","path":"2019/06/14/flink-architecture-deploy-test/","text":"Apache Flink 是一个面向分布式数据流处理和批量数据处理的开源计算平台，它能够基于同一个 Flink 运行时，提供支持流处理和批处理两种类型应用的功能。 本文转载自博客园，作者：Florian 原文地址：https://www.cnblogs.com/fanzhidongyzby/p/6297723.html 现有的开源计算方案，会把流处理和批处理作为两种不同的应用类型，因为它们所提供的 SLA（Service-Level-Aggreement）是完全不相同的：流处理一般需要支持低延迟、Exactly-once 保证，而批处理需要支持高吞吐、高效处理。 Flink 从另一个视角看待流处理和批处理，将二者统一起来：Flink 是完全支持流处理，也就是说作为流处理看待时输入数据流是无界的；批处理被作为一种特殊的流处理，只是它的输入数据流被定义为有界的。 Flink 流处理特性： 支持高吞吐、低延迟、高性能的流处理 支持带有事件时间的窗口（Window）操作 支持有状态计算的 Exactly-once 语义 支持高度灵活的窗口（Window）操作，支持基于 time、count、session，以及 data-driven 的窗口操作 支持具有 Backpressure 功能的持续流模型 支持基于轻量级分布式快照（Snapshot）实现的容错 一个运行时同时支持 Batch on Streaming 处理和 Streaming 处理 Flink 在 JVM 内部实现了自己的内存管理 支持迭代计算 支持程序自动优化：避免特定情况下 Shuffle、排序等昂贵操作，中间结果有必要进行缓存 一、架构Flink 以层级式系统形式组件其软件栈，不同层的栈建立在其下层基础上，并且各层接受程序不同层的抽象形式。 1、运行时层以 JobGraph 形式接收程序。JobGraph 即为一个一般化的并行数据流图（data flow），它拥有任意数量的 Task 来接收和产生 data stream。 2、DataStream API 和 DataSet API 都会使用单独编译的处理方式生成 JobGraph。DataSet API 使用optimizer 来决定针对程序的优化方法，而 DataStream API 则使用 stream builder 来完成该任务。 3、在执行 JobGraph 时，Flink 提供了多种候选部署方案（如 local，remote，YARN 等）。 4、Flink 附随了一些产生 DataSet 或 DataStream API 程序的的类库和 API：处理逻辑表查询的 Table，机器学习的 FlinkML，图像处理的 Gelly，复杂事件处理的 CEP。 二、原理1. 流、转换、操作符Flink 程序是由 Stream 和 Transformation 这两个基本构建块组成，其中 Stream 是一个中间结果数据，而Transformation 是一个操作，它对一个或多个输入 Stream 进行计算处理，输出一个或多个结果 Stream。 Flink 程序被执行的时候，它会被映射为 Streaming Dataflow。一个 Streaming Dataflow 是由一组 Stream 和Transformation Operator 组成，它类似于一个 DAG 图，在启动的时候从一个或多个 Source Operator 开始，结束于一个或多个 Sink Operator。 2. 并行数据流一个 Stream 可以被分成多个 Stream 分区（Stream Partitions），一个 Operator 可以被分成多个 Operator Subtask，每一个 Operator Subtask 是在不同的线程中独立执行的。一个 Operator 的并行度，等于 Operator Subtask 的个数，一个 Stream 的并行度总是等于生成它的 Operator 的并行度。 One-to-one 模式 比如从 Source[1] 到 map()[1]，它保持了 Source 的分区特性（Partitioning）和分区内元素处理的有序性，也就是说 map()[1] 的 Subtask 看到数据流中记录的顺序，与 Source[1] 中看到的记录顺序是一致的。 Redistribution模式 这种模式改变了输入数据流的分区，比如从 map()[1]、map()[2] 到 keyBy()/window()/apply()[1]、keyBy()/window()/apply()[2]，上游的 Subtask 向下游的多个不同的 Subtask 发送数据，改变了数据流的分区，这与实际应用所选择的 Operator 有关系。 3. 任务、操作符链Flink 分布式执行环境中，会将多个 Operator Subtask 串起来组成一个 Operator Chain，实际上就是一个执行链，每个执行链会在 TaskManager 上一个独立的线程中执行。 4. 时间处理 Stream 中的记录时，记录中通常会包含各种典型的时间字段： Event Time：表示事件创建时间 Ingestion Time：表示事件进入到 Flink Dataflow 的时间 Processing Time：表示某个 Operator 对事件进行处理的本地系统时间 Flink 使用 WaterMark 衡量时间的时间，WaterMark 携带时间戳 t，并被插入到 stream 中。 WaterMark 的含义是所有时间 t’&lt; t 的事件都已经发生。 针对乱序的的流，WaterMark 至关重要，这样可以允许一些事件到达延迟，而不至于过于影响 window 窗口的计算。 并行数据流中，当 Operator 有多个输入流时，Operator 的 event time 以最小流 event time 为准。 5. 窗口Flink 支持基于时间窗口操作，也支持基于数据的窗口操作： 窗口分类： 按分割标准划分：timeWindow、countWindow 按窗口行为划分：Tumbling Window、Sliding Window、自定义窗口 Tumbling/Sliding Time Window 12345678910111213141516// Stream of (sensorId, carCnt)val vehicleCnts: DataStream[(Int, Int)] = ...val tumblingCnts: DataStream[(Int, Int)] = vehicleCnts // key stream by sensorId .keyBy(0) // tumbling time window of 1 minute length .timeWindow(Time.minutes(1)) // compute sum over carCnt .sum(1) val slidingCnts: DataStream[(Int, Int)] = vehicleCnts .keyBy(0) // sliding time window of 1 minute length and 30 secs trigger interval .timeWindow(Time.minutes(1), Time.seconds(30)) .sum(1) Tumbling/Sliding Count Window 12345678910111213141516// Stream of (sensorId, carCnt) val vehicleCnts: DataStream[(Int, Int)] = ... val tumblingCnts: DataStream[(Int, Int)] = vehicleCnts // key stream by sensorId .keyBy(0) // tumbling count window of 100 elements size .countWindow(100) // compute the carCnt sum .sum(1)val slidingCnts: DataStream[(Int, Int)] = vehicleCnts .keyBy(0) // sliding count window of 100 elements size and 10 elements trigger interval .countWindow(100, 10) .sum(1) 自定义窗口 基本操作： window：创建自定义窗口 trigger：自定义触发器 evictor：自定义evictor apply：自定义window function 6. 容错Barrier 机制： 出现一个 Barrier，在该 Barrier 之前出现的记录都属于该 Barrier 对应的 Snapshot，在该 Barrier 之后出现的记录属于下一个 Snapshot。 来自不同Snapshot多个Barrier可能同时出现在数据流中，也就是说同一个时刻可能并发生成多个Snapshot。 当一个中间（Intermediate）Operator接收到一个Barrier后，它会发送Barrier到属于该Barrier的Snapshot的数据流中，等到Sink Operator接收到该Barrier后会向Checkpoint Coordinator确认该Snapshot，直到所有的Sink Operator都确认了该Snapshot，才被认为完成了该Snapshot。 对齐： 当Operator接收到多个输入的数据流时，需要在Snapshot Barrier中对数据流进行排列对齐： Operator从一个incoming Stream接收到Snapshot Barrier n，然后暂停处理，直到其它的incoming Stream的Barrier n（否则属于2个Snapshot的记录就混在一起了）到达该Operator 接收到Barrier n的Stream被临时搁置，来自这些Stream的记录不会被处理，而是被放在一个Buffer中。 一旦最后一个Stream接收到Barrier n，Operator会emit所有暂存在Buffer中的记录，然后向Checkpoint Coordinator发送Snapshot n。 继续处理来自多个Stream的记录 基于Stream Aligning操作能够实现Exactly Once语义，但是也会给流处理应用带来延迟，因为为了排列对齐Barrier，会暂时缓存一部分Stream的记录到Buffer中，尤其是在数据流并行度很高的场景下可能更加明显，通常以最迟对齐Barrier的一个Stream为处理Buffer中缓存记录的时刻点。在Flink中，提供了一个开关，选择是否使用Stream Aligning，如果关掉则Exactly Once会变成At least once。 CheckPoint： Snapshot并不仅仅是对数据流做了一个状态的Checkpoint，它也包含了一个Operator内部所持有的状态，这样才能够在保证在流处理系统失败时能够正确地恢复数据流处理。状态包含两种： 系统状态：一个Operator进行计算处理的时候需要对数据进行缓冲，所以数据缓冲区的状态是与Operator相关联的。以窗口操作的缓冲区为例，Flink系统会收集或聚合记录数据并放到缓冲区中，直到该缓冲区中的数据被处理完成。 一种是用户自定义状态（状态可以通过转换函数进行创建和修改），它可以是函数中的Java对象这样的简单变量，也可以是与函数相关的Key/Value状态。 7. 调度在JobManager端，会接收到Client提交的JobGraph形式的Flink Job，JobManager会将一个JobGraph转换映射为一个ExecutionGraph，ExecutionGraph是JobGraph的并行表示，也就是实际JobManager调度一个Job在TaskManager上运行的逻辑视图。 物理上进行调度，基于资源的分配与使用的一个例子： 左上子图：有2个TaskManager，每个TaskManager有3个Task Slot 左下子图：一个Flink Job，逻辑上包含了1个data source、1个MapFunction、1个ReduceFunction，对应一个JobGraph 左下子图：用户提交的Flink Job对各个Operator进行的配置——data source的并行度设置为4，MapFunction的并行度也为4，ReduceFunction的并行度为3，在JobManager端对应于ExecutionGraph 右上子图：TaskManager 1上，有2个并行的ExecutionVertex组成的DAG图，它们各占用一个Task Slot 右下子图：TaskManager 2上，也有2个并行的ExecutionVertex组成的DAG图，它们也各占用一个Task Slot 在2个TaskManager上运行的4个Execution是并行执行的 8. 迭代机器学习和图计算应用，都会使用到迭代计算，Flink通过在迭代Operator中定义Step函数来实现迭代算法，这种迭代算法包括Iterate和Delta Iterate两种类型。 Iterate Iterate Operator是一种简单的迭代形式：每一轮迭代，Step函数的输入或者是输入的整个数据集，或者是上一轮迭代的结果，通过该轮迭代计算出下一轮计算所需要的输入（也称为Next Partial Solution），满足迭代的终止条件后，会输出最终迭代结果。 流程伪代码： 1234567IterationState state = getInitialState();while (!terminationCriterion()) &#123; state = step(state);&#125;setFinalState(state); Delta Iterate Delta Iterate Operator实现了增量迭代。 流程伪代码： 12345678910IterationState workset = getInitialState();IterationState solution = getInitialSolution();while (!terminationCriterion()) &#123; (delta, workset) = step(workset, solution); solution.update(delta)&#125;setFinalState(solution); 最小值传播： 9. Back Pressure监控流处理系统中，当下游Operator处理速度跟不上的情况，如果下游Operator能够将自己处理状态传播给上游Operator，使得上游Operator处理速度慢下来就会缓解上述问题，比如通过告警的方式通知现有流处理系统存在的问题。 Flink Web界面上提供了对运行Job的Backpressure行为的监控，它通过使用Sampling线程对正在运行的Task进行堆栈跟踪采样来实现。 默认情况下，JobManager会每间隔50ms触发对一个Job的每个Task依次进行100次堆栈跟踪调用，过计算得到一个比值，例如，radio=0.01，表示100次中仅有1次方法调用阻塞。Flink目前定义了如下Backpressure状态：OK: 0 &lt;= Ratio &lt;= 0.10LOW: 0.10 &lt; Ratio &lt;= 0.5HIGH: 0.5 &lt; Ratio &lt;= 1 三、库1. TableFlink的Table API实现了使用类SQL进行流和批处理。 详情参考：https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/table_api.html 2. CEPFlink的CEP（Complex Event Processing）支持在流中发现复杂的事件模式，快速筛选用户感兴趣的数据。 详情参考：https://ci.apache.org/projects/flink/flink-docs-release-1.2/concepts/programming-model.html#next-steps 3. GellyGelly是Flink提供的图计算API，提供了简化开发和构建图计算分析应用的接口。 详情参考：https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/libs/gelly/index.html 4. FlinkMLFlinkML是Flink提供的机器学习库，提供了可扩展的机器学习算法、简洁的API和工具简化机器学习系统的开发。 详情参考：https://ci.apache.org/projects/flink/flink-docs-release-1.2/dev/libs/ml/index.html 四、部署当Flink系统启动时，首先启动JobManager和一至多个TaskManager。JobManager负责协调Flink系统，TaskManager则是执行并行程序的worker。当系统以本地形式启动时，一个JobManager和一个TaskManager会启动在同一个JVM中。当一个程序被提交后，系统会创建一个Client来进行预处理，将程序转变成一个并行数据流的形式，交给JobManager和TaskManager执行。 1. 启动测试编译flink，本地启动。 12345678$ java -versionjava version \"1.8.0_111\"$ git clone https://github.com/apache/flink.git$ git checkout release-1.1.4 -b release-1.1.4$ cd flink$ mvn clean package -DskipTests$ cd flink-dist/target/flink-1.1.4-bin/flink-1.1.4$ ./bin/start-local.sh 编写本地流处理demo。 SocketWindowWordCount.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091public class SocketWindowWordCount &#123; public static void main(String[] args) throws Exception &#123; // the port to connect to final int port; try &#123; final ParameterTool params = ParameterTool.fromArgs(args); port = params.getInt(\"port\"); &#125; catch (Exception e) &#123; System.err.println(\"No port specified. Please run 'SocketWindowWordCount --port &lt;port&gt;'\"); return; &#125; // get the execution environment final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // get input data by connecting to the socket DataStream&lt;String&gt; text = env.socketTextStream(\"localhost\", port, \"\\n\"); // parse the data, group it, window it, and aggregate the counts DataStream&lt;WordWithCount&gt; windowCounts = text .flatMap(new FlatMapFunction&lt;String, WordWithCount&gt;() &#123; public void flatMap(String value, Collector&lt;WordWithCount&gt; out) &#123; for (String word : value.split(\"\\s\")) &#123; out.collect(new WordWithCount(word, 1L)); &#125; &#125; &#125;) .keyBy(\"word\") .timeWindow(Time.seconds(5), Time.seconds(1)) .reduce(new ReduceFunction&lt;WordWithCount&gt;() &#123; public WordWithCount reduce(WordWithCount a, WordWithCount b) &#123; return new WordWithCount(a.word, a.count + b.count); &#125; &#125;); // print the results with a single thread, rather than in parallel windowCounts.print().setParallelism(1); env.execute(\"Socket Window WordCount\"); &#125; // Data type for words with count public static class WordWithCount &#123; public String word; public long count; public WordWithCount() &#123;&#125; public WordWithCount(String word, long count) &#123; this.word = word; this.count = count; &#125; @Override public String toString() &#123; return word + \" : \" + count; &#125; &#125;&#125; pom.xml 1234567891011121314151617&lt;!-- Use this dependency if you are using the DataStream API --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_2.10&lt;/artifactId&gt; &lt;version&gt;1.1.4&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Use this dependency if you are using the DataSet API --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;1.1.4&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-clients_2.10&lt;/artifactId&gt; &lt;version&gt;1.1.4&lt;/version&gt;&lt;/dependency&gt; 执行mvn构建。 12mvn clean install$ ls target/flink-demo-1.0-SNAPSHOT.jar 开启9000端口，用于输入数据: 1$ nc -l 9000 提交flink任务： 1$ ./bin/flink run -c com.demo.florian.WordCount $DEMO_DIR/target/flink-demo-1.0-SNAPSHOT.jar --port 9000 在nc里输入数据后，查看执行结果： 1$ tail -f log/flink-*-jobmanager-*.out 查看flink web页面：localhost:8081 2. 代码结构Flink系统核心可分为多个子项目。分割项目旨在减少开发Flink程序需要的依赖数量，并对测试和开发小组件提供便捷。 Flink当前还包括以下子项目： Flink-dist：distribution项目。它定义了如何将编译后的代码、脚本和其他资源整合到最终可用的目录结构中。 Flink-quick-start：有关quickstart和教程的脚本、maven原型和示例程序 flink-contrib：一系列有用户开发的早起版本和有用的工具的项目。后期的代码主要由外部贡献者继续维护，被flink-contirb接受的代码的要求低于其他项目的要求。 3. Flink On YARNFlink在YARN集群上运行时：Flink YARN Client负责与YARN RM通信协商资源请求，Flink JobManager和Flink TaskManager分别申请到Container去运行各自的进程。 YARN AM与Flink JobManager在同一个Container中，这样AM可以知道Flink JobManager的地址，从而AM可以申请Container去启动Flink TaskManager。待Flink成功运行在YARN集群上，Flink YARN Client就可以提交Flink Job到Flink JobManager，并进行后续的映射、调度和计算处理。 1、设置Hadoop环境变量 1$ export HADOOP_CONF_DIR=/etc/hadoop/conf 2、以集群模式提交任务，每次都会新建flink集群 1$ ./bin/flink run -m yarn-cluster -c com.demo.florian.WordCount $DEMO_DIR/target/flink-demo-1.0-SNAPSHOT.jar 3、启动共享flink集群，提交任务 12$ ./bin/yarn-session.sh -n 4 -jm 1024 -tm 4096 -d$ ./bin/flink run -c com.demo.florian.WordCount $DEMO_DIR/target/flink-demo-1.0.SNAPSHOT.jar ###参考资料 http://shiyanjun.cn/archives/1508.htmlhttps://ci.apache.org/projects/flink/flink-docs-release-1.2/index.html 最后GitHub Flink 学习代码地址：https://github.com/zhisheng17/flink-learning 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文","date":"2019-06-12T16:00:00.000Z","path":"2019/06/13/flink-book-paper/","text":"前言之前也分享了不少自己的文章，但是对于 Flink 来说，还是有不少新入门的朋友，这里给大家分享点 Flink 相关的资料（国外数据 pdf 和流处理相关的 Paper），期望可以帮你更好的理解 Flink。 书籍1、《Introduction to Apache Flink book》 这本书比较薄，简单介绍了 Flink，也有中文版，读完可以对 Flink 有个大概的了解。 2、《Learning Apache Flink》 这本书还是讲的比较多的 API 使用，不仅有 Java 版本还有 Scala 版本，入门看这本我觉得还是 OK 的。 3、《Stream Processing with Apache Flink》 这本书是 Flink PMC 写的，质量还是很好的，对 Flink 中的概念讲的很清楚，还有不少图片帮忙理解，美中不足的是没有 Table 和 SQL API 相关的介绍。 4、《Streaming System》 这本书是讲流处理引擎的，对流处理引擎的发展带来不少的推动，书本的质量非常高，配了大量的图，目的就是让你很容易的懂流处理引擎中的概念（比如时间、窗口、水印等），我强烈的推荐大家都看一下，这本书的内容被很多博客和书籍都引用了。 Paper这是一份 streaming systems 领域相关的论文列表 20+ 篇，涉及 streaming systems 的设计，实现，故障恢复，弹性扩展等各方面。也包含自 2014 年以来 streaming system 和 batch system 的统一模型的论文。 2016 年 Drizzle: Fast and Adaptable Stream Processing at Scale (Draft): Record-at-a-time 的系统，如 Naiad, Flink，处理延迟较低、但恢复延迟较高；micro-batch 系统，如 Spark Streaming，恢复延迟低但处理延迟略高。Drizzle 则采用 group scheduling + pre-scheduling shuffles 的方式对 Spark Streaming 做了改进，保留低恢复延迟的同时，降低了处理延迟至 100ms 量级。 Realtime Data Processing at Facebook (SIGMOD): Facebook 明确自己实时的使用场景是 seconds of latency, not milliseconds，并基于自己的需求构建了 3 个实时处理组件：Puma, Swift, 以及 Stylus。Puma, Swift 和 Stylus 都从 Scribe 读数据，并可向 Scribe 写回数据（Scribe 是 Facebook 内部的分布式消息系统，类似 Kafka）。 2015 年 The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing (VLDB): 来自 Google 的将 stream processing 模型和 batch processing 模型统一的尝试。在 Dataflow model 下，底层依赖 FlumeJava 支持 batch processing，依赖 MillWheel 支持 stream processing。Dataflow model 的开源实现是 Apache Beam 项目。 Apache Flink: Stream and Batch Processing in a Single Engine Apache Flink 是一个处理 streaming data 和 batch data 的开源系统。Flink 的设计哲学是，包括实时分析 (real-time analytics)、持续数据处理 (continuous data pipelines)、历史数据处理 (historic data processing / batch)、迭代式算法 (iterative algorithms - machine learning, graph analysis) 等的很多类数据处理应用，都能用 pipelined fault-tolerant 的 dataflows 执行模型来表达。 Lightweight asynchronous snapshots for distributed dataflows: Apache Flink 所实现的一个轻量级的、异步做状态快照的方法。基于此，Flink 得以保证分布式状态的一致性，从而保证整个系统的 exactly-once 语义。具体的，Flink 会持续性的在 stream 里插入 barrier markers，制造一个分布式的顺序关系，使得不同的节点能够在同一批 barrier marker 上达成整个系统的一致性状态。 Twitter Heron: Stream Processing at Scale (SIGMOD): Heron 是 Twitter 开发的用于代替 Storm 的实时处理系统，解决了 Storm 在扩展性、调试能力、性能、管理方式上的一些问题。Heron 实现了 Storm 的接口，因此对 Storm 有很好的兼容性，也成为了 Twitter 内部实时处理系统的事实上的标准。 2014 年 Trill: A High-Performance Incremental Query Processor for Diverse Analytics (VLDB): 此篇介绍了 Microsoft 的 Trill - 一个新的分析查询处理器。Trill 很好的结合以下 3 方面需求：(1) Query Model: Trill 是基于时间-关系 (tempo-relational) 模型，所以很好的支持从实时到离线计算的延迟需求；(2) Fabric and Language Integration: Trill 作为一个类库，可以很好的与高级语言、已有类库结合；以及 (3) Performance: 无论实时还是离线，Trill 的 throughput 都很高 —— 实时计算比流处理引擎高 2-4 个数量级，离线计算与商业的列式 DBMS 同等。从实现角度讲，包括 punctuation 的使用来分 batch 满足 latency 需求，batch 内使用列式存储、code-gen 等技术来提高 performance，都具有很好的借鉴意义 —— 尤其注意这是 2014 年发表的论文。 Summingbird: A Framework for Integrating Batch and Online MapReduce Computations (VLDB): Twitter 开发的目标是将 online Storm 计算和 batch MapReduce 计算逻辑统一描述的一套 domain-specific language。Summingbird 抽象了 sources, sinks, 以及 stores 等，基于此抽象，上层应用就不必为 streaming 和 batch 维护两套计算逻辑，而可以使用同一套计算逻辑，只在运行时分别编译后跑在 streaming 的 Storm 上和 batch 的 MapReduce 上。 Storm@Twitter (SIGMOD): 这是一篇来迟的论文。Apache Storm 最初在 Backtype 及 Twitter，而后在业界范围都有广泛的应用，甚至曾经一度也是事实上的流处理系统标准。此篇介绍了 Storm 的设计，及在 Twitter 内部的应用情况。当然后面我们知道 Apache Storm 也暴露出一些问题，业界也出现了一些更优秀的流处理系统。Twitter 虽没有在 2012 年 Storm 时代开启时发声，但在 2014 年 Storm 落幕时以此文发声向其致敬，也算是弥补了些许遗憾吧。 2013 年 Discretized Streams: Fault-Tolerant Streaming Computation at Scale (SOSP): Spark Streaming 是基于 Spark 执行引擎、micro-batch 模式的准实时处理系统。对比 RDD 是 Spark 引擎的数据抽象，DStream (Discretized Stream) 则是 Spark Streaming 引擎的数据抽象。DStream 像 RDD 一样，具有分布式、可故障恢复的特点，并且能够充分利用 Spark 引擎的推测执行，应对 straggler 的出现。 MillWheel: Fault-Tolerant Stream Processing at Internet Scale (VLDB): MillWheel 是 Google 内部研发的实时流数据处理系统，具有分布式、低延迟、高可用、支持 exactly-once 语义的特点。不出意外，MillWheel 是 Google 强大 infra structure 和强大 engeering 能力的综合体现 —— 利用 Bigtable/Spanner 作为后备状态存储、保证 exactly-once 特性等等。另外，MillWheel 将 watermark 机制发扬光大，对 event time 有着非常好的支持。推荐对 streaming system 感兴趣的朋友一定多读几遍此篇论文 —— 虽然此篇已经发表了几年，但工业界开源的系统尚未完全达到 MillWheel 的水平。 Integrating Scale Out and Fault Tolerance in Stream Processing using Operator State Management (SIGMOD): 针对有状态的算子的状态，此篇的基本洞察是，scale out 和 fault tolerance 其实很相通，应该结合到一起考虑和实现，而不是将其割裂开来。文章提出了算子的 3 类状态：(a) processing state, (b) buffer state, 和 (c) routing state，并提出了算子状态的 4 个操作原语：(1) checkpoint state, (2) backup state, (3) restore state, (4) partition state。 2010 年 S4: Distributed Stream Computing Platform (ICDMW): 2010 年算是 general stream processing engine 元年 —— Yahoo! 研发并发布了 S4, Backtype 开始研发了 Storm 并将在 1 年后（由 Twitter）将其开源。S4 和 Storm 都是 general-purpose 的 stream processing engine，允许用户通过代码自定义计算逻辑，而不是仅仅是使用声明式的语言或算子。 2008 年 Out-of-Order Processing: A New Architecture for HighPerformance Stream System (VLDB): 这篇文章提出了一种新的处理模型，即 out-of-order processing (OOP)，取消了以往 streaming system 里对事件有序的假设。重要的是，这篇文章提出了并实现了 low watermark: lwm(n, S, A) is the smallest value for A that occurs after prefix Sn of stream S。我们看到，在 2 年后 Google 开始研发的 MillWheel 里，watermark 将被发扬光大。 Fast and Highly-Available Stream Processing over Wide Area Networks (ICDE): 针对广域网 (wide area networks) 的 stream processing 设计的快速、高可用方案。主要思想是依靠 replication。 2007 年 A Cooperative, Self-Configuring High-Availability Solution for Stream Processing (ICDE): 与 2005 年 ICDE 的文章一样，此篇也讨论 stream processing 的高可用问题。与 2005 年文章做法不同的是，此篇的 checkpointing 方法更细粒度一些，所以一个节点上的不同状态能够备份到不同的节点上去，因而在恢复的时候能够并行恢复以提高速度。 2005 年 The 8 Requirements of Real-Time Stream Processing (SIGMOD): 图领奖得主 Michael Stonebraker 老爷子与他在 StreamBase 的小伙伴们勾画的 stream processing applications 应当满足的 8 条规则，如 Rule 1: Keep the Data Moving, Rule 2: Query using SQL on Streams (StreamSQL), Rule 3: Handle Stream Imperfections (Delayed, Missing and Out-of-Order Data) … 等等。虽然此篇有引导舆论的嫌疑 —— 不知是先有了这流 8 条、再有了 StreamBase，还是先有了 StreamBase、再有了这流 8 条 —— 但其内容还是有相当的借鉴意义。 The Design of the Borealis Stream Processing Engine (CIDR): Borealis 是 Aurora 的分布式、更优化版本的续作。Borealis 提出并解决了 3 个新一代系统的基础问题：(1) dynamic revision of query results, (2) dynamic query modification, 以及 (3) flexible and highly-scalable optimization. 此篇讲解了 Borealis 的设计与实现 —— p.s. 下，Aurora 及续作 Borealis 的命名还真是非常讲究，是学院派的风格 :-D High-availability algorithms for distributed stream processing (ICDE): 此篇主要聚焦在 streaming system 的高可用性，即故障恢复。文章提出了 3 种 recovery types: (a) precise, (b) gap, 和 (c) rollback，并通过 (1) passive standby, (2) upstream backup, (3) active standby 的方式进行 recover。可与 2007 年 ICDE 的文章对比阅读。 2004 年 STREAM: The Stanford Data Stream Management System (Technique Report): 这篇 technique report 定义了一种 Continuous Query Language (CQL)，讲解了 Query Plans 和 Execution，讨论了一些 Performance Issues。系统也注意到并讨论了 Adaptivity 和 Approximation 的问题。从这篇 technique report 可以看出，这时的流式计算，更多是传统 RDBMS 的思路，扩展到了处理实时流式数据；这大约也是 2010 以前的 stream processing 相关研究的缩影。 2002 年 Monitoring Streams – A New Class of Data Management Applications (VLDB): 大约在 2002 年前后，从实时数据监控（如监控 sensors 数据等）应用出发，大家已经开始区分传统的查询主动、数据被动 (Human-Active, DBMS-Passive) 模式和新兴的数据主动、查询被动 (DBMS-Active, Human-Passive) 模式的区别 —— 此篇即是其中的典型代表。此篇提出了新式的 DBMS 的 Aurora，描述了其基本系统模型、面向流式数据的操作算子集、 优化策略、及实时应用。 Exploiting Punctuation Semantics in Continuous Data Streams (TKDE): 此篇很早的注意到了一些传统的操作算子不能用于无尽的数据流入的场景，因为将导致无尽的状态（考虑 outer join），或者无尽的阻塞（考虑 count 或 max）等。此篇提出，如果在 stream 里加入一些特殊的 punctuation，来标识一段一段的数据，那么我们就可以把无限的 stream 划分为多个有限的数据集的集合，从而使得之前提到的算子变得可用。此篇的价值更多体现在给了 2008 年 watermark 相关的文章以基础，乃至集大成在了 2010 年 Google MillWheel 中。 总结本文分享了四本 Flink 相关的书籍和一份 streaming systems 领域相关的论文列表 20+ 篇，涉及 streaming systems 的设计，实现，故障恢复，弹性扩展等各方面。 如何获取呢？你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ 另外你如果感兴趣的话，也可以关注我的公众号。 本篇文章连接是：http://www.54tianzhisheng.cn/2019/06/13/flink-book-paper/ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从0到1学习—— Flink 不可以连续 Split(分流)？","date":"2019-06-11T16:00:00.000Z","path":"2019/06/12/flink-split/","text":"前言今天上午被 Flink 的一个算子困惑了下，具体问题是什么呢？ 我有这么个需求：有不同种类型的告警数据流(包含恢复数据)，然后我要将这些数据流做一个拆分，拆分后的话，每种告警里面的数据又想将告警数据和恢复数据拆分出来。 结果，这个需求用 Flink 的 Split 运算符出现了问题。 分析需求如下图所示： 我是期望如上这样将数据流进行拆分的，最后将每种告警和恢复用不同的消息模版做一个渲染，渲染后再通过各种其他的方式（钉钉群邮件、短信）进行告警通知。 于是我的代码大概的结构如下代码所示： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879//dataStream 是总的数据流//split 是拆分后的数据流SplitStream&lt;AlertEvent&gt; split = dataStream.split(new OutputSelector&lt;AlertEvent&gt;() &#123; @Override public Iterable&lt;String&gt; select(AlertEvent value) &#123; List&lt;String&gt; tags = new ArrayList&lt;&gt;(); switch (value.getType()) &#123; case MIDDLEWARE: tags.add(MIDDLEWARE); break; case HEALTH_CHECK: tags.add(HEALTH_CHECK); break; case DOCKER: tags.add(DOCKER); break; //... //当然这里还可以很多种类型 &#125; return tags; &#125;&#125;);//然后你想获取每种不同的数据类型，你可以使用 selectDataStream&lt;AlertEvent&gt; middleware = split.select(MIDDLEWARE); //选出中间件的数据流//然后你又要将中间件的数据流分流成告警和恢复SplitStream&lt;AlertEvent&gt; middlewareSplit = middleware.split(new OutputSelector&lt;AlertEvent&gt;() &#123; @Override public Iterable&lt;String&gt; select(AlertEvent value) &#123; List&lt;String&gt; tags = new ArrayList&lt;&gt;(); if(value.isRecover()) &#123; tags.add(RECOVER) &#125; else &#123; tags.add(ALERT) &#125; return tags; &#125;&#125;);middlewareSplit.select(ALERT).print(); DataStream&lt;AlertEvent&gt; healthCheck = split.select(HEALTH_CHECK); //选出健康检查的数据流//然后你又要将健康检查的数据流分流成告警和恢复SplitStream&lt;AlertEvent&gt; healthCheckSplit = healthCheck.split(new OutputSelector&lt;AlertEvent&gt;() &#123; @Override public Iterable&lt;String&gt; select(AlertEvent value) &#123; List&lt;String&gt; tags = new ArrayList&lt;&gt;(); if(value.isRecover()) &#123; tags.add(RECOVER) &#125; else &#123; tags.add(ALERT) &#125; return tags; &#125;&#125;);healthCheckSplit.select(ALERT).print();DataStream&lt;AlertEvent&gt; docekr = split.select(DOCKER); //选出容器的数据流//然后你又要将容器的数据流分流成告警和恢复SplitStream&lt;AlertEvent&gt; dockerSplit = docekr.split(new OutputSelector&lt;AlertEvent&gt;() &#123; @Override public Iterable&lt;String&gt; select(AlertEvent value) &#123; List&lt;String&gt; tags = new ArrayList&lt;&gt;(); if(value.isRecover()) &#123; tags.add(RECOVER) &#125; else &#123; tags.add(ALERT) &#125; return tags; &#125;&#125;);dockerSplit.select(ALERT).print(); 结构我抽象后大概就长上面这样，然后我先本地测试的时候只把容器的数据那块代码打开了，其他种告警的分流代码注释掉了，一运行，发现竟然容器告警的数据怎么还掺杂着健康检查的数据也一起打印出来了，一开始我以为自己出了啥问题，就再起码运行了三遍 IDEA 才发现结果一直都是这样的。 于是，我只好在第二步分流前将 docekr 数据流打印出来，发现是没什么问题，打印出来的数据都是容器相关的，没有掺杂着其他种的数据啊。这会儿遍陷入了沉思，懵逼发呆了一会。 解决问题于是还是开始面向 Google 编程： 发现第一条就找到答案了，简直不要太快，点进去可以看到他也有这样的需求： 然后这个小伙伴还挣扎了下用不同的方法（虽然结果更惨）： 最后换了个姿势就好了（果然小伙子会的姿势挺多的）： 但从这篇文章中，我找到了关联到的两个 Flink Issue，分别是： 1、https://issues.apache.org/jira/browse/FLINK-5031 2、https://issues.apache.org/jira/browse/FLINK-11084 然后呢，从第二个 Issue 的讨论中我发现了一些很有趣的讨论： 对话很有趣，但是我突然想到之前我的知识星球里面一位很细心的小伙伴问的一个问题了： 可以发现代码上确实是标明了过期了，但是注释里面没写清楚推荐用啥，幸好我看到了这个 Issue，不然脑子里面估计这个问题一直会存着呢。 那么这个问题解决方法是不是意味着就可以利用 Side Outputs 来解决呢？当然可以啦，官方都推荐了，还不能都话，那么不是打脸啪啪啪的响吗？不过这里还是卖个关子将 Side Outputs 后面专门用一篇文章来讲，感兴趣的可以先看看官网介绍：https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/side_output.html 另外其实也可以通过 split + filter 组合来解决这个问题，反正关键就是不要连续的用 split 来分流。 用 split + filter 的方案代码大概如下： 12345678910111213141516171819DataStream&lt;AlertEvent&gt; docekr = split.select(DOCKER); //选出容器的数据流//容器告警的数据流docekr.filter(new FilterFunction&lt;AlertEvent&gt;() &#123; @Override public boolean filter(AlertEvent value) throws Exception &#123; return !value.isRecover(); &#125;&#125;).print(); //容器恢复的数据流 docekr.filter(new FilterFunction&lt;AlertEvent&gt;() &#123; @Override public boolean filter(AlertEvent value) throws Exception &#123; return value.isRecover(); &#125;&#125;).print(); 上面这种就是多次 filter 也可以满足需求，但是就是代码有点啰嗦。 总结Flink 中不支持连续的 Split/Select 分流操作，要实现连续分流也可以通过其他的方式（split + filter 或者 side output）来实现 本篇文章连接是：http://www.54tianzhisheng.cn/2019/06/12/flink-split/ 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程","date":"2019-03-27T16:00:00.000Z","path":"2019/03/28/Flink-code-TaskManager-submitJob/","text":"TaskManager 处理 SubmitJob 的过程 https://t.zsxq.com/eu7mQZj 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从0到1学习 —— Flink 中如何管理配置？","date":"2019-03-27T16:00:00.000Z","path":"2019/03/28/flink-additional-data/","text":"前言如果你了解 Apache Flink 的话，那么你应该熟悉该如何像 Flink 发送数据或者如何从 Flink 获取数据。但是在某些情况下，我们需要将配置数据发送到 Flink 集群并从中接收一些额外的数据。 在本文的第一部分中，我将描述如何将配置数据发送到 Flink 集群。我们需要配置很多东西:方法参数、配置文件、机器学习模型。Flink 提供了几种不同的方法，我们将介绍如何使用它们以及何时使用它们。在本文的第二部分中，我将描述如何从 Flink 集群中获取数据。 如何发送数据给 TaskManager？在我们深入研究如何在 Apache Flink 中的不同组件之间发送数据之前，让我们先谈谈 Flink 集群中的组件，下图展示了 Flink 中的主要组件以及它们是如何相互作用的： 当我们运行 Flink 应用程序时，它会与 Flink JobManager 进行交互，这个 Flink JobManager 存储了那些正在运行的 Job 的详细信息，例如执行图。JobManager 它控制着 TaskManager，每个 TaskManager 中包含了一部分数据来执行我们定义的数据处理方法。 在许多的情况下，我们希望能够去配置 Flink Job 中某些运行的函数参数。根据用例，我们可能需要设置单个变量或者提交具有静态配置的文件，我们下面将讨论在 Flink 中该如何实现？ 除了向 TaskManager 发送配置数据外，有时我们可能还希望从 Flink Job 的函数方法中返回数据。 如何配置用户自定义函数？假设我们有一个从 CSV 文件中读取电影列表的应用程序（它要过滤特定类型的所有电影）: 1234567891011121314//读取电影列表数据集合DataSet&lt;Tuple3&lt;Long, String, String&gt;&gt; lines = env.readCsvFile(\"movies.csv\") .ignoreFirstLine() .parseQuotedStrings('\"') .ignoreInvalidLines() .types(Long.class, String.class, String.class);lines.filter((FilterFunction&lt;Tuple3&lt;Long, String, String&gt;&gt;) movie -&gt; &#123; // 以“|”符号分隔电影类型 String[] genres = movie.f2.split(\"\\\\|\"); // 查找所有 “动作” 类型的电影 return Stream.of(genres).anyMatch(g -&gt; g.equals(\"Action\"));&#125;).print(); 我们很可能想要提取不同类型的电影，为此我们需要能够配置我们的过滤功能。 当你要实现这样的函数时，最直接的配置方法是实现构造函数： 123456789101112131415161718192021// 传递类型名称lines.filter(new FilterGenre(\"Action\")) .print();...class FilterGenre implements FilterFunction&lt;Tuple3&lt;Long, String, String&gt;&gt; &#123; //类型 String genre; //初始化构造方法 public FilterGenre(String genre) &#123; this.genre = genre; &#125; @Override public boolean filter(Tuple3&lt;Long, String, String&gt; movie) throws Exception &#123; String[] genres = movie.f2.split(\"\\\\|\"); return Stream.of(genres).anyMatch(g -&gt; g.equals(genre)); &#125;&#125; 或者，如果你使用 lambda 函数，你可以简单地使用它的闭包中的一个变量: 12345678final String genre = \"Action\";lines.filter((FilterFunction&lt;Tuple3&lt;Long, String, String&gt;&gt;) movie -&gt; &#123; String[] genres = movie.f2.split(\"\\\\|\"); //使用变量 return Stream.of(genres).anyMatch(g -&gt; g.equals(genre));&#125;).print(); Flink 将序列化此变量并将其与函数一起发送到集群。 如果你需要将大量变量传递给函数，那么这些方法就会变得非常烦人了。 为了解决这个问题，Flink 提供了 withParameters 方法。 要使用它，你需要实现那些 Rich 函数，比如你不必实现 MapFunction 接口，而是实现 RichMapFunction。 Rich 函数允许你使用 withParameters 方法传递许多参数： 12345678// Configuration 类来存储参数Configuration configuration = new Configuration();configuration.setString(\"genre\", \"Action\");lines.filter(new FilterGenreWithParameters()) // 将参数传递给函数 .withParameters(configuration) .print(); 要读取这些参数，我们需要实现 “open” 方法并读取其中的参数: 1234567891011121314151617class FilterGenreWithParameters extends RichFilterFunction&lt;Tuple3&lt;Long, String, String&gt;&gt; &#123; String genre; @Override public void open(Configuration parameters) throws Exception &#123; //读取配置 genre = parameters.getString(\"genre\", \"\"); &#125; @Override public boolean filter(Tuple3&lt;Long, String, String&gt; movie) throws Exception &#123; String[] genres = movie.f2.split(\"\\\\|\"); return Stream.of(genres).anyMatch(g -&gt; g.equals(genre)); &#125;&#125; 所有这些选项都可以使用，但如果需要为多个函数设置相同的参数，则可能会很繁琐。在 Flink 中要处理此种情况， 你可以设置所有 TaskManager 都可以访问的全局环境变量。 为此，首先需要使用 ParameterTool.fromArgs 从命令行读取参数： 12345public static void main(String... args) &#123; //读取命令行参数 ParameterTool parameterTool = ParameterTool.fromArgs(args); ...&#125; 然后使用 setGlobalJobParameters 设置全局作业参数: 1234567final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();env.getConfig().setGlobalJobParameters(parameterTool);...//该函数将能够读取这些全局参数lines.filter(new FilterGenreWithGlobalEnv()) //这个函数是自己定义的 .print(); 现在我们来看看这个读取这些参数的函数，和上面说的一样，它是一个 Rich 函数： 12345678910111213class FilterGenreWithGlobalEnv extends RichFilterFunction&lt;Tuple3&lt;Long, String, String&gt;&gt; &#123; @Override public boolean filter(Tuple3&lt;Long, String, String&gt; movie) throws Exception &#123; String[] genres = movie.f2.split(\"\\\\|\"); //获取全局的配置 ParameterTool parameterTool = (ParameterTool) getRuntimeContext().getExecutionConfig().getGlobalJobParameters(); //读取配置 String genre = parameterTool.get(\"genre\"); return Stream.of(genres).anyMatch(g -&gt; g.equals(genre)); &#125;&#125; 要读取配置，我们需要调用 getGlobalJobParameter 来获取所有全局参数，然后使用 get 方法获取我们要的参数。 广播变量如果你想将数据从客户端发送到 TaskManager，上面文章中讨论的方法都适合你，但如果数据以数据集的形式存在于 TaskManager 中，该怎么办？ 在这种情况下，最好使用 Flink 中的另一个功能 —— 广播变量。 它只允许将数据集发送给那些执行你 Job 里面函数的任务管理器。 假设我们有一个数据集，其中包含我们在进行文本处理时应忽略的单词，并且我们希望将其设置为我们的函数。 要为单个函数设置广播变量，我们需要使用 withBroadcastSet 方法和数据集。 12345678910111213141516171819202122232425DataSet&lt;Integer&gt; toBroadcast = env.fromElements(1, 2, 3);// 获取要忽略的单词集合DataSet&lt;String&gt; wordsToIgnore = ...data.map(new RichFlatMapFunction&lt;String, String&gt;() &#123; // 存储要忽略的单词集合. 这将存储在 TaskManager 的内存中 Collection&lt;String&gt; wordsToIgnore; @Override public void open(Configuration parameters) throws Exception &#123; //读取要忽略的单词的集合 wordsToIgnore = getRuntimeContext().getBroadcastVariable(\"wordsToIgnore\"); &#125; @Override public String map(String line, Collector&lt;String&gt; out) throws Exception &#123; String[] words = line.split(\"\\\\W+\"); for (String word : words) //使用要忽略的单词集合 if (wordsToIgnore.contains(word)) out.collect(new Tuple2&lt;&gt;(word, 1)); &#125; //通过广播变量传递数据集&#125;).withBroadcastSet(wordsToIgnore, \"wordsToIgnore\"); 你应该记住，如果要使用广播变量，那么数据集将会存储在 TaskManager 的内存中，如果数据集和越大，那么占用的内存就会越大，因此使用广播变量适用于较小的数据集。 如果要向每个 TaskManager 发送更多数据并且不希望将这些数据存储在内存中，可以使用 Flink 的分布式缓存向 TaskManager 发送静态文件。 要使用 Flink 的分布式缓存，你首先需要将文件存储在一个分布式文件系统（如 HDFS）中，然后在缓存中注册该文件： 12345678ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();//从 HDFS 注册文件env.registerCachedFile(\"hdfs:///path/to/file\", \"machineLearningModel\")...env.execute() 为了访问分布式缓存，我们需要实现一个 Rich 函数： 12345678910111213class MyClassifier extends RichMapFunction&lt;String, Integer&gt; &#123; @Override public void open(Configuration config) &#123; File machineLearningModel = getRuntimeContext().getDistributedCache().getFile(\"machineLearningModel\"); ... &#125; @Override public Integer map(String value) throws Exception &#123; ... &#125;&#125; 请注意，要访问分布式缓存中的文件，我们需要使用我们用于注册文件的 key，比如上面代码中的 machineLearningModel。 Accumulator(累加器)我们前面已经介绍了如何将数据发送给 TaskManager，但现在我们将讨论如何从 TaskManager 中返回数据。 你可能想知道为什么我们需要做这种事情。 毕竟，Apache Flink 就是建立数据处理流水线，读取输入数据，处理数据并返回结果。 为了表达清楚，让我们来看一个例子。假设我们需要计算每个单词在文本中出现的次数，同时我们要计算文本中有多少行： 1234567891011121314151617181920//要处理的数据集合DataSet&lt;String&gt; lines = ...// Word count 算法lines.flatMap(new FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123; @Override public void flatMap(String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) throws Exception &#123; String[] words = line.split(\"\\\\W+\"); for (String word : words) &#123; out.collect(new Tuple2&lt;&gt;(word, 1)); &#125; &#125;&#125;).groupBy(0).sum(1).print();// 计算要处理的文本中的行数int linesCount = lines.count()System.out.println(linesCount); 问题是如果我们运行这个应用程序，它将运行两个 Flink 作业！首先得到单词统计数，然后计算行数。 这绝对是低效的，但我们怎样才能避免这种情况呢？一种方法是使用累加器。它们允许你从 TaskManager 发送数据，并使用预定义的功能聚合此数据。 Flink 有以下内置累加器： IntCounter，LongCounter，DoubleCounter：允许将 TaskManager 发送的 int，long，double 值汇总在一起 AverageAccumulator：计算双精度值的平均值 LongMaximum，LongMinimum，IntMaximum，IntMinimum，DoubleMaximum，DoubleMinimum：累加器，用于确定不同类型的最大值和最小值 直方图 - 用于计算 TaskManager 的值分布 要使用累加器，我们需要创建并注册一个用户定义的函数，然后在客户端上读取结果。下面我们来看看该如何使用呢： 1234567891011121314151617181920212223242526272829lines.flatMap(new RichFlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123; //创建一个累加器 private IntCounter linesNum = new IntCounter(); @Override public void open(Configuration parameters) throws Exception &#123; //注册一个累加器 getRuntimeContext().addAccumulator(\"linesNum\", linesNum); &#125; @Override public void flatMap(String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) throws Exception &#123; String[] words = line.split(\"\\\\W+\"); for (String word : words) &#123; out.collect(new Tuple2&lt;&gt;(word, 1)); &#125; // 处理每一行数据后 linesNum 递增 linesNum.add(1); &#125;&#125;).groupBy(0).sum(1).print();//获取累加器结果int linesNum = env.getLastJobExecutionResult().getAccumulatorResult(\"linesNum\");System.out.println(linesNum); 这样计算就可以统计输入文本中每个单词出现的次数以及它有多少行。 如果需要自定义累加器，还可以使用 Accumulator 或 SimpleAccumulator 接口实现自己的累加器。 最后本篇文章由 zhisheng 翻译，禁止任何无授权的转载。 翻译后地址：http://www.54tianzhisheng.cn/2019/03/28/flink-additional-data/ 原文地址：https://brewing.codes/2017/10/24/flink-additional-data/ 本文部分代码地址：https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-examples/src/main/java/com/zhisheng/examples/batch/accumulator 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 源码解析 —— JobManager 处理 SubmitJob 的过程","date":"2019-03-26T16:00:00.000Z","path":"2019/03/27/Flink-code-JobManager-submitJob/","text":"JobManager 处理 SubmitJobhttps://t.zsxq.com/3JQJMzZ 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 源码解析 —— 如何获取 ExecutionGraph ？","date":"2019-03-25T16:00:00.000Z","path":"2019/03/26/Flink-code-ExecutionGraph/","text":"ExecutionGraph https://t.zsxq.com/UnA2jIi 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 源码解析 —— Flink JobManager 有什么作用？","date":"2019-03-24T16:00:00.000Z","path":"2019/03/25/Flink-code-jobmanager/","text":"JobManager 的作用https://t.zsxq.com/2VRrbuf 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 源码解析 —— Flink TaskManager 有什么作用？","date":"2019-03-24T16:00:00.000Z","path":"2019/03/25/Flink-code-taskmanager/","text":"TaskManager 有什么作用 https://t.zsxq.com/RZbu7yN 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？","date":"2019-03-23T16:00:00.000Z","path":"2019/03/24/Flink-code-memory-management/","text":"前言如今，许多用于分析大型数据集的开源系统都是用 Java 或者是基于 JVM 的编程语言实现的。最着名的例子是 Apache Hadoop，还有较新的框架，如 Apache Spark、Apache Drill、Apache Flink。基于 JVM 的数据分析引擎面临的一个常见挑战就是如何在内存中存储大量的数据（包括缓存和高效处理）。合理的管理好 JVM 内存可以将 难以配置且不可预测的系统 与 少量配置且稳定运行的系统区分开来。 在这篇文章中，我们将讨论 Apache Flink 如何管理内存，讨论其自定义序列化与反序列化机制，以及它是如何操作二进制数据的。 数据对象直接放在堆内存中在 JVM 中处理大量数据最直接的方式就是将这些数据做为对象存储在堆内存中，然后直接在内存中操作这些数据，如果想进行排序则就是对对象列表进行排序。然而这种方法有一些明显的缺点，首先，在频繁的创建和销毁大量对象的时候，监视和控制堆内存的使用并不是一件很简单的事情。如果对象分配过多的话，那么会导致内存过度使用，从而触发 OutOfMemoryError，导致 JVM 进程直接被杀死。另一个方面就是因为这些对象大都是生存在新生代，当 JVM 进行垃圾回收时，垃圾收集的开销很容易达到 50% 甚至更多。最后就是 Java 对象具有一定的空间开销（具体取决于 JVM 和平台）。对于具有许多小对象的数据集，这可以显著减少有效可用的内存量。如果你精通系统设计和系统调优，你可以根据系统进行特定的参数调整，可以或多或少的控制出现 OutOfMemoryError 的次数和避免堆内存的过多使用，但是这种设置和调优的作用有限，尤其是在数据量较大和执行环境发生变化的情况下。 Flink 是怎么做的?Apache Flink 起源于一个研究项目，该项目旨在结合基于 MapReduce 的系统和并行数据库系统的最佳技术。在此背景下，Flink 一直有自己的内存数据处理方法。Flink 将对象序列化为固定数量的预先分配的内存段，而不是直接把对象放在堆内存上。它的 DBMS 风格的排序和连接算法尽可能多地对这个二进制数据进行操作，以此将序列化和反序列化开销降到最低。如果需要处理的数据多于可以保存在内存中的数据，Flink 的运算符会将部分数据溢出到磁盘。事实上，很多Flink 的内部实现看起来更像是 C / C ++，而不是普通的 Java。下图概述了 Flink 如何在内存段中存储序列化数据并在必要时溢出到磁盘： Flink 的主动内存管理和操作二进制数据有几个好处： 1、内存安全执行和高效的核外算法 由于分配的内存段的数量是固定的，因此监控剩余的内存资源是非常简单的。在内存不足的情况下，处理操作符可以有效地将更大批的内存段写入磁盘，后面再将它们读回到内存。因此，OutOfMemoryError 就有效的防止了。 2、减少垃圾收集压力 因为所有长生命周期的数据都是在 Flink 的管理内存中以二进制表示的，所以所有数据对象都是短暂的，甚至是可变的，并且可以重用。短生命周期的对象可以更有效地进行垃圾收集，这大大降低了垃圾收集的压力。现在，预先分配的内存段是 JVM 堆上的长期存在的对象，为了降低垃圾收集的压力，Flink 社区正在积极地将其分配到堆外内存。这种努力将使得 JVM 堆变得更小，垃圾收集所消耗的时间将更少。 3、节省空间的数据存储 Java 对象具有存储开销，如果数据以二进制的形式存储，则可以避免这种开销。 4、高效的二进制操作和缓存敏感性 在给定合适的二进制表示的情况下，可以有效地比较和操作二进制数据。此外，二进制表示可以将相关值、哈希码、键和指针等相邻地存储在内存中。这使得数据结构通常具有更高效的缓存访问模式。 主动内存管理的这些特性在用于大规模数据分析的数据处理系统中是非常可取的，但是要实现这些功能的代价也是高昂的。要实现对二进制数据的自动内存管理和操作并非易事，使用 java.util.HashMap 比实现一个可溢出的 hash-table （由字节数组和自定义序列化支持）。当然，Apache Flink 并不是唯一一个基于 JVM 且对二进制数据进行操作的数据处理系统。例如 Apache Drill、Apache Ignite、Apache Geode 也有应用类似技术，最近 Apache Spark 也宣布将向这个方向演进。 下面我们将详细讨论 Flink 如何分配内存、如果对对象进行序列化和反序列化以及如果对二进制数据进行操作。我们还将通过一些性能表现数据来比较处理堆内存上的对象和对二进制数据的操作。 Flink 如何分配内存?Flink TaskManager 是由几个内部组件组成的：actor 系统（负责与 Flink master 协调）、IOManager（负责将数据溢出到磁盘并将其读取回来）、MemoryManager（负责协调内存使用）。在本篇文章中，我们主要讲解 MemoryManager。 MemoryManager 负责将 MemorySegments 分配、计算和分发给数据处理操作符，例如 sort 和 join 等操作符。MemorySegment 是 Flink 的内存分配单元，由常规 Java 字节数组支持(默认大小为 32 KB)。MemorySegment 通过使用 Java 的 unsafe 方法对其支持的字节数组提供非常有效的读写访问。你可以将 MemorySegment 看作是 Java 的 NIO ByteBuffer 的定制版本。为了在更大的连续内存块上操作多个 MemorySegment，Flink 使用了实现 Java 的 java.io.DataOutput 和 java.io.DataInput 接口的逻辑视图。 MemorySegments 在 TaskManager 启动时分配一次，并在 TaskManager 关闭时销毁。因此，在 TaskManager 的整个生命周期中，MemorySegment 是重用的，而不会被垃圾收集的。在初始化 TaskManager 的所有内部数据结构并且已启动所有核心服务之后，MemoryManager 开始创建 MemorySegments。默认情况下，服务初始化后，70％ 可用的 JVM 堆内存由 MemoryManager 分配（也可以配置全部）。剩余的 JVM 堆内存用于在任务处理期间实例化的对象，包括由用户定义的函数创建的对象。下图显示了启动后 TaskManager JVM 中的内存分布： Flink 如何序列化对象？Java 生态系统提供了几个库，可以将对象转换为二进制表示形式并返回。常见的替代方案是标准 Java 序列化，Kryo，Apache Avro，Apache Thrift 或 Google 的 Protobuf。Flink 包含自己的自定义序列化框架，以便控制数据的二进制表示。这一点很重要，因为对二进制数据进行操作需要对序列化布局有准确的了解。此外，根据在二进制数据上执行的操作配置序列化布局可以显著提升性能。Flink 的序列化机制利用了这一特性，即在执行程序之前，要序列化和反序列化的对象的类型是完全已知的。 Flink 程序可以处理表示为任意 Java 或 Scala 对象的数据。在优化程序之前，需要识别程序数据流的每个处理步骤中的数据类型。对于 Java 程序，Flink 提供了一个基于反射的类型提取组件，用于分析用户定义函数的返回类型。Scala 程序可以在 Scala 编译器的帮助下进行分析。Flink 使用 TypeInformation 表示每种数据类型。 注：该图选自董伟柯的文章《Apache Flink 类型和序列化机制简介》，侵删 Flink 有如下几种数据类型的 TypeInformations： BasicTypeInfo：所有 Java 的基础类型或 java.lang.String BasicArrayTypeInfo：Java 基本类型构成的数组或 java.lang.String WritableTypeInfo：Hadoop 的 Writable 接口的任何实现 TupleTypeInfo：任何 Flink tuple（Tuple1 到 Tuple25）。Flink tuples 是具有类型化字段的固定长度元组的 Java 表示 CaseClassTypeInfo：任何 Scala CaseClass（包括 Scala tuples） PojoTypeInfo：任何 POJO（Java 或 Scala），即所有字段都是 public 的或通过 getter 和 setter 访问的对象，遵循通用命名约定 GenericTypeInfo：不能标识为其他类型的任何数据类型 注：该图选自董伟柯的文章《Apache Flink 类型和序列化机制简介》，侵删 每个 TypeInformation 都为它所代表的数据类型提供了一个序列化器。例如，BasicTypeInfo 返回一个序列化器，该序列化器写入相应的基本类型；WritableTypeInfo 的序列化器将序列化和反序列化委托给实现 Hadoop 的 Writable 接口的对象的 write() 和 readFields() 方法；GenericTypeInfo 返回一个序列化器，该序列化器将序列化委托给 Kryo。对象将自动通过 Java 中高效的 Unsafe 方法来序列化到 Flink MemorySegments 支持的 DataOutput。对于可用作键的数据类型，例如哈希值，TypeInformation 提供了 TypeComparators，TypeComparators 比较和哈希对象，并且可以根据具体的数据类型有效的比较二进制并提取固定长度的二进制 key 前缀。 Tuple，Pojo 和 CaseClass 类型是复合类型，它们可能嵌套一个或者多个数据类型。因此，它们的序列化和比较也都比较复杂，一般将其成员数据类型的序列化和比较都交给各自的 Serializers（序列化器） 和 Comparators（比较器）。下图说明了 Tuple3&lt;Integer, Double, Person&gt;对象的序列化，其中Person 是 POJO 并定义如下： 1234public class Person &#123; public int id; public String name;&#125; 通过提供定制的 TypeInformations、Serializers（序列化器） 和 Comparators（比较器），可以方便地扩展 Flink 的类型系统，从而提高序列化和比较自定义数据类型的性能。 Flink 如何对二进制数据进行操作？与其他的数据处理框架的 API（包括 SQL）类似，Flink 的 API 也提供了对数据集进行分组、排序和连接等转换操作。这些转换操作的数据集可能非常大。关系数据库系统具有非常高效的算法，比如 merge-sort、merge-join 和 hash-join。Flink 建立在这种技术的基础上，但是主要分为使用自定义序列化和自定义比较器来处理任意对象。在下面文章中我们将通过 Flink 的内存排序算法示例演示 Flink 如何使用二进制数据进行操作。 Flink 为其数据处理操作符预先分配内存，初始化时，排序算法从 MemoryManager 请求内存预算，并接收一组相应的 MemorySegments。这些 MemorySegments 变成了缓冲区的内存池，缓冲区中收集要排序的数据。下图说明了如何将数据对象序列化到排序缓冲区中： 排序缓冲区在内部分为两个内存区域：第一个区域保存所有对象的完整二进制数据，第二个区域包含指向完整二进制对象数据的指针（取决于 key 的数据类型）。将对象添加到排序缓冲区时，它的二进制数据会追加到第一个区域，指针(可能还有一个 key)被追加到第二个区域。分离实际数据和指针以及固定长度的 key 有两个目的：它可以有效的交换固定长度的 entries（key 和指针），还可以减少排序时需要移动的数据。如果排序的 key 是可变长度的数据类型（比如 String），则固定长度的排序 key 必须是前缀 key，比如字符串的前 n 个字符。请注意：并非所有数据类型都提供固定长度的前缀排序 key。将对象序列化到排序缓冲区时，两个内存区域都使用内存池中的 MemorySegments 进行扩展。一旦内存池为空且不能再添加对象时，则排序缓冲区将会被完全填充并可以进行排序。Flink 的排序缓冲区提供了比较和交换元素的方法，这使得实际的排序算法是可插拔的。默认情况下， Flink 使用了 Quicksort（快速排序）实现，可以使用 HeapSort（堆排序）。下图显示了如何比较两个对象： 排序缓冲区通过比较它们的二进制固定长度排序 key 来比较两个元素。如果元素的完整 key（不是前缀 key） 或者二进制前缀 key 不相等，则代表比较成功。如果前缀 key 相等(或者排序 key 的数据类型不提供二进制前缀 key)，则排序缓冲区遵循指向实际对象数据的指针，对两个对象进行反序列化并比较对象。根据比较结果，排序算法决定是否交换比较的元素。排序缓冲区通过移动其固定长度 key 和指针来交换两个元素，实际数据不会移动，排序算法完成后，排序缓冲区中的指针被正确排序。下图演示了如何从排序缓冲区返回已排序的数据： 通过顺序读取排序缓冲区的指针区域，跳过排序 key 并按照实际数据的排序指针返回排序数据。此数据要么反序列化并作为对象返回，要么在外部合并排序的情况下复制二进制数据并将其写入磁盘。 基准测试数据那么，对二进制数据进行操作对性能意味着什么？我们将运行一个基准测试，对 1000 万个Tuple2&lt;Integer, String&gt;对象进行排序以找出答案。整数字段的值从均匀分布中采样。String 字段值的长度为 12 个字符，并从长尾分布中进行采样。输入数据由返回可变对象的迭代器提供，即返回具有不同字段值的相同 Tuple 对象实例。Flink 在从内存，网络或磁盘读取数据时使用此技术，以避免不必要的对象实例化。基准测试在具有 900 MB 堆大小的 JVM 中运行，在堆上存储和排序 1000 万个 Tuple 对象并且不会导致触发 OutOfMemoryError 大约需要这么大的内存。我们使用三种排序方法在Integer 字段和 String 字段上对 Tuple 对象进行排序： 1、对象存在堆中：Tuple 对象存储在常用的 java.util.ArrayList 中，初始容量设置为 1000 万，并使用 Java 中常用的集合排序进行排序。 Flink 序列化：使用 Flink 的自定义序列化程序将 Tuple 字段序列化为 600 MB 大小的排序缓冲区，如上所述排序，最后再次反序列化。在 Integer 字段上进行排序时，完整的 Integer 用作排序 key，以便排序完全发生在二进制数据上（不需要对象的反序列化）。对于 String 字段的排序，使用 8 字节前缀 key，如果前缀 key 相等，则对 Tuple 对象进行反序列化。 3、Kryo 序列化：使用 Kryo 序列化将 Tuple 字段序列化为 600 MB 大小的排序缓冲区，并在没有二进制排序 key 的情况下进行排序。这意味着每次比较需要对两个对象进行反序列化。 所有排序方法都使用单线程实现。结果的时间是十次运行结果的平均值。在每次运行之后，我们调用System.gc()请求垃圾收集运行，该运行不会进入测量的执行时间。下图显示了将输入数据存储在内存中，对其进行排序并将其作为对象读回的时间。 我们看到 Flink 使用自己的序列化器对二进制数据进行排序明显优于其他两种方法。与存储在堆内存上相比，我们看到将数据加载到内存中要快得多。因为我们实际上是在收集对象，没有机会重用对象实例，但必须重新创建每个 Tuple。这比 Flink 的序列化器（或Kryo序列化）效率低。另一方面，与反序列化相比，从堆中读取对象是无性能消耗的。在我们的基准测试中，对象克隆比序列化和反序列化组合更耗性能。查看排序时间，我们看到对二进制数据的排序也比 Java 的集合排序更快。使用没有二进制排序 key 的 Kryo 序列化的数据排序比其他方法慢得多。这是因为反序列化带来很大的开销。在String 字段上对 Tuple 进行排序比在 Integer 字段上排序更快，因为长尾值分布显着减少了成对比较的数量。为了更好地了解排序过程中发生的状况，我们使用 VisualVM 监控执行的 JVM。以下截图显示了执行 10次 运行时的堆内存使用情况、垃圾收集情况和 CPU 使用情况。 测试是在 8 核机器上运行单线程，因此一个核心的完全利用仅对应 12.5％ 的总体利用率。截图显示，对二进制数据进行操作可显著减少垃圾回收活动。对于对象存在堆中，垃圾收集器在排序缓冲区被填满时以非常短的时间间隔运行，并且即使对于单个处理线程也会导致大量 CPU 使用（排序本身不会触发垃圾收集器）。JVM 垃圾收集多个并行线程，解释了高CPU 总体利用率。另一方面，对序列化数据进行操作的方法很少触发垃圾收集器并且 CPU 利用率低得多。实际上，如果使用 Flink 序列化的方式在 Integer 字段上对 Tuple 进行排序，则垃圾收集器根本不运行，因为对于成对比较，不需要反序列化任何对象。Kryo 序列化需要比较多的垃圾收集，因为它不使用二进制排序 key 并且每次排序都要反序列化两个对象。 内存使用情况上图显示 Flink 序列化和 Kryo 序列化不断的占用大量内存 存使用情况图表显示flink-serialized和kryo-serialized不断占用大量内存。这是由于 MemorySegments 的预分配。实际内存使用率要低得多，因为排序缓冲区并未完全填充。下表显示了每种方法的内存消耗。1000 万条数据产生大约 280 MB 的二进制数据（对象数据、指针和排序 key），具体取决于使用的序列化程序以及二进制排序 key 的存在和大小。将其与数据存储在堆上的方法进行比较，我们发现对二进制数据进行操作可以显著提高内存效率。在我们的基准测试中，如果序列化为排序缓冲区而不是将其作为堆上的对象保存，则可以在内存中对两倍以上的数据进行排序。 占用内存 对象存在堆中 Flink 序列化 Kryo 序列化 对 Integer 排序 约 700 MB（堆内存） 277 MB（排序缓冲区） 266 MB（排序缓冲区） 对 String 排序 约 700 MB（堆内存） 315 MB（排序缓冲区） 266 MB（排序缓冲区） 总而言之，测试验证了文章前面说的对二进制数据进行操作的好处。 展望未来Apache Flink 具有相当多的高级技术，可以通过有限的内存资源安全有效地处理大量数据。但是有几点可以使 Flink 更有效率。Flink 社区正在努力将管理内存移动到堆外内存。这将允许更小的 JVM，更低的垃圾收集开销，以及更容易的系统配置。使用 Flink 的 Table API，所有操作（如 aggregation 和 projection）的语义都是已知的（与黑盒用户定义的函数相反）。因此，我们可以为直接对二进制数据进行操作的 Table API 操作生成代码。进一步的改进包括序列化设计，这些设计针对应用于二进制数据的操作和针对序列化器和比较器的代码生成而定制。 总结 Flink 的主动内存管理减少了因触发 OutOfMemoryErrors 而杀死 JVM 进程和垃圾收集开销的问题。 Flink 具有高效的数据序列化和反序列化机制，有助于对二进制数据进行操作，并使更多数据适合内存。 Flink 的 DBMS 风格的运算符本身在二进制数据上运行，在必要时可以在内存中高性能地传输到磁盘。 本文地址: http://www.54tianzhisheng.cn/2019/03/24/Flink-code-memory-management/ 本文翻译自：https://flink.apache.org/news/2015/05/11/Juggling-with-Bits-and-Bytes.html翻译：zhisheng，二次转载请注明地址，否则保留追究法律责任。 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 源码解析 —— 深度解析 Flink Checkpoint 机制","date":"2019-03-22T16:00:00.000Z","path":"2019/03/23/Flink-code-checkpoint/","text":"Flink Checkpoint 机制 https://t.zsxq.com/ynQNbeM 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 源码解析 —— 深度解析 Flink 序列化机制","date":"2019-03-21T16:00:00.000Z","path":"2019/03/22/Flink-code-serialize/","text":"Flink 序列化机制 https://t.zsxq.com/JaQfeMf 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 源码解析 —— 如何获取 JobGraph？","date":"2019-03-20T16:00:00.000Z","path":"2019/03/21/Flink-code-JobGraph/","text":"JobGraph https://t.zsxq.com/naaMf6y 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 源码解析 —— 如何获取 StreamGraph？","date":"2019-03-19T16:00:00.000Z","path":"2019/03/20/Flink-code-StreamGraph/","text":"StreamGraph https://t.zsxq.com/qRFIm6I 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程","date":"2019-03-18T16:00:00.000Z","path":"2019/03/19/Flink-code-streaming-wordcount-start/","text":"流处理 WordCount 程序 https://t.zsxq.com/qnMFEUJ 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程","date":"2019-03-17T16:00:00.000Z","path":"2019/03/18/Flink-code-batch-wordcount-start/","text":"批处理的 WordCount 程序分析： https://t.zsxq.com/YJ2Zrfi 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动","date":"2019-03-16T16:00:00.000Z","path":"2019/03/17/Flink-code-Standalone-TaskManager-start/","text":"Task Manager 启动 https://t.zsxq.com/qjEUFau 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动","date":"2019-03-15T16:00:00.000Z","path":"2019/03/16/Flink-code-Standalone-JobManager-start/","text":"Job Manager 启动 https://t.zsxq.com/AurR3rN 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 源码解析 —— Standalone session 模式启动流程","date":"2019-03-14T16:00:00.000Z","path":"2019/03/15/Flink-code-Standalone-start/","text":"Standalone session 模式启动流程 https://t.zsxq.com/EemAEIi 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 源码解析 —— 项目结构一览","date":"2019-03-13T16:00:00.000Z","path":"2019/03/14/Flink-code-structure/","text":"Flink 源码项目结构一览 https://t.zsxq.com/MNfAYne 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从 0 到 1 学习 —— 你上传的 jar 包藏到哪里去了?","date":"2019-03-12T16:00:00.000Z","path":"2019/03/13/flink-job-jars/","text":"前言写这篇文章其实也是知识星球里面的一个小伙伴问了这样一个问题： 通过 flink UI 仪表盘提交的 jar 是存储在哪个目录下？ 这个问题其实我自己也有问过，但是自己因为自己的问题没有啥压力也就没深入去思考，现在可是知识星球的付费小伙伴问的，所以自然要逼着自己去深入然后才能给出正确的答案。 跟着我一起来看看我的探寻步骤吧！小小的 jar 竟然还敢和我捉迷藏？ 查看配置文件首先想到的是这个肯定可以在配置文件中有设置的地方的： 谷歌大法好虽然有个是 upload 的，但是并不是我们想要的目录！于是，只好动用我的“谷歌大法好”。 找到了一条，点进去看 Issue 如下： 发现这 tm 不就是想要的吗？都支持配置文件来填写上传的 jar 后存储的目录了！赶紧点进去看一波源码： 源码确认这个 jobmanager.web.upload.dir 是不是？我去看下 1.8 的源码确认一下： 发现这个 jobmanager.web.upload.dir 还过期了，用 WebOptions 类中的 UPLOAD_DIR 替代了！ 继续跟进去看看这个 UPLOAD_DIR 是啥玩意？ 看这注释的意思是说，如果这个配置 web.upload.dir 没有配置具体的路径的话就会使用 JOB_MANAGER_WEB_TMPDIR_KEY 目录，那么我们来看看是否配置了这个目录呢？ 确实没有配置这个 jar 文件上传的目录，那么我们来看看这个临时目录 JOB_MANAGER_WEB_TMPDIR_KEY 是在哪里的？ 又是一个过期的目录，mmp，继续跟下去看下这个目录 TMP_DIR。 我们查看下配置文件是否有配置这个 web.tmpdir 的值，又是没有： so，它肯定使用的是 System.getProperty(&quot;java.io.tmpdir&quot;) 这个目录了， 我查看了下我本地电脑起的 job 它的配置中有这个配置如下： 1java.io.tmpdir /var/folders/mb/3vpbvkkx13l2jmpt2kmmt0fr0000gn/T/ 再观察了下 job，发现 jobManager 这里有个 web.tmpdir 的配置： 1web.tmpdir /var/folders/mb/3vpbvkkx13l2jmpt2kmmt0fr0000gn/T/flink-web-ea909e9e-4bac-452d-8450-b4ff082298c7 发现这个 web.tmpdir 的就是由 java.io.tmpdir + “flink-web-” + UUID 组成的！ 水落石出进入这个目录发现我们上传的 jar 终于被找到了： 配置上传 jar 目录确认上面我们虽然已经知道我们上传的 jar 是存储在这个临时目录里，那么我们现在要验证一下，我们在配置文件中配置一下上传 jar 的固定位置，我们先在目录下创建一个 jars 目录，然后在配置文件中加入这个配置： 1web.tmpdir: /usr/local/blink-1.5.1/jars 更改之后再看 web.tmpdir 是这样的: 从 Flink UI 上上传了三个 jar，查看 /usr/local/blink-1.5.1/jars/flink-web-7a98165b-1d56-44be-be8c-d0cd9166b179 目录下就出现了我们的 jar 了。 我们重启 Flink，发现这三个 jar 又没有了，这也能解释之前我自己也遇到过的问题了，Flink 重启后之前所有上传的 jar 都被删除了！作为生产环境，这样玩，肯定不行的，所以我们还是得固定一个目录来存储所有的上传 jar 包，并且不能够被删除，要配置固定的目录（Flink 重启也不删除的话）需要配置如下： 1web.upload.dir: /usr/local/blink-1.5.1/jars 这样的话，就可以保证你的 jar 不再会被删除了！ 再来看看源码是咋写的哈： 12345678//从配置文件中找 UPLOAD_DIRfinal Path uploadDir = Paths.get( config.getString(WebOptions.UPLOAD_DIR, config.getString(WebOptions.TMP_DIR)), \"flink-web-upload\");return new RestServerEndpointConfiguration( restAddress,restBindAddress,port,sslEngineFactory, uploadDir,maxContentLength,responseHeaders); 他就是从配置文件中找 UPLOAD_DIR，如果为 null 就找 TMP_DIR 目录来当作 jar 上传的路径！ 总结本文从知识星球一个朋友的问题，从现象到本质再到解决方案的讲解了下如何找到 Flink UI 上上传的 jar 包藏身之处，并提出了如何解决 Flink 上传的 jar 包被删除的问题。 本篇文章连接是：http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/ 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"阿里巴巴开源的 Blink 实时计算框架真香","date":"2019-02-27T16:00:00.000Z","path":"2019/02/28/blink/","text":"Blink 开源了有一段时间了，竟然没发现有人写相关的博客，其实我已经在我的知识星球里开始写了，今天来看看 Blink 为什么香？ 我们先看看 Blink 黑色版本： 对比下 Flink 版本你就知道黑色版本多好看了。 你上传 jar 包的时候是这样的： 我们来看看 Blink 运行的 job 长啥样？ 再来对比一下 Flink 的样子： 查看 Job Task 的详情，可以看到开始时间、接收记录、并行度、duration、Queue in/out、TPS 查看 subTask，这里可以直接点击这个日志就可以查看 task 日志： 查看背压： 查看 task metric，可以手动添加，支持的有很多，这点很重要，可以根据每个算子的监控以及时对每个算子进行调优： 查看 job 运行时间段的情况： 查看 running 的 job： 查看已经完成的 job： 查看 Task Manager： Task Manager 分配的资源详情： Task Manager metric 监控信息详情： Task Manager log 文件详情，包含运行产生的日志和 GC 日志： Task Manager 日志详情，支持高亮和分页，特别友好，妈妈再也不担心我看不见 “刷刷刷” 的日志了。 总结介绍了 Flink 的 Blink 分支编译后运行的界面情况，总体来说很棒，期待后面 Blink 合并到 Flink！ 本文原创地址是: http://www.54tianzhisheng.cn/2019/02/28/blink/ , 未经允许禁止转载。 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。 相关文章1、《从0到1学习Flink》—— Apache Flink 介绍 2、《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、《从0到1学习Flink》—— Flink 配置文件详解 4、《从0到1学习Flink》—— Data Source 介绍 5、《从0到1学习Flink》—— 如何自定义 Data Source ？ 6、《从0到1学习Flink》—— Data Sink 介绍 7、《从0到1学习Flink》—— 如何自定义 Data Sink ？ 8、《从0到1学习Flink》—— Flink Data transformation(转换) 9、《从0到1学习Flink》—— 介绍Flink中的Stream Windows 10、《从0到1学习Flink》—— Flink 中的几种 Time 详解 11、《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch 12、《从0到1学习Flink》—— Flink 项目如何运行？ 13、《从0到1学习Flink》—— Flink 写入数据到 Kafka 14、《从0到1学习Flink》—— Flink JobManager 高可用性配置 15、《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍 16、《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"},{"name":"Blink","slug":"Blink","permalink":"http://www.54tianzhisheng.cn/tags/Blink/"}]},{"title":"Flink 源码解析 —— 源码编译运行","date":"2019-01-29T16:00:00.000Z","path":"2019/01/30/Flink-code-compile/","text":"更新一篇知识星球里面的源码分析文章，去年写的，周末自己录了个视频，大家看下效果好吗？如果好的话，后面补录发在知识星球里面的其他源码解析文章。 前言之前自己本地 clone 了 Flink 的源码，编译过，然后 share 到了 GitHub 上去了，自己也写了一些源码的中文注释，并且 push 到了 GitHub 上去了。这几天阿里开源了宣传已久的 Blink，结果我那个分支不能够继续 pull 下新的代码，再加上自己对 Flink 研究了也有点时间了，所以打算将这两个东西对比着来看，这样可能会学到不少更多东西，因为 Blink 是另外一个分支，所以自己干脆再重新 fork 了一份，拉到本地来看源码。 fork执行下面命令： 1git clone git@github.com:apache/flink.git 拉取的时候找个网络好点的地方，这样速度可能会更快点。 编译因为自己想看下 Blink 分支的代码，所以需要切换到 blink 分支来， 1git checkout blink 这样你就到了 blink 分支了，接下来我们将 blink 源码编译一下，执行如下命令： 1mvn clean install -Dmaven.test.skip=true -Dhadoop.version=2.7.6 -Dmaven.javadoc.skip=true -Dcheckstyle.skip=true maven 编译的时候跳过测试代码、javadoc 和代码风格检查，这样可以减少不少时间。 注意：你的 maven 的 settings.xml 文件的 mirror 添加下面这个：(这样下载依赖才能飞起来) 12345678910111213&lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;*,!jeecg,!jeecg-snapshots,!mapr-releases&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;&lt;/mirror&gt;&lt;mirror&gt; &lt;id&gt;mapr-public&lt;/id&gt; &lt;mirrorOf&gt;mapr-releases&lt;/mirrorOf&gt; &lt;name&gt;mapr-releases&lt;/name&gt; &lt;url&gt;https://maven.aliyun.com/repository/mapr-public&lt;/url&gt;&lt;/mirror&gt; 执行完这个命令后，然后呢，你可以掏出手机，打开微信，搜索下微信号：zhisheng_tian , 然后点击一波添加好友，欢迎来探讨技术。 等了一波时间之后，你可能会遇到这个问题(看到不少童鞋都遇到这个问题，之前编译 Flink 的时候也遇到过)： 1[ERROR] Failed to execute goal on project flink-mapr-fs: Could not resolve dependencies for project com.alibaba.blink:flink-mapr-fs:jar:1.5.1: Failure to find com.mapr.hadoop:maprfs:jar:5.2.1-mapr in http://maven.aliyun.com/nexus/content/groups/public was cached in the local repository, resolution will not be reattempted until the update interval of nexus-aliyun has elapsed or updates are forced -&gt; [Help 1] 如果你试了两遍都没编译通过，那么我这里就教大家一种方法（执行完编译命令后啥也没动就 OK 的请跳过，谁叫你运气这么好呢）： 在 flink-filesystems 中把 flink-mapr-fs module 给注释掉。 上图这是我给大家的忠告，特别管用。 再次执行命令编译起来就没有错误了，如果你还有其他的错误，我猜估计还是网络的问题，导致一些国外的 maven 依赖下载不下来或者不完整，导致的错误，暴力的方法就是和我一样，把这些下载不下来的依赖 module 注释掉，或者你可以像已经编译好的童鞋要下 maven 的 .m2 文件里面已经下载好了的依赖，然后复制粘贴到你的本地路径去，注意路径包名不要弄错了，这样一般可以解决所有的问题了，如果还有问题，我也无能为力了。 编译成功就长下图这样： 运行然后我们的目录是长这样的： 标记的那个就是我们的可执行文件，就跟我们在 Flink 官网下载的一样，我们可以将它运行起来看下效果。 我把它移到了 /usr/local/blink-1.5.1 下了，个人习惯，喜欢把一些安装的软件安装在 /usr/local/ 目录下面。 目录结构和我以前的安装介绍文章类似，就是多了 batch_conf 目录，和 conf 目录是一样的东西，不知道为啥要弄两个配置文件目录，问过负责的人，没理我，哈哈哈。 那么我们接下来就是运行下 Blink，进入到 bin 目录，执行可执行文件： 1./start-cluster.sh windows 可以点击 start-cluster.bat 启动，这点对 windows 用户比较友好。 执行完后命令后，在浏览器里访问地址，, 出现下图这样就代表 Blink 成功启动了：123456789101112131415161718192021222324![](https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/XQLzMv.jpg)上图是开源版本的白色主题，骚气的黑色主题通过在 Flink 群里得知如何改之后，编译运行后的效果如下：![](https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/JfcR5E.jpg)一次好奇的执行了多次上面启动命令，发现也能够正常的运行。然后启动的日志是这样的：![](https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/fcAhYL.jpg)说明已经启动了 9 个 Task Manager，然后看到我们页面的监控信息如下：![](https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/9Hxhsg.jpg)可以看到监控信息里面已经有 40 个可用的 slot，这是因为 Blink 默认的是一个 Task Manager 4 个 slot，我们总共启动了 10 个 Task Manager，所以才会有 40 个可用的 slot，注意：Flink 默认的配置是 1 个 Task Manager 只含有 1 个 slot，不过这个是可以自己分配的。注意：开启了多个 Task Manager 后，要关闭的话，得执行同样次数的关闭命令：```shell./stop-cluster.sh 中文源码分析https://github.com/zhisheng17/flink 配套视频解析视频录制过程难免说错，还请大家可以指教 相关更多源码解析的文章和 Flink 资料请加知识星球！ 本文地址是：http://www.54tianzhisheng.cn/2019/01/30/Flink-code-compile/，未经允许，禁止转载！ 总结本篇文章是《从1到100深入学习Flink》的第一篇，zhisheng 我带带大家一起如何 clone 项目源码，进行源码编译，然后运行编译后的可执行文件 blink。下篇文章会分析项目源码的结构组成。 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从 0 到 1 学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ","date":"2019-01-19T16:00:00.000Z","path":"2019/01/20/Flink-RabbitMQ-sink/","text":"前言之前有文章 《从0到1学习Flink》—— Flink 写入数据到 Kafka 写过 Flink 将处理后的数据后发到 Kafka 消息队列中去，当然我们常用的消息队列可不止这一种，还有 RocketMQ、RabbitMQ 等，刚好 Flink 也支持将数据写入到 RabbitMQ，所以今天我们就来写篇文章讲讲如何将 Flink 处理后的数据写入到 RabbitMQ。 前提准备安装 RabbitMQ这里我直接用 docker 命令安装吧，先把 docker 在 mac 上启动起来。 在命令行中执行下面的命令： 1docker run -d -p 15672:15672 -p 5672:5672 -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=admin --name rabbitmq rabbitmq:3-management 对这个命令不懂的童鞋可以看看我以前的文章：http://www.54tianzhisheng.cn/2018/01/26/SpringBoot-RabbitMQ/ 登录用户名和密码分别是：admin / admin ，登录进去是这个样子就代表安装成功了： 依赖pom.xml 中添加 Flink connector rabbitmq 的依赖如下： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-rabbitmq_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt; 生产者这里我们依旧自己写一个工具类一直的往 RabbitMQ 中的某个 queue 中发数据，然后由 Flink 去消费这些数据。 注意按照我的步骤来一步步操作，否则可能会出现一些错误！ RabbitMQProducerUtil.java 12345678910111213141516171819202122232425262728293031323334353637383940import com.rabbitmq.client.Channel;import com.rabbitmq.client.Connection;import com.rabbitmq.client.ConnectionFactory;public class RabbitMQProducerUtil &#123; public final static String QUEUE_NAME = \"zhisheng\"; public static void main(String[] args) throws Exception &#123; //创建连接工厂 ConnectionFactory factory = new ConnectionFactory(); //设置RabbitMQ相关信息 factory.setHost(\"localhost\"); factory.setUsername(\"admin\"); factory.setPassword(\"admin\"); factory.setPort(5672); //创建一个新的连接 Connection connection = factory.newConnection(); //创建一个通道 Channel channel = connection.createChannel(); // 声明一个队列// channel.queueDeclare(QUEUE_NAME, false, false, false, null); //发送消息到队列中 String message = \"Hello zhisheng\"; //我们这里演示发送一千条数据 for (int i = 0; i &lt; 1000; i++) &#123; channel.basicPublish(\"\", QUEUE_NAME, null, (message + i).getBytes(\"UTF-8\")); System.out.println(\"Producer Send +'\" + message + i); &#125; //关闭通道和连接 channel.close(); connection.close(); &#125;&#125; Flink 主程序12345678910111213141516171819202122232425262728293031323334import com.zhisheng.common.utils.ExecutionEnvUtil;import org.apache.flink.api.common.serialization.SimpleStringSchema;import org.apache.flink.api.java.utils.ParameterTool;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.connectors.rabbitmq.RMQSource;import org.apache.flink.streaming.connectors.rabbitmq.common.RMQConnectionConfig;/** * 从 rabbitmq 读取数据 */public class Main &#123; public static void main(String[] args) throws Exception &#123; final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); ParameterTool parameterTool = ExecutionEnvUtil.PARAMETER_TOOL; //这些配置建议可以放在配置文件中，然后通过 parameterTool 来获取对应的参数值 final RMQConnectionConfig connectionConfig = new RMQConnectionConfig .Builder().setHost(\"localhost\").setVirtualHost(\"/\") .setPort(5672).setUserName(\"admin\").setPassword(\"admin\") .build(); DataStreamSource&lt;String&gt; zhisheng = env.addSource(new RMQSource&lt;&gt;(connectionConfig, \"zhisheng\", true, new SimpleStringSchema())) .setParallelism(1); zhisheng.print(); //如果想保证 exactly-once 或 at-least-once 需要把 checkpoint 开启// env.enableCheckpointing(10000); env.execute(\"flink learning connectors rabbitmq\"); &#125;&#125; 运行 RabbitMQProducerUtil 类，再运行 Main 类！ 注意⚠️： 1、RMQConnectionConfig 中设置的用户名和密码要设置成 admin/admin，如果你换成是 guest/guest，其实是在 RabbitMQ 里面是没有这个用户名和密码的，所以就会报这个错误： 1nested exception is com.rabbitmq.client.AuthenticationFailureException: ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile. 不出意外的话应该你运行 RabbitMQProducerUtil 类后，立马两个运行的结果都会出来，速度还是很快的。 2、如果你在 RabbitMQProducerUtil 工具类中把注释的那行代码打开的话： 12 // 声明一个队列// channel.queueDeclare(QUEUE_NAME, false, false, false, null); 就会出现这种错误： 1Caused by: com.rabbitmq.client.ShutdownSignalException: channel error; protocol method: #method&lt;channel.close&gt;(reply-code=406, reply-text=PRECONDITION_FAILED - inequivalent arg &apos;durable&apos; for queue &apos;zhisheng&apos; in vhost &apos;/&apos;: received &apos;true&apos; but current is &apos;false&apos;, class-id=50, method-id=10) 这是因为你打开那个注释的话，一旦你运行了该类就会创建一个叫做 zhisheng 的 Queue，当你再运行 Main 类中的时候，它又会创建这样一个叫 zhisheng 的 Queue，然后因为已经有同名的 Queue 了，所以就有了冲突，解决方法就是把那行代码注释就好了。 3、该 connector（连接器）中提供了 RMQSource 类去消费 RabbitMQ queue 中的消息和确认 checkpoints 上的消息，它提供了三种不一样的保证： Exactly-once(只消费一次): 前提条件有，1 是要开启 checkpoint，因为只有在 checkpoint 完成后，才会返回确认消息给 RabbitMQ（这时，消息才会在 RabbitMQ 队列中删除)；2 是要使用 Correlation ID，在将消息发往 RabbitMQ 时，必须在消息属性中设置 Correlation ID。数据源根据 Correlation ID 把从 checkpoint 恢复的数据进行去重；3 是数据源不能并行，这种限制主要是由于 RabbitMQ 将消息从单个队列分派给多个消费者。 At-least-once(至少消费一次): 开启了 checkpoint，但未使用相 Correlation ID 或 数据源是并行的时候，那么就只能保证数据至少消费一次了 No guarantees(无法保证): Flink 接收到数据就返回确认消息给 RabbitMQ Sink 数据到 RabbitMQRabbitMQ 除了可以作为数据源，也可以当作下游，Flink 消费数据做了一些处理之后也能把数据发往 RabbitMQ，下面演示下 Flink 消费 Kafka 数据后写入到 RabbitMQ。 12345678910111213141516public class Main1 &#123; public static void main(String[] args) throws Exception &#123; final ParameterTool parameterTool = ExecutionEnvUtil.createParameterTool(args); StreamExecutionEnvironment env = ExecutionEnvUtil.prepare(parameterTool); DataStreamSource&lt;Metrics&gt; data = KafkaConfigUtil.buildSource(env); final RMQConnectionConfig connectionConfig = new RMQConnectionConfig .Builder().setHost(\"localhost\").setVirtualHost(\"/\") .setPort(5672).setUserName(\"admin\").setPassword(\"admin\") .build(); //注意，换一个新的 queue，否则也会报错 data.addSink(new RMQSink&lt;&gt;(connectionConfig, \"zhisheng001\", new MetricSchema())); env.execute(\"flink learning connectors rabbitmq\"); &#125;&#125; 是不是很简单？但是需要注意的是，要换一个之前不存在的 queue，否则是会报错的。 不出意外的话，你可以看到 RabbitMQ 的监控页面会出现新的一个 queue 出来，如下图： 总结本文先把 RabbitMQ 作为数据源，写了个 Flink 消费 RabbitMQ 队列里面的数据进行打印出来，然后又写了个 Flink 消费 Kafka 数据后写入到 RabbitMQ 的例子！ 本文原创地址是: http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/ , 未经允许禁止转载。 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。 本文的项目代码在 https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-rabbitmq 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"},{"name":"Kafka","slug":"Kafka","permalink":"http://www.54tianzhisheng.cn/tags/Kafka/"},{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://www.54tianzhisheng.cn/tags/RabbitMQ/"}]},{"title":"Flink 从 0 到 1 学习 —— Flink 读取 Kafka 数据批量写入到 MySQL","date":"2019-01-14T16:00:00.000Z","path":"2019/01/15/Flink-MySQL-sink/","text":"前言之前其实在 《从0到1学习Flink》—— 如何自定义 Data Sink ？ 文章中其实已经写了点将数据写入到 MySQL，但是一些配置化的东西当时是写死的，不能够通用，最近知识星球里有朋友叫我: 写个从 kafka 中读取数据，经过 Flink 做个预聚合，然后创建数据库连接池将数据批量写入到 mysql 的例子。 于是才有了这篇文章，更多提问和想要我写的文章可以在知识星球里像我提问，我会根据提问及时回答和尽可能作出文章的修改。 准备你需要将这两个依赖添加到 pom.xml 中 12345&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.34&lt;/version&gt;&lt;/dependency&gt; 读取 kafka 数据这里我依旧用的以前的 student 类，自己本地起了 kafka 然后造一些测试数据，这里我们测试发送一条数据则 sleep 10s，意味着往 kafka 中一分钟发 6 条数据。 123456789101112131415161718192021222324252627282930313233343536373839package com.zhisheng.connectors.mysql.utils;import com.zhisheng.common.utils.GsonUtil;import com.zhisheng.connectors.mysql.model.Student;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.ProducerRecord;import java.util.Properties;/** * Desc: 往kafka中写数据,可以使用这个main函数进行测试 * Created by zhisheng on 2019-02-17 * Blog: http://www.54tianzhisheng.cn/tags/Flink/ */public class KafkaUtil &#123; public static final String broker_list = \"localhost:9092\"; public static final String topic = \"student\"; //kafka topic 需要和 flink 程序用同一个 topic public static void writeToKafka() throws InterruptedException &#123; Properties props = new Properties(); props.put(\"bootstrap.servers\", broker_list); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); KafkaProducer producer = new KafkaProducer&lt;String, String&gt;(props); for (int i = 1; i &lt;= 100; i++) &#123; Student student = new Student(i, \"zhisheng\" + i, \"password\" + i, 18 + i); ProducerRecord record = new ProducerRecord&lt;String, String&gt;(topic, null, null, GsonUtil.toJson(student)); producer.send(record); System.out.println(\"发送数据: \" + GsonUtil.toJson(student)); Thread.sleep(10 * 1000); //发送一条数据 sleep 10s，相当于 1 分钟 6 条 &#125; producer.flush(); &#125; public static void main(String[] args) throws InterruptedException &#123; writeToKafka(); &#125;&#125; 从 kafka 中读取数据，然后序列化成 student 对象。 1234567891011121314final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();Properties props = new Properties();props.put(\"bootstrap.servers\", \"localhost:9092\");props.put(\"zookeeper.connect\", \"localhost:2181\");props.put(\"group.id\", \"metric-group\");props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\");props.put(\"auto.offset.reset\", \"latest\");SingleOutputStreamOperator&lt;Student&gt; student = env.addSource(new FlinkKafkaConsumer011&lt;&gt;( \"student\", //这个 kafka topic 需要和上面的工具类的 topic 一致 new SimpleStringSchema(), props)).setParallelism(1) .map(string -&gt; GsonUtil.fromJson(string, Student.class)); //，解析字符串成 student 对象 因为 RichSinkFunction 中如果 sink 一条数据到 mysql 中就会调用 invoke 方法一次，所以如果要实现批量写的话，我们最好在 sink 之前就把数据聚合一下。那这里我们开个一分钟的窗口去聚合 Student 数据。 12345678910student.timeWindowAll(Time.minutes(1)).apply(new AllWindowFunction&lt;Student, List&lt;Student&gt;, TimeWindow&gt;() &#123; @Override public void apply(TimeWindow window, Iterable&lt;Student&gt; values, Collector&lt;List&lt;Student&gt;&gt; out) throws Exception &#123; ArrayList&lt;Student&gt; students = Lists.newArrayList(values); if (students.size() &gt; 0) &#123; System.out.println(\"1 分钟内收集到 student 的数据条数是：\" + students.size()); out.collect(students); &#125; &#125;&#125;); 写入数据库这里使用 DBCP 连接池连接数据库 mysql，pom.xml 中添加依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-dbcp2&lt;/artifactId&gt; &lt;version&gt;2.1.1&lt;/version&gt;&lt;/dependency&gt; 如果你想使用其他的数据库连接池请加入对应的依赖。 这里将数据写入到 MySQL 中，依旧是和之前文章一样继承 RichSinkFunction 类，重写里面的方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293package com.zhisheng.connectors.mysql.sinks;import com.zhisheng.connectors.mysql.model.Student;import org.apache.commons.dbcp2.BasicDataSource;import org.apache.flink.configuration.Configuration;import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;import javax.sql.DataSource;import java.sql.Connection;import java.sql.DriverManager;import java.sql.PreparedStatement;import java.util.List;/** * Desc: 数据批量 sink 数据到 mysql * Created by zhisheng_tian on 2019-02-17 * Blog: http://www.54tianzhisheng.cn/tags/Flink/ */public class SinkToMySQL extends RichSinkFunction&lt;List&lt;Student&gt;&gt; &#123; PreparedStatement ps; BasicDataSource dataSource; private Connection connection; /** * open() 方法中建立连接，这样不用每次 invoke 的时候都要建立连接和释放连接 * * @param parameters * @throws Exception */ @Override public void open(Configuration parameters) throws Exception &#123; super.open(parameters); dataSource = new BasicDataSource(); connection = getConnection(dataSource); String sql = \"insert into Student(id, name, password, age) values(?, ?, ?, ?);\"; ps = this.connection.prepareStatement(sql); &#125; @Override public void close() throws Exception &#123; super.close(); //关闭连接和释放资源 if (connection != null) &#123; connection.close(); &#125; if (ps != null) &#123; ps.close(); &#125; &#125; /** * 每条数据的插入都要调用一次 invoke() 方法 * * @param value * @param context * @throws Exception */ @Override public void invoke(List&lt;Student&gt; value, Context context) throws Exception &#123; //遍历数据集合 for (Student student : value) &#123; ps.setInt(1, student.getId()); ps.setString(2, student.getName()); ps.setString(3, student.getPassword()); ps.setInt(4, student.getAge()); ps.addBatch(); &#125; int[] count = ps.executeBatch();//批量后执行 System.out.println(\"成功了插入了\" + count.length + \"行数据\"); &#125; private static Connection getConnection(BasicDataSource dataSource) &#123; dataSource.setDriverClassName(\"com.mysql.jdbc.Driver\"); //注意，替换成自己本地的 mysql 数据库地址和用户名、密码 dataSource.setUrl(\"jdbc:mysql://localhost:3306/test\"); dataSource.setUsername(\"root\"); dataSource.setPassword(\"root123456\"); //设置连接池的一些参数 dataSource.setInitialSize(10); dataSource.setMaxTotal(50); dataSource.setMinIdle(2); Connection con = null; try &#123; con = dataSource.getConnection(); System.out.println(\"创建连接池：\" + con); &#125; catch (Exception e) &#123; System.out.println(\"-----------mysql get connection has exception , msg = \" + e.getMessage()); &#125; return con; &#125;&#125; 核心类 Main核心程序如下： 123456789101112131415161718192021222324252627282930public class Main &#123; public static void main(String[] args) throws Exception&#123; final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); Properties props = new Properties(); props.put(\"bootstrap.servers\", \"localhost:9092\"); props.put(\"zookeeper.connect\", \"localhost:2181\"); props.put(\"group.id\", \"metric-group\"); props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(\"auto.offset.reset\", \"latest\"); SingleOutputStreamOperator&lt;Student&gt; student = env.addSource(new FlinkKafkaConsumer011&lt;&gt;( \"student\", //这个 kafka topic 需要和上面的工具类的 topic 一致 new SimpleStringSchema(), props)).setParallelism(1) .map(string -&gt; GsonUtil.fromJson(string, Student.class)); // student.timeWindowAll(Time.minutes(1)).apply(new AllWindowFunction&lt;Student, List&lt;Student&gt;, TimeWindow&gt;() &#123; @Override public void apply(TimeWindow window, Iterable&lt;Student&gt; values, Collector&lt;List&lt;Student&gt;&gt; out) throws Exception &#123; ArrayList&lt;Student&gt; students = Lists.newArrayList(values); if (students.size() &gt; 0) &#123; System.out.println(\"1 分钟内收集到 student 的数据条数是：\" + students.size()); out.collect(students); &#125; &#125; &#125;).addSink(new SinkToMySQL()); env.execute(\"flink learning connectors kafka\"); &#125;&#125; 运行项目运行 Main 类后再运行 KafkaUtils.java 类！ 下图是往 Kafka 中发送的数据： 下图是运行 Main 类的日志，会创建 4 个连接池是因为默认的 4 个并行度，你如果在 addSink 这个算子设置并行度为 1 的话就会创建一个连接池： 下图是批量插入数据库的结果： 总结本文从知识星球一位朋友的疑问来写的，应该都满足了他的条件（批量/数据库连接池/写入mysql），的确网上很多的例子都是简单的 demo 形式，都是单条数据就创建数据库连接插入 MySQL，如果要写的数据量很大的话，会对 MySQL 的写有很大的压力。这也是我之前在 《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch 中，数据写 ES 强调过的，如果要提高性能必定要批量的写。就拿我们现在这篇文章来说，如果数据量大的话，聚合一分钟数据达万条，那么这样批量写会比来一条写一条性能提高不知道有多少。 本文原创地址是: http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/ , 未经允许禁止转载。 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。 本文的项目代码在 https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-mysql 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://www.54tianzhisheng.cn/tags/MySQL/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从 0 到 1 学习 —— Flink parallelism 和 Slot 介绍","date":"2019-01-13T16:00:00.000Z","path":"2019/01/14/Flink-parallelism-slot/","text":"前言之所以写这个是因为前段时间自己的项目出现过这样的一个问题： 123Caused by: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/taskmanager_0#15608456]] after [10000 ms]. Sender[null] sent message of type &quot;org.apache.flink.runtime.rpc.messages.LocalRpcInvocation&quot;. 跟着这问题在 Flink 的 Issue 列表里看到了一个类似的问题：https://issues.apache.org/jira/browse/FLINK-9056，看下面的评论差不多就是 TaskManager 的 slot 数量不足的原因，导致 job 提交失败。在 Flink 1.63 中已经修复了变成抛出异常了。 竟然知道了是因为 slot 不足的原因了，那么我们就要先了解下 slot 是什么东东呢？不过文章这里先介绍下 parallelism。 什么是 parallelism？ 如翻译这样，parallelism 是并行的意思，在 Flink 里面代表每个任务的并行度，适当的提高并行度可以大大提高 job 的执行效率，比如你的 job 消费 kafka 数据过慢，适当调大可能就消费正常了。 那么在 Flink 中怎么设置并行度呢？ 如何设置 parallelism？ 如上图，在 flink 配置文件中可以查看到默认并行度是 1， 1234cat flink-conf.yaml | grep parallelism# The parallelism used for programs that did not specify and other parallelism.parallelism.default: 1 所以你如何在你的 flink job 里面不设置任何的 parallelism 的话，那么他也会有一个默认的 parallelism = 1。那也意味着你可以修改这个配置文件的默认并行度。 如果你是用命令行启动你的 Flink job，那么你也可以这样设置并行度(使用 -p 并行度)： 1./bin/flink run -p 10 ../word-count.jar 你也可以通过这样来设置你整个程序的并行度： 12StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env.setParallelism(10); 注意：这样设置的并行度是你整个程序的并行度，那么后面如果你的每个算子不单独设置并行度覆盖的话，那么后面每个算子的并行度就都是这里设置的并行度的值了。 如何给每个算子单独设置并行度呢？ 1234data.keyBy(new xxxKey()) .flatMap(new XxxFlatMapFunction()).setParallelism(5) .map(new XxxMapFunction).setParallelism(5) .addSink(new XxxSink()).setParallelism(1) 如上，就是在每个算子后面单独的设置并行度，这样的话，就算你前面设置了 env.setParallelism(10) 也是会被覆盖的。 这也说明优先级是：算子设置并行度 &gt; env 设置并行度 &gt; 配置文件默认并行度 并行度讲到这里应该都懂了，下面 zhisheng 就继续跟你讲讲 什么是 slot？ 什么是 slot？其实什么是 slot 这个问题之前在第一篇文章 《从0到1学习Flink》—— Apache Flink 介绍 中就介绍过了，这里再讲细一点。 图中 Task Manager 是从 Job Manager 处接收需要部署的 Task，任务的并行性由每个 Task Manager 上可用的 slot 决定。每个任务代表分配给任务槽的一组资源，slot 在 Flink 里面可以认为是资源组，Flink 将每个任务分成子任务并且将这些子任务分配到 slot 来并行执行程序。 例如，如果 Task Manager 有四个 slot，那么它将为每个 slot 分配 25％ 的内存。 可以在一个 slot 中运行一个或多个线程。 同一 slot 中的线程共享相同的 JVM。 同一 JVM 中的任务共享 TCP 连接和心跳消息。Task Manager 的一个 Slot 代表一个可用线程，该线程具有固定的内存，注意 Slot 只对内存隔离，没有对 CPU 隔离。默认情况下，Flink 允许子任务共享 Slot，即使它们是不同 task 的 subtask，只要它们来自相同的 job。这种共享可以有更好的资源利用率。 文字说的比较干，zhisheng 这里我就拿下面的图片来讲解： 上面图片中有两个 Task Manager，每个 Task Manager 有三个 slot，这样我们的算子最大并行度那么就可以达到 6 个，在同一个 slot 里面可以执行 1 至多个子任务。 那么再看上面的图片，source/map/keyby/window/apply 最大可以有 6 个并行度，sink 只用了 1 个并行。 每个 Flink TaskManager 在集群中提供 slot。 slot 的数量通常与每个 TaskManager 的可用 CPU 内核数成比例。一般情况下你的 slot 数是你每个 TaskManager 的 cpu 的核数。 但是 flink 配置文件中设置的 task manager 默认的 slot 是 1。 slot 和 parallelism下面给出官方的图片来更加深刻的理解下 slot： 1、slot 是指 taskmanager 的并发执行能力 taskmanager.numberOfTaskSlots:3 每一个 taskmanager 中的分配 3 个 TaskSlot, 3 个 taskmanager 一共有 9 个 TaskSlot。 2、parallelism 是指 taskmanager 实际使用的并发能力 parallelism.default:1 运行程序默认的并行度为 1，9 个 TaskSlot 只用了 1 个，有 8 个空闲。设置合适的并行度才能提高效率。 3、parallelism 是可配置、可指定的 上图中 example2 每个算子设置的并行度是 2， example3 每个算子设置的并行度是 9。 example4 除了 sink 是设置的并行度为 1，其他算子设置的并行度都是 9。 好了，既然并行度和 slot zhisheng 都带大家过了一遍了，那么再来看文章开头的问题：slot 资源不够。 问题原因现在这个问题的答案其实就已经很明显了，就是我们设置的并行度 parallelism 超过了 Task Manager 能提供的最大 slot 数量，所以才会报这个错误。 再来拿我的代码来看吧，当时我就是只设置了整个项目的并行度： 1env.setParallelism(15); 为什么要设置 15 呢，因为我项目消费的 Kafka topic 有 15 个 parttion，就想着让一个并行去消费一个 parttion，没曾想到 Flink 资源的不够，稍微降低下 并行度为 10 后就没出现这个错误了。 总结本文由自己项目生产环境的一个问题来讲解了自己对 Flink parallelism 和 slot 的理解，并告诉大家如何去设置这两个参数，最后也指出了问题的原因所在。 关注我转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/ , 未经允许禁止转载。 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从 0 到 1 学习 —— Flink JobManager 高可用性配置","date":"2019-01-12T16:00:00.000Z","path":"2019/01/13/Flink-JobManager-High-availability/","text":"前言之前在 《从0到1学习Flink》—— Flink 配置文件详解 讲过 Flink 的配置，但是后面陆续有人来问我一些配置相关的东西，在加上我现在对 Flink 也更熟悉了些，这里我就再写下 Flink JobManager 的配置相关信息。 在 《从0到1学习Flink》—— Apache Flink 介绍 一文中介绍过了 Flink Job 的运行架构图： JobManager 协调每个 Flink 作业的部署。它负责调度和资源管理。 默认情况下，每个 Flink 集群都有一个 JobManager 实例。这会产生单点故障（SPOF）：如果 JobManager 崩溃，则无法提交新作业且运行中的作业也会失败。 如果我们使用 JobManager 高可用模式，可以避免这个问题。您可以为 standalone 集群和 YARN 集群配置高可用模式。 standalone 集群高可用性standalone 集群的 JobManager 高可用性的概念是，任何时候都有一个主 JobManager 和 多个备 JobManagers，以便在主节点失败时有新的 JobNamager 接管集群。这样就保证了没有单点故障，一旦备 JobManager 接管集群，作业就可以依旧正常运行。主备 JobManager 实例之间没有明确的区别。每个 JobManager 都可以充当主备节点。 例如，请考虑以下三个 JobManager 实例的设置： 如何配置要启用 JobManager 高可用性功能，您必须将高可用性模式设置为 zookeeper，配置 ZooKeeper quorum，将所有 JobManagers 主机及其 Web UI 端口写入配置文件。 Flink 利用 ZooKeeper 在所有正在运行的 JobManager 实例之间进行分布式协调。ZooKeeper 是独立于 Flink 的服务，通过 leader 选举和轻量级一致性状态存储提供高可靠的分布式协调服务。Flink 包含用于 Bootstrap ZooKeeper 安装的脚本。他在我们的 Flink 安装路径下面 /conf/zoo.cfg 。 Masters 文件要启动 HA 集群，请在以下位置配置 Masters 文件 conf/masters： 12localhost:8081xxx.xxx.xxx.xxx:8081 masters 文件包含启动 JobManagers 的所有主机以及 Web 用户界面绑定的端口，上面一行写一个。 默认情况下，job manager 选一个随机端口作为进程通信端口。您可以通过 high-availability.jobmanager.port 更改此设置。此配置接受单个端口（例如 50010），范围（50000-50025）或两者的组合（50010,50011,50020-50025,50050-50075）。 配置文件 (flink-conf.yaml)要启动 HA 集群，请将以下配置键添加到 conf/flink-conf.yaml： 高可用性模式（必需）：在 conf/flink-conf.yaml中，必须将高可用性模式设置为 zookeeper，以打开高可用模式。 1high-availability: zookeeper ZooKeeper quorum（必需）：ZooKeeper quorum 是一组 ZooKeeper 服务器，它提供分布式协调服务。 1high-availability.zookeeper.quorum: ip1:2181 [,...],ip2:2181 每个 ip:port 都是一个 ZooKeeper 服务器的 ip 及其端口，Flink 可以通过指定的地址和端口访问 zookeeper。 另外就是高可用存储目录，JobManager 元数据保存在文件系统 storageDir 中，在 ZooKeeper 中仅保存了指向此状态的指针, 推荐这个目录是 HDFS, S3, Ceph, nfs 等，该 storageDir 中保存了 JobManager 恢复状态需要的所有元数据。 1high-availability.storageDir: hdfs:///flink/ha/ 配置 master 文件和 ZooKeeper 配置后，您可以使用提供的集群启动脚本。他们将启动 HA 集群。请注意，启动 Flink HA 集群前，必须启动 Zookeeper 集群，并确保为要启动的每个 HA 集群配置单独的 ZooKeeper 根路径。 示例具有 2 个 JobManagers 的 Standalone 集群： 1、在 conf/flink-conf.yaml 中配置高可用模式和 Zookeeper : 123high-availability: zookeeperhigh-availability.zookeeper.quorum: localhost:2181high-availability.storageDir: hdfs:///flink/recovery 2、在 conf/masters 中 配置 masters: 12localhost:8081localhost:8082 3、在 conf/zoo.cfg 中配置 Zookeeper 服务: 1server.0=localhost:2888:3888 4、启动 ZooKeeper 集群: 12$ bin/start-zookeeper-quorum.shStarting zookeeper daemon on host localhost. 5、启动一个 Flink HA 集群: 12345$ bin/start-cluster.shStarting HA cluster with 2 masters and 1 peers in ZooKeeper quorum.Starting jobmanager daemon on host localhost.Starting jobmanager daemon on host localhost.Starting taskmanager daemon on host localhost. 6、停止 ZooKeeper 和集群: 1234567$ bin/stop-cluster.shStopping taskmanager daemon (pid: 7647) on localhost.Stopping jobmanager daemon (pid: 7495) on host localhost.Stopping jobmanager daemon (pid: 7349) on host localhost.$ bin/stop-zookeeper-quorum.shStopping zookeeper daemon (pid: 7101) on host localhost. 上面的执行脚本如下图可见： YARN 集群高可用性当运行高可用的 YARN 集群时，我们不会运行多个 JobManager 实例，而只会运行一个，该 JobManager 实例失败时，YARN 会将其重新启动。Yarn 的具体行为取决于您使用的 YARN 版本。 如何配置？Application Master 最大重试次数 (yarn-site.xml)在 YARN 配置文件 yarn-site.xml 中，需要配置 application master 的最大重试次数： 1234567&lt;property&gt; &lt;name&gt;yarn.resourcemanager.am.max-attempts&lt;/name&gt; &lt;value&gt;4&lt;/value&gt; &lt;description&gt; The maximum number of application master execution attempts. &lt;/description&gt;&lt;/property&gt; 当前 YARN 版本的默认值为 2（表示允许单个 JobManager 失败两次）。 Application Attempts (flink-conf.yaml)除了上面可以配置最大重试次数外，你还可以在 flink-conf.yaml 配置如下： 1yarn.application-attempts: 10 这意味着在如果程序启动失败，YARN 会再重试 9 次（9 次重试 + 1 次启动），如果启动 10 次作业还失败，yarn 才会将该任务的状态置为失败。如果因为节点硬件故障或重启，NodeManager 重新同步等操作，需要 YARN 继续尝试启动应用。这些重启尝试不计入 yarn.application-attempts 个数中。 容器关闭行为 YARN 2.3.0 &lt; 版本 &lt; 2.4.0. 如果 application master 进程失败，则所有的 container 都会重启。 YARN 2.4.0 &lt; 版本 &lt; 2.6.0. TaskManager container 在 application master 故障期间，会继续工作。这具有以下优点：作业恢复时间更快，且缩短所有 task manager 启动时申请资源的时间。 YARN 2.6.0 &lt;= version: 将尝试失败有效性间隔设置为 Flink 的 Akka 超时值。尝试失败有效性间隔表示只有在系统在一个间隔期间看到最大应用程序尝试次数后才会终止应用程序。这避免了持久的工作会耗尽它的应用程序尝试。 示例：高可用的 YARN Session1、配置 HA 模式和 Zookeeper 集群 在 conf/flink-conf.yaml:123high-availability: zookeeperhigh-availability.zookeeper.quorum: localhost:2181yarn.application-attempts: 10 2、配置 ZooKeeper 服务 在 conf/zoo.cfg：1server.0=localhost:2888:3888 3、启动 Zookeeper 集群:12$ bin/start-zookeeper-quorum.shStarting zookeeper daemon on host localhost. 4、启动 HA 集群:1$ bin/yarn-session.sh -n 2 总结本篇文章再次写了下 Flink JobManager 的高可用配置，如何在 standalone 集群和 YARN 集群中配置高可用。 关注我转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/ , 未经允许禁止转载。 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从 0 到 1 学习 —— Flink 写入数据到 Kafka","date":"2019-01-05T16:00:00.000Z","path":"2019/01/06/Flink-Kafka-sink/","text":"前言之前文章 《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch 写了如何将 Kafka 中的数据存储到 ElasticSearch 中，里面其实就已经用到了 Flink 自带的 Kafka source connector（FlinkKafkaConsumer）。存入到 ES 只是其中一种情况，那么如果我们有多个地方需要这份通过 Flink 转换后的数据，是不是又要我们继续写个 sink 的插件呢？确实，所以 Flink 里面就默认支持了不少 sink，比如也支持 Kafka sink connector（FlinkKafkaProducer），那么这篇文章我们就讲讲如何将数据写入到 Kafka。 准备添加依赖Flink 里面支持 Kafka 0.8、0.9、0.10、0.11 ，以后有时间可以分析下源码的实现。 这里我们需要安装下 Kafka，请对应添加对应的 Flink Kafka connector 依赖的版本，这里我们使用的是 0.11 版本： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka-0.11_2.11&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt; Kafka 安装这里就不写这块内容了，可以参考我以前的文章 Kafka 安装及快速入门。 这里我们演示把其他 Kafka 集群中 topic 数据原样写入到自己本地起的 Kafka 中去。 配置文件12345678910kafka.brokers=xxx:9092,xxx:9092,xxx:9092kafka.group.id=metrics-group-testkafka.zookeeper.connect=xxx:2181metrics.topic=xxxstream.parallelism=5kafka.sink.brokers=localhost:9092kafka.sink.topic=metric-teststream.checkpoint.interval=1000stream.checkpoint.enable=falsestream.sink.parallelism=5 目前我们先看下本地 Kafka 是否有这个 metric-test topic 呢？需要执行下这个命令： 1bin/kafka-topics.sh --list --zookeeper localhost:2181 可以看到本地的 Kafka 是没有任何 topic 的，如果等下我们的程序运行起来后，再次执行这个命令出现 metric-test topic，那么证明我的程序确实起作用了，已经将其他集群的 Kafka 数据写入到本地 Kafka 了。 程序代码Main.java 12345678910111213141516public class Main &#123; public static void main(String[] args) throws Exception&#123; final ParameterTool parameterTool = ExecutionEnvUtil.createParameterTool(args); StreamExecutionEnvironment env = ExecutionEnvUtil.prepare(parameterTool); DataStreamSource&lt;Metrics&gt; data = KafkaConfigUtil.buildSource(env); data.addSink(new FlinkKafkaProducer011&lt;Metrics&gt;( parameterTool.get(\"kafka.sink.brokers\"), parameterTool.get(\"kafka.sink.topic\"), new MetricSchema() )).name(\"flink-connectors-kafka\") .setParallelism(parameterTool.getInt(\"stream.sink.parallelism\")); env.execute(\"flink learning connectors kafka\"); &#125;&#125; 运行结果启动程序，查看运行结果，不段执行上面命令，查看是否有新的 topic 出来： 执行命令可以查看该 topic 的信息： 1bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic metric-test 分析上面代码我们使用 Flink Kafka Producer 只传了三个参数：brokerList、topicId、serializationSchema（序列化） 其实也可以传入多个参数进去，现在有的参数用的是默认参数，因为这个内容比较多，后面可以抽出一篇文章单独来讲。 总结本篇文章写了 Flink 读取其他 Kafka 集群的数据，然后写入到本地的 Kafka 上。我在 Flink 这层没做什么数据转换，只是原样的将数据转发了下，如果你们有什么其他的需求，是可以在 Flink 这层将数据进行各种转换操作，比如这篇文章中的一些转换：《从0到1学习Flink》—— Flink Data transformation(转换)，然后将转换后的数据发到 Kafka 上去。 本文原创地址是: http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/ , 未经允许禁止转载。 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"},{"name":"Kafka","slug":"Kafka","permalink":"http://www.54tianzhisheng.cn/tags/Kafka/"}]},{"title":"Flink 从 0 到 1 学习 —— Flink 项目如何运行？","date":"2019-01-04T16:00:00.000Z","path":"2019/01/05/Flink-run/","text":"前言之前写了不少 Flink 文章了，也有不少 demo，但是文章写的时候都是在本地直接运行 Main 类的 main 方法，其实 Flink 是支持在 UI 上上传 Flink Job 的 jar 包，然后运行得。最开始在第一篇 《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 中其实提到过了 Flink 自带的 UI 界面，今天我们就来看看如何将我们的项目打包在这里发布运行。 准备编译打包项目代码就拿我之前的文章 《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch 吧，代码地址是在 GitHub 仓库地址：https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-es6 ，如果感兴趣的可以直接拿来打包试试水。 我们在整个项目 （flink-learning）pom.xml 所在文件夹执行以下命令打包： 1mvn clean install 然后你会发现在 flink-learning-connectors-es6 的 target 目录下有 flink-learning-connectors-es6-1.0-SNAPSHOT.jar 。 启动 ES注意你的 Kafka 数据源和 ES 都已经启动好了, 清空了下 ES 目录下的 data 数据，为了就是查看是不是真的有数据存入进来了。 提交 jar 包将此文件提交到 Flinkserver 上，如下图： 点击下图红框中的”Upload”按钮： 如下图，选中刚刚上传的文件，填写类名，再点击”Submit”按钮即可启动 Job： 查看运行结果如下图，在 Overview 页面可见正在运行的任务： 你可以看到 Task Manager 中关于任务的 metric 数据、日志信息以及 Stdout 信息。 查看 Kibana ，此时 ES 中已经有数据了： 我们可以在 flink ui 界面上的 overview cancel 这个 job，那么可以看到 job 的日志： 总结本篇文章写了下如何将我们的 job 编译打包并提交到 Flink 自带到 Server UI 上面去运行，也算是对前面文章的一个补充，当然了，Flink job 不仅支持这种模式的运行，它还可以运行在 K8s，Mesos，等上面，等以后我接触到再写写。 本文原创地址是: http://www.54tianzhisheng.cn/2019/01/05/Flink-run/ , 未经允许禁止转载。 关注我微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从 0 到 1 学习 —— Flink 写入数据到 ElasticSearch","date":"2018-12-29T16:00:00.000Z","path":"2018/12/30/Flink-ElasticSearch-Sink/","text":"前言前面 FLink 的文章中我们已经介绍了说 Flink 已经有很多自带的 Connector。 1、《从0到1学习Flink》—— Data Source 介绍 2、《从0到1学习Flink》—— Data Sink 介绍 其中包括了 Source 和 Sink 的，后面我也讲了下如何自定义自己的 Source 和 Sink。 那么今天要做的事情是啥呢？就是介绍一下 Flink 自带的 ElasticSearch Connector，我们今天就用他来做 Sink，将 Kafka 中的数据经过 Flink 处理后然后存储到 ElasticSearch。 准备安装 ElasticSearch，这里就忽略，自己找我以前的文章，建议安装 ElasticSearch 6.0 版本以上的，毕竟要跟上时代的节奏。 下面就讲解一下生产环境中如何使用 Elasticsearch Sink 以及一些注意点，及其内部实现机制。 Elasticsearch Sink添加依赖12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-elasticsearch6_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt; 上面这依赖版本号请自己根据使用的版本对应改变下。 下面所有的代码都没有把 import 引入到这里来，如果需要查看更详细的代码，请查看我的 GitHub 仓库地址： https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-es6 这个 module 含有本文的所有代码实现，当然越写到后面自己可能会做一些抽象，所以如果有代码改变很正常，请直接查看全部项目代码。 ElasticSearchSinkUtil 工具类这个工具类是自己封装的，getEsAddresses 方法将传入的配置文件 es 地址解析出来，可以是域名方式，也可以是 ip + port 形式。addSink 方法是利用了 Flink 自带的 ElasticsearchSink 来封装了一层，传入了一些必要的调优参数和 es 配置参数，下面文章还会再讲些其他的配置。 ElasticSearchSinkUtil.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class ElasticSearchSinkUtil &#123; /** * es sink * * @param hosts es hosts * @param bulkFlushMaxActions bulk flush size * @param parallelism 并行数 * @param data 数据 * @param func * @param &lt;T&gt; */ public static &lt;T&gt; void addSink(List&lt;HttpHost&gt; hosts, int bulkFlushMaxActions, int parallelism, SingleOutputStreamOperator&lt;T&gt; data, ElasticsearchSinkFunction&lt;T&gt; func) &#123; ElasticsearchSink.Builder&lt;T&gt; esSinkBuilder = new ElasticsearchSink.Builder&lt;&gt;(hosts, func); esSinkBuilder.setBulkFlushMaxActions(bulkFlushMaxActions); data.addSink(esSinkBuilder.build()).setParallelism(parallelism); &#125; /** * 解析配置文件的 es hosts * * @param hosts * @return * @throws MalformedURLException */ public static List&lt;HttpHost&gt; getEsAddresses(String hosts) throws MalformedURLException &#123; String[] hostList = hosts.split(\",\"); List&lt;HttpHost&gt; addresses = new ArrayList&lt;&gt;(); for (String host : hostList) &#123; if (host.startsWith(\"http\")) &#123; URL url = new URL(host); addresses.add(new HttpHost(url.getHost(), url.getPort())); &#125; else &#123; String[] parts = host.split(\":\", 2); if (parts.length &gt; 1) &#123; addresses.add(new HttpHost(parts[0], Integer.parseInt(parts[1]))); &#125; else &#123; throw new MalformedURLException(\"invalid elasticsearch hosts format\"); &#125; &#125; &#125; return addresses; &#125;&#125; Main 启动类Main.java 123456789101112131415161718192021222324252627public class Main &#123; public static void main(String[] args) throws Exception &#123; //获取所有参数 final ParameterTool parameterTool = ExecutionEnvUtil.createParameterTool(args); //准备好环境 StreamExecutionEnvironment env = ExecutionEnvUtil.prepare(parameterTool); //从kafka读取数据 DataStreamSource&lt;Metrics&gt; data = KafkaConfigUtil.buildSource(env); //从配置文件中读取 es 的地址 List&lt;HttpHost&gt; esAddresses = ElasticSearchSinkUtil.getEsAddresses(parameterTool.get(ELASTICSEARCH_HOSTS)); //从配置文件中读取 bulk flush size，代表一次批处理的数量，这个可是性能调优参数，特别提醒 int bulkSize = parameterTool.getInt(ELASTICSEARCH_BULK_FLUSH_MAX_ACTIONS, 40); //从配置文件中读取并行 sink 数，这个也是性能调优参数，特别提醒，这样才能够更快的消费，防止 kafka 数据堆积 int sinkParallelism = parameterTool.getInt(STREAM_SINK_PARALLELISM, 5); //自己再自带的 es sink 上一层封装了下 ElasticSearchSinkUtil.addSink(esAddresses, bulkSize, sinkParallelism, data, (Metrics metric, RuntimeContext runtimeContext, RequestIndexer requestIndexer) -&gt; &#123; requestIndexer.add(Requests.indexRequest() .index(ZHISHENG + \"_\" + metric.getName()) //es 索引名 .type(ZHISHENG) //es type .source(GsonUtil.toJSONBytes(metric), XContentType.JSON)); &#125;); env.execute(\"flink learning connectors es6\"); &#125;&#125; 配置文件配置都支持集群模式填写，注意用 , 分隔！ 12345678910kafka.brokers=localhost:9092kafka.group.id=zhisheng-metrics-group-testkafka.zookeeper.connect=localhost:2181metrics.topic=zhisheng-metricsstream.parallelism=5stream.checkpoint.interval=1000stream.checkpoint.enable=falseelasticsearch.hosts=localhost:9200elasticsearch.bulk.flush.max.actions=40stream.sink.parallelism=5 运行结果执行 Main 类的 main 方法，我们的程序是只打印 flink 的日志，没有打印存入的日志（因为我们这里没有打日志）： 所以看起来不知道我们的 sink 是否有用，数据是否从 kafka 读取出来后存入到 es 了。 你可以查看下本地起的 es 终端或者服务器的 es 日志就可以看到效果了。 es 日志如下： 上图是我本地 Mac 电脑终端的 es 日志，可以看到我们的索引了。 如果还不放心，你也可以在你的电脑装个 kibana，然后更加的直观查看下 es 的索引情况（或者直接敲 es 的命令） 我们用 kibana 查看存入 es 的索引如下： 程序执行了一会，存入 es 的数据量就很大了。 扩展配置上面代码已经可以实现你的大部分场景了，但是如果你的业务场景需要保证数据的完整性（不能出现丢数据的情况），那么就需要添加一些重试策略，因为在我们的生产环境中，很有可能会因为某些组件不稳定性导致各种问题，所以这里我们就要在数据存入失败的时候做重试操作，这里 flink 自带的 es sink 就支持了，常用的失败重试配置有: 123456789101112131、bulk.flush.backoff.enable 用来表示是否开启重试机制2、bulk.flush.backoff.type 重试策略，有两种：EXPONENTIAL 指数型（表示多次重试之间的时间间隔按照指数方式进行增长）、CONSTANT 常数型（表示多次重试之间的时间间隔为固定常数）3、bulk.flush.backoff.delay 进行重试的时间间隔4、bulk.flush.backoff.retries 失败重试的次数5、bulk.flush.max.actions: 批量写入时的最大写入条数6、bulk.flush.max.size.mb: 批量写入时的最大数据量7、bulk.flush.interval.ms: 批量写入的时间间隔，配置后则会按照该时间间隔严格执行，无视上面的两个批量写入配置 看下啦，就是如下这些配置了，如果你需要的话，可以在这个地方配置扩充了。 FailureHandler 失败处理器写入 ES 的时候会有这些情况会导致写入 ES 失败： 1、ES 集群队列满了，报如下错误 112:08:07.326 [I/O dispatcher 13] ERROR o.a.f.s.c.e.ElasticsearchSinkBase - Failed Elasticsearch item request: ElasticsearchException[Elasticsearch exception [type=es_rejected_execution_exception, reason=rejected execution of org.elasticsearch.transport.TransportService$7@566c9379 on EsThreadPoolExecutor[name = node-1/write, queue capacity = 200, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@f00b373[Running, pool size = 4, active threads = 4, queued tasks = 200, completed tasks = 6277]]]] 是这样的，我电脑安装的 es 队列容量默认应该是 200，我没有修改过。我这里如果配置的 bulk flush size * 并发 sink 数量 这个值如果大于这个 queue capacity ，那么就很容易导致出现这种因为 es 队列满了而写入失败。 当然这里你也可以通过调大点 es 的队列。参考：https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-threadpool.html 2、ES 集群某个节点挂了 这个就不用说了，肯定写入失败的。跟过源码可以发现 RestClient 类里的 performRequestAsync 方法一开始会随机的从集群中的某个节点进行写入数据，如果这台机器掉线，会进行重试在其他的机器上写入，那么当时写入的这台机器的请求就需要进行失败重试，否则就会把数据丢失！ 3、ES 集群某个节点的磁盘满了 这里说的磁盘满了，并不是磁盘真的就没有一点剩余空间的，是 es 会在写入的时候检查磁盘的使用情况，在 85% 的时候会打印日志警告。 这里我看了下源码如下图： 如果你想继续让 es 写入的话就需要去重新配一下 es 让它继续写入，或者你也可以清空些不必要的数据腾出磁盘空间来。 解决方法123456789101112131415161718192021222324DataStream&lt;String&gt; input = ...;input.addSink(new ElasticsearchSink&lt;&gt;( config, transportAddresses, new ElasticsearchSinkFunction&lt;String&gt;() &#123;...&#125;, new ActionRequestFailureHandler() &#123; @Override void onFailure(ActionRequest action, Throwable failure, int restStatusCode, RequestIndexer indexer) throw Throwable &#123; if (ExceptionUtils.containsThrowable(failure, EsRejectedExecutionException.class)) &#123; // full queue; re-add document for indexing indexer.add(action); &#125; else if (ExceptionUtils.containsThrowable(failure, ElasticsearchParseException.class)) &#123; // malformed document; simply drop request without failing sink &#125; else &#123; // for all other failures, fail the sink // here the failure is simply rethrown, but users can also choose to throw custom exceptions throw failure; &#125; &#125;&#125;)); 如果仅仅只是想做失败重试，也可以直接使用官方提供的默认的 RetryRejectedExecutionFailureHandler ，该处理器会对 EsRejectedExecutionException 导致到失败写入做重试处理。如果你没有设置失败处理器(failure handler)，那么就会使用默认的 NoOpFailureHandler 来简单处理所有的异常。 总结本文写了 Flink connector es，将 Kafka 中的数据读取并存储到 ElasticSearch 中，文中讲了如何封装自带的 sink，然后一些扩展配置以及 FailureHandler 情况下要怎么处理。（这个问题可是线上很容易遇到的） 关注我转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/ 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://www.54tianzhisheng.cn/tags/ElasticSearch/"},{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从 0 到 1 学习 —— Flink 中几种 Time 详解","date":"2018-12-10T16:00:00.000Z","path":"2018/12/11/Flink-time/","text":"前言Flink 在流程序中支持不同的 Time 概念，就比如有 Processing Time、Event Time 和 Ingestion Time。 下面我们一起来看看这几个 Time： Processing TimeProcessing Time 是指事件被处理时机器的系统时间。 当流程序在 Processing Time 上运行时，所有基于时间的操作(如时间窗口)将使用当时机器的系统时间。每小时 Processing Time 窗口将包括在系统时钟指示整个小时之间到达特定操作的所有事件。 例如，如果应用程序在上午 9:15 开始运行，则第一个每小时 Processing Time 窗口将包括在上午 9:15 到上午 10:00 之间处理的事件，下一个窗口将包括在上午 10:00 到 11:00 之间处理的事件。 Processing Time 是最简单的 “Time” 概念，不需要流和机器之间的协调，它提供了最好的性能和最低的延迟。但是，在分布式和异步的环境下，Processing Time 不能提供确定性，因为它容易受到事件到达系统的速度（例如从消息队列）、事件在系统内操作流动的速度以及中断的影响。 Event TimeEvent Time 是事件发生的时间，一般就是数据本身携带的时间。这个时间通常是在事件到达 Flink 之前就确定的，并且可以从每个事件中获取到事件时间戳。在 Event Time 中，时间取决于数据，而跟其他没什么关系。Event Time 程序必须指定如何生成 Event Time 水印，这是表示 Event Time 进度的机制。 完美的说，无论事件什么时候到达或者其怎么排序，最后处理 Event Time 将产生完全一致和确定的结果。但是，除非事件按照已知顺序（按照事件的时间）到达，否则处理 Event Time 时将会因为要等待一些无序事件而产生一些延迟。由于只能等待一段有限的时间，因此就难以保证处理 Event Time 将产生完全一致和确定的结果。 假设所有数据都已到达， Event Time 操作将按照预期运行，即使在处理无序事件、延迟事件、重新处理历史数据时也会产生正确且一致的结果。 例如，每小时事件时间窗口将包含带有落入该小时的事件时间戳的所有记录，无论它们到达的顺序如何。 请注意，有时当 Event Time 程序实时处理实时数据时，它们将使用一些 Processing Time 操作，以确保它们及时进行。 Ingestion TimeIngestion Time 是事件进入 Flink 的时间。 在源操作处，每个事件将源的当前时间作为时间戳，并且基于时间的操作（如时间窗口）会利用这个时间戳。 Ingestion Time 在概念上位于 Event Time 和 Processing Time 之间。 与 Processing Time 相比，它稍微贵一些，但结果更可预测。因为 Ingestion Time 使用稳定的时间戳（在源处分配一次），所以对事件的不同窗口操作将引用相同的时间戳，而在 Processing Time 中，每个窗口操作符可以将事件分配给不同的窗口（基于机器系统时间和到达延迟）。 与 Event Time 相比，Ingestion Time 程序无法处理任何无序事件或延迟数据，但程序不必指定如何生成水印。 在 Flink 中，，Ingestion Time 与 Event Time 非常相似，但 Ingestion Time 具有自动分配时间戳和自动生成水印功能。 说了这么多概念比较干涩，下面直接看图： 设定时间特性Flink DataStream 程序的第一部分通常是设置基本时间特性。 该设置定义了数据流源的行为方式（例如：它们是否将分配时间戳），以及像 KeyedStream.timeWindow(Time.seconds(30)) 这样的窗口操作应该使用上面哪种时间概念。 以下示例显示了一个 Flink 程序，该程序在每小时时间窗口中聚合事件。 123456789101112131415final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime);// 其他// env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);// env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);DataStream&lt;MyEvent&gt; stream = env.addSource(new FlinkKafkaConsumer09&lt;MyEvent&gt;(topic, schema, props));stream .keyBy( (event) -&gt; event.getUser() ) .timeWindow(Time.hours(1)) .reduce( (a, b) -&gt; a.add(b) ) .addSink(...); Event Time 和 Watermarks注意：Flink 实现了数据流模型中的许多技术。有关 Event Time 和 Watermarks 的详细介绍，请查看以下文章： https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101 https://research.google.com/pubs/archive/43864.pdf 支持 Event Time 的流处理器需要一种方法来衡量 Event Time 的进度。 例如，当 Event Time 超过一小时结束时，需要通知构建每小时窗口的窗口操作符，以便操作员可以关闭正在进行的窗口。 Event Time 可以独立于 Processing Time 进行。 例如，在一个程序中，操作员的当前 Event Time 可能略微落后于 Processing Time （考虑到接收事件的延迟），而两者都以相同的速度进行。另一方面，另一个流程序可能只需要几秒钟的时间就可以处理完 Kafka Topic 中数周的 Event Time 数据。 Flink 中用于衡量 Event Time 进度的机制是 Watermarks。 Watermarks 作为数据流的一部分流动并带有时间戳 t。 Watermark（t）声明 Event Time 已到达该流中的时间 t，这意味着流中不应再有具有时间戳 t’&lt;= t 的元素（即时间戳大于或等于水印的事件） 下图显示了带有(逻辑)时间戳和内联水印的事件流。在本例中，事件是按顺序排列的(相对于它们的时间戳)，这意味着水印只是流中的周期性标记。 Watermark 对于无序流是至关重要的，如下所示，其中事件不按时间戳排序。通常，Watermark 是一种声明，通过流中的该点，到达某个时间戳的所有事件都应该到达。一旦水印到达操作员，操作员就可以将其内部事件时间提前到水印的值。 平行流中的水印水印是在源函数处生成的，或直接在源函数之后生成的。源函数的每个并行子任务通常独立生成其水印。这些水印定义了特定并行源处的事件时间。 当水印通过流程序时，它们会提前到达操作人员处的事件时间。当一个操作符提前它的事件时间时，它为它的后续操作符在下游生成一个新的水印。 一些操作员消耗多个输入流; 例如，一个 union，或者跟随 keyBy（…）或 partition（…）函数的运算符。 这样的操作员当前事件时间是其输入流的事件时间的最小值。 由于其输入流更新其事件时间，因此操作员也是如此。 下图显示了流经并行流的事件和水印的示例，以及跟踪事件时间的运算符。 参考https://github.com/zhisheng17/flink/blob/feature%2Fzhisheng_release_1.6/docs/dev/event_time.md 关注我转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/12/11/Flink-time/ 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从 0 到 1 学习 —— 介绍Flink中的Stream Windows","date":"2018-12-07T16:00:00.000Z","path":"2018/12/08/Flink-Stream-Windows/","text":"前言目前有许多数据分析的场景从批处理到流处理的演变， 虽然可以将批处理作为流处理的特殊情况来处理，但是分析无穷集的流数据通常需要思维方式的转变并且具有其自己的术语（例如，“windowing（窗口化）”、“at-least-once（至少一次）”、“exactly-once（只有一次）” ）。 对于刚刚接触流处理的人来说，这种转变和新术语可能会非常混乱。 Apache Flink 是一个为生产环境而生的流处理器，具有易于使用的 API，可以用于定义高级流分析程序。 Flink 的 API 在数据流上具有非常灵活的窗口定义，使其在其他开源流处理框架中脱颖而出。 在这篇文章中，我们将讨论用于流处理的窗口的概念，介绍 Flink 的内置窗口，并解释它对自定义窗口语义的支持。 什么是 Windows？下面我们结合一个现实的例子来说明。 就拿交通传感器的示例：统计经过某红绿灯的汽车数量之和？ 假设在一个红绿灯处，我们每隔 15 秒统计一次通过此红绿灯的汽车数量，如下图： 可以把汽车的经过看成一个流，无穷的流，不断有汽车经过此红绿灯，因此无法统计总共的汽车数量。但是，我们可以换一种思路，每隔 15 秒，我们都将与上一次的结果进行 sum 操作（滑动聚合），如下： 这个结果似乎还是无法回答我们的问题，根本原因在于流是无界的，我们不能限制流，但可以在有一个有界的范围内处理无界的流数据。 因此，我们需要换一个问题的提法：每分钟经过某红绿灯的汽车数量之和？这个问题，就相当于一个定义了一个 Window（窗口），window 的界限是1分钟，且每分钟内的数据互不干扰，因此也可以称为翻滚（不重合）窗口，如下图： 第一分钟的数量为8，第二分钟是22，第三分钟是27。。。这样，1个小时内会有60个window。 再考虑一种情况，每30秒统计一次过去1分钟的汽车数量之和： 此时，window 出现了重合。这样，1个小时内会有120个 window。 扩展一下，我们可以在某个地区，收集每一个红绿灯处汽车经过的数量，然后每个红绿灯处都做一次基于1分钟的window统计，即并行处理： 它有什么作用？通常来讲，Window 就是用来对一个无限的流设置一个有限的集合，在有界的数据集上进行操作的一种机制。window 又可以分为基于时间（Time-based）的 window 以及基于数量（Count-based）的 window。 Flink 自带的 windowFlink DataStream API 提供了 Time 和 Count 的 window，同时增加了基于 Session 的 window。同时，由于某些特殊的需要，DataStream API 也提供了定制化的 window 操作，供用户自定义 window。 下面，主要介绍 Time-Based window 以及 Count-Based window，以及自定义的 window 操作，Session-Based Window 操作将会在后续的文章中讲到。 Time Windows正如命名那样，Time Windows 根据时间来聚合流数据。例如：一分钟的 tumbling time window 收集一分钟的元素，并在一分钟过后对窗口中的所有元素应用于一个函数。 在 Flink 中定义 tumbling time windows(翻滚时间窗口) 和 sliding time windows(滑动时间窗口) 非常简单： tumbling time windows(翻滚时间窗口) 输入一个时间参数 123data.keyBy(1) .timeWindow(Time.minutes(1)) //tumbling time window 每分钟统计一次数量和 .sum(1); sliding time windows(滑动时间窗口) 输入两个时间参数 123data.keyBy(1) .timeWindow(Time.minutes(1), Time.seconds(30)) //sliding time window 每隔 30s 统计过去一分钟的数量和 .sum(1); 有一点我们还没有讨论，即“收集一分钟的元素”的确切含义，它可以归结为一个问题，“流处理器如何解释时间?” Apache Flink 具有三个不同的时间概念，即 processing time, event time 和 ingestion time。 这里可以参考我下一篇文章： 《从0到1学习Flink》—— 介绍Flink中的Event Time、Processing Time和Ingestion Time Count WindowsApache Flink 还提供计数窗口功能。如果计数窗口设置的为 100 ，那么将会在窗口中收集 100 个事件，并在添加第 100 个元素时计算窗口的值。 在 Flink 的 DataStream API 中，tumbling count window 和 sliding count window 的定义如下: tumbling count window 输入一个时间参数 123data.keyBy(1) .countWindow(100) //统计每 100 个元素的数量之和 .sum(1); sliding count window 输入两个时间参数 123data.keyBy(1) .countWindow(100, 10) //每 10 个元素统计过去 100 个元素的数量之和 .sum(1); 解剖 Flink 的窗口机制Flink 的内置 time window 和 count window 已经覆盖了大多数应用场景，但是有时候也需要定制窗口逻辑，此时 Flink 的内置的 window 无法解决这些问题。为了还支持自定义 window 实现不同的逻辑，DataStream API 为其窗口机制提供了接口。 下图描述了 Flink 的窗口机制，并介绍了所涉及的组件： 到达窗口操作符的元素被传递给 WindowAssigner。WindowAssigner 将元素分配给一个或多个窗口，可能会创建新的窗口。窗口本身只是元素列表的标识符，它可能提供一些可选的元信息，例如 TimeWindow 中的开始和结束时间。注意，元素可以被添加到多个窗口，这也意味着一个元素可以同时在多个窗口存在。 每个窗口都拥有一个 Trigger(触发器)，该 Trigger(触发器) 决定何时计算和清除窗口。当先前注册的计时器超时时，将为插入窗口的每个元素调用触发器。在每个事件上，触发器都可以决定触发(即、清除(删除窗口并丢弃其内容)，或者启动并清除窗口。一个窗口可以被求值多次，并且在被清除之前一直存在。注意，在清除窗口之前，窗口将一直消耗内存。 当 Trigger(触发器) 触发时，可以将窗口元素列表提供给可选的 Evictor，Evictor 可以遍历窗口元素列表，并可以决定从列表的开头删除首先进入窗口的一些元素。然后其余的元素被赋给一个计算函数，如果没有定义 Evictor，触发器直接将所有窗口元素交给计算函数。 计算函数接收 Evictor 过滤后的窗口元素，并计算窗口的一个或多个元素的结果。 DataStream API 接受不同类型的计算函数，包括预定义的聚合函数，如 sum（），min（），max（），以及 ReduceFunction，FoldFunction 或 WindowFunction。 这些是构成 Flink 窗口机制的组件。 接下来我们逐步演示如何使用 DataStream API 实现自定义窗口逻辑。 我们从 DataStream [IN] 类型的流开始，并使用 key 选择器函数对其分组，该函数将 key 相同类型的数据分组在一块。 12SingleOutputStreamOperator&lt;xxx&gt; data = env.addSource(...);data.keyBy() 如何自定义 Window？1、Window Assigner 负责将元素分配到不同的 window。 Window API 提供了自定义的 WindowAssigner 接口，我们可以实现 WindowAssigner 的 1public abstract Collection&lt;W&gt; assignWindows(T element, long timestamp) 方法。同时，对于基于 Count 的 window 而言，默认采用了 GlobalWindow 的 window assigner，例如： 1keyBy.window(GlobalWindows.create()) 2、Trigger Trigger 即触发器，定义何时或什么情况下移除 window 我们可以指定触发器来覆盖 WindowAssigner 提供的默认触发器。 请注意，指定的触发器不会添加其他触发条件，但会替换当前触发器。 3、Evictor（可选） 驱逐者，即保留上一 window 留下的某些元素 4、通过 apply WindowFunction 来返回 DataStream 类型数据。 利用 Flink 的内部窗口机制和 DataStream API 可以实现自定义的窗口逻辑，例如 session window。 结论对于现代流处理器来说，支持连续数据流上的各种类型的窗口是必不可少的。 Apache Flink 是一个具有强大功能集的流处理器，包括一个非常灵活的机制，可以在连续数据流上构建窗口。 Flink 为常见场景提供内置的窗口运算符，以及允许用户自定义窗口逻辑。 参考1、https://flink.apache.org/news/2015/12/04/Introducing-windows.html 2、https://blog.csdn.net/lmalds/article/details/51604501 关注我转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/ 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从 0 到 1 学习 —— Flink Data transformation(转换)","date":"2018-11-03T16:00:00.000Z","path":"2018/11/04/Flink-Data-transformation/","text":"前言在第一篇介绍 Flink 的文章 《《从0到1学习Flink》—— Apache Flink 介绍》 中就说过 Flink 程序的结构 Flink 应用程序结构就是如上图所示： 1、Source: 数据源，Flink 在流处理和批处理上的 source 大概有 4 类：基于本地集合的 source、基于文件的 source、基于网络套接字的 source、自定义的 source。自定义的 source 常见的有 Apache kafka、Amazon Kinesis Streams、RabbitMQ、Twitter Streaming API、Apache NiFi 等，当然你也可以定义自己的 source。 2、Transformation：数据转换的各种操作，有 Map / FlatMap / Filter / KeyBy / Reduce / Fold / Aggregations / Window / WindowAll / Union / Window join / Split / Select / Project 等，操作很多，可以将数据转换计算成你想要的数据。 3、Sink：接收器，Flink 将转换计算后的数据发送的地点 ，你可能需要存储下来，Flink 常见的 Sink 大概有如下几类：写入文件、打印出来、写入 socket 、自定义的 sink 。自定义的 sink 常见的有 Apache kafka、RabbitMQ、MySQL、ElasticSearch、Apache Cassandra、Hadoop FileSystem 等，同理你也可以定义自己的 Sink。 在上四篇文章介绍了 Source 和 Sink： 1、《从0到1学习Flink》—— Data Source 介绍 2、《从0到1学习Flink》—— 如何自定义 Data Source ？ 3、《从0到1学习Flink》—— Data Sink 介绍 4、《从0到1学习Flink》—— 如何自定义 Data Sink ？ 那么这篇文章我们就来看下 Flink Data Transformation 吧，数据转换操作还是蛮多的，需要好好讲讲！ TransformationMap这是最简单的转换之一，其中输入是一个数据流，输出的也是一个数据流： 还是拿上一篇文章的案例来将数据进行 map 转换操作： 123456789101112SingleOutputStreamOperator&lt;Student&gt; map = student.map(new MapFunction&lt;Student, Student&gt;() &#123; @Override public Student map(Student value) throws Exception &#123; Student s1 = new Student(); s1.id = value.id; s1.name = value.name; s1.password = value.password; s1.age = value.age + 5; return s1; &#125;&#125;);map.print(); 将每个人的年龄都增加 5 岁，其他不变。 FlatMapFlatMap 采用一条记录并输出零个，一个或多个记录。 123456789SingleOutputStreamOperator&lt;Student&gt; flatMap = student.flatMap(new FlatMapFunction&lt;Student, Student&gt;() &#123; @Override public void flatMap(Student value, Collector&lt;Student&gt; out) throws Exception &#123; if (value.id % 2 == 0) &#123; out.collect(value); &#125; &#125;&#125;);flatMap.print(); 这里将 id 为偶数的聚集出来。 FilterFilter 函数根据条件判断出结果。 12345678910SingleOutputStreamOperator&lt;Student&gt; filter = student.filter(new FilterFunction&lt;Student&gt;() &#123; @Override public boolean filter(Student value) throws Exception &#123; if (value.id &gt; 95) &#123; return true; &#125; return false; &#125;&#125;);filter.print(); 这里将 id 大于 95 的过滤出来，然后打印出来。 KeyByKeyBy 在逻辑上是基于 key 对流进行分区。在内部，它使用 hash 函数对流进行分区。它返回 KeyedDataStream 数据流。 1234567KeyedStream&lt;Student, Integer&gt; keyBy = student.keyBy(new KeySelector&lt;Student, Integer&gt;() &#123; @Override public Integer getKey(Student value) throws Exception &#123; return value.age; &#125;&#125;);keyBy.print(); 上面对 student 的 age 做 KeyBy 操作分区 ReduceReduce 返回单个的结果值，并且 reduce 操作每处理一个元素总是创建一个新值。常用的方法有 average, sum, min, max, count，使用 reduce 方法都可实现。 1234567891011121314151617SingleOutputStreamOperator&lt;Student&gt; reduce = student.keyBy(new KeySelector&lt;Student, Integer&gt;() &#123; @Override public Integer getKey(Student value) throws Exception &#123; return value.age; &#125;&#125;).reduce(new ReduceFunction&lt;Student&gt;() &#123; @Override public Student reduce(Student value1, Student value2) throws Exception &#123; Student student1 = new Student(); student1.name = value1.name + value2.name; student1.id = (value1.id + value2.id) / 2; student1.password = value1.password + value2.password; student1.age = (value1.age + value2.age) / 2; return student1; &#125;&#125;);reduce.print(); 上面先将数据流进行 keyby 操作，因为执行 reduce 操作只能是 KeyedStream，然后将 student 对象的 age 做了一个求平均值的操作。 FoldFold 通过将最后一个文件夹流与当前记录组合来推出 KeyedStream。 它会发回数据流。 123456KeyedStream.fold(\"1\", new FoldFunction&lt;Integer, String&gt;() &#123; @Override public String fold(String accumulator, Integer value) throws Exception &#123; return accumulator + \"=\" + value; &#125;&#125;) AggregationsDataStream API 支持各种聚合，例如 min，max，sum 等。 这些函数可以应用于 KeyedStream 以获得 Aggregations 聚合。 12345678910KeyedStream.sum(0) KeyedStream.sum(\"key\") KeyedStream.min(0) KeyedStream.min(\"key\") KeyedStream.max(0) KeyedStream.max(\"key\") KeyedStream.minBy(0) KeyedStream.minBy(\"key\") KeyedStream.maxBy(0) KeyedStream.maxBy(\"key\") max 和 maxBy 之间的区别在于 max 返回流中的最大值，但 maxBy 返回具有最大值的键， min 和 minBy 同理。 WindowWindow 函数允许按时间或其他条件对现有 KeyedStream 进行分组。 以下是以 10 秒的时间窗口聚合： 1inputStream.keyBy(0).window(Time.seconds(10)); Flink 定义数据片段以便（可能）处理无限数据流。 这些切片称为窗口。 此切片有助于通过应用转换处理数据块。 要对流进行窗口化，我们需要分配一个可以进行分发的键和一个描述要对窗口化流执行哪些转换的函数 要将流切片到窗口，我们可以使用 Flink 自带的窗口分配器。 我们有选项，如 tumbling windows, sliding windows, global 和 session windows。 Flink 还允许您通过扩展 WindowAssginer 类来编写自定义窗口分配器。 这里先预留下篇文章来讲解这些不同的 windows 是如何工作的。 WindowAllwindowAll 函数允许对常规数据流进行分组。 通常，这是非并行数据转换，因为它在非分区数据流上运行。 与常规数据流功能类似，我们也有窗口数据流功能。 唯一的区别是它们处理窗口数据流。 所以窗口缩小就像 Reduce 函数一样，Window fold 就像 Fold 函数一样，并且还有聚合。 1inputStream.keyBy(0).windowAll(Time.seconds(10)); UnionUnion 函数将两个或多个数据流结合在一起。 这样就可以并行地组合数据流。 如果我们将一个流与自身组合，那么它会输出每个记录两次。 1inputStream.union(inputStream1, inputStream2, ...); Window join我们可以通过一些 key 将同一个 window 的两个数据流 join 起来。 1234inputStream.join(inputStream1) .where(0).equalTo(1) .window(Time.seconds(5)) .apply (new JoinFunction () &#123;...&#125;); 以上示例是在 5 秒的窗口中连接两个流，其中第一个流的第一个属性的连接条件等于另一个流的第二个属性。 Split此功能根据条件将流拆分为两个或多个流。 当您获得混合流并且您可能希望单独处理每个数据流时，可以使用此方法。 12345678910111213SplitStream&lt;Integer&gt; split = inputStream.split(new OutputSelector&lt;Integer&gt;() &#123; @Override public Iterable&lt;String&gt; select(Integer value) &#123; List&lt;String&gt; output = new ArrayList&lt;String&gt;(); if (value % 2 == 0) &#123; output.add(\"even\"); &#125; else &#123; output.add(\"odd\"); &#125; return output; &#125;&#125;); Select此功能允许您从拆分流中选择特定流。 1234SplitStream&lt;Integer&gt; split;DataStream&lt;Integer&gt; even = split.select(\"even\"); DataStream&lt;Integer&gt; odd = split.select(\"odd\"); DataStream&lt;Integer&gt; all = split.select(\"even\",\"odd\"); ProjectProject 函数允许您从事件流中选择属性子集，并仅将所选元素发送到下一个处理流。 12DataStream&lt;Tuple4&lt;Integer, Double, String, String&gt;&gt; in = // [...] DataStream&lt;Tuple2&lt;String, String&gt;&gt; out = in.project(3,2); 上述函数从给定记录中选择属性号 2 和 3。 以下是示例输入和输出记录： 12(1,10.0,A,B)=&gt; (B,A)(2,20.0,C,D)=&gt; (D,C) 最后本文主要介绍了 Flink Data 的常用转换方式：Map、FlatMap、Filter、KeyBy、Reduce、Fold、Aggregations、Window、WindowAll、Union、Window Join、Split、Select、Project 等。并用了点简单的 demo 介绍了如何使用，具体在项目中该如何将数据流转换成我们想要的格式，还需要根据实际情况对待。 关注我转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/ 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从 0 到 1 学习 —— 如何自定义 Data Sink ？","date":"2018-10-30T16:00:00.000Z","path":"2018/10/31/flink-create-sink/","text":"前言前篇文章 《从0到1学习Flink》—— Data Sink 介绍 介绍了 Flink Data Sink，也介绍了 Flink 自带的 Sink，那么如何自定义自己的 Sink 呢？这篇文章将写一个 demo 教大家将从 Kafka Source 的数据 Sink 到 MySQL 中去。 准备工作我们先来看下 Flink 从 Kafka topic 中获取数据的 demo，首先你需要安装好了 FLink 和 Kafka 。 运行启动 Flink、Zookepeer、Kafka， 好了，都启动了！ 数据库建表12345678DROP TABLE IF EXISTS `student`;CREATE TABLE `student` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT, `name` varchar(25) COLLATE utf8_bin DEFAULT NULL, `password` varchar(25) COLLATE utf8_bin DEFAULT NULL, `age` int(10) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8 COLLATE=utf8_bin; 实体类Student.java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package com.zhisheng.flink.model;/** * Desc: * weixin: zhisheng_tian * blog: http://www.54tianzhisheng.cn/ */public class Student &#123; public int id; public String name; public String password; public int age; public Student() &#123; &#125; public Student(int id, String name, String password, int age) &#123; this.id = id; this.name = name; this.password = password; this.age = age; &#125; @Override public String toString() &#123; return \"Student&#123;\" + \"id=\" + id + \", name='\" + name + '\\'' + \", password='\" + password + '\\'' + \", age=\" + age + '&#125;'; &#125; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getPassword() &#123; return password; &#125; public void setPassword(String password) &#123; this.password = password; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125;&#125; 工具类工具类往 kafka topic student 发送数据 12345678910111213141516171819202122232425262728293031323334353637383940import com.alibaba.fastjson.JSON;import com.zhisheng.flink.model.Metric;import com.zhisheng.flink.model.Student;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.ProducerRecord;import java.util.HashMap;import java.util.Map;import java.util.Properties;/** * 往kafka中写数据 * 可以使用这个main函数进行测试一下 * weixin: zhisheng_tian * blog: http://www.54tianzhisheng.cn/ */public class KafkaUtils2 &#123; public static final String broker_list = \"localhost:9092\"; public static final String topic = \"student\"; //kafka topic 需要和 flink 程序用同一个 topic public static void writeToKafka() throws InterruptedException &#123; Properties props = new Properties(); props.put(\"bootstrap.servers\", broker_list); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); KafkaProducer producer = new KafkaProducer&lt;String, String&gt;(props); for (int i = 1; i &lt;= 100; i++) &#123; Student student = new Student(i, \"zhisheng\" + i, \"password\" + i, 18 + i); ProducerRecord record = new ProducerRecord&lt;String, String&gt;(topic, null, null, JSON.toJSONString(student)); producer.send(record); System.out.println(\"发送数据: \" + JSON.toJSONString(student)); &#125; producer.flush(); &#125; public static void main(String[] args) throws InterruptedException &#123; writeToKafka(); &#125;&#125; SinkToMySQL该类就是 Sink Function，继承了 RichSinkFunction ，然后重写了里面的方法。在 invoke 方法中将数据插入到 MySQL 中。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273package com.zhisheng.flink.sink;import com.zhisheng.flink.model.Student;import org.apache.flink.configuration.Configuration;import org.apache.flink.streaming.api.functions.sink.RichSinkFunction;import java.sql.Connection;import java.sql.DriverManager;import java.sql.PreparedStatement;/** * Desc: * weixin: zhisheng_tian * blog: http://www.54tianzhisheng.cn/ */public class SinkToMySQL extends RichSinkFunction&lt;Student&gt; &#123; PreparedStatement ps; private Connection connection; /** * open() 方法中建立连接，这样不用每次 invoke 的时候都要建立连接和释放连接 * * @param parameters * @throws Exception */ @Override public void open(Configuration parameters) throws Exception &#123; super.open(parameters); connection = getConnection(); String sql = \"insert into Student(id, name, password, age) values(?, ?, ?, ?);\"; ps = this.connection.prepareStatement(sql); &#125; @Override public void close() throws Exception &#123; super.close(); //关闭连接和释放资源 if (connection != null) &#123; connection.close(); &#125; if (ps != null) &#123; ps.close(); &#125; &#125; /** * 每条数据的插入都要调用一次 invoke() 方法 * * @param value * @param context * @throws Exception */ @Override public void invoke(Student value, Context context) throws Exception &#123; //组装数据，执行插入操作 ps.setInt(1, value.getId()); ps.setString(2, value.getName()); ps.setString(3, value.getPassword()); ps.setInt(4, value.getAge()); ps.executeUpdate(); &#125; private static Connection getConnection() &#123; Connection con = null; try &#123; Class.forName(\"com.mysql.jdbc.Driver\"); con = DriverManager.getConnection(\"jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=UTF-8\", \"root\", \"root123456\"); &#125; catch (Exception e) &#123; System.out.println(\"-----------mysql get connection has exception , msg = \"+ e.getMessage()); &#125; return con; &#125;&#125; Flink 程序这里的 source 是从 kafka 读取数据的，然后 Flink 从 Kafka 读取到数据（JSON）后用阿里 fastjson 来解析成 student 对象，然后在 addSink 中使用我们创建的 SinkToMySQL，这样就可以把数据存储到 MySQL 了。 12345678910111213141516171819202122232425262728293031323334353637383940414243package com.zhisheng.flink;import com.alibaba.fastjson.JSON;import com.zhisheng.flink.model.Student;import com.zhisheng.flink.sink.SinkToMySQL;import org.apache.flink.api.common.serialization.SimpleStringSchema;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.api.functions.sink.PrintSinkFunction;import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer011;import org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer011;import java.util.Properties;/** * Desc: * weixin: zhisheng_tian * blog: http://www.54tianzhisheng.cn/ */public class Main3 &#123; public static void main(String[] args) throws Exception &#123; final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); Properties props = new Properties(); props.put(\"bootstrap.servers\", \"localhost:9092\"); props.put(\"zookeeper.connect\", \"localhost:2181\"); props.put(\"group.id\", \"metric-group\"); props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(\"auto.offset.reset\", \"latest\"); SingleOutputStreamOperator&lt;Student&gt; student = env.addSource(new FlinkKafkaConsumer011&lt;&gt;( \"student\", //这个 kafka topic 需要和上面的工具类的 topic 一致 new SimpleStringSchema(), props)).setParallelism(1) .map(string -&gt; JSON.parseObject(string, Student.class)); //Fastjson 解析字符串成 student 对象 student.addSink(new SinkToMySQL()); //数据 sink 到 mysql env.execute(\"Flink add sink\"); &#125;&#125; 结果运行 Flink 程序，然后再运行 KafkaUtils2.java 工具类，这样就可以了。 如果数据插入成功了，那么我们查看下我们的数据库： 数据库中已经插入了 100 条我们从 Kafka 发送的数据了。证明我们的 SinkToMySQL 起作用了。是不是很简单？ 项目结构怕大家不知道我的项目结构，这里发个截图看下： 最后本文主要利用一个 demo，告诉大家如何自定义 Sink Function，将从 Kafka 的数据 Sink 到 MySQL 中，如果你项目中有其他的数据来源，你也可以换成对应的 Source，也有可能你的 Sink 是到其他的地方或者其他不同的方式，那么依旧是这个套路：继承 RichSinkFunction 抽象类，重写 invoke 方法。 关注我转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/ 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从 0 到 1 学习 —— 如何自定义 Data Source ？","date":"2018-10-29T16:00:00.000Z","path":"2018/10/30/flink-create-source/","text":"前言在 《从0到1学习Flink》—— Data Source 介绍 文章中，我给大家介绍了 Flink Data Source 以及简短的介绍了一下自定义 Data Source，这篇文章更详细的介绍下，并写一个 demo 出来让大家理解。 Flink Kafka source准备工作我们先来看下 Flink 从 Kafka topic 中获取数据的 demo，首先你需要安装好了 FLink 和 Kafka 。 运行启动 Flink、Zookepeer、Kafka， 好了，都启动了！ maven 依赖1234567891011121314151617181920212223242526272829303132333435363738&lt;!--flink java--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-java&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-java_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;!--日志--&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;version&gt;1.7.7&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt; &lt;scope&gt;runtime&lt;/scope&gt;&lt;/dependency&gt;&lt;!--flink kafka connector--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka-0.11_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;!--alibaba fastjson--&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.51&lt;/version&gt;&lt;/dependency&gt; 测试发送数据到 kafka topic实体类，Metric.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package com.zhisheng.flink.model;import java.util.Map;/** * Desc: * weixi: zhisheng_tian * blog: http://www.54tianzhisheng.cn/ */public class Metric &#123; public String name; public long timestamp; public Map&lt;String, Object&gt; fields; public Map&lt;String, String&gt; tags; public Metric() &#123; &#125; public Metric(String name, long timestamp, Map&lt;String, Object&gt; fields, Map&lt;String, String&gt; tags) &#123; this.name = name; this.timestamp = timestamp; this.fields = fields; this.tags = tags; &#125; @Override public String toString() &#123; return \"Metric&#123;\" + \"name='\" + name + '\\'' + \", timestamp='\" + timestamp + '\\'' + \", fields=\" + fields + \", tags=\" + tags + '&#125;'; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public long getTimestamp() &#123; return timestamp; &#125; public void setTimestamp(long timestamp) &#123; this.timestamp = timestamp; &#125; public Map&lt;String, Object&gt; getFields() &#123; return fields; &#125; public void setFields(Map&lt;String, Object&gt; fields) &#123; this.fields = fields; &#125; public Map&lt;String, String&gt; getTags() &#123; return tags; &#125; public void setTags(Map&lt;String, String&gt; tags) &#123; this.tags = tags; &#125;&#125; 往 kafka 中写数据工具类：KafkaUtils.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import com.alibaba.fastjson.JSON;import com.zhisheng.flink.model.Metric;import org.apache.kafka.clients.producer.KafkaProducer;import org.apache.kafka.clients.producer.ProducerRecord;import java.util.HashMap;import java.util.Map;import java.util.Properties;/** * 往kafka中写数据 * 可以使用这个main函数进行测试一下 * weixin: zhisheng_tian * blog: http://www.54tianzhisheng.cn/ */public class KafkaUtils &#123; public static final String broker_list = \"localhost:9092\"; public static final String topic = \"metric\"; // kafka topic，Flink 程序中需要和这个统一 public static void writeToKafka() throws InterruptedException &#123; Properties props = new Properties(); props.put(\"bootstrap.servers\", broker_list); props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); //key 序列化 props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\"); //value 序列化 KafkaProducer producer = new KafkaProducer&lt;String, String&gt;(props); Metric metric = new Metric(); metric.setTimestamp(System.currentTimeMillis()); metric.setName(\"mem\"); Map&lt;String, String&gt; tags = new HashMap&lt;&gt;(); Map&lt;String, Object&gt; fields = new HashMap&lt;&gt;(); tags.put(\"cluster\", \"zhisheng\"); tags.put(\"host_ip\", \"101.147.022.106\"); fields.put(\"used_percent\", 90d); fields.put(\"max\", 27244873d); fields.put(\"used\", 17244873d); fields.put(\"init\", 27244873d); metric.setTags(tags); metric.setFields(fields); ProducerRecord record = new ProducerRecord&lt;String, String&gt;(topic, null, null, JSON.toJSONString(metric)); producer.send(record); System.out.println(\"发送数据: \" + JSON.toJSONString(metric)); producer.flush(); &#125; public static void main(String[] args) throws InterruptedException &#123; while (true) &#123; Thread.sleep(300); writeToKafka(); &#125; &#125;&#125; 运行： 如果出现如上图标记的，即代表能够不断的往 kafka 发送数据的。 Flink 程序Main.java 123456789101112131415161718192021222324252627282930313233343536package com.zhisheng.flink;import org.apache.flink.api.common.serialization.SimpleStringSchema;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer011;import java.util.Properties;/** * Desc: * weixi: zhisheng_tian * blog: http://www.54tianzhisheng.cn/ */public class Main &#123; public static void main(String[] args) throws Exception &#123; final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); Properties props = new Properties(); props.put(\"bootstrap.servers\", \"localhost:9092\"); props.put(\"zookeeper.connect\", \"localhost:2181\"); props.put(\"group.id\", \"metric-group\"); props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); //key 反序列化 props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\"); props.put(\"auto.offset.reset\", \"latest\"); //value 反序列化 DataStreamSource&lt;String&gt; dataStreamSource = env.addSource(new FlinkKafkaConsumer011&lt;&gt;( \"metric\", //kafka topic new SimpleStringSchema(), // String 序列化 props)).setParallelism(1); dataStreamSource.print(); //把从 kafka 读取到的数据打印在控制台 env.execute(\"Flink add data source\"); &#125;&#125; 运行起来： 看到没程序，Flink 程序控制台能够源源不断的打印数据呢。 自定义 Source上面就是 Flink 自带的 Kafka source，那么接下来就模仿着写一个从 MySQL 中读取数据的 Source。 首先 pom.xml 中添加 MySQL 依赖： 12345&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.34&lt;/version&gt;&lt;/dependency&gt; 数据库建表如下： 12345678DROP TABLE IF EXISTS `student`;CREATE TABLE `student` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT, `name` varchar(25) COLLATE utf8_bin DEFAULT NULL, `password` varchar(25) COLLATE utf8_bin DEFAULT NULL, `age` int(10) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8 COLLATE=utf8_bin; 插入数据： 12INSERT INTO `student` VALUES ('1', 'zhisheng01', '123456', '18'), ('2', 'zhisheng02', '123', '17'), ('3', 'zhisheng03', '1234', '18'), ('4', 'zhisheng04', '12345', '16');COMMIT; 新建实体类：Student.java 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package com.zhisheng.flink.model;/** * Desc: * weixi: zhisheng_tian * blog: http://www.54tianzhisheng.cn/ */public class Student &#123; public int id; public String name; public String password; public int age; public Student() &#123; &#125; public Student(int id, String name, String password, int age) &#123; this.id = id; this.name = name; this.password = password; this.age = age; &#125; @Override public String toString() &#123; return \"Student&#123;\" + \"id=\" + id + \", name='\" + name + '\\'' + \", password='\" + password + '\\'' + \", age=\" + age + '&#125;'; &#125; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getPassword() &#123; return password; &#125; public void setPassword(String password) &#123; this.password = password; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125;&#125; 新建 Source 类 SourceFromMySQL.java，该类继承 RichSourceFunction ，实现里面的 open、close、run、cancel 方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586 package com.zhisheng.flink.source;import com.zhisheng.flink.model.Student;import org.apache.flink.configuration.Configuration;import org.apache.flink.streaming.api.functions.source.RichSourceFunction;import java.sql.Connection;import java.sql.DriverManager;import java.sql.PreparedStatement;import java.sql.ResultSet;/** * Desc: * weixi: zhisheng_tian * blog: http://www.54tianzhisheng.cn/ */public class SourceFromMySQL extends RichSourceFunction&lt;Student&gt; &#123; PreparedStatement ps; private Connection connection; /** * open() 方法中建立连接，这样不用每次 invoke 的时候都要建立连接和释放连接。 * * @param parameters * @throws Exception */ @Override public void open(Configuration parameters) throws Exception &#123; super.open(parameters); connection = getConnection(); String sql = \"select * from Student;\"; ps = this.connection.prepareStatement(sql); &#125; /** * 程序执行完毕就可以进行，关闭连接和释放资源的动作了 * * @throws Exception */ @Override public void close() throws Exception &#123; super.close(); if (connection != null) &#123; //关闭连接和释放资源 connection.close(); &#125; if (ps != null) &#123; ps.close(); &#125; &#125; /** * DataStream 调用一次 run() 方法用来获取数据 * * @param ctx * @throws Exception */ @Override public void run(SourceContext&lt;Student&gt; ctx) throws Exception &#123; ResultSet resultSet = ps.executeQuery(); while (resultSet.next()) &#123; Student student = new Student( resultSet.getInt(\"id\"), resultSet.getString(\"name\").trim(), resultSet.getString(\"password\").trim(), resultSet.getInt(\"age\")); ctx.collect(student); &#125; &#125; @Override public void cancel() &#123; &#125; private static Connection getConnection() &#123; Connection con = null; try &#123; Class.forName(\"com.mysql.jdbc.Driver\"); con = DriverManager.getConnection(\"jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=UTF-8\", \"root\", \"root123456\"); &#125; catch (Exception e) &#123; System.out.println(\"-----------mysql get connection has exception , msg = \"+ e.getMessage()); &#125; return con; &#125;&#125; Flink 程序： 12345678910111213141516171819package com.zhisheng.flink;import com.zhisheng.flink.source.SourceFromMySQL;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;/** * Desc: * weixi: zhisheng_tian * blog: http://www.54tianzhisheng.cn/ */public class Main2 &#123; public static void main(String[] args) throws Exception &#123; final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.addSource(new SourceFromMySQL()).print(); env.execute(\"Flink add data sourc\"); &#125;&#125; 运行 Flink 程序，控制台日志中可以看见打印的 student 信息。 RichSourceFunction从上面自定义的 Source 可以看到我们继承的就是这个 RichSourceFunction 类，那么来了解一下： 一个抽象类，继承自 AbstractRichFunction。为实现一个 Rich SourceFunction 提供基础能力。该类的子类有三个，两个是抽象类，在此基础上提供了更具体的实现，另一个是 ContinuousFileMonitoringFunction。 MessageAcknowledgingSourceBase ：它针对的是数据源是消息队列的场景并且提供了基于 ID 的应答机制。 MultipleIdsMessageAcknowledgingSourceBase ： 在 MessageAcknowledgingSourceBase 的基础上针对 ID 应答机制进行了更为细分的处理，支持两种 ID 应答模型：session id 和 unique message id。 ContinuousFileMonitoringFunction：这是单个（非并行）监视任务，它接受 FileInputFormat，并且根据 FileProcessingMode 和 FilePathFilter，它负责监视用户提供的路径；决定应该进一步读取和处理哪些文件；创建与这些文件对应的 FileInputSplit 拆分，将它们分配给下游任务以进行进一步处理。 最后本文主要讲了下 Flink 使用 Kafka Source 的使用，并提供了一个 demo 教大家如何自定义 Source，从 MySQL 中读取数据，当然你也可以从其他地方读取，实现自己的数据源 source。可能平时工作会比这个更复杂，需要大家灵活应对！ 关注我转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从 0 到 1 学习 —— Data Sink 介绍","date":"2018-10-28T16:00:00.000Z","path":"2018/10/29/flink-sink/","text":"前言再上一篇文章中 《从0到1学习Flink》—— Data Source 介绍 讲解了 Flink Data Source ，那么这里就来讲讲 Flink Data Sink 吧。 首先 Sink 的意思是： 大概可以猜到了吧！Data sink 有点把数据存储下来（落库）的意思。 如上图，Source 就是数据的来源，中间的 Compute 其实就是 Flink 干的事情，可以做一系列的操作，操作完后就把计算后的数据结果 Sink 到某个地方。（可以是 MySQL、ElasticSearch、Kafka、Cassandra 等）。这里我说下自己目前做告警这块就是把 Compute 计算后的结果 Sink 直接告警出来了（发送告警消息到钉钉群、邮件、短信等），这个 sink 的意思也不一定非得说成要把数据存储到某个地方去。其实官网用的 Connector 来形容要去的地方更合适，这个 Connector 可以有 MySQL、ElasticSearch、Kafka、Cassandra RabbitMQ 等。 Flink Data Sink前面文章 《从0到1学习Flink》—— Data Source 介绍 介绍了 Flink Data Source 有哪些，这里也看看 Flink Data Sink 支持的有哪些。 看下源码有哪些呢？ 可以看到有 Kafka、ElasticSearch、Socket、RabbitMQ、JDBC、Cassandra POJO、File、Print 等 Sink 的方式。 SinkFunction 从上图可以看到 SinkFunction 接口有 invoke 方法，它有一个 RichSinkFunction 抽象类。 上面的那些自带的 Sink 可以看到都是继承了 RichSinkFunction 抽象类，实现了其中的方法，那么我们要是自己定义自己的 Sink 的话其实也是要按照这个套路来做的。 这里就拿个较为简单的 PrintSinkFunction 源码来讲下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566@PublicEvolvingpublic class PrintSinkFunction&lt;IN&gt; extends RichSinkFunction&lt;IN&gt; &#123; private static final long serialVersionUID = 1L; private static final boolean STD_OUT = false; private static final boolean STD_ERR = true; private boolean target; private transient PrintStream stream; private transient String prefix; /** * Instantiates a print sink function that prints to standard out. */ public PrintSinkFunction() &#123;&#125; /** * Instantiates a print sink function that prints to standard out. * * @param stdErr True, if the format should print to standard error instead of standard out. */ public PrintSinkFunction(boolean stdErr) &#123; target = stdErr; &#125; public void setTargetToStandardOut() &#123; target = STD_OUT; &#125; public void setTargetToStandardErr() &#123; target = STD_ERR; &#125; @Override public void open(Configuration parameters) throws Exception &#123; super.open(parameters); StreamingRuntimeContext context = (StreamingRuntimeContext) getRuntimeContext(); // get the target stream stream = target == STD_OUT ? System.out : System.err; // set the prefix if we have a &gt;1 parallelism prefix = (context.getNumberOfParallelSubtasks() &gt; 1) ? ((context.getIndexOfThisSubtask() + 1) + \"&gt; \") : null; &#125; @Override public void invoke(IN record) &#123; if (prefix != null) &#123; stream.println(prefix + record.toString()); &#125; else &#123; stream.println(record.toString()); &#125; &#125; @Override public void close() &#123; this.stream = null; this.prefix = null; &#125; @Override public String toString() &#123; return \"Print to \" + (target == STD_OUT ? \"System.out\" : \"System.err\"); &#125;&#125; 可以看到它就是实现了 RichSinkFunction 抽象类，然后实现了 invoke 方法，这里 invoke 方法就是把记录打印出来了就是，没做其他的额外操作。 如何使用？1SingleOutputStreamOperator.addSink(new PrintSinkFunction&lt;&gt;(); 这样就可以了，如果是其他的 Sink Function 的话需要换成对应的。 使用这个 Function 其效果就是打印从 Source 过来的数据，和直接 Source.print() 效果一样。 下篇文章我们将讲解下如何自定义自己的 Sink Function，并使用一个 demo 来教大家，让大家知道这个套路，且能够在自己工作中自定义自己需要的 Sink Function，来完成自己的工作需求。 最后本文主要讲了下 Flink 的 Data Sink，并介绍了常见的 Data Sink，也看了下源码的 SinkFunction，介绍了一个简单的 Function 使用, 告诉了大家自定义 Sink Function 的套路，下篇文章带大家写个。 关注我转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/10/29/flink-sink/ 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从 0 到 1 学习 —— Data Source 介绍","date":"2018-10-27T16:00:00.000Z","path":"2018/10/28/flink-sources/","text":"前言Data Sources 是什么呢？就字面意思其实就可以知道：数据来源。 Flink 做为一款流式计算框架，它可用来做批处理，即处理静态的数据集、历史的数据集；也可以用来做流处理，即实时的处理些实时数据流，实时的产生数据流结果，只要数据源源不断的过来，Flink 就能够一直计算下去，这个 Data Sources 就是数据的来源地。 Flink 中你可以使用 StreamExecutionEnvironment.addSource(sourceFunction) 来为你的程序添加数据来源。 Flink 已经提供了若干实现好了的 source functions，当然你也可以通过实现 SourceFunction 来自定义非并行的 source 或者实现 ParallelSourceFunction 接口或者扩展 RichParallelSourceFunction 来自定义并行的 source， FlinkStreamExecutionEnvironment 中可以使用以下几个已实现的 stream sources， 总的来说可以分为下面几大类： 基于集合1、fromCollection(Collection) - 从 Java 的 Java.util.Collection 创建数据流。集合中的所有元素类型必须相同。 2、fromCollection(Iterator, Class) - 从一个迭代器中创建数据流。Class 指定了该迭代器返回元素的类型。 3、fromElements(T …) - 从给定的对象序列中创建数据流。所有对象类型必须相同。 12345678StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();DataStream&lt;Event&gt; input = env.fromElements( new Event(1, \"barfoo\", 1.0), new Event(2, \"start\", 2.0), new Event(3, \"foobar\", 3.0), ...); 4、fromParallelCollection(SplittableIterator, Class) - 从一个迭代器中创建并行数据流。Class 指定了该迭代器返回元素的类型。 5、generateSequence(from, to) - 创建一个生成指定区间范围内的数字序列的并行数据流。 基于文件1、readTextFile(path) - 读取文本文件，即符合 TextInputFormat 规范的文件，并将其作为字符串返回。 123final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();DataStream&lt;String&gt; text = env.readTextFile(\"file:///path/to/file\"); 2、readFile(fileInputFormat, path) - 根据指定的文件输入格式读取文件（一次）。 3、readFile(fileInputFormat, path, watchType, interval, pathFilter, typeInfo) - 这是上面两个方法内部调用的方法。它根据给定的 fileInputFormat 和读取路径读取文件。根据提供的 watchType，这个 source 可以定期（每隔 interval 毫秒）监测给定路径的新数据（FileProcessingMode.PROCESS_CONTINUOUSLY），或者处理一次路径对应文件的数据并退出（FileProcessingMode.PROCESS_ONCE）。你可以通过 pathFilter 进一步排除掉需要处理的文件。 12345final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();DataStream&lt;MyEvent&gt; stream = env.readFile( myFormat, myFilePath, FileProcessingMode.PROCESS_CONTINUOUSLY, 100, FilePathFilter.createDefaultFilter(), typeInfo); 实现: 在具体实现上，Flink 把文件读取过程分为两个子任务，即目录监控和数据读取。每个子任务都由单独的实体实现。目录监控由单个非并行（并行度为1）的任务执行，而数据读取由并行运行的多个任务执行。后者的并行性等于作业的并行性。单个目录监控任务的作用是扫描目录（根据 watchType 定期扫描或仅扫描一次），查找要处理的文件并把文件分割成切分片（splits），然后将这些切分片分配给下游 reader。reader 负责读取数据。每个切分片只能由一个 reader 读取，但一个 reader 可以逐个读取多个切分片。 重要注意： 如果 watchType 设置为 FileProcessingMode.PROCESS_CONTINUOUSLY，则当文件被修改时，其内容将被重新处理。这会打破“exactly-once”语义，因为在文件末尾附加数据将导致其所有内容被重新处理。 如果 watchType 设置为 FileProcessingMode.PROCESS_ONCE，则 source 仅扫描路径一次然后退出，而不等待 reader 完成文件内容的读取。当然 reader 会继续阅读，直到读取所有的文件内容。关闭 source 后就不会再有检查点。这可能导致节点故障后的恢复速度较慢，因为该作业将从最后一个检查点恢复读取。 基于 Socket：socketTextStream(String hostname, int port) - 从 socket 读取。元素可以用分隔符切分。 12345678StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; dataStream = env .socketTextStream(\"localhost\", 9999) // 监听 localhost 的 9999 端口过来的数据 .flatMap(new Splitter()) .keyBy(0) .timeWindow(Time.seconds(5)) .sum(1); 这个在 《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 文章里用的就是基于 Socket 的 Word Count 程序。 自定义：addSource - 添加一个新的 source function。例如，你可以 addSource(new FlinkKafkaConsumer011&lt;&gt;(…)) 以从 Apache Kafka 读取数据 说下上面几种的特点吧： 1、基于集合：有界数据集，更偏向于本地测试用 2、基于文件：适合监听文件修改并读取其内容 3、基于 Socket：监听主机的 host port，从 Socket 中获取数据 4、自定义 addSource：大多数的场景数据都是无界的，会源源不断的过来。比如去消费 Kafka 某个 topic 上的数据，这时候就需要用到这个 addSource，可能因为用的比较多的原因吧，Flink 直接提供了 FlinkKafkaConsumer011 等类可供你直接使用。你可以去看看 FlinkKafkaConsumerBase 这个基础类，它是 Flink Kafka 消费的最根本的类。 123456789StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();DataStream&lt;KafkaEvent&gt; input = env .addSource( new FlinkKafkaConsumer011&lt;&gt;( parameterTool.getRequired(\"input-topic\"), //从参数中获取传进来的 topic new KafkaEventSchema(), parameterTool.getProperties()) .assignTimestampsAndWatermarks(new CustomWatermarkExtractor())); Flink 目前支持如下图里面常见的 Source： 如果你想自己自定义自己的 Source 呢？ 那么你就需要去了解一下 SourceFunction 接口了，它是所有 stream source 的根接口，它继承自一个标记接口（空接口）Function。 SourceFunction 定义了两个接口方法： 1、run ： 启动一个 source，即对接一个外部数据源然后 emit 元素形成 stream（大部分情况下会通过在该方法里运行一个 while 循环的形式来产生 stream）。 2、cancel ： 取消一个 source，也即将 run 中的循环 emit 元素的行为终止。 正常情况下，一个 SourceFunction 实现这两个接口方法就可以了。其实这两个接口方法也固定了一种实现模板。 比如，实现一个 XXXSourceFunction，那么大致的模板是这样的：(直接拿 FLink 源码的实例给你看看) 最后本文主要讲了下 Flink 的常见 Source 有哪些并且简单的提了下如何自定义 Source。 关注我转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/10/28/flink-sources/ 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从 0 到 1 学习 —— Flink 配置文件详解","date":"2018-10-26T16:00:00.000Z","path":"2018/10/27/flink-config/","text":"前面文章我们已经知道 Flink 是什么东西了，安装好 Flink 后，我们再来看下安装路径下的配置文件吧。 安装目录下主要有 flink-conf.yaml 配置、日志的配置文件、zk 配置、Flink SQL Client 配置。 flink-conf.yaml基础配置123456789101112131415161718192021# jobManager 的IP地址jobmanager.rpc.address: localhost# JobManager 的端口号jobmanager.rpc.port: 6123# JobManager JVM heap 内存大小jobmanager.heap.size: 1024m# TaskManager JVM heap 内存大小taskmanager.heap.size: 1024m# 每个 TaskManager 提供的任务 slots 数量大小taskmanager.numberOfTaskSlots: 1# 程序默认并行计算的个数parallelism.default: 1# 文件系统来源# fs.default-scheme 高可用性配置1234567891011# 可以选择 &apos;NONE&apos; 或者 &apos;zookeeper&apos;.# high-availability: zookeeper# 文件系统路径，让 Flink 在高可用性设置中持久保存元数据# high-availability.storageDir: hdfs:///flink/ha/# zookeeper 集群中仲裁者的机器 ip 和 port 端口号# high-availability.zookeeper.quorum: localhost:2181# 默认是 open，如果 zookeeper security 启用了该值会更改成 creator# high-availability.zookeeper.client.acl: open 容错和检查点 配置1234567891011# 用于存储和检查点状态# state.backend: filesystem# 存储检查点的数据文件和元数据的默认目录# state.checkpoints.dir: hdfs://namenode-host:port/flink-checkpoints# savepoints 的默认目标目录(可选)# state.savepoints.dir: hdfs://namenode-host:port/flink-checkpoints# 用于启用/禁用增量 checkpoints 的标志# state.backend.incremental: false web 前端配置12345678# 基于 Web 的运行时监视器侦听的地址.#jobmanager.web.address: 0.0.0.0# Web 的运行时监视器端口rest.port: 8081# 是否从基于 Web 的 jobmanager 启用作业提交# jobmanager.web.submit.enable: false 高级配置123456789101112131415# io.tmp.dirs: /tmp# 是否应在 TaskManager 启动时预先分配 TaskManager 管理的内存# taskmanager.memory.preallocate: false# 类加载解析顺序，是先检查用户代码 jar（“child-first”）还是应用程序类路径（“parent-first”）。 默认设置指示首先从用户代码 jar 加载类# classloader.resolve-order: child-first# 用于网络缓冲区的 JVM 内存的分数。 这决定了 TaskManager 可以同时拥有多少流数据交换通道以及通道缓冲的程度。 如果作业被拒绝或者您收到系统没有足够缓冲区的警告，请增加此值或下面的最小/最大值。 另请注意，“taskmanager.network.memory.min”和“taskmanager.network.memory.max”可能会覆盖此分数# taskmanager.network.memory.fraction: 0.1# taskmanager.network.memory.min: 67108864# taskmanager.network.memory.max: 1073741824 Flink 集群安全配置1234567891011# 指示是否从 Kerberos ticket 缓存中读取# security.kerberos.login.use-ticket-cache: true# 包含用户凭据的 Kerberos 密钥表文件的绝对路径# security.kerberos.login.keytab: /path/to/kerberos/keytab# 与 keytab 关联的 Kerberos 主体名称# security.kerberos.login.principal: flink-user# 以逗号分隔的登录上下文列表，用于提供 Kerberos 凭据（例如，`Client，KafkaClient`使用凭证进行 ZooKeeper 身份验证和 Kafka 身份验证）# security.kerberos.login.contexts: Client,KafkaClient Zookeeper 安全配置12345# 覆盖以下配置以提供自定义 ZK 服务名称# zookeeper.sasl.service-name: zookeeper# 该配置必须匹配 &quot;security.kerberos.login.contexts&quot; 中的列表（含有一个）# zookeeper.sasl.login-context-name: Client HistoryServer12345678910111213141516# 你可以通过 bin/historyserver.sh (start|stop) 命令启动和关闭 HistoryServer# 将已完成的作业上传到的目录# jobmanager.archive.fs.dir: hdfs:///completed-jobs/# 基于 Web 的 HistoryServer 的地址# historyserver.web.address: 0.0.0.0# 基于 Web 的 HistoryServer 的端口号# historyserver.web.port: 8082# 以逗号分隔的目录列表，用于监视已完成的作业# historyserver.archive.fs.dir: hdfs:///completed-jobs/# 刷新受监控目录的时间间隔（以毫秒为单位）# historyserver.archive.fs.refresh-interval: 10000 查看下另外两个配置 slaves / master 2、slaves里面是每个 worker 节点的 IP/Hostname，每一个 worker 结点之后都会运行一个 TaskManager，一个一行。 1localhost 3、mastershost:port 1localhost:8081 4、zoo.cfg123456789101112131415161718# 每个 tick 的毫秒数tickTime=2000# 初始同步阶段可以采用的 tick 数initLimit=10# 在发送请求和获取确认之间可以传递的 tick 数syncLimit=5# 存储快照的目录# dataDir=/tmp/zookeeper# 客户端将连接的端口clientPort=2181# ZooKeeper quorum peersserver.1=localhost:2888:3888# server.2=host:peer-port:leader-port 5、日志配置Flink 在不同平台下运行的日志文件 1234567log4j-cli.propertieslog4j-console.propertieslog4j-yarn-session.propertieslog4j.propertieslogback-console.xmllogback-yarn.xmllogback.xml sql-client-defaults.yaml12345678910111213141516171819202122232425execution: # 'batch' or 'streaming' execution type: streaming # allow 'event-time' or only 'processing-time' in sources time-characteristic: event-time # interval in ms for emitting periodic watermarks periodic-watermarks-interval: 200 # 'changelog' or 'table' presentation of results result-mode: changelog # parallelism of the program parallelism: 1 # maximum parallelism max-parallelism: 128 # minimum idle state retention in ms min-idle-state-retention: 0 # maximum idle state retention in ms max-idle-state-retention: 0 deployment: # general cluster communication timeout in ms response-timeout: 5000 # (optional) address from cluster to gateway gateway-address: \"\" # (optional) port from cluster to gateway gateway-port: 0 Flink sql client ：你可以从官网这里了解 https://ci.apache.org/projects/flink/flink-docs-stable/dev/table/sqlClient.html 总结本文拿安装目录文件下的配置文件讲解了下 Flink 目录下的所有配置。 你也可以通过官网这里学习更多：https://ci.apache.org/projects/flink/flink-docs-stable/ops/config.html 关注我本篇文章地址是：http://www.54tianzhisheng.cn/2018/10/27/flink-config/ 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从 0 到 1 学习 —— Apache Flink 介绍","date":"2018-10-12T16:00:00.000Z","path":"2018/10/13/flink-introduction/","text":"前言Flink 是一种流式计算框架，为什么我会接触到 Flink 呢？ 因为我目前在负责的是监控平台的告警部分，负责采集到的监控数据会直接往 kafka 里塞，然后告警这边需要从 kafka topic 里面实时读取到监控数据，并将读取到的监控数据做一些 聚合/转换/计算 等操作，然后将计算后的结果与告警规则的阈值进行比较，然后做出相应的告警措施（钉钉群、邮件、短信、电话等）。画了个简单的图如下： 目前告警这块的架构是这样的结构，刚进公司那会的时候，架构是所有的监控数据直接存在 ElasticSearch 中，然后我们告警是去 ElasticSearch 中搜索我们监控指标需要的数据，幸好 ElasticSearch 的搜索能力够强大。但是你有没有发现一个问题，就是所有的监控数据从采集、采集后的数据做一些 计算/转换/聚合、再通过 Kafka 消息队列、再存进 ElasticSearch 中，再而去 ElasticSearch 中查找我们的监控数据，然后做出告警策略。整个流程对监控来说看起来很按照常理，但是对于告警来说，如果中间某个环节出了问题，比如 Kafka 消息队列延迟、监控数据存到 ElasticSearch 中写入时间较长、你的查询姿势写的不对等原因，这都将导致告警从 ElasticSearch 查到的数据是有延迟的。也许是 30 秒、一分钟、或者更长，这样对于告警来说这无疑将导致告警的消息没有任何的意义。 为什么这么说呢？为什么需要监控告警平台呢？无非就是希望我们能够尽早的发现问题，把问题给告警出来，这样开发和运维人员才能够及时的处理解决好线上的问题，以免给公司造成巨大的损失。 更何况现在还有更多的公司在做那种提前预警呢！这种又该如何做呢？需要用大数据和机器学习的技术去分析周期性的历史数据，然后根据这些数据可以整理出来某些监控指标的一些周期性（一天/七天/一月/一季度/一年）走势图，这样就大概可以绘图出来。然后根据这个走势图，可以将当前时间点的监控指标的数据使用量和走势图进行对比，在快要达到我们告警规则的阈值时，这时就可以提前告一个预警出来，让运维提前知道预警，然后提前查找问题，这样就能够提早发现问题所在，避免损失，将损失降到最小！当然，这种也是我打算做的，应该可以学到不少东西的。 于是乎，我现在就在接触流式计算框架 Flink，类似的还有常用的 Spark 等。 自己也接触了 Flink 一段时间了，这块中文资料目前书籍是只有一本很薄的，英文书籍也是三本不超过。 我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以关注我的公众号：zhisheng，然后回复关键字：Flink 即可无条件获取到。 另外这里也推荐一些博客可以看看： 1、官网：https://flink.apache.org/ 2、GitHub: https://github.com/apache/flink 3、https://blog.csdn.net/column/details/apacheflink.html 4、https://blog.csdn.net/lmalds/article/category/6263085 5、http://wuchong.me/ 6、https://blog.csdn.net/liguohuabigdata/article/category/7279020 下面的介绍可能也有不少参考以上所有的资料，感谢他们！在介绍 Flink 前，我们先看看 数据集类型 和 数据运算模型 的种类。 数据集类型有哪些呢： 无穷数据集：无穷的持续集成的数据集合 有界数据集：有限不会改变的数据集合 那么那些常见的无穷数据集有哪些呢？ 用户与客户端的实时交互数据 应用实时产生的日志 金融市场的实时交易记录 … 数据运算模型有哪些呢： 流式：只要数据一直在产生，计算就持续地进行 批处理：在预先定义的时间内运行计算，当完成时释放计算机资源 Flink 它可以处理有界的数据集、也可以处理无界的数据集、它可以流式的处理数据、也可以批量的处理数据。 Flink 是什么 ？ 上面三张图转自 云邪 成都站 《Flink 技术介绍与未来展望》，侵删。 从下至上，Flink 整体结构 从下至上： 1、部署：Flink 支持本地运行、能在独立集群或者在被 YARN 或 Mesos 管理的集群上运行， 也能部署在云上。 2、运行：Flink 的核心是分布式流式数据引擎，意味着数据以一次一个事件的形式被处理。 3、API：DataStream、DataSet、Table、SQL API。 4、扩展库：Flink 还包括用于复杂事件处理，机器学习，图形处理和 Apache Storm 兼容性的专用代码库。 Flink 数据流编程模型抽象级别Flink 提供了不同的抽象级别以开发流式或批处理应用。 最底层提供了有状态流。它将通过 过程函数（Process Function）嵌入到 DataStream API 中。它允许用户可以自由地处理来自一个或多个流数据的事件，并使用一致、容错的状态。除此之外，用户可以注册事件时间和处理事件回调，从而使程序可以实现复杂的计算。 DataStream / DataSet API 是 Flink 提供的核心 API ，DataSet 处理有界的数据集，DataStream 处理有界或者无界的数据流。用户可以通过各种方法（map / flatmap / window / keyby / sum / max / min / avg / join 等）将数据进行转换 / 计算。 Table API 是以 表 为中心的声明式 DSL，其中表可能会动态变化（在表达流数据时）。Table API 提供了例如 select、project、join、group-by、aggregate 等操作，使用起来却更加简洁（代码量更少）。 你可以在表与 DataStream/DataSet 之间无缝切换，也允许程序将 Table API 与 DataStream 以及 DataSet 混合使用。 Flink 提供的最高层级的抽象是 SQL 。这一层抽象在语法与表达能力上与 Table API 类似，但是是以 SQL查询表达式的形式表现程序。SQL 抽象与 Table API 交互密切，同时 SQL 查询可以直接在 Table API 定义的表上执行。 Flink 程序与数据流结构 Flink 应用程序结构就是如上图所示： 1、Source: 数据源，Flink 在流处理和批处理上的 source 大概有 4 类：基于本地集合的 source、基于文件的 source、基于网络套接字的 source、自定义的 source。自定义的 source 常见的有 Apache kafka、Amazon Kinesis Streams、RabbitMQ、Twitter Streaming API、Apache NiFi 等，当然你也可以定义自己的 source。 2、Transformation：数据转换的各种操作，有 Map / FlatMap / Filter / KeyBy / Reduce / Fold / Aggregations / Window / WindowAll / Union / Window join / Split / Select / Project 等，操作很多，可以将数据转换计算成你想要的数据。 3、Sink：接收器，Flink 将转换计算后的数据发送的地点 ，你可能需要存储下来，Flink 常见的 Sink 大概有如下几类：写入文件、打印出来、写入 socket 、自定义的 sink 。自定义的 sink 常见的有 Apache kafka、RabbitMQ、MySQL、ElasticSearch、Apache Cassandra、Hadoop FileSystem 等，同理你也可以定义自己的 sink。 为什么选择 Flink？Flink 是一个开源的分布式流式处理框架： ①提供准确的结果，甚至在出现无序或者延迟加载的数据的情况下。 ②它是状态化的容错的，同时在维护一次完整的的应用状态时，能无缝修复错误。 ③大规模运行，在上千个节点运行时有很好的吞吐量和低延迟。 更早的时候，我们讨论了数据集类型（有界 vs 无穷）和运算模型（批处理 vs 流式）的匹配。Flink 的流式计算模型启用了很多功能特性，如状态管理，处理无序数据，灵活的视窗，这些功能对于得出无穷数据集的精确结果是很重要的。 Flink 保证状态化计算强一致性。”状态化“意味着应用可以维护随着时间推移已经产生的数据聚合或者，并且 Filnk 的检查点机制在一次失败的事件中一个应用状态的强一致性。 Flink 支持流式计算和带有事件时间语义的视窗。事件时间机制使得那些事件无序到达甚至延迟到达的数据流能够计算出精确的结果。 除了提供数据驱动的视窗外，Flink 还支持基于时间，计数，session 等的灵活视窗。视窗能够用灵活的触发条件定制化从而达到对复杂的流传输模式的支持。Flink 的视窗使得模拟真实的创建数据的环境成为可能。 Flink 的容错能力是轻量级的，允许系统保持高并发，同时在相同时间内提供强一致性保证。Flink 以零数据丢失的方式从故障中恢复，但没有考虑可靠性和延迟之间的折衷。 Flink 能满足高并发和低延迟（计算大量数据很快）。下图显示了 Apache Flink 与 Apache Storm 在完成流数据清洗的分布式任务的性能对比。 Flink 保存点提供了一个状态化的版本机制，使得能以无丢失状态和最短停机时间的方式更新应用或者回退历史数据。 Flink 被设计成能用上千个点在大规模集群上运行。除了支持独立集群部署外，Flink 还支持 YARN 和Mesos 方式部署。 Flink 的程序内在是并行和分布式的，数据流可以被分区成 stream partitions，operators 被划分为operator subtasks; 这些 subtasks 在不同的机器或容器中分不同的线程独立运行；operator subtasks 的数量在具体的 operator 就是并行计算数，程序不同的 operator 阶段可能有不同的并行数；如下图所示，source operator 的并行数为 2，但最后的 sink operator 为1； 自己的内存管理 Flink 在 JVM 中提供了自己的内存管理，使其独立于 Java 的默认垃圾收集器。 它通过使用散列，索引，缓存和排序有效地进行内存管理。 丰富的库 Flink 拥有丰富的库来进行机器学习，图形处理，关系数据处理等。 由于其架构，很容易执行复杂的事件处理和警报。 分布式运行flink 作业提交架构流程可见下图： 1、Program Code：我们编写的 Flink 应用程序代码 2、Job Client：Job Client 不是 Flink 程序执行的内部部分，但它是任务执行的起点。 Job Client 负责接受用户的程序代码，然后创建数据流，将数据流提交给 Job Manager 以便进一步执行。 执行完成后，Job Client 将结果返回给用户 3、Job Manager：主进程（也称为作业管理器）协调和管理程序的执行。 它的主要职责包括安排任务，管理checkpoint ，故障恢复等。机器集群中至少要有一个 master，master 负责调度 task，协调 checkpoints 和容灾，高可用设置的话可以有多个 master，但要保证一个是 leader, 其他是 standby; Job Manager 包含 Actor system、Scheduler、Check pointing 三个重要的组件 4、Task Manager：从 Job Manager 处接收需要部署的 Task。Task Manager 是在 JVM 中的一个或多个线程中执行任务的工作节点。 任务执行的并行性由每个 Task Manager 上可用的任务槽决定。 每个任务代表分配给任务槽的一组资源。 例如，如果 Task Manager 有四个插槽，那么它将为每个插槽分配 25％ 的内存。 可以在任务槽中运行一个或多个线程。 同一插槽中的线程共享相同的 JVM。 同一 JVM 中的任务共享 TCP 连接和心跳消息。Task Manager 的一个 Slot 代表一个可用线程，该线程具有固定的内存，注意 Slot 只对内存隔离，没有对 CPU 隔离。默认情况下，Flink 允许子任务共享 Slot，即使它们是不同 task 的 subtask，只要它们来自相同的 job。这种共享可以有更好的资源利用率。 最后本文主要讲了我接触到 Flink 的缘由，然后从数据集类型和数据运算模型开始讲起，接着介绍了下 Flink 是什么、Flink 的整体架构、提供的 API、Flink 的优点所在以及 Flink 的分布式作业运行的方式。水文一篇，希望你能够对 Flink 稍微有一点概念了。 关注我转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/ 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Flink 从 0 到 1 学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门","date":"2018-09-17T16:00:00.000Z","path":"2018/09/18/flink-install/","text":"准备工作1、安装查看 Java 的版本号，推荐使用 Java 8。 安装 Flink2、在 Mac OS X 上安装 Flink 是非常方便的。推荐通过 homebrew 来安装。 1brew install apache-flink 3、检查安装： 1flink --version 结果： 1Version: 1.6.0, Commit ID: ff472b4 4、启动 flink 1234zhisheng@zhisheng  /usr/local/Cellar/apache-flink/1.6.0/libexec/bin  ./start-cluster.shStarting cluster.Starting standalonesession daemon on host zhisheng.Starting taskexecutor daemon on host zhisheng. 接着就可以进入 web 页面(http://localhost:8081/) 查看 demo1、新建一个 maven 项目 创建一个 SocketTextStreamWordCount 文件，加入以下代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package com.zhisheng.flink;import org.apache.flink.api.common.functions.FlatMapFunction;import org.apache.flink.api.java.tuple.Tuple2;import org.apache.flink.streaming.api.datastream.DataStreamSource;import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.util.Collector;/** * Created by zhisheng_tian on 2018/9/18 */public class SocketTextStreamWordCount &#123; public static void main(String[] args) throws Exception &#123; //参数检查 if (args.length != 2) &#123; System.err.println(\"USAGE:\\nSocketTextStreamWordCount &lt;hostname&gt; &lt;port&gt;\"); return; &#125; String hostname = args[0]; Integer port = Integer.parseInt(args[1]); // set up the streaming execution environment final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); //获取数据 DataStreamSource&lt;String&gt; stream = env.socketTextStream(hostname, port); //计数 SingleOutputStreamOperator&lt;Tuple2&lt;String, Integer&gt;&gt; sum = stream.flatMap(new LineSplitter()) .keyBy(0) .sum(1); sum.print(); env.execute(\"Java WordCount from SocketTextStream Example\"); &#125; public static final class LineSplitter implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; &#123; @Override public void flatMap(String s, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; collector) &#123; String[] tokens = s.toLowerCase().split(\"\\\\W+\"); for (String token: tokens) &#123; if (token.length() &gt; 0) &#123; collector.collect(new Tuple2&lt;String, Integer&gt;(token, 1)); &#125; &#125; &#125; &#125;&#125; 接着进入工程目录，使用以下命令打包。 1mvn clean package -Dmaven.test.skip=true 然后我们开启监听 9000 端口: 1nc -l 9000 最后进入 flink 安装目录 bin 下执行以下命令跑程序： 1flink run -c com.zhisheng.flink.SocketTextStreamWordCount /Users/zhisheng/IdeaProjects/flink/word-count/target/original-word-count-1.0-SNAPSHOT.jar 127.0.0.1 9000 注意换成你自己项目的路径。 执行完上述命令后，我们可以在 webUI 中看到正在运行的程序： 我们可以在 nc 监听端口中输入 text，比如： 然后我们通过 tail 命令看一下输出的 log 文件，来观察统计结果。进入目录 apache-flink/1.6.0/libexec/log，执行以下命令: 1tail -f flink-zhisheng-taskexecutor-0-zhisheng.out 注意：切换成你自己的路径和查看自己的目录。 总结本文描述了如何在 Mac 电脑上安装 Flink，及运行它。接着通过一个简单的 Flink 程序来介绍如何构建及运行Flink 程序。 关注我转载请注明地址：http://www.54tianzhisheng.cn/2018/09/18/flink-install 微信公众号：zhisheng 另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：zhisheng_tian，然后回复关键字：Flink 即可无条件获取到。 更多私密资料请加入知识星球！ Github 代码仓库https://github.com/zhisheng17/flink-learning/ 以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客 博客1、Flink 从0到1学习 —— Apache Flink 介绍 2、Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门 3、Flink 从0到1学习 —— Flink 配置文件详解 4、Flink 从0到1学习 —— Data Source 介绍 5、Flink 从0到1学习 —— 如何自定义 Data Source ？ 6、Flink 从0到1学习 —— Data Sink 介绍 7、Flink 从0到1学习 —— 如何自定义 Data Sink ？ 8、Flink 从0到1学习 —— Flink Data transformation(转换) 9、Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows 10、Flink 从0到1学习 —— Flink 中的几种 Time 详解 11、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch 12、Flink 从0到1学习 —— Flink 项目如何运行？ 13、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka 14、Flink 从0到1学习 —— Flink JobManager 高可用性配置 15、Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍 16、Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL 17、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ 18、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase 19、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS 20、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis 21、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra 22、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume 23、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB 24、Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ 25、Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了 26、Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了 27、阿里巴巴开源的 Blink 实时计算框架真香 28、Flink 从0到1学习 —— Flink 中如何管理配置？ 29、Flink 从0到1学习—— Flink 不可以连续 Split(分流)？ 30、Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文 31、Flink 架构、原理与部署测试 32、为什么说流处理即未来？ 33、OPPO 数据中台之基石：基于 Flink SQL 构建实时数据仓库 34、流计算框架 Flink 与 Storm 的性能对比 35、Flink状态管理和容错机制介绍 36、Apache Flink 结合 Kafka 构建端到端的 Exactly-Once 处理 37、360深度实践：Flink与Storm协议级对比 38、如何基于Flink+TensorFlow打造实时智能异常检测平台？只看这一篇就够了 39、Apache Flink 1.9 重大特性提前解读 40、Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新） 41、Flink 灵魂两百问，这谁顶得住？ 42、Flink 从0到1学习 —— 如何使用 Side Output 来分流？ 43、你公司到底需不需要引入实时计算引擎？ 44、一文让你彻底了解大数据实时计算引擎 Flink 源码解析1、Flink 源码解析 —— 源码编译运行 2、Flink 源码解析 —— 项目结构一览 3、Flink 源码解析—— local 模式启动流程 4、Flink 源码解析 —— standalone session 模式启动流程 5、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动 6、Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动 7、Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程 8、Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程 9、Flink 源码解析 —— 如何获取 JobGraph？ 10、Flink 源码解析 —— 如何获取 StreamGraph？ 11、Flink 源码解析 —— Flink JobManager 有什么作用？ 12、Flink 源码解析 —— Flink TaskManager 有什么作用？ 13、Flink 源码解析 —— JobManager 处理 SubmitJob 的过程 14、Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程 15、Flink 源码解析 —— 深度解析 Flink Checkpoint 机制 16、Flink 源码解析 —— 深度解析 Flink 序列化机制 17、Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？ 18、Flink Metrics 源码解析 —— Flink-metrics-core 19、Flink Metrics 源码解析 —— Flink-metrics-datadog 20、Flink Metrics 源码解析 —— Flink-metrics-dropwizard 21、Flink Metrics 源码解析 —— Flink-metrics-graphite 22、Flink Metrics 源码解析 —— Flink-metrics-influxdb 23、Flink Metrics 源码解析 —— Flink-metrics-jmx 24、Flink Metrics 源码解析 —— Flink-metrics-slf4j 25、Flink Metrics 源码解析 —— Flink-metrics-statsd 26、Flink Metrics 源码解析 —— Flink-metrics-prometheus 26、Flink Annotations 源码解析 27、Flink 源码解析 —— 如何获取 ExecutionGraph ？ 28、大数据重磅炸弹——实时计算框架 Flink 29、Flink Checkpoint-\u0007\b轻量级分布式快照 30、Flink Clients 源码解析","tags":[{"name":"Flink","slug":"Flink","permalink":"http://www.54tianzhisheng.cn/tags/Flink/"},{"name":"大数据","slug":"大数据","permalink":"http://www.54tianzhisheng.cn/tags/大数据/"},{"name":"流式计算","slug":"流式计算","permalink":"http://www.54tianzhisheng.cn/tags/流式计算/"}]},{"title":"Go 并发——实现协程同步的几种方式","date":"2018-08-29T16:00:00.000Z","path":"2018/08/30/go-sync/","text":"前言Java 中有一系列的线程同步的方法，go 里面有 goroutine（协程），先看下下面的代码执行的结果是什么呢？ 123456789101112131415package mainimport ( \"fmt\")func main() &#123; go func() &#123; fmt.Println(\"Goroutine 1\") &#125;() go func() &#123; fmt.Println(\"Goroutine 2\") &#125;()&#125; 执行以上代码很可能看不到输出。 因为有可能这两个协程还没得到执行，主协程就已经结束了，而主协程结束时会结束所有其他协程，所以导致代码运行的结果什么都没有。 估计不少新接触 go 的童鞋都会对此郁闷😒，可能会问那么该如何等待主协程中创建的协程执行完毕之后再结束主协程呢？ 下面说几种可以解决的方法： Sleep 一段时间在 main 方法退出之前 sleep 一段时间就可能会出现结果了，如下代码： 123456789101112131415161718package mainimport ( \"fmt\" \"time\")func main() &#123; go func() &#123; fmt.Println(\"Goroutine 1\") &#125;() go func() &#123; fmt.Println(\"Goroutine 2\") &#125;() time.Sleep(time.Second * 1) // 睡眠1秒，等待上面两个协程结束&#125; 这两个简单的协程执行消耗的时间很短的，所以你会发现现在就有结果出现了。 12Goroutine 1Goroutine 2 为什么上面我要说 “可能会出现” ？ 因为 sleep 这个时间目前是设置的 1s，如果我这两个协程里面执行了很复杂的逻辑操作（时间大于 1s），那么就会发现依旧也是无结果打印出来的。 那么就可以发现这种方式得到问题所在了：我们无法确定需要睡眠多久 上面那种方式有问题，go 里面其实也可以用管道来实现同步的。 管道实现同步那么用管道怎么实现同步呢？show code： 123456789101112131415161718192021222324252627282930package mainimport ( \"fmt\")func main() &#123; ch := make(chan struct&#123;&#125;) count := 2 // count 表示活动的协程个数 go func() &#123; fmt.Println(\"Goroutine 1\") ch &lt;- struct&#123;&#125;&#123;&#125; // 协程结束，发出信号 &#125;() go func() &#123; fmt.Println(\"Goroutine 2\") ch &lt;- struct&#123;&#125;&#123;&#125; // 协程结束，发出信号 &#125;() for range ch &#123; // 每次从ch中接收数据，表明一个活动的协程结束 count-- // 当所有活动的协程都结束时，关闭管道 if count == 0 &#123; close(ch) &#125; &#125;&#125; 这种方式是一种比较完美的解决方案， goroutine / channel 它们也是在 go 里面经常搭配在一起的一对。 sync.WaitGroup其实 go 里面也提供了更简单的方式 —— 使用 sync.WaitGroup。 WaitGroup 顾名思义，就是用来等待一组操作完成的。WaitGroup 内部实现了一个计数器，用来记录未完成的操作个数，它提供了三个方法： Add() 用来添加计数 Done() 用来在操作结束时调用，使计数减一 Wait() 用来等待所有的操作结束，即计数变为 0，该函数会在计数不为 0 时等待，在计数为 0 时立即返回 继续 show code： 123456789101112131415161718192021222324package mainimport ( \"fmt\" \"sync\")func main() &#123; var wg sync.WaitGroup wg.Add(2) // 因为有两个动作，所以增加2个计数 go func() &#123; fmt.Println(\"Goroutine 1\") wg.Done() // 操作完成，减少一个计数 &#125;() go func() &#123; fmt.Println(\"Goroutine 2\") wg.Done() // 操作完成，减少一个计数 &#125;() wg.Wait() // 等待，直到计数为0&#125; 你会发现也是可以看到运行结果的，是不是发现这种方式是很简单的。 总结多看别人写的代码；多想想为啥要这样写；多查自己不理解的地方；多写 demo 测试；多写文章总结。 关注我 本文地址为：http://www.54tianzhisheng.cn/2018/08/30/go-sync/ ，转载请注明原文出处！","tags":[{"name":"GO","slug":"GO","permalink":"http://www.54tianzhisheng.cn/tags/GO/"}]},{"title":"教你如何在 IDEA 远程 Debug ElasticSearch","date":"2018-08-13T16:00:00.000Z","path":"2018/08/14/idea-remote-debug-elasticsearch/","text":"前提之前在源码阅读环境搭建文章中写过我遇到的一个问题迟迟没有解决，也一直困扰着我。问题如下，在启动的时候解决掉其他异常和报错后，最后剩下这个错误一直解决不了： 12345678910111213141516171819202122[2018-08-01T09:44:27,370][ERROR][o.e.b.ElasticsearchUncaughtExceptionHandler] [] fatal error in thread [main], exitingjava.lang.NoClassDefFoundError: org/elasticsearch/plugins/ExtendedPluginsClassLoader at org.elasticsearch.plugins.PluginsService.loadBundle(PluginsService.java:632) ~[main/:?] at org.elasticsearch.plugins.PluginsService.loadBundles(PluginsService.java:557) ~[main/:?] at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:162) ~[main/:?] at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:311) ~[main/:?] at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:252) ~[main/:?] at org.elasticsearch.bootstrap.Bootstrap$5.&lt;init&gt;(Bootstrap.java:213) ~[main/:?] at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:213) ~[main/:?] at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:326) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:136) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:127) ~[main/:?] at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) ~[main/:?] at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) ~[main/:?] at org.elasticsearch.cli.Command.main(Command.java:90) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:93) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:86) ~[main/:?]Caused by: java.lang.ClassNotFoundException: org.elasticsearch.plugins.ExtendedPluginsClassLoader at jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:582) ~[?:?] at jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:190) ~[?:?] at java.lang.ClassLoader.loadClass(ClassLoader.java:499) ~[?:?] ... 15 more 网上的解决办法也试了很多遍，包括自己也在 GitHub issue 提问了，也没能解决。然后后面自己分享文章在掘金也发现有人和我有同样的问题。 下面讲讲另一种可以让你继续看源码的方法。 远程 Debug前提条件是你之前已经把项目导入进 IDEA 了，如果你还没了解，请看之前的文章，这里不重复了。 启动一个实例在你 git 拉取下的代码，切换你要阅读的分支代码后，执行下面这条命令启动一个 debug 的实例： 1./gradlew run --debug-jvm 启动等会后，就可以看到启动好后的端口号为 8000 了。 配置 IDEA新建一个远程的 debug： 配置如下图： 接下来点击 OK 就好了。 然后点击下面的 debug 图标： 启动后如下： 这时就可以发现是可以把整个流程全启动了，也不会报什么错误！ 流程全启动后，你会发现终端的日志都打印出来了（注意：这时不是打印在你的 IDEA 控制台） 总结遇到问题，多思考，多搜索，多想办法解决！这样才能够不断的提升你解决问题的能力！ 关注我最后转载请务必注明文章出处为： http://www.54tianzhisheng.cn/2018/08/14/idea-remote-debug-elasticsearch/ 相关文章1、渣渣菜鸡为什么要看 ElasticSearch 源码？ 2、渣渣菜鸡的 ElasticSearch 源码解析 —— 环境搭建 3、渣渣菜鸡的 ElasticSearch 源码解析 —— 启动流程(上) 4、渣渣菜鸡的 ElasticSearch 源码解析 —— 启动流程(下) 5、Elasticsearch 系列文章（一）：Elasticsearch 默认分词器和中分分词器之间的比较及使用方法 6、Elasticsearch 系列文章（二）：全文搜索引擎 Elasticsearch 集群搭建入门教程 7、Elasticsearch 系列文章（三）：ElasticSearch 集群监控 8、Elasticsearch 系列文章（四）：ElasticSearch 单个节点监控 9、Elasticsearch 系列文章（五）：ELK 实时日志分析平台环境搭建 10、教你如何在 IDEA 远程 Debug ElasticSearch","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://www.54tianzhisheng.cn/tags/ElasticSearch/"}]},{"title":"渣渣菜鸡的 ElasticSearch 源码解析 —— 启动流程（下）","date":"2018-08-11T16:00:00.000Z","path":"2018/08/12/es-code03/","text":"关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/08/12/es-code03/ 前提上篇文章写完了 ES 流程启动的一部分，main 方法都入口，以及创建 Elasticsearch 运行的必须环境以及相关配置，接着就是创建该环境的节点了。 Node 的创建看下新建节点的代码：(代码比较多，这里是比较关键的地方，我就把注释直接写在代码上面了，实在不好拆开这段代码，300 多行代码) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278public Node(Environment environment) &#123; this(environment, Collections.emptyList()); //执行下面的代码 &#125;protected Node(final Environment environment, Collection&lt;Class&lt;? extends Plugin&gt;&gt; classpathPlugins) &#123; final List&lt;Closeable&gt; resourcesToClose = new ArrayList&lt;&gt;(); // register everything we need to release in the case of an error boolean success = false; &#123;// use temp logger just to say we are starting. we can't use it later on because the node name might not be set Logger logger = Loggers.getLogger(Node.class, NODE_NAME_SETTING.get(environment.settings())); logger.info(\"initializing ...\"); &#125; try &#123; originalSettings = environment.settings(); Settings tmpSettings = Settings.builder().put(environment.settings()) .put(Client.CLIENT_TYPE_SETTING_S.getKey(), CLIENT_TYPE).build();// create the node environment as soon as possible, to recover the node id and enable logging try &#123; nodeEnvironment = new NodeEnvironment(tmpSettings, environment); //1、创建节点环境,比如节点名称,节点ID,分片信息,存储元,以及分配内存准备给节点使用 resourcesToClose.add(nodeEnvironment); &#125; catch (IOException ex) &#123; throw new IllegalStateException(\"Failed to create node environment\", ex); &#125; final boolean hadPredefinedNodeName = NODE_NAME_SETTING.exists(tmpSettings); final String nodeId = nodeEnvironment.nodeId(); tmpSettings = addNodeNameIfNeeded(tmpSettings, nodeId); final Logger logger = Loggers.getLogger(Node.class, tmpSettings);// this must be captured after the node name is possibly added to the settings final String nodeName = NODE_NAME_SETTING.get(tmpSettings); if (hadPredefinedNodeName == false) &#123; logger.info(\"node name derived from node ID [&#123;&#125;]; set [&#123;&#125;] to override\", nodeId, NODE_NAME_SETTING.getKey()); &#125; else &#123; logger.info(\"node name [&#123;&#125;], node ID [&#123;&#125;]\", nodeName, nodeId); &#125; //2、打印出JVM相关信息 final JvmInfo jvmInfo = JvmInfo.jvmInfo(); logger.info(\"version[&#123;&#125;], pid[&#123;&#125;], build[&#123;&#125;/&#123;&#125;/&#123;&#125;/&#123;&#125;], OS[&#123;&#125;/&#123;&#125;/&#123;&#125;], JVM[&#123;&#125;/&#123;&#125;/&#123;&#125;/&#123;&#125;]\", Version.displayVersion(Version.CURRENT, Build.CURRENT.isSnapshot()), jvmInfo.pid(), Build.CURRENT.flavor().displayName(), Build.CURRENT.type().displayName(), Build.CURRENT.shortHash(), Build.CURRENT.date(), Constants.OS_NAME, Constants.OS_VERSION, Constants.OS_ARCH,Constants.JVM_VENDOR,Constants.JVM_NAME, Constants.JAVA_VERSION,Constants.JVM_VERSION); logger.info(\"JVM arguments &#123;&#125;\", Arrays.toString(jvmInfo.getInputArguments())); //检查当前版本是不是 pre-release 版本（Snapshot）， warnIfPreRelease(Version.CURRENT, Build.CURRENT.isSnapshot(), logger); 。。。 this.pluginsService = new PluginsService(tmpSettings, environment.configFile(), environment.modulesFile(), environment.pluginsFile(), classpathPlugins); //3、利用PluginsService加载相应的模块和插件 this.settings = pluginsService.updatedSettings(); localNodeFactory = new LocalNodeFactory(settings, nodeEnvironment.nodeId());// create the environment based on the finalized (processed) view of the settings// this is just to makes sure that people get the same settings, no matter where they ask them from this.environment = new Environment(this.settings, environment.configFile()); Environment.assertEquivalent(environment, this.environment); final List&lt;ExecutorBuilder&lt;?&gt;&gt; executorBuilders = pluginsService.getExecutorBuilders(settings); //线程池 final ThreadPool threadPool = new ThreadPool(settings, executorBuilders.toArray(new ExecutorBuilder[0])); resourcesToClose.add(() -&gt; ThreadPool.terminate(threadPool, 10, TimeUnit.SECONDS)); // adds the context to the DeprecationLogger so that it does not need to be injected everywhere DeprecationLogger.setThreadContext(threadPool.getThreadContext()); resourcesToClose.add(() -&gt; DeprecationLogger.removeThreadContext(threadPool.getThreadContext())); final List&lt;Setting&lt;?&gt;&gt; additionalSettings = new ArrayList&lt;&gt;(pluginsService.getPluginSettings()); //额外配置 final List&lt;String&gt; additionalSettingsFilter = new ArrayList&lt;&gt;(pluginsService.getPluginSettingsFilter()); for (final ExecutorBuilder&lt;?&gt; builder : threadPool.builders()) &#123; //4、加载一些额外配置 additionalSettings.addAll(builder.getRegisteredSettings()); &#125; client = new NodeClient(settings, threadPool);//5、创建一个节点客户端 //6、缓存一系列模块,如NodeModule,ClusterModule,IndicesModule,ActionModule,GatewayModule,SettingsModule,RepositioriesModule，scriptModule，analysisModule final ResourceWatcherService resourceWatcherService = new ResourceWatcherService(settings, threadPool); final ScriptModule scriptModule = new ScriptModule(settings, pluginsService.filterPlugins(ScriptPlugin.class)); AnalysisModule analysisModule = new AnalysisModule(this.environment, pluginsService.filterPlugins(AnalysisPlugin.class)); // this is as early as we can validate settings at this point. we already pass them to ScriptModule as well as ThreadPool so we might be late here already final SettingsModule settingsModule = new SettingsModule(this.settings, additionalSettings, additionalSettingsFilter);scriptModule.registerClusterSettingsListeners(settingsModule.getClusterSettings()); resourcesToClose.add(resourceWatcherService); final NetworkService networkService = new NetworkService( getCustomNameResolvers(pluginsService.filterPlugins(DiscoveryPlugin.class))); List&lt;ClusterPlugin&gt; clusterPlugins = pluginsService.filterPlugins(ClusterPlugin.class); final ClusterService clusterService = new ClusterService(settings, settingsModule.getClusterSettings(), threadPool, ClusterModule.getClusterStateCustomSuppliers(clusterPlugins)); clusterService.addStateApplier(scriptModule.getScriptService()); resourcesToClose.add(clusterService); final IngestService ingestService = new IngestService(settings, threadPool, this.environment, scriptModule.getScriptService(), analysisModule.getAnalysisRegistry(), pluginsService.filterPlugins(IngestPlugin.class)); final DiskThresholdMonitor listener = new DiskThresholdMonitor(settings, clusterService::state, clusterService.getClusterSettings(), client); final ClusterInfoService clusterInfoService = newClusterInfoService(settings, clusterService, threadPool, client,listener::onNewInfo); final UsageService usageService = new UsageService(settings); ModulesBuilder modules = new ModulesBuilder();// plugin modules must be added here, before others or we can get crazy injection errors... for (Module pluginModule : pluginsService.createGuiceModules()) &#123; modules.add(pluginModule); &#125; final MonitorService monitorService = new MonitorService(settings, nodeEnvironment, threadPool, clusterInfoService); ClusterModule clusterModule = new ClusterModule(settings, clusterService, clusterPlugins, clusterInfoService); modules.add(clusterModule); IndicesModule indicesModule = new IndicesModule(pluginsService.filterPlugins(MapperPlugin.class)); modules.add(indicesModule); SearchModule searchModule = new SearchModule(settings, false, pluginsService.filterPlugins(SearchPlugin.class)); CircuitBreakerService circuitBreakerService = createCircuitBreakerService(settingsModule.getSettings(), settingsModule.getClusterSettings()); resourcesToClose.add(circuitBreakerService); modules.add(new GatewayModule()); PageCacheRecycler pageCacheRecycler = createPageCacheRecycler(settings); BigArrays bigArrays = createBigArrays(pageCacheRecycler, circuitBreakerService); resourcesToClose.add(bigArrays); modules.add(settingsModule); List&lt;NamedWriteableRegistry.Entry&gt; namedWriteables = Stream.of( NetworkModule.getNamedWriteables().stream(), indicesModule.getNamedWriteables().stream(), searchModule.getNamedWriteables().stream(), pluginsService.filterPlugins(Plugin.class).stream() .flatMap(p -&gt; p.getNamedWriteables().stream()), ClusterModule.getNamedWriteables().stream()) .flatMap(Function.identity()).collect(Collectors.toList()); final NamedWriteableRegistry namedWriteableRegistry = new NamedWriteableRegistry(namedWriteables); NamedXContentRegistry xContentRegistry = new NamedXContentRegistry(Stream.of( NetworkModule.getNamedXContents().stream(), searchModule.getNamedXContents().stream(), pluginsService.filterPlugins(Plugin.class).stream() .flatMap(p -&gt; p.getNamedXContent().stream()), ClusterModule.getNamedXWriteables().stream()).flatMap(Function.identity()).collect(toList())); modules.add(new RepositoriesModule(this.environment, pluginsService.filterPlugins(RepositoryPlugin.class), xContentRegistry)); final MetaStateService metaStateService = new MetaStateService(settings, nodeEnvironment, xContentRegistry); final IndicesService indicesService = new IndicesService(settings, pluginsService, nodeEnvironment, xContentRegistry,analysisModule.getAnalysisRegistry(), clusterModule.getIndexNameExpressionResolver(), indicesModule.getMapperRegistry(), namedWriteableRegistry,threadPool, settingsModule.getIndexScopedSettings(), circuitBreakerService, bigArrays, scriptModule.getScriptService(),client, metaStateService); Collection&lt;Object&gt; pluginComponents = pluginsService.filterPlugins(Plugin.class).stream() .flatMap(p -&gt; p.createComponents(client, clusterService, threadPool, resourceWatcherService,scriptModule.getScriptService(), xContentRegistry, environment, nodeEnvironment,namedWriteableRegistry).stream()).collect(Collectors.toList()); ActionModule actionModule = new ActionModule(false, settings, clusterModule.getIndexNameExpressionResolver(), settingsModule.getIndexScopedSettings(), settingsModule.getClusterSettings(), settingsModule.getSettingsFilter(),threadPool, pluginsService.filterPlugins(ActionPlugin.class), client, circuitBreakerService, usageService); modules.add(actionModule); //7、获取RestController,用于处理各种Elasticsearch的rest命令,如_cat,_all,_cat/health,_clusters等rest命令(Elasticsearch称之为action) final RestController restController = actionModule.getRestController(); final NetworkModule networkModule = new NetworkModule(settings, false, pluginsService.filterPlugins(NetworkPlugin.class),threadPool, bigArrays, pageCacheRecycler, circuitBreakerService, namedWriteableRegistry, xContentRegistry,networkService, restController); Collection&lt;UnaryOperator&lt;Map&lt;String, MetaData.Custom&gt;&gt;&gt; customMetaDataUpgraders = pluginsService.filterPlugins(Plugin.class).stream() .map(Plugin::getCustomMetaDataUpgrader) .collect(Collectors.toList()); Collection&lt;UnaryOperator&lt;Map&lt;String, IndexTemplateMetaData&gt;&gt;&gt; indexTemplateMetaDataUpgraders = pluginsService.filterPlugins(Plugin.class).stream() .map(Plugin::getIndexTemplateMetaDataUpgrader) .collect(Collectors.toList()); Collection&lt;UnaryOperator&lt;IndexMetaData&gt;&gt; indexMetaDataUpgraders = pluginsService.filterPlugins(Plugin.class).stream() .map(Plugin::getIndexMetaDataUpgrader).collect(Collectors.toList()); final MetaDataUpgrader metaDataUpgrader = new MetaDataUpgrader(customMetaDataUpgraders, indexTemplateMetaDataUpgraders); final MetaDataIndexUpgradeService metaDataIndexUpgradeService = new MetaDataIndexUpgradeService(settings, xContentRegistry, indicesModule.getMapperRegistry(), settingsModule.getIndexScopedSettings(), indexMetaDataUpgraders); final GatewayMetaState gatewayMetaState = new GatewayMetaState(settings, nodeEnvironment, metaStateService, metaDataIndexUpgradeService, metaDataUpgrader); new TemplateUpgradeService(settings, client, clusterService, threadPool, indexTemplateMetaDataUpgraders); final Transport transport = networkModule.getTransportSupplier().get(); Set&lt;String&gt; taskHeaders = Stream.concat( pluginsService.filterPlugins(ActionPlugin.class).stream().flatMap(p -&gt; p.getTaskHeaders().stream()), Stream.of(\"X-Opaque-Id\") ).collect(Collectors.toSet()); final TransportService transportService = newTransportService(settings, transport, threadPool, networkModule.getTransportInterceptor(), localNodeFactory, settingsModule.getClusterSettings(), taskHeaders); final ResponseCollectorService responseCollectorService = new ResponseCollectorService(this.settings, clusterService); final SearchTransportService searchTransportService = new SearchTransportService(settings, transportService, SearchExecutionStatsCollector.makeWrapper(responseCollectorService)); final Consumer&lt;Binder&gt; httpBind; final HttpServerTransport httpServerTransport; if (networkModule.isHttpEnabled()) &#123; httpServerTransport = networkModule.getHttpServerTransportSupplier().get(); httpBind = b -&gt; &#123;b.bind(HttpServerTransport.class).toInstance(httpServerTransport); &#125;; &#125; else &#123; httpBind = b -&gt; &#123; b.bind(HttpServerTransport.class).toProvider(Providers.of(null)); &#125;; httpServerTransport = null; &#125; final DiscoveryModule discoveryModule = new DiscoveryModule(this.settings, threadPool, transportService, namedWriteableRegistry,networkService, clusterService.getMasterService(), clusterService.getClusterApplierService(),clusterService.getClusterSettings(), pluginsService.filterPlugins(DiscoveryPlugin.class),clusterModule.getAllocationService()); this.nodeService = new NodeService(settings, threadPool, monitorService, discoveryModule.getDiscovery(),transportService, indicesService, pluginsService, circuitBreakerService, scriptModule.getScriptService(),httpServerTransport, ingestService, clusterService, settingsModule.getSettingsFilter(), responseCollectorService,searchTransportService); final SearchService searchService = newSearchService(clusterService, indicesService, threadPool, scriptModule.getScriptService(), bigArrays, searchModule.getFetchPhase(),responseCollectorService); final List&lt;PersistentTasksExecutor&lt;?&gt;&gt; tasksExecutors = pluginsService .filterPlugins(PersistentTaskPlugin.class).stream() .map(p -&gt; p.getPersistentTasksExecutor(clusterService, threadPool, client)) .flatMap(List::stream) .collect(toList()); final PersistentTasksExecutorRegistry registry = new PersistentTasksExecutorRegistry(settings, tasksExecutors); final PersistentTasksClusterService persistentTasksClusterService = new PersistentTasksClusterService(settings, registry, clusterService); final PersistentTasksService persistentTasksService = new PersistentTasksService(settings, clusterService, threadPool, client);//8、绑定处理各种服务的实例,这里是最核心的地方,也是Elasticsearch能处理各种服务的核心. modules.add(b -&gt; &#123; b.bind(Node.class).toInstance(this); b.bind(NodeService.class).toInstance(nodeService); b.bind(NamedXContentRegistry.class).toInstance(xContentRegistry); b.bind(PluginsService.class).toInstance(pluginsService); b.bind(Client.class).toInstance(client); b.bind(NodeClient.class).toInstance(client); b.bind(Environment.class).toInstance(this.environment); b.bind(ThreadPool.class).toInstance(threadPool); b.bind(NodeEnvironment.class).toInstance(nodeEnvironment); b.bind(ResourceWatcherService.class).toInstance(resourceWatcherService);b.bind(CircuitBreakerService.class).toInstance(circuitBreakerService); b.bind(BigArrays.class).toInstance(bigArrays); b.bind(ScriptService.class).toInstance(scriptModule.getScriptService()); b.bind(AnalysisRegistry.class).toInstance(analysisModule.getAnalysisRegistry()); b.bind(IngestService.class).toInstance(ingestService); b.bind(UsageService.class).toInstance(usageService); b.bind(NamedWriteableRegistry.class).toInstance(namedWriteableRegistry); b.bind(MetaDataUpgrader.class).toInstance(metaDataUpgrader); b.bind(MetaStateService.class).toInstance(metaStateService); b.bind(IndicesService.class).toInstance(indicesService); b.bind(SearchService.class).toInstance(searchService); b.bind(SearchTransportService.class).toInstance(searchTransportService);b.bind(SearchPhaseController.class).toInstance(new SearchPhaseController(settings, searchService::createReduceContext)); b.bind(Transport.class).toInstance(transport); b.bind(TransportService.class).toInstance(transportService); b.bind(NetworkService.class).toInstance(networkService); b.bind(UpdateHelper.class).toInstance(new UpdateHelper(settings, scriptModule.getScriptService()));b.bind(MetaDataIndexUpgradeService.class).toInstance(metaDataIndexUpgradeService); b.bind(ClusterInfoService.class).toInstance(clusterInfoService); b.bind(GatewayMetaState.class).toInstance(gatewayMetaState); b.bind(Discovery.class).toInstance(discoveryModule.getDiscovery()); &#123; RecoverySettings recoverySettings = new RecoverySettings(settings, settingsModule.getClusterSettings()); processRecoverySettings(settingsModule.getClusterSettings(), recoverySettings); b.bind(PeerRecoverySourceService.class).toInstance(new PeerRecoverySourceService(settings, transportService,indicesService, recoverySettings)); b.bind(PeerRecoveryTargetService.class).toInstance(new PeerRecoveryTargetService(settings, threadPool,transportService, recoverySettings, clusterService)); &#125; httpBind.accept(b); pluginComponents.stream().forEach(p -&gt; b.bind((Class) p.getClass()).toInstance(p));b.bind(PersistentTasksService.class).toInstance(persistentTasksService); b.bind(PersistentTasksClusterService.class).toInstance(persistentTasksClusterService);b.bind(PersistentTasksExecutorRegistry.class).toInstance(registry); &#125;); injector = modules.createInjector(); // TODO hack around circular dependencies problems in AllocationServiceclusterModule.getAllocationService().setGatewayAllocator(injector.getInstance(GatewayAllocator.class)); List&lt;LifecycleComponent&gt; pluginLifecycleComponents = pluginComponents.stream() .filter(p -&gt; p instanceof LifecycleComponent) .map(p -&gt; (LifecycleComponent) p).collect(Collectors.toList()); //9、利用Guice将各种模块以及服务(xxxService)注入到Elasticsearch环境中pluginLifecycleComponents.addAll(pluginsService.getGuiceServiceClasses().stream() .map(injector::getInstance).collect(Collectors.toList())); resourcesToClose.addAll(pluginLifecycleComponents); this.pluginLifecycleComponents = Collections.unmodifiableList(pluginLifecycleComponents); client.initialize(injector.getInstance(new Key&lt;Map&lt;GenericAction, TransportAction&gt;&gt;() &#123;&#125;), () -&gt; clusterService.localNode().getId(), transportService.getRemoteClusterService()); if (NetworkModule.HTTP_ENABLED.get(settings)) &#123; //如果elasticsearch.yml文件中配置了http.enabled参数(默认为true),则会初始化RestHandlers logger.debug(\"initializing HTTP handlers ...\"); actionModule.initRestHandlers(() -&gt; clusterService.state().nodes()); //初始化RestHandlers, 解析集群命令,如_cat/,_cat/health &#125; //10、初始化工作完成 logger.info(\"initialized\"); success = true; &#125; catch (IOException ex) &#123; throw new ElasticsearchException(\"failed to bind service\", ex); &#125; finally &#123; if (!success) &#123; IOUtils.closeWhileHandlingException(resourcesToClose); &#125; &#125;&#125; 上面代码真的很多，这里再说下上面这么多代码主要干了什么吧：（具体是哪行代码执行的如下流程，上面代码中也标记了） 1、创建节点环境,比如节点名称,节点 ID,分片信息,存储元,以及分配内存准备给节点使用 2、打印出 JVM 相关信息 3、利用 PluginsService 加载相应的模块和插件，具体哪些模块可以去 modules 目录下查看 4、加载一些额外的配置参数 5、创建一个节点客户端 6、缓存一系列模块,如NodeModule,ClusterModule,IndicesModule,ActionModule,GatewayModule,SettingsModule,RepositioriesModule，scriptModule，analysisModule 7、获取 RestController，用于处理各种 Elasticsearch 的 rest 命令,如 _cat, _all, _cat/health, _clusters 等 rest命令 8、绑定处理各种服务的实例 9、利用 Guice 将各种模块以及服务(xxxService)注入到 Elasticsearch 环境中 10、初始化工作完成（打印日志） JarHell 报错解释前一篇阅读源码环境搭建的文章写过用 JDK 1.8 编译 ES 源码是会遇到如下异常： 1org.elasticsearch.bootstrap.StartupException: java.lang.IllegalStateException: jar hell! 这里说下就是 setup 方法中的如下代码导致的 1234567try &#123; // look for jar hell final Logger logger = ESLoggerFactory.getLogger(JarHell.class); JarHell.checkJarHell(logger::debug);&#125; catch (IOException | URISyntaxException e) &#123; throw new BootstrapException(e);&#125; 所以你如果是用 JDK 1.8 编译的，那么就需要把所有的有这块的代码给注释掉就可以编译成功的。 我自己试过用 JDK 10 编译是没有出现这里报错的。 正式启动 ES 节点回到上面 Bootstrap 中的静态 init 方法中，接下来就是正式启动 elasticsearch 节点了： 123456INSTANCE.start(); //调用下面的 start 方法private void start() throws NodeValidationException &#123; node.start(); //正式启动 Elasticsearch 节点 keepAliveThread.start();&#125; 接下来看看这个 start 方法里面的 node.start() 方法源码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120public Node start() throws NodeValidationException &#123; if (!lifecycle.moveToStarted()) &#123; return this; &#125; Logger logger = Loggers.getLogger(Node.class, NODE_NAME_SETTING.get(settings)); logger.info(\"starting ...\"); pluginLifecycleComponents.forEach(LifecycleComponent::start); //1、利用Guice获取上述注册的各种模块以及服务 //Node 的启动其实就是 node 里每个组件的启动，同样的，分别调用不同的实例的 start 方法来启动这个组件, 如下： injector.getInstance(MappingUpdatedAction.class).setClient(client); injector.getInstance(IndicesService.class).start(); injector.getInstance(IndicesClusterStateService.class).start(); injector.getInstance(SnapshotsService.class).start(); injector.getInstance(SnapshotShardsService.class).start(); injector.getInstance(RoutingService.class).start(); injector.getInstance(SearchService.class).start(); nodeService.getMonitorService().start(); final ClusterService clusterService = injector.getInstance(ClusterService.class); final NodeConnectionsService nodeConnectionsService = injector.getInstance(NodeConnectionsService.class); nodeConnectionsService.start(); clusterService.setNodeConnectionsService(nodeConnectionsService); injector.getInstance(ResourceWatcherService.class).start(); injector.getInstance(GatewayService.class).start(); Discovery discovery = injector.getInstance(Discovery.class); clusterService.getMasterService().setClusterStatePublisher(discovery::publish); // Start the transport service now so the publish address will be added to the local disco node in ClusterService TransportService transportService = injector.getInstance(TransportService.class); transportService.getTaskManager().setTaskResultsService(injector.getInstance(TaskResultsService.class)); transportService.start(); assert localNodeFactory.getNode() != null; assert transportService.getLocalNode().equals(localNodeFactory.getNode()) : \"transportService has a different local node than the factory provided\"; final MetaData onDiskMetadata; try &#123; // we load the global state here (the persistent part of the cluster state stored on disk) to // pass it to the bootstrap checks to allow plugins to enforce certain preconditions based on the recovered state. if (DiscoveryNode.isMasterNode(settings) || DiscoveryNode.isDataNode(settings)) &#123;//根据配置文件看当前节点是master还是data节点 onDiskMetadata = injector.getInstance(GatewayMetaState.class).loadMetaState(); &#125; else &#123; onDiskMetadata = MetaData.EMPTY_META_DATA; &#125; assert onDiskMetadata != null : \"metadata is null but shouldn't\"; // this is never null &#125; catch (IOException e) &#123; throw new UncheckedIOException(e); &#125; validateNodeBeforeAcceptingRequests(new BootstrapContext(settings, onDiskMetadata), transportService.boundAddress(), pluginsService .filterPlugins(Plugin .class) .stream() .flatMap(p -&gt; p.getBootstrapChecks().stream()).collect(Collectors.toList())); //2、将当前节点加入到一个集群簇中去,并启动当前节点 clusterService.addStateApplier(transportService.getTaskManager()); // start after transport service so the local disco is known discovery.start(); // start before cluster service so that it can set initial state on ClusterApplierService clusterService.start(); assert clusterService.localNode().equals(localNodeFactory.getNode()) : \"clusterService has a different local node than the factory provided\"; transportService.acceptIncomingRequests(); discovery.startInitialJoin(); // tribe nodes don't have a master so we shouldn't register an observer s final TimeValue initialStateTimeout = DiscoverySettings.INITIAL_STATE_TIMEOUT_SETTING.get(settings); if (initialStateTimeout.millis() &gt; 0) &#123; final ThreadPool thread = injector.getInstance(ThreadPool.class); ClusterState clusterState = clusterService.state(); ClusterStateObserver observer = new ClusterStateObserver(clusterState, clusterService, null, logger, thread.getThreadContext()); if (clusterState.nodes().getMasterNodeId() == null) &#123; logger.debug(\"waiting to join the cluster. timeout [&#123;&#125;]\", initialStateTimeout); final CountDownLatch latch = new CountDownLatch(1); observer.waitForNextChange(new ClusterStateObserver.Listener() &#123; @Override public void onNewClusterState(ClusterState state) &#123; latch.countDown(); &#125; @Override public void onClusterServiceClose() &#123; latch.countDown(); &#125; @Override public void onTimeout(TimeValue timeout) &#123; logger.warn(\"timed out while waiting for initial discovery state - timeout: &#123;&#125;\", initialStateTimeout); latch.countDown(); &#125; &#125;, state -&gt; state.nodes().getMasterNodeId() != null, initialStateTimeout); try &#123; latch.await(); &#125; catch (InterruptedException e) &#123; throw new ElasticsearchTimeoutException(\"Interrupted while waiting for initial discovery state\"); &#125; &#125; &#125; if (NetworkModule.HTTP_ENABLED.get(settings)) &#123; injector.getInstance(HttpServerTransport.class).start(); &#125; if (WRITE_PORTS_FILE_SETTING.get(settings)) &#123; if (NetworkModule.HTTP_ENABLED.get(settings)) &#123; HttpServerTransport http = injector.getInstance(HttpServerTransport.class); writePortsFile(\"http\", http.boundAddress()); &#125; TransportService transport = injector.getInstance(TransportService.class); writePortsFile(\"transport\", transport.boundAddress()); &#125; logger.info(\"started\"); pluginsService.filterPlugins(ClusterPlugin.class).forEach(ClusterPlugin::onNodeStarted); return this;&#125; 上面代码主要是： 1、利用 Guice 获取上述注册的各种模块以及服务，然后启动 node 里每个组件（分别调用不同的实例的 start 方法来启动） 2、打印日志（启动节点完成） 总结这篇文章主要把大概启动流程串通了，讲了下 node 节点的创建和正式启动 ES 节点了。因为篇幅较多所以拆开成两篇，先不扣细节了，后面流程启动文章写完后我们再单一的扣细节。 相关文章1、渣渣菜鸡为什么要看 ElasticSearch 源码？ 2、渣渣菜鸡的 ElasticSearch 源码解析 —— 环境搭建 3、渣渣菜鸡的 ElasticSearch 源码解析 —— 启动流程(上) 4、渣渣菜鸡的 ElasticSearch 源码解析 —— 启动流程(下) 5、Elasticsearch 系列文章（一）：Elasticsearch 默认分词器和中分分词器之间的比较及使用方法 6、Elasticsearch 系列文章（二）：全文搜索引擎 Elasticsearch 集群搭建入门教程 7、Elasticsearch 系列文章（三）：ElasticSearch 集群监控 8、Elasticsearch 系列文章（四）：ElasticSearch 单个节点监控 9、Elasticsearch 系列文章（五）：ELK 实时日志分析平台环境搭建 10、教你如何在 IDEA 远程 Debug ElasticSearch","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://www.54tianzhisheng.cn/tags/ElasticSearch/"}]},{"title":"渣渣菜鸡的 ElasticSearch 源码解析 —— 启动流程（上）","date":"2018-08-10T16:00:00.000Z","path":"2018/08/11/es-code02/","text":"关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/08/11/es-code02/ 前提上篇文章写了 ElasticSearch 源码解析 —— 环境搭建 ，其中里面说了启动 打开 server 模块下的 Elasticsearch 类：org.elasticsearch.bootstrap.Elasticsearch，运行里面的 main 函数就可以启动 ElasticSearch 了，这篇文章讲讲启动流程，因为篇幅会很多，所以分了两篇来写。 启动流程main 方法入口可以看到入口其实是一个 main 方法，方法里面先是检查权限，然后是一个错误日志监听器（确保在日志配置之前状态日志没有出现 error），然后是 Elasticsearch 对象的创建，然后调用了静态方法 main 方法（18 行），并把创建的对象和参数以及 Terminal 默认值传进去。静态的 main 方法里面调用 elasticsearch.main 方法。 1234567891011121314151617181920public static void main(final String[] args) throws Exception &#123; //1、入口 // we want the JVM to think there is a security manager installed so that if internal policy decisions that would be based on the // presence of a security manager or lack thereof act as if there is a security manager present (e.g., DNS cache policy) System.setSecurityManager(new SecurityManager() &#123; @Override public void checkPermission(Permission perm) &#123; // grant all permissions so that we can later set the security manager to the one that we want &#125; &#125;); LogConfigurator.registerErrorListener(); // final Elasticsearch elasticsearch = new Elasticsearch(); int status = main(args, elasticsearch, Terminal.DEFAULT); //2、调用Elasticsearch.main方法 if (status != ExitCodes.OK) &#123; exit(status); &#125;&#125;static int main(final String[] args, final Elasticsearch elasticsearch, final Terminal terminal) throws Exception &#123; return elasticsearch.main(args, terminal); //3、command main&#125; 因为 Elasticsearch 类是继承了 EnvironmentAwareCommand 类，EnvironmentAwareCommand 类继承了 Command 类，但是 Elasticsearch 类并没有重写 main 方法，所以上面调用的 elasticsearch.main 其实是调用了 Command 的 main 方法，代码如下： 123456789101112131415161718192021222324252627282930313233343536373839/** Parses options for this command from args and executes it. */public final int main(String[] args, Terminal terminal) throws Exception &#123; if (addShutdownHook()) &#123; //利用Runtime.getRuntime().addShutdownHook方法加入一个Hook，在程序退出时触发该Hook shutdownHookThread = new Thread(() -&gt; &#123; try &#123; this.close(); &#125; catch (final IOException e) &#123; try ( StringWriter sw = new StringWriter(); PrintWriter pw = new PrintWriter(sw)) &#123; e.printStackTrace(pw); terminal.println(sw.toString()); &#125; catch (final IOException impossible) &#123; // StringWriter#close declares a checked IOException from the Closeable interface but the Javadocs for StringWriter // say that an exception here is impossible throw new AssertionError(impossible); &#125; &#125; &#125;); Runtime.getRuntime().addShutdownHook(shutdownHookThread); &#125; beforeMain.run(); try &#123; mainWithoutErrorHandling(args, terminal);//4、mainWithoutErrorHandling &#125; catch (OptionException e) &#123; printHelp(terminal); terminal.println(Terminal.Verbosity.SILENT, \"ERROR: \" + e.getMessage()); return ExitCodes.USAGE; &#125; catch (UserException e) &#123; if (e.exitCode == ExitCodes.USAGE) &#123; printHelp(terminal); &#125; terminal.println(Terminal.Verbosity.SILENT, \"ERROR: \" + e.getMessage()); return e.exitCode; &#125; return ExitCodes.OK;&#125; 上面代码一开始利用一个勾子函数，在程序退出时触发该 Hook，该方法主要代码是 mainWithoutErrorHandling() 方法，然后下面的是 catch 住方法抛出的异常，方法代码如下： 12345678910111213141516/*** Executes the command, but all errors are thrown. */void mainWithoutErrorHandling(String[] args, Terminal terminal) throws Exception &#123; final OptionSet options = parser.parse(args); if (options.has(helpOption)) &#123; printHelp(terminal); return; &#125; if (options.has(silentOption)) &#123; terminal.setVerbosity(Terminal.Verbosity.SILENT); &#125; else if (options.has(verboseOption)) &#123; terminal.setVerbosity(Terminal.Verbosity.VERBOSE); &#125; else &#123; terminal.setVerbosity(Terminal.Verbosity.NORMAL); &#125; execute(terminal, options);//5、执行 EnvironmentAwareCommand 中的 execute()，（重写了command里面抽象的execute方法）&#125; 上面的代码从 3 ～ 14 行是解析传进来的参数并配置 terminal，重要的 execute() 方法，执行的是 EnvironmentAwareCommand 中的 execute() （重写了 Command 类里面的抽象 execute 方法），从上面那个继承图可以看到 EnvironmentAwareCommand 继承了 Command，重写的 execute 方法代码如下： 1234567891011121314151617181920212223@Overrideprotected void execute(Terminal terminal, OptionSet options) throws Exception &#123; final Map&lt;String, String&gt; settings = new HashMap&lt;&gt;(); for (final KeyValuePair kvp : settingOption.values(options)) &#123; if (kvp.value.isEmpty()) &#123; throw new UserException(ExitCodes.USAGE, \"setting [\" + kvp.key + \"] must not be empty\"); &#125; if (settings.containsKey(kvp.key)) &#123; final String message = String.format( Locale.ROOT, \"setting [%s] already set, saw [%s] and [%s]\", kvp.key, settings.get(kvp.key), kvp.value); throw new UserException(ExitCodes.USAGE, message); &#125; settings.put(kvp.key, kvp.value); &#125; //6、根据我们ide配置的 vm options 进行设置path.data、path.home、path.logs putSystemPropertyIfSettingIsMissing(settings, \"path.data\", \"es.path.data\"); putSystemPropertyIfSettingIsMissing(settings, \"path.home\", \"es.path.home\"); putSystemPropertyIfSettingIsMissing(settings, \"path.logs\", \"es.path.logs\"); execute(terminal, options, createEnv(terminal, settings));//7、先调用 createEnv 创建环境 //9、执行elasticsearch的execute方法，elasticsearch中重写了EnvironmentAwareCommand中的抽象execute方法&#125; 方法前面是根据传参去判断配置的，如果配置为空，就会直接跳到执行 putSystemPropertyIfSettingIsMissing 方法，这里会配置三个属性：path.data、path.home、path.logs 设置 es 的 data、home、logs 目录，它这里是根据我们 ide 配置的 vm options 进行设置的，这也是为什么我们上篇文章说的配置信息，如果不配置的话就会直接报错。下面看看 putSystemPropertyIfSettingIsMissing 方法代码里面怎么做到的： 12345678910111213141516/** Ensure the given setting exists, reading it from system properties if not already set. */private static void putSystemPropertyIfSettingIsMissing(final Map&lt;String, String&gt; settings, final String setting, final String key) &#123; final String value = System.getProperty(key);//获取key（es.path.data）找系统设置 if (value != null) &#123; if (settings.containsKey(setting)) &#123; final String message = String.format( Locale.ROOT, \"duplicate setting [%s] found via command-line [%s] and system property [%s]\", setting, settings.get(setting), value); throw new IllegalArgumentException(message); &#125; else &#123; settings.put(setting, value); &#125; &#125;&#125; 执行这三个方法后： 跳出此方法，继续看会发现 execute 方法调用了方法， 1execute(terminal, options, createEnv(terminal, settings)); 这里我们先看看 createEnv(terminal, settings) 方法： 1234567protected Environment createEnv(final Terminal terminal, final Map&lt;String, String&gt; settings) throws UserException &#123; final String esPathConf = System.getProperty(\"es.path.conf\");//8、读取我们 vm options 中配置的 es.path.conf if (esPathConf == null) &#123; throw new UserException(ExitCodes.CONFIG, \"the system property [es.path.conf] must be set\"); &#125; return InternalSettingsPreparer.prepareEnvironment(Settings.EMPTY, terminal, settings, getConfigPath(esPathConf)); //8、准备环境 prepareEnvironment&#125; 读取我们 ide vm options 中配置的 es.path.conf，同上篇文章也讲了这个一定要配置的，因为 es 启动的时候会加载我们的配置和一些插件。这里继续看下上面代码第 6 行的 prepareEnvironment 方法： 1234567891011121314151617181920212223242526272829303132333435public static Environment prepareEnvironment(Settings input, Terminal terminal, Map&lt;String, String&gt; properties, Path configPath) &#123; // just create enough settings to build the environment, to get the config dir Settings.Builder output = Settings.builder(); initializeSettings(output, input, properties); Environment environment = new Environment(output.build(), configPath); //查看 es.path.conf 目录下的配置文件是不是 yml 格式的，如果不是则抛出一个异常 if (Files.exists(environment.configFile().resolve(\"elasticsearch.yaml\"))) &#123; throw new SettingsException(\"elasticsearch.yaml was deprecated in 5.5.0 and must be renamed to elasticsearch.yml\"); &#125; if (Files.exists(environment.configFile().resolve(\"elasticsearch.json\"))) &#123; throw new SettingsException(\"elasticsearch.json was deprecated in 5.5.0 and must be converted to elasticsearch.yml\"); &#125; output = Settings.builder(); // start with a fresh output Path path = environment.configFile().resolve(\"elasticsearch.yml\"); if (Files.exists(path)) &#123; try &#123; output.loadFromPath(path); //加载文件并读取配置文件内容 &#125; catch (IOException e) &#123; throw new SettingsException(\"Failed to load settings from \" + path.toString(), e); &#125; &#125; // re-initialize settings now that the config file has been loaded initializeSettings(output, input, properties); //再一次初始化设置 finalizeSettings(output, terminal); environment = new Environment(output.build(), configPath); // we put back the path.logs so we can use it in the logging configuration file output.put(Environment.PATH_LOGS_SETTING.getKey(), environment.logsFile().toAbsolutePath().normalize().toString()); return new Environment(output.build(), configPath);&#125; 准备的环境如上图，通过构建的环境查看配置文件 elasticsearch.yml 是不是以 yml 结尾，如果是 yaml 或者 json 结尾的则抛出异常（在 5.5.0 版本其他两种格式过期了，只能使用 yml 格式），然后加载该配置文件并读取里面的内容（KV结构）。 跳出 createEnv 方法，我们继续看 execute 方法吧。 EnvironmentAwareCommand 类的 execute 方法代码如下： 1protected abstract void execute(Terminal terminal, OptionSet options, Environment env) throws Exception; 这是个抽象方法，那么它的实现方法在 Elasticsearch 类中，代码如下： 1234567891011121314151617181920212223242526272829303132333435@Overrideprotected void execute(Terminal terminal, OptionSet options, Environment env) throws UserException &#123; if (options.nonOptionArguments().isEmpty() == false) &#123; throw new UserException(ExitCodes.USAGE, \"Positional arguments not allowed, found \" + options.nonOptionArguments()); &#125; if (options.has(versionOption)) &#123; final String versionOutput = String.format( Locale.ROOT, \"Version: %s, Build: %s/%s/%s/%s, JVM: %s\", Version.displayVersion(Version.CURRENT, Build.CURRENT.isSnapshot()), Build.CURRENT.flavor().displayName(), Build.CURRENT.type().displayName(), Build.CURRENT.shortHash(), Build.CURRENT.date(), JvmInfo.jvmInfo().version()); terminal.println(versionOutput); return; &#125; final boolean daemonize = options.has(daemonizeOption); final Path pidFile = pidfileOption.value(options); final boolean quiet = options.has(quietOption); // a misconfigured java.io.tmpdir can cause hard-to-diagnose problems later, so reject it immediately try &#123; env.validateTmpFile(); &#125; catch (IOException e) &#123; throw new UserException(ExitCodes.CONFIG, e.getMessage()); &#125; try &#123; init(daemonize, pidFile, quiet, env); //10、初始化 &#125; catch (NodeValidationException e) &#123; throw new UserException(ExitCodes.CONFIG, e.getMessage()); &#125;&#125; 上面代码里主要还是看看 init(daemonize, pidFile, quiet, env); 初始化方法吧。 12345678910void init(final boolean daemonize, final Path pidFile, final boolean quiet, Environment initialEnv) throws NodeValidationException, UserException &#123; try &#123; Bootstrap.init(!daemonize, pidFile, quiet, initialEnv); //11、执行 Bootstrap 中的 init 方法 &#125; catch (BootstrapException | RuntimeException e) &#123; // format exceptions to the console in a special way // to avoid 2MB stacktraces from guice, etc. throw new StartupException(e); &#125;&#125; init 方法Bootstrap 中的静态 init 方法如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101static void init( final boolean foreground, final Path pidFile, final boolean quiet, final Environment initialEnv) throws BootstrapException, NodeValidationException, UserException &#123; // force the class initializer for BootstrapInfo to run before // the security manager is installed BootstrapInfo.init(); INSTANCE = new Bootstrap(); //12、创建一个 Bootstrap 实例 final SecureSettings keystore = loadSecureSettings(initialEnv);//如果注册了安全模块则将相关配置加载进来 final Environment environment = createEnvironment(foreground, pidFile, keystore, initialEnv.settings(), initialEnv.configFile()); //干之前干过的事情 try &#123; LogConfigurator.configure(environment); //13、log 配置环境 &#125; catch (IOException e) &#123; throw new BootstrapException(e); &#125; if (environment.pidFile() != null) &#123; try &#123; PidFile.create(environment.pidFile(), true); &#125; catch (IOException e) &#123; throw new BootstrapException(e); &#125; &#125; final boolean closeStandardStreams = (foreground == false) || quiet; try &#123; if (closeStandardStreams) &#123; final Logger rootLogger = ESLoggerFactory.getRootLogger(); final Appender maybeConsoleAppender = Loggers.findAppender(rootLogger, ConsoleAppender.class); if (maybeConsoleAppender != null) &#123; Loggers.removeAppender(rootLogger, maybeConsoleAppender); &#125; closeSystOut(); &#125; // fail if somebody replaced the lucene jars checkLucene(); //14、检查Lucene版本// install the default uncaught exception handler; must be done before security is initialized as we do not want to grant the runtime permission setDefaultUncaughtExceptionHandler Thread.setDefaultUncaughtExceptionHandler( new ElasticsearchUncaughtExceptionHandler(() -&gt; Node.NODE_NAME_SETTING.get(environment.settings()))); INSTANCE.setup(true, environment); //15、调用 setup 方法 try &#123; // any secure settings must be read during node construction IOUtils.close(keystore); &#125; catch (IOException e) &#123; throw new BootstrapException(e); &#125; INSTANCE.start(); //26、调用 start 方法 if (closeStandardStreams) &#123; closeSysError(); &#125; &#125; catch (NodeValidationException | RuntimeException e) &#123; // disable console logging, so user does not see the exception twice (jvm will show it already) final Logger rootLogger = ESLoggerFactory.getRootLogger(); final Appender maybeConsoleAppender = Loggers.findAppender(rootLogger, ConsoleAppender.class); if (foreground &amp;&amp; maybeConsoleAppender != null) &#123; Loggers.removeAppender(rootLogger, maybeConsoleAppender); &#125; Logger logger = Loggers.getLogger(Bootstrap.class); if (INSTANCE.node != null) &#123; logger = Loggers.getLogger(Bootstrap.class, Node.NODE_NAME_SETTING.get(INSTANCE.node.settings())); &#125; // HACK, it sucks to do this, but we will run users out of disk space otherwise if (e instanceof CreationException) &#123; // guice: log the shortened exc to the log file ByteArrayOutputStream os = new ByteArrayOutputStream(); PrintStream ps = null; try &#123; ps = new PrintStream(os, false, \"UTF-8\"); &#125; catch (UnsupportedEncodingException uee) &#123; assert false; e.addSuppressed(uee); &#125; new StartupException(e).printStackTrace(ps); ps.flush(); try &#123; logger.error(\"Guice Exception: &#123;&#125;\", os.toString(\"UTF-8\")); &#125; catch (UnsupportedEncodingException uee) &#123; assert false; e.addSuppressed(uee); &#125; &#125; else if (e instanceof NodeValidationException) &#123; logger.error(\"node validation exception\\n&#123;&#125;\", e.getMessage()); &#125; else &#123; // full exception logger.error(\"Exception\", e); &#125; // re-enable it if appropriate, so they can see any logging during the shutdown process if (foreground &amp;&amp; maybeConsoleAppender != null) &#123; Loggers.addAppender(rootLogger, maybeConsoleAppender); &#125; throw e; &#125;&#125; 该方法主要有： 1、创建 Bootstrap 实例 2、如果注册了安全模块则将相关配置加载进来 3、创建 Elasticsearch 运行的必须环境以及相关配置, 如将 config、scripts、plugins、modules、logs、lib、bin 等配置目录加载到运行环境中 4、log 配置环境，创建日志上下文 5、检查是否存在 PID 文件，如果不存在，创建 PID 文件 6、检查 Lucene 版本 7、调用 setup 方法（用当前环境来创建一个节点） setup 方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152private void setup(boolean addShutdownHook, Environment environment) throws BootstrapException &#123; Settings settings = environment.settings();//根据环境得到配置 try &#123; spawner.spawnNativeControllers(environment); &#125; catch (IOException e) &#123; throw new BootstrapException(e); &#125; initializeNatives( environment.tmpFile(), BootstrapSettings.MEMORY_LOCK_SETTING.get(settings), BootstrapSettings.SYSTEM_CALL_FILTER_SETTING.get(settings), BootstrapSettings.CTRLHANDLER_SETTING.get(settings)); // initialize probes before the security manager is installed initializeProbes(); if (addShutdownHook) &#123; Runtime.getRuntime().addShutdownHook(new Thread() &#123; @Override public void run() &#123; try &#123; IOUtils.close(node, spawner); LoggerContext context = (LoggerContext) LogManager.getContext(false); Configurator.shutdown(context); &#125; catch (IOException ex) &#123; throw new ElasticsearchException(\"failed to stop node\", ex); &#125; &#125; &#125;); &#125; try &#123; // look for jar hell final Logger logger = ESLoggerFactory.getLogger(JarHell.class); JarHell.checkJarHell(logger::debug); &#125; catch (IOException | URISyntaxException e) &#123; throw new BootstrapException(e); &#125; // Log ifconfig output before SecurityManager is installed IfConfig.logIfNecessary(); // install SM after natives, shutdown hooks, etc. try &#123; Security.configure(environment, BootstrapSettings.SECURITY_FILTER_BAD_DEFAULTS_SETTING.get(settings)); &#125; catch (IOException | NoSuchAlgorithmException e) &#123; throw new BootstrapException(e); &#125; node = new Node(environment) &#123; //16、新建节点 @Override protected void validateNodeBeforeAcceptingRequests( final BootstrapContext context, final BoundTransportAddress boundTransportAddress, List&lt;BootstrapCheck&gt; checks) throws NodeValidationException &#123; BootstrapChecks.check(context, boundTransportAddress, checks); &#125; &#125;;&#125; 上面代码最后就是 Node 节点的创建，这篇文章就不讲 Node 的创建了，下篇文章会好好讲一下 Node 节点的创建和正式启动 ES 节点的。 总结这篇文章主要先把大概启动流程串通，因为篇幅较多所以拆开成两篇，先不扣细节了，后面流程启动文章写完后我们再单一的扣细节。 相关文章1、渣渣菜鸡为什么要看 ElasticSearch 源码？ 2、渣渣菜鸡的 ElasticSearch 源码解析 —— 环境搭建 3、渣渣菜鸡的 ElasticSearch 源码解析 —— 启动流程(上) 4、渣渣菜鸡的 ElasticSearch 源码解析 —— 启动流程(下) 5、Elasticsearch 系列文章（一）：Elasticsearch 默认分词器和中分分词器之间的比较及使用方法 6、Elasticsearch 系列文章（二）：全文搜索引擎 Elasticsearch 集群搭建入门教程 7、Elasticsearch 系列文章（三）：ElasticSearch 集群监控 8、Elasticsearch 系列文章（四）：ElasticSearch 单个节点监控 9、Elasticsearch 系列文章（五）：ELK 实时日志分析平台环境搭建 10、教你如何在 IDEA 远程 Debug ElasticSearch","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://www.54tianzhisheng.cn/tags/ElasticSearch/"}]},{"title":"渣渣菜鸡的 ElasticSearch 源码解析 —— 环境搭建","date":"2018-08-04T16:00:00.000Z","path":"2018/08/05/es-code01/","text":"关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/08/05/es-code01/ 软件环境1、Intellij Idea:2018.2版本 2、Elasticsearch 源码版本: 6.3.2 3、JDK:10.0.2 4、Gradle : 建议 4.5 及以上版本 5、Macbook Pro 2017 安装 ElasticSearch 去 https://www.elastic.co/downloads/past-releases 这里找到 ElasticSearch 6.3.2 版本，下载后然后解压就好了。（注意：这个版本需要和下面的源码版本一致） 下载源码从 https://github.com/elastic/elasticsearch 上下载相应版本的源代码，这里建议用 git clone ，这样的话后面你可以随意切换到 ElasticSearch 的其他版本去。 1git clone git@github.com:elastic/elasticsearch.git 我们看下有哪些版本的： 1git tag 找到了目前源码版本最新的版本的稳定版为：v6.3.2 切换到该版本： 1git checkout v6.3.2 于是就可以切换到该稳定版本了。接下来不要直接导入到 IDEA/Eclipse 中。 编译GitHub 这里已经有描述如何导入 IDEA/Eclipse 中： 1234567891011121314151617181920JDK 10 is required to build Elasticsearch. You must have a JDK 10 installation with the environment variable JAVA_HOME referencing the path to Java home for your JDK 10 installation. By default, tests use the same runtime as JAVA_HOME. However, since Elasticsearch, supports JDK 8 the build supports compiling with JDK 10 and testing on a JDK 8 runtime; to do this, set RUNTIME_JAVA_HOME pointing to the Java home of a JDK 8 installation. Note that this mechanism can be used to test against other JDKs as well, this is not only limited to JDK 8.Note: It is also required to have JAVA7_HOME, JAVA8_HOME and JAVA10_HOME available so that the tests can pass.Warning: do not use sdkman for Java installations which do not have proper jrunscript for jdk distributions.Elasticsearch uses the Gradle wrapper for its build. You can execute Gradle using the wrapper via the gradlew script in the root of the repository.Configuring IDEs And Running TestsEclipse users can automatically configure their IDE: ./gradlew eclipse then File: Import: Existing Projects into Workspace. Select the option Search for nested projects. Additionally you will want to ensure that Eclipse is using 2048m of heap by modifying eclipse.ini accordingly to avoid GC overhead errors.IntelliJ users can automatically configure their IDE: ./gradlew idea then File-&gt;New Project From Existing Sources. Point to the root of the source directory, select Import project from external model-&gt;Gradle, enable Use auto-import. In order to run tests directly from IDEA 2017.2 and above, it is required to disable the IDEA run launcher in order to avoid idea_rt.jar causing &quot;jar hell&quot;. This can be achieved by adding the -Didea.no.launcher=true JVM option. Alternatively, idea.no.launcher=true can be set in the idea.properties file which can be accessed under Help &gt; Edit Custom Properties (this will require a restart of IDEA). For IDEA 2017.3 and above, in addition to the JVM option, you will need to go to Run-&gt;Edit Configurations-&gt;...-&gt;Defaults-&gt;JUnit and verify that the Shorten command line setting is set to user-local default: none. You may also need to remove ant-javafx.jar from your classpath if that is reported as a source of jar hell.To run an instance of elasticsearch from the source code run ./gradlew runThe Elasticsearch codebase makes heavy use of Java asserts and the test runner requires that assertions be enabled within the JVM. This can be accomplished by passing the flag -ea to the JVM on startup.For IntelliJ, go to Run-&gt;Edit Configurations...-&gt;Defaults-&gt;JUnit-&gt;VM options and input -ea.For Eclipse, go to Preferences-&gt;Java-&gt;Installed JREs and add -ea to VM Arguments. 上面说了下如何编译 Elasticsearch 和如何在 ide 中配置好环境。下面说下步骤吧：（这里我只是演示在 IDEA 中如何导入） 1、在我们下载的 Elasticsearch 根目录下执行命令：(执行已经写好的脚本 gradlew) 1./gradlew idea 请注意版本和我的一致，早的版本可能没有该执行脚本，需要执行 gradle idea 命令 最后结果如下： 2、导入 IDEA idea 中 File -&gt; New Project From Existing Sources 选择你下载的 Elasticsearch 根目录，然后点 open ，之后 Import project from external model -&gt; Gradle , 选中 Use auto-import, 然后就可以了。 导入进去后，gradle 又会编译一遍，需要等一会，好了之后如下： 运行打开 server 模块下的 Elasticsearch 类：org.elasticsearch.bootstrap.Elasticsearch，运行里面的 main 函数。 1、报错如下： 1ERROR: the system property [es.path.conf] must be set 我们在运行的配置 vm options 如下：（后面启动流程会写为什么会报这个错误） 1-Des.path.conf=&quot;/usr/local/elasticsearch-6.3.2/config&quot; 2、再次运行，报错如下： 12345678910Exception in thread &quot;main&quot; java.lang.IllegalStateException: path.home is not configured at org.elasticsearch.env.Environment.&lt;init&gt;(Environment.java:103) at org.elasticsearch.env.Environment.&lt;init&gt;(Environment.java:94) at org.elasticsearch.node.InternalSettingsPreparer.prepareEnvironment(InternalSettingsPreparer.java:86) at org.elasticsearch.cli.EnvironmentAwareCommand.createEnv(EnvironmentAwareCommand.java:95) at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) at org.elasticsearch.cli.Command.main(Command.java:90) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:93) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:86) 我们在运行的配置 vm options 如下：（后面启动流程会写为什么会报这个错误） 1-Des.path.home=&quot;/usr/local/elasticsearch-6.3.2&quot; 3、再次运行，报错如下： 123456789101112131415161718192021222324252018-08-01 09:38:03,974 main ERROR Could not register mbeans java.security.AccessControlException: access denied (&quot;javax.management.MBeanTrustPermission&quot; &quot;register&quot;) at java.base/java.security.AccessControlContext.checkPermission(AccessControlContext.java:472) at java.base/java.lang.SecurityManager.checkPermission(SecurityManager.java:371) at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.checkMBeanTrustPermission(DefaultMBeanServerInterceptor.java:1805) at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:318) at java.management/com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522) at org.apache.logging.log4j.core.jmx.Server.register(Server.java:389) at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:167) at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:140) at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:556) at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:261) at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:206) at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:220) at org.apache.logging.log4j.core.config.Configurator.initialize(Configurator.java:197) at org.elasticsearch.common.logging.LogConfigurator.configureStatusLogger(LogConfigurator.java:171) at org.elasticsearch.common.logging.LogConfigurator.configure(LogConfigurator.java:140) at org.elasticsearch.common.logging.LogConfigurator.configure(LogConfigurator.java:119) at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:294) at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:136) at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:127) at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) at org.elasticsearch.cli.Command.main(Command.java:90) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:93) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:86) 我们在运行的配置 vm options 如下： 1-Dlog4j2.disable.jmx=true 4、如果你用的是 JDK 1.8 编译的应该还会报这个错误 123456789101112131415161718192021222324[2018-08-01T11:02:24,663][WARN ][o.e.b.ElasticsearchUncaughtExceptionHandler] [] uncaught exception in thread [main]org.elasticsearch.bootstrap.StartupException: java.lang.IllegalStateException: jar hell!class: jdk.packager.services.UserJvmOptionsServicejar1: /Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home/lib/ant-javafx.jarjar2: /Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home/lib/packager.jar at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:140) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:127) ~[main/:?] at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) ~[main/:?] at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) ~[main/:?] at org.elasticsearch.cli.Command.main(Command.java:90) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:93) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:86) ~[main/:?]Caused by: java.lang.IllegalStateException: jar hell!class: jdk.packager.services.UserJvmOptionsServicejar1: /Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home/lib/ant-javafx.jarjar2: /Library/Java/JavaVirtualMachines/jdk1.8.0_152.jdk/Contents/Home/lib/packager.jar at org.elasticsearch.bootstrap.JarHell.checkClass(JarHell.java:273) ~[main/:?] at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:190) ~[main/:?] at org.elasticsearch.bootstrap.JarHell.checkJarHell(JarHell.java:86) ~[main/:?] at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:198) ~[main/:?] at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:326) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:136) ~[main/:?] ... 6 more2018-08-01 11:02:24,713 Thread-2 ERROR No log4j2 configuration file found. Using default configuration: logging only errors to the console. Set system property &apos;log4j2.debug&apos; to show Log4j2 internal initialization logging. 有两个解决方法就是， （1）、把源码中有关使用了 JarHell.checkJarHell 代码的地方全部注释掉就好了 （2）、换成 JDK 10 编译 两种方法我都试了是可行的，建议直接换第二种方案吧！ 5、然后再启动的话，应该没问题了,出现下面日志：（网上很多人在这步就好了） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[elasticsearch] Java HotSpot(TM) 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.[elasticsearch] [2018-08-04T16:42:26,073][INFO ][o.e.n.Node ] [node-0] initializing ...[elasticsearch] [2018-08-04T16:42:26,185][INFO ][o.e.e.NodeEnvironment ] [node-0] using [1] data paths, mounts [[/ (/dev/disk1s1)]], net usable_space [109.3gb], net total_space [233.4gb], types [apfs][elasticsearch] [2018-08-04T16:42:26,187][INFO ][o.e.e.NodeEnvironment ] [node-0] heap size [494.9mb], compressed ordinary object pointers [true][elasticsearch] [2018-08-04T16:42:26,190][INFO ][o.e.n.Node ] [node-0] node name [node-0], node ID [o9SuMXP-R7uvJLtE3h37Rw][elasticsearch] [2018-08-04T16:42:26,191][INFO ][o.e.n.Node ] [node-0] version[6.3.2-SNAPSHOT], pid[61499], build[default/zip/053779d/2018-08-04T08:39:59.714654Z], OS[Mac OS X/10.13.5/x86_64], JVM[&quot;Oracle Corporation&quot;/Java HotSpot(TM) 64-Bit Server VM/10.0.2/10.0.2+13][elasticsearch] [2018-08-04T16:42:26,191][INFO ][o.e.n.Node ] [node-0] JVM arguments [-Xms1g, -Xmx1g, -XX:+UseConcMarkSweepGC, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Djava.io.tmpdir=/var/folders/mb/3vpbvkkx13l2jmpt2kmmt0fr0000gn/T/elasticsearch.URRKTybG, -XX:+HeapDumpOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -Xlog:gc*,gc+age=trace,safepoint:file=logs/gc.log:utctime,pid,tags:filecount=32,filesize=64m, -Djava.locale.providers=COMPAT, -XX:UseAVX=2, -ea, -esa, -Xms512m, -Xmx512m, -Des.path.home=/Users/zhisheng/IdeaProjects/github/elasticsearch/distribution/build/cluster/run node0/elasticsearch-6.3.2-SNAPSHOT, -Des.path.conf=/Users/zhisheng/IdeaProjects/github/elasticsearch/distribution/build/cluster/run node0/elasticsearch-6.3.2-SNAPSHOT/config, -Des.distribution.flavor=default, -Des.distribution.type=zip][elasticsearch] [2018-08-04T16:42:26,191][WARN ][o.e.n.Node ] [node-0] version [6.3.2-SNAPSHOT] is a pre-release version of Elasticsearch and is not suitable for production[elasticsearch] [2018-08-04T16:42:28,808][INFO ][o.e.p.PluginsService ] [node-0] loaded module [aggs-matrix-stats][elasticsearch] [2018-08-04T16:42:28,809][INFO ][o.e.p.PluginsService ] [node-0] loaded module [analysis-common][elasticsearch] [2018-08-04T16:42:28,809][INFO ][o.e.p.PluginsService ] [node-0] loaded module [ingest-common][elasticsearch] [2018-08-04T16:42:28,809][INFO ][o.e.p.PluginsService ] [node-0] loaded module [lang-expression][elasticsearch] [2018-08-04T16:42:28,809][INFO ][o.e.p.PluginsService ] [node-0] loaded module [lang-mustache][elasticsearch] [2018-08-04T16:42:28,809][INFO ][o.e.p.PluginsService ] [node-0] loaded module [lang-painless][elasticsearch] [2018-08-04T16:42:28,809][INFO ][o.e.p.PluginsService ] [node-0] loaded module [mapper-extras][elasticsearch] [2018-08-04T16:42:28,809][INFO ][o.e.p.PluginsService ] [node-0] loaded module [parent-join][elasticsearch] [2018-08-04T16:42:28,809][INFO ][o.e.p.PluginsService ] [node-0] loaded module [percolator][elasticsearch] [2018-08-04T16:42:28,809][INFO ][o.e.p.PluginsService ] [node-0] loaded module [rank-eval][elasticsearch] [2018-08-04T16:42:28,809][INFO ][o.e.p.PluginsService ] [node-0] loaded module [reindex][elasticsearch] [2018-08-04T16:42:28,810][INFO ][o.e.p.PluginsService ] [node-0] loaded module [repository-url][elasticsearch] [2018-08-04T16:42:28,810][INFO ][o.e.p.PluginsService ] [node-0] loaded module [transport-netty4][elasticsearch] [2018-08-04T16:42:28,810][INFO ][o.e.p.PluginsService ] [node-0] loaded module [tribe][elasticsearch] [2018-08-04T16:42:28,810][INFO ][o.e.p.PluginsService ] [node-0] loaded module [x-pack-core][elasticsearch] [2018-08-04T16:42:28,810][INFO ][o.e.p.PluginsService ] [node-0] loaded module [x-pack-deprecation][elasticsearch] [2018-08-04T16:42:28,810][INFO ][o.e.p.PluginsService ] [node-0] loaded module [x-pack-graph][elasticsearch] [2018-08-04T16:42:28,810][INFO ][o.e.p.PluginsService ] [node-0] loaded module [x-pack-logstash][elasticsearch] [2018-08-04T16:42:28,810][INFO ][o.e.p.PluginsService ] [node-0] loaded module [x-pack-ml][elasticsearch] [2018-08-04T16:42:28,810][INFO ][o.e.p.PluginsService ] [node-0] loaded module [x-pack-monitoring][elasticsearch] [2018-08-04T16:42:28,810][INFO ][o.e.p.PluginsService ] [node-0] loaded module [x-pack-rollup][elasticsearch] [2018-08-04T16:42:28,810][INFO ][o.e.p.PluginsService ] [node-0] loaded module [x-pack-security][elasticsearch] [2018-08-04T16:42:28,811][INFO ][o.e.p.PluginsService ] [node-0] loaded module [x-pack-sql][elasticsearch] [2018-08-04T16:42:28,811][INFO ][o.e.p.PluginsService ] [node-0] loaded module [x-pack-upgrade][elasticsearch] [2018-08-04T16:42:28,811][INFO ][o.e.p.PluginsService ] [node-0] loaded module [x-pack-watcher][elasticsearch] [2018-08-04T16:42:28,811][INFO ][o.e.p.PluginsService ] [node-0] no plugins loaded[elasticsearch] [2018-08-04T16:42:32,722][INFO ][o.e.x.s.a.s.FileRolesStore] [node-0] parsed [0] roles from file [/Users/zhisheng/IdeaProjects/github/elasticsearch/distribution/build/cluster/run node0/elasticsearch-6.3.2-SNAPSHOT/config/roles.yml][elasticsearch] [2018-08-04T16:42:33,358][INFO ][o.e.x.m.j.p.l.CppLogMessageHandler] [controller/61517] [Main.cc@109] controller (64 bit): Version 6.3.2-SNAPSHOT (Build 903094f295d249) Copyright (c) 2018 Elasticsearch BV[elasticsearch] [2018-08-04T16:42:33,783][DEBUG][o.e.a.ActionModule ] Using REST wrapper from plugin org.elasticsearch.xpack.security.Security[elasticsearch] [2018-08-04T16:42:34,110][INFO ][o.e.d.DiscoveryModule ] [node-0] using discovery type [zen][elasticsearch] [2018-08-04T16:42:34,971][INFO ][o.e.n.Node ] [node-0] initialized[elasticsearch] [2018-08-04T16:42:34,971][INFO ][o.e.n.Node ] [node-0] starting ...[elasticsearch] [2018-08-04T16:42:35,217][INFO ][o.e.t.TransportService ] [node-0] publish_address &#123;127.0.0.1:9300&#125;, bound_addresses &#123;[::1]:9300&#125;, &#123;127.0.0.1:9300&#125;[elasticsearch] [2018-08-04T16:42:38,291][INFO ][o.e.c.s.MasterService ] [node-0] zen-disco-elected-as-master ([0] nodes joined)[, ], reason: new_master &#123;node-0&#125;&#123;o9SuMXP-R7uvJLtE3h37Rw&#125;&#123;xjoT1zvpRsm1ZDGLCab1sA&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;&#123;ml.machine_memory=17179869184, xpack.installed=true, testattr=test, ml.max_open_jobs=20, ml.enabled=true&#125;[elasticsearch] [2018-08-04T16:42:38,295][INFO ][o.e.c.s.ClusterApplierService] [node-0] new_master &#123;node-0&#125;&#123;o9SuMXP-R7uvJLtE3h37Rw&#125;&#123;xjoT1zvpRsm1ZDGLCab1sA&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;&#123;ml.machine_memory=17179869184, xpack.installed=true, testattr=test, ml.max_open_jobs=20, ml.enabled=true&#125;, reason: apply cluster state (from master [master &#123;node-0&#125;&#123;o9SuMXP-R7uvJLtE3h37Rw&#125;&#123;xjoT1zvpRsm1ZDGLCab1sA&#125;&#123;127.0.0.1&#125;&#123;127.0.0.1:9300&#125;&#123;ml.machine_memory=17179869184, xpack.installed=true, testattr=test, ml.max_open_jobs=20, ml.enabled=true&#125; committed version [1] source [zen-disco-elected-as-master ([0] nodes joined)[, ]]])[elasticsearch] [2018-08-04T16:42:38,317][INFO ][o.e.x.s.t.n.SecurityNetty4HttpServerTransport] [node-0] publish_address &#123;127.0.0.1:9200&#125;, bound_addresses &#123;[::1]:9200&#125;, &#123;127.0.0.1:9200&#125;[elasticsearch] [2018-08-04T16:42:38,319][INFO ][o.e.n.Node ] [node-0] started[elasticsearch] [2018-08-04T16:42:38,358][WARN ][o.e.x.s.a.s.m.NativeRoleMappingStore] [node-0] Failed to clear cache for realms [[]][elasticsearch] [2018-08-04T16:42:38,413][INFO ][o.e.g.GatewayService ] [node-0] recovered [0] indices into cluster_state[elasticsearch] [2018-08-04T16:42:38,597][INFO ][o.e.c.m.MetaDataIndexTemplateService] [node-0] adding template [.watch-history-7] for index patterns [.watcher-history-7*][elasticsearch] [2018-08-04T16:42:38,660][INFO ][o.e.c.m.MetaDataIndexTemplateService] [node-0] adding template [.watches] for index patterns [.watches*][elasticsearch] [2018-08-04T16:42:38,707][INFO ][o.e.c.m.MetaDataIndexTemplateService] [node-0] adding template [.triggered_watches] for index patterns [.triggered_watches*][elasticsearch] [2018-08-04T16:42:38,771][INFO ][o.e.c.m.MetaDataIndexTemplateService] [node-0] adding template [.monitoring-logstash] for index patterns [.monitoring-logstash-6-*][elasticsearch] [2018-08-04T16:42:38,836][INFO ][o.e.c.m.MetaDataIndexTemplateService] [node-0] adding template [.monitoring-es] for index patterns [.monitoring-es-6-*][elasticsearch] [2018-08-04T16:42:38,878][INFO ][o.e.c.m.MetaDataIndexTemplateService] [node-0] adding template [.monitoring-alerts] for index patterns [.monitoring-alerts-6][elasticsearch] [2018-08-04T16:42:38,926][INFO ][o.e.c.m.MetaDataIndexTemplateService] [node-0] adding template [.monitoring-beats] for index patterns [.monitoring-beats-6-*][elasticsearch] [2018-08-04T16:42:38,970][INFO ][o.e.c.m.MetaDataIndexTemplateService] [node-0] adding template [.monitoring-kibana] for index patterns [.monitoring-kibana-6-*][elasticsearch] [2018-08-04T16:42:39,055][INFO ][o.e.l.LicenseService ] [node-0] license [79704513-d3c4-4535-8276-beeb146765de] mode [basic] - valid 6、但是我出现了下面这个问题，一直困扰着我呢，我是直接跳过去的。 12345678910111213141516171819202122[2018-08-01T09:44:27,370][ERROR][o.e.b.ElasticsearchUncaughtExceptionHandler] [] fatal error in thread [main], exitingjava.lang.NoClassDefFoundError: org/elasticsearch/plugins/ExtendedPluginsClassLoader at org.elasticsearch.plugins.PluginsService.loadBundle(PluginsService.java:632) ~[main/:?] at org.elasticsearch.plugins.PluginsService.loadBundles(PluginsService.java:557) ~[main/:?] at org.elasticsearch.plugins.PluginsService.&lt;init&gt;(PluginsService.java:162) ~[main/:?] at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:311) ~[main/:?] at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:252) ~[main/:?] at org.elasticsearch.bootstrap.Bootstrap$5.&lt;init&gt;(Bootstrap.java:213) ~[main/:?] at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:213) ~[main/:?] at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:326) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:136) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:127) ~[main/:?] at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) ~[main/:?] at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) ~[main/:?] at org.elasticsearch.cli.Command.main(Command.java:90) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:93) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:86) ~[main/:?]Caused by: java.lang.ClassNotFoundException: org.elasticsearch.plugins.ExtendedPluginsClassLoader at jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:582) ~[?:?] at jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:190) ~[?:?] at java.lang.ClassLoader.loadClass(ClassLoader.java:499) ~[?:?] ... 15 more 遇到的这个问题，我在 GitHub 求助信息如下： https://github.com/elastic/elasticsearch/issues/30774 但是并没有解决我的问题，这里暂时先记录下来！我自己也跟了下源码，定位到错误信息是怎么产生的，但是没有解决方案！ 后面写了篇文章：教你如何在 IDEA 远程 Debug ElasticSearch 或许可以帮你解决上面问题带给你的困扰！ 更新后面有一个读者提醒了我一下，他自己也遇到这个问题，然后他的解决方案挺好的，完美解决我的问题。这里做个记录： 解决方法： 打开 IDEA Edit Configurations ，给 Include dependencies with Provided scope 打上勾即可解决，很简单吧！！ 继续RUN，又来一个 EXceptin： 1234567891011121314151617[2018-08-23T01:13:38,551][WARN ][o.e.b.ElasticsearchUncaughtExceptionHandler] [] uncaught exception in thread [main]org.elasticsearch.bootstrap.StartupException: java.security.AccessControlException: access denied (&quot;java.lang.RuntimePermission&quot; &quot;createClassLoader&quot;) at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:140) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:127) ~[main/:?] at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) ~[main/:?] at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) ~[main/:?] at org.elasticsearch.cli.Command.main(Command.java:90) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:93) ~[main/:?] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:86) ~[main/:?]Caused by: java.security.AccessControlException: access denied (&quot;java.lang.RuntimePermission&quot; &quot;createClassLoader&quot;) at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472) ~[?:?] at java.security.AccessController.checkPermission(AccessController.java:895) ~[?:?] at java.lang.SecurityManager.checkPermission(SecurityManager.java:335) ~[?:?] at java.lang.SecurityManager.checkCreateClassLoader(SecurityManager.java:397) ~[?:?]...Exception: java.security.AccessControlException thrown from the UncaughtExceptionHandler in thread &quot;Thread-2&quot; 第一种： 在 config 目录下新建 java.policy 文件，填入下面内容: 123grant &#123; permission java.lang.RuntimePermission &quot;createClassLoader&quot;;&#125;; 然后在 VM options 加入 java.security.policy 的设置，指向该文件即可 1-Djava.security.policy=/usr/local/elasticsearch-6.3.2/config/java.policy 第二种： 就是在 %JAVA_HOME%/conf/security 目录下（JDK10是这个路径，之前的版本不确定），我的目录是 /Library/Java/JavaVirtualMachines/jdk-10.0.2.jdk/Contents/Home/conf/security，打开 java.policy 文件，在 grant 中加入下面这句，赋予权限。 12//for es 6.3.2permission java.lang.RuntimePermission &quot;createClassLoader&quot;; 再 RUN，这次可终于运行起来了！！！ 再次感谢下读者，他的文章地址是：http://laijianfeng.org/2018/08/%E6%95%99%E4%BD%A0%E7%BC%96%E8%AF%91%E8%B0%83%E8%AF%95Elasticsearch-6-3-2%E6%BA%90%E7%A0%81/ 总结折腾的路上少不了各种错误烦扰你，学会解决问题！ 相关文章1、渣渣菜鸡为什么要看 ElasticSearch 源码？ 2、渣渣菜鸡的 ElasticSearch 源码解析 —— 环境搭建 3、渣渣菜鸡的 ElasticSearch 源码解析 —— 启动流程(上) 4、渣渣菜鸡的 ElasticSearch 源码解析 —— 启动流程(下) 5、Elasticsearch 系列文章（一）：Elasticsearch 默认分词器和中分分词器之间的比较及使用方法 6、Elasticsearch 系列文章（二）：全文搜索引擎 Elasticsearch 集群搭建入门教程 7、Elasticsearch 系列文章（三）：ElasticSearch 集群监控 8、Elasticsearch 系列文章（四）：ElasticSearch 单个节点监控 9、Elasticsearch 系列文章（五）：ELK 实时日志分析平台环境搭建 10、教你如何在 IDEA 远程 Debug ElasticSearch","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://www.54tianzhisheng.cn/tags/ElasticSearch/"}]},{"title":"渣渣菜鸡为什么要看 ElasticSearch 源码？","date":"2018-08-03T16:00:00.000Z","path":"2018/08/04/why-see-es-code/","text":"前提人工智能、大数据快速发展的今天，对于 TB 甚至 PB 级大数据的快速检索已然成为刚需，大型企业早已淹没在系统生成的浩瀚数据流当中。大数据技术业已集中在如何存储和处理这些海量的数据上。Elasticsearch 作为开源领域的后起之秀，从2010年至今得到飞跃式的发展。 Elasticsearch 以其开源、分布式、RESTFul API 三大优势，已经成为当下风口中“会飞的猪”。 在我的电脑本地写了几篇 ElasticSearch 的源码解析了，回过头来想想应该也写一篇为何我会去看它的源码？ 为什么呢？下面我讲讲自己从接触搜索到现在看源码的过程！ 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/08/24/why-see-es-code/ 第一次接触搜索搜索，我们首先想到的是搜索引擎：Google、百度，这个就算是接触的最早的了。 我自己项目里面接触搜索是大二暑假那时练习的一个项目，里面用了 Solr，然后当时自己也稍微了解了下，并用在了项目里面。 第二次接触搜索从第一次项目里面用到了搜索，后面自己对这方面就比较感兴趣。再一次接触搜索是实习的时候进公司。第一件事情就是被老大叫的去学习搭建 Elasticsearch 集群，于是乎，电脑就装了三个虚拟机，Elasticsearch 就一个个的装了起来了。也记录了博客下来：Elasticsearch 系列文章（二）：全文搜索引擎 Elasticsearch 集群搭建入门教程，当时搭建的时候 ES 的版本才刚从 2.x 升级到 5.x 呢，截止本文时间 2018.08.04，现在 ES 版本已经是到 7.0 了，这版本升级是真的的快，这也说明了 ES 的活跃度很高，背后的开发工程师维护也快，侧面突出要去看它源码的重要性。 当时自己在本地测试搭建集群后，给分配了另外一个任务就是去了解 ES 中的自带分词、英文分词、中文分词的相同与差异、以及自己建立分词需要注意的点。于是乎：当时在公司 wiki 贡献了这篇文章：Elasticsearch 系列文章（一）：Elasticsearch 默认分词器和中分分词器之间的比较及使用方法。这篇文章几乎已经把市面上所有的分词都写进去了，包括他们的相同点、不同点、如何使用、如何自定义分词器。 然后还有就是我同组的一个同学，她的任务就是 2.x 升级到 5.x 中 mapping 的大改变有哪些？后面我也看了她总结的文档，很详细！ 在这次接触了 ES 后，因为我自己本地已经有环境了，所以自己测试了一些功能，给 ES 安装插件（IK、x-pack、支持 sql 的、），后面自己也去测试 ES 的索引、文档、REST API。 第三次接触搜索由于是自己对其感兴趣，所以后面就去找了些相关的视频，比如：中华石衫的《Elasticsearch 顶尖高手系列-高手进阶篇》几个系列视频教程个人感觉还是不错的，看完这几个系列估计入门肯定是没有问题的。版权原因，不提供下载链接。 另外就是《Elasticsearch 权威指南》翻译的版本，翻译还没有全，可以去看看，讲得很详细的，市面上应该还没有哪本书讲的有这么清楚，如果英文不错的可以直接啃英文吧。 还有就是官网的文档了，非常非常详细，还有 demo，2.x 版本的是有中文的官方文档，可以凑合着看。 学习新东西，要学会先看官方文档，何况 Elasticsearch 的官方文档这么详细呢！ 第四次接触搜索后面实习的时候，又分配了公司中间件监控的两个模块：Elasticsearch 和 HBase 组件的监控。于是乎，再次有机会接触 Elasticsearch 了，这次主要还是利用 Elasticsearch 自带的 REST API ：_cluster/health 、_cluster/stats、_nodes、_nodes/stats 去获取到集群的健康信息、节点信息（内存、CPU、网络、JVM等信息）。为了做这个项目自己当时也去找了网上很多类似的文章参考常用的监控指标和他们是怎么做监控的。我当时的任务主要还是采集信息，然后存到公司大项目的 influxdb 中，最后用 grafana 展示出来，后面我组的运维大佬给我看了监控大盘，界面挺酷炫的，哈哈哈，牛逼！ 当时写的两篇博客： 1、Elasticsearch 系列文章（三）：ElasticSearch 集群监控 2、Elasticsearch 系列文章（四）：ElasticSearch 单个节点监控 取之网络，还之网络，希望给后面做类似任务的小伙伴给点参考意见！ 再就是自己搭建 ELK（ElasticSearch, Logstash, Kibana）日志分析平台，然后玩了下！ 搭建环境博客：Elasticsearch 系列文章（五）：ELK 实时日志分析平台环境搭建 第五次接触搜索后面就没怎么接触 ElasticSearch 了，一直忙着其他的东西。 实习辞职后，毕业出来找工作的那段日子，自己又花了一星期稍微过了一遍 《Elasticsearch 权威指南》 这本书，话说还帮我面试挺过不少关呢，哈哈哈！因为我项目里写了 Elasticsearch 的监控，如果你对 Elasticsearch 其他的不熟悉，面试官稍微问些其他关于这方面的，那就不知道就有点尴尬😅了，所以还是准备了下。看完之后应付面试没多大的问题。 第六次接触看起来我接触了 Elasticsearch 很久了，其实真正项目里面是没有用到 Elasticsearch 做过项目的，没有用到 Elasticsearch 的搜索做什么项目，于是自己当时找工作其实也打算找到工作后看能不能自己做个项目或者公司项目里面用用 Elasticsearch 呢？ 结果在新公司新项目里，很快就用到了。只不过这次不是 Java 项目里面用了，而是和 GoLang 整合。不过 API 都差不多，多熟悉几次就很快上手了，关键还是要懂 Elasticsearch 如何构造 DSL 查询语句，这样再转换成 GO 里面的 API 就快了。 还有就是公司里刚好有个中科院研究生大佬，他就写过 Elasticsearch 这块的书籍《从 lucene 到 Elasticsearch 全文检索实战》，另外他的 CSDN 博客也很火，阅读量很高，感兴趣的可以买本书支持下。 中途自己遇到 Elasticsearch 实在不会的问题也会主动去找大佬咨询，然后大佬耐心教教我这个渣渣菜鸡，在文章这里感谢下大佬这段时间的关照。 萌生阅读源码的想法既然接触了这么久的 Elasticsearch ，项目里用过，书籍也看过，虽然还不是很熟，但是如果看看它的源码是不是会让我对它的印象更深呢？ 说干就干，晚上回家就从 GitHub clone 了源码在本地，那时刚好回家，就在火车上直接用 VS code 看了会源码，也没有在 ide 里 debug 起来。 写这篇文章的时候已经把 Elasticsearch 的整个启动流程（加载读取配置、加载插件等）、如何支持 REST API 看了下，后面会在下班后回家继续阅读源码，继续分享我的源码解析的。 有想法就去干，不尝试下，怎么知道适不适合你？ 总结其实阅读源码的主要原因还是自己感兴趣；另外就是这东西现在项目里确实也用到了，如果我对源码熟悉的话可能会对我的理解会更加透彻点；还有就是 Elasticsearch 确实火，公司几乎都用的，所以学习下还是有必要的。","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://www.54tianzhisheng.cn/tags/ElasticSearch/"}]},{"title":"渣渣菜鸡的蚂蚁金服面试经历(二)","date":"2018-07-30T16:00:00.000Z","path":"2018/07/31/alipay02/","text":"蚂蚁金服电话二面（85 分钟）1、自我介绍、工作经历、技术栈 2、项目中你学到了什么技术？（把三项目具体描述了很久） 3、微服务划分的粒度 4、微服务的高可用怎么保证的？ 5、常用的负载均衡，该怎么用，你能说下吗？ 6、网关能够为后端服务带来哪些好处？ 7、Spring Bean 的生命周期 8、xml 中配置的 init、destroy 方法怎么可以做到调用具体的方法？ 9、反射的机制 10、Object 类中的方法 11、hashcode 和 equals 方法常用地方 12、对象比较是否相同 13、hashmap put 方法存放的时候怎么判断是否是重复的 14、Object toString 方法常用的地方，为什么要重写该方法 15、Set 和 List 区别？ 16、ArrayList 和 LinkedList 区别 17、如果存取相同的数据，ArrayList 和 LinkedList 谁占用空间更大？ 18、Set 存的顺序是有序的吗？ 19、常见 Set 的实现有哪些？ 20、TreeSet 对存入对数据有什么要求呢？ 21、HashSet 的底层实现呢 22、TreeSet 底层源码有看过吗？ 23、HashSet 是不是线程安全的？为什么不是线程安全的？ 24、Java 中有哪些线程安全的 Map？ 25、Concurrenthashmap 是怎么做到线程安全的？ 26、HashTable 你了解过吗？ 27、如何保证线程安全问题？ 28、synchronized、lock 29、volatile 的原子性问题？为什么 i++ 这种不支持原子性？从计算机原理的设计来讲下不能保证原子性的原因 30、happens before 原理 31、cas 操作 32、lock 和 synchronized 的区别？ 33、公平锁和非公平锁 34、Java 读写锁 35、读写锁设计主要解决什么问题？ 36、你项目除了写 Java 代码，还有前端代码，那你知道前端有哪些框架吗？ 37、MySQL 分页查询语句 38、MySQL 事务特性和隔离级别 39、不可重复读会出现在什么场景？ 40、sql having 的使用场景 41、前端浏览器地址的一个 http 请求到后端整个流程是怎么样？能够说下吗？ 42、http 默认端口，https 默认端口 43、DNS 你知道是干嘛的吗？ 44、你们开发用的 ide 是啥？你能说下 idea 的常用几个快捷键吧？ 45、代码版本管理你们用的是啥？ 46、git rebase 和 merge 有什么区别？ 47、你们公司加班多吗？ 48、后面一起聊 high 了，之间扯了些蛋，哈哈哈 相关文章：1、秋招第一站 —— 亚信科技 2、秋招第二站 —— 内推爱奇艺 3、秋招第三站 —— 内推阿里（一面） 4、那些年我看过的书 —— 致敬我的大学生活 —— Say Good Bye ！ 5、面试过阿里等互联网大公司，我知道了这些套路 6、渣渣菜鸡的有赞面试经历（一） 7、渣渣菜鸡的蚂蚁金服面试经历（一） 最后本地地址：http://www.54tianzhisheng.cn/2018/07/31/alipay02 ，转载请授权，否则禁止转载！ 本文首发在我的知识星球，最近自己一直在写前段时间的所有面试情况，已经分享在我的知识星球，如果感兴趣，可以加入我的知识星球！","tags":[{"name":"面经","slug":"面经","permalink":"http://www.54tianzhisheng.cn/tags/面经/"}]},{"title":"渣渣菜鸡的蚂蚁金服面试经历(一)","date":"2018-07-29T16:00:00.000Z","path":"2018/07/30/alipay01/","text":"蚂蚁金服电话一面1、自我介绍、自己做的项目和技术领域 2、项目中的监控：那个监控指标常见的哪些？ 3、微服务涉及到的技术以及需要注意的问题有哪些？ 4、注册中心你了解了哪些？ 5、consul 的可靠性你了解吗？ 6、consul 的机制你有没有具体深入过？有没有和其他的注册中心对比过？ 7、项目用 Spring 比较多，有没有了解 Spring 的原理？AOP 和 IOC 的原理 8、Spring Boot除了自动配置，相比传统的 Spring 有什么其他的区别？ 9、Spring Cloud 有了解多少？ 10、Spring Bean 的生命周期 11、HashMap 和 hashTable 区别？ 12、Object 的 hashcode 方法重写了，equals 方法要不要改？ 13、Hashmap 线程不安全的出现场景 14、线上服务 CPU 很高该怎么做？有哪些措施可以找到问题 15、JDK 中有哪几个线程池？顺带把线程池讲了个遍 16、SQL 优化的常见方法有哪些 17、SQL 索引的顺序，字段的顺序 18、查看 SQL 是不是使用了索引？（有什么工具） 19、TCP 和 UDP 的区别？TCP 数据传输过程中怎么做到可靠的？ 20、说下你知道的排序算法吧 21、查找一个数组的中位数？ 22、你有什么问题想问我的吗？ 相关文章：1、秋招第一站 —— 亚信科技 2、秋招第二站 —— 内推爱奇艺 3、秋招第三站 —— 内推阿里（一面） 4、那些年我看过的书 —— 致敬我的大学生活 —— Say Good Bye ！ 5、面试过阿里等互联网大公司，我知道了这些套路 6、渣渣菜鸡的有赞面试经历（一） 最后本地地址：http://www.54tianzhisheng.cn/2018/07/30/alipay01 ，转载请授权，否则禁止转载！ 本文首发在我的知识星球，最近自己一直在写前段时间的所有面试情况，已经分享在我的知识星球，如果感兴趣，可以加入我的知识星球！","tags":[{"name":"面经","slug":"面经","permalink":"http://www.54tianzhisheng.cn/tags/面经/"}]},{"title":"渣渣菜鸡的有赞面试经历（一）","date":"2018-07-11T16:00:00.000Z","path":"2018/07/12/youzan/","text":"出去面试的话还是得好好准备，不然会被虐的有点惨！ 有赞（框架组）四月份面试有赞的时候，自己还在实习，所以也没有复习，是在 Boss 直聘上投的，当时看到了有赞的 2018 届春招，就投了下，然后不知道怎么就被推到了框架组，结果后面就感觉自己被虐的可惨了。 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/07/12/youzan/ 电话一面好像是清明节还是五一劳动节来着，我还在睡觉，就接到一面面试官的电话，说现在有时间吗，能够接受下电话面试吗？我勒个去，今天过节、我被电话吵醒的，现在人都没清醒、这面试那肯定得一面就挂了，所以就老实得说现在还是不方便呢，约了周一上午 10 点面试。 周一 10 点面试官准时打电话过来了！ 以下是面试的问题： 1、自我介绍 2、Map 的底层结构？（HashMap） 3、线程安全的 Map （concurrentHashMap）简单的说了下这两 1。7 和 1.8 的区别，本想问下要不要深入的讲下（源码级别），结果面试官说不用了。 4、项目 MySQL 的数据量和并发量有多大？ 5、你对数据库了解多少？ 6、你说下数据库的索引实现和非主键的二级索引 7、项目用的是 SpringBoot ，你能说下 Spring Boot 与 Spring 的区别吗？ 8、SpringBoot 的自动配置是怎么做的？ 9、MyBatis 定义的接口，怎么找到实现的？ 10、Java 内存结构 11、对象是否可 GC？ 12、Minor GC 和 Full GC 13、垃圾回收算法 14、垃圾回收器 G1 15、项目里用过 ElasticSearch 和 Hbase，有深入了解他们的调优技巧吗？ 16、Spring RestTemplate 的具体实现 17、描述下网页一个 Http 请求，到后端的整个请求过程 18、多线程的常用方法和接口类及线程池的机制 19、总结我的 Java 基础还是不错，但是一些主流的框架源码还是处在使用的状态，需要继续去看源码 20、死锁 21、自己研究比较新的技术，说下成果！ 22、你有什么想问的？我就问了下公司那边的情况，这个自由发挥！ 最后我知道有二面的面试机会了。 10 来分钟不到，就再次打电话过来约了明早上午 10 点的视频面试。 视频二面二面面试官先打电话过来，然后加了个微信，开始微信视频面试 这个面试我也不太记得具体面试题目了，下面写的是大概方向的： 1、HashMap，源码级别的问了，包括为什么线程不安全 2、死锁 3、Synchronized 和 ReentrantLock 锁机制，怎么判断重入锁的，会不会是死锁？ 4、进程和线程的区别？ 5、进程之间如何保证同步？ 6、分布式锁 7、对象 GC 8、垃圾回收算法 9、JVM 参数 10、OOM 出现的有哪些场景？为什么会发生？ 11、JVM 内存结构说下吧 12、堆和栈的共享问题？ 13、有比较过 Http 和 RPC 吗？ 14、HttpClient 你说说里面的具体实现吧？（涉及了哪些东西） 15、那要你设计一个高性能的 Http ，你会怎么设计？ 二面微信视频面试只记得这么多了。 本文首发在我的知识星球，最近自己一直在写前段时间的所有面试情况，后面会一篇一篇分享在我的知识星球的，如果感兴趣，可以加入我的知识星球！ 知识星球更多面经文章： 1、蚂蚁金服电话一面 2、蚂蚁金服电话二面——后面聊的有点high 3、club factory 面经分享 4、作为面试官得到的经验 5、史上最强最全面经合集 6、公司需要什么样的人 7、如何介绍项目","tags":[{"name":"面经","slug":"面经","permalink":"http://www.54tianzhisheng.cn/tags/面经/"}]},{"title":"20 个案例教你在 Java 8 中如何处理日期和时间?","date":"2018-06-19T16:00:00.000Z","path":"2018/06/20/java-8-date/","text":"前言前面一篇文章写了《SimpleDateFormat 如何安全的使用？》, 里面介绍了 SimpleDateFormat 如何处理日期／时间，以及如何保证线程安全，及其介绍了在 Java 8 中的处理时间／日期默认就线程安全的 DateTimeFormatter 类。那么 Java 8 中该怎么样处理生活中常见的一些日期／时间呢？比如：计算一周后的日期；计算一年前或一年后的日期；检查闰年等。 接下来创建了 20 个基于任务的实例来学习 Java 8 的新特性。从最简单创建当天的日期开始，然后创建时间及时区，接着模拟一个日期提醒应用中的任务——计算重要日期的到期天数，例如生日、纪念日、账单日、保费到期日、信用卡过期日等。 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/06/20/java-8-date/ 示例 1、在 Java 8 中获取今天的日期Java 8 中的 LocalDate 用于表示当天日期。和 java.util.Date 不同，它只有日期，不包含时间。当你仅需要表示日期时就用这个类。 12LocalDate now = LocalDate.now();System.out.println(now); 结果是： 12018-06-20 上面的代码创建了当天的日期，不含时间信息。打印出的日期格式非常友好，不像老的 Date 类打印出一堆没有格式化的信息。 示例 2、在 Java 8 中获取年、月、日信息LocalDate 类提供了获取年、月、日的快捷方法，其实例还包含很多其它的日期属性。通过调用这些方法就可以很方便的得到需要的日期信息，不用像以前一样需要依赖 java.util.Calendar 类了 12345LocalDate now = LocalDate.now();int year = now.getYear();int monthValue = now.getMonthValue();int dayOfMonth = now.getDayOfMonth();System.out.printf(\"year = %d, month = %d, day = %d\", year, monthValue, dayOfMonth); 结果是： 1year = 2018, month = 6, day = 20 示例 3、在 Java 8 中处理特定日期在第一个例子里，我们通过静态工厂方法 now() 非常容易地创建了当天日期，你还可以调用另一个有用的工厂方法LocalDate.of() 创建任意日期， 该方法需要传入年、月、日做参数，返回对应的 LocalDate 实例。这个方法的好处是没再犯老 API 的设计错误，比如年度起始于 1900，月份是从 0 开始等等。日期所见即所得，就像下面这个例子表示了 6 月 20 日，没有任何隐藏机关。 12LocalDate date = LocalDate.of(2018, 06, 20);System.out.println(date); 可以看到创建的日期完全符合预期，与写入的 2018 年 6 月 20 日完全一致。 示例 4、在 Java 8 中判断两个日期是否相等现实生活中有一类时间处理就是判断两个日期是否相等。你常常会检查今天是不是个特殊的日子，比如生日、纪念日或非交易日。这时就需要把指定的日期与某个特定日期做比较，例如判断这一天是否是假期。下面这个例子会帮助你用 Java 8 的方式去解决，你肯定已经想到了，LocalDate 重载了 equal 方法，请看下面的例子： 12345LocalDate now = LocalDate.now();LocalDate date = LocalDate.of(2018, 06, 20);if (date.equals(now)) &#123; System.out.println(\"同一天\");&#125; 这个例子中我们比较的两个日期相同。注意，如果比较的日期是字符型的，需要先解析成日期对象再作判断。 示例 5、在 Java 8 中检查像生日这种周期性事件Java 中另一个日期时间的处理就是检查类似每月账单、结婚纪念日、EMI日或保险缴费日这些周期性事件。如果你在电子商务网站工作，那么一定会有一个模块用来在圣诞节、感恩节这种节日时向客户发送问候邮件。Java 中如何检查这些节日或其它周期性事件呢？答案就是 MonthDay 类。这个类组合了月份和日，去掉了年，这意味着你可以用它判断每年都会发生事件。和这个类相似的还有一个 YearMonth 类。这些类也都是不可变并且线程安全的值类型。下面我们通过 MonthDay 来检查周期性事件： 123456789LocalDate now = LocalDate.now();LocalDate dateOfBirth = LocalDate.of(2018, 06, 20);MonthDay birthday = MonthDay.of(dateOfBirth.getMonth(), dateOfBirth.getDayOfMonth());MonthDay currentMonthDay = MonthDay.from(now);if (currentMonthDay.equals(birthday)) &#123; System.out.println(\"Happy Birthday\");&#125; else &#123; System.out.println(\"Sorry, today is not your birthday\");&#125; 结果：（注意：获取当前时间可能与你看的时候不对，所以这个结果可能和你看的时候运行结果不一样） 1Happy Birthday 只要当天的日期和生日匹配，无论是哪一年都会打印出祝贺信息。你可以把程序整合进系统时钟，看看生日时是否会受到提醒，或者写一个单元测试来检测代码是否运行正确。 示例 6、在 Java 8 中获取当前时间与 Java 8 获取日期的例子很像，获取时间使用的是 LocalTime 类，一个只有时间没有日期的 LocalDate 近亲。可以调用静态工厂方法 now() 来获取当前时间。默认的格式是 hh:mm:ss:nnn。 12LocalTime localTime = LocalTime.now();System.out.println(localTime); 结果： 113:35:56.155 可以看到当前时间就只包含时间信息，没有日期。 示例 7、如何在现有的时间上增加小时通过增加小时、分、秒来计算将来的时间很常见。Java 8 除了不变类型和线程安全的好处之外，还提供了更好的plusHours() 方法替换 add()，并且是兼容的。注意，这些方法返回一个全新的 LocalTime 实例，由于其不可变性，返回后一定要用变量赋值。 1234LocalTime localTime = LocalTime.now();System.out.println(localTime);LocalTime localTime1 = localTime.plusHours(2);//增加2小时System.out.println(localTime1); 结果： 1213:41:20.72115:41:20.721 可以看到，新的时间在当前时间 13:41:20.721 的基础上增加了 2 个小时。 示例 8、如何计算一周后的日期和上个例子计算两小时以后的时间类似，这个例子会计算一周后的日期。LocalDate 日期不包含时间信息，它的 plus()方法用来增加天、周、月，ChronoUnit 类声明了这些时间单位。由于 LocalDate 也是不变类型，返回后一定要用变量赋值。 1234LocalDate now = LocalDate.now();LocalDate plusDate = now.plus(1, ChronoUnit.WEEKS);System.out.println(now);System.out.println(plusDate); 结果： 122018-06-202018-06-27 可以看到新日期离当天日期是 7 天，也就是一周。你可以用同样的方法增加 1 个月、1 年、1 小时、1 分钟甚至一个世纪，更多选项可以查看 Java 8 API 中的 ChronoUnit 类。 示例 9、计算一年前或一年后的日期继续上面的例子，上个例子中我们通过 LocalDate 的 plus() 方法增加天数、周数或月数，这个例子我们利用 minus() 方法计算一年前的日期。 12345LocalDate now = LocalDate.now();LocalDate minusDate = now.minus(1, ChronoUnit.YEARS);LocalDate plusDate1 = now.plus(1, ChronoUnit.YEARS);System.out.println(minusDate);System.out.println(plusDate1); 结果： 122017-06-202019-06-20 示例 10、使用 Java 8 的 Clock 时钟类Java 8 增加了一个 Clock 时钟类用于获取当时的时间戳，或当前时区下的日期时间信息。以前用到System.currentTimeInMillis() 和 TimeZone.getDefault() 的地方都可用 Clock 替换。 1234Clock clock = Clock.systemUTC();Clock clock1 = Clock.systemDefaultZone();System.out.println(clock);System.out.println(clock1); 结果： 12SystemClock[Z]SystemClock[Asia/Shanghai] 示例 11、如何用 Java 判断日期是早于还是晚于另一个日期另一个工作中常见的操作就是如何判断给定的一个日期是大于某天还是小于某天？在 Java 8 中，LocalDate 类有两类方法 isBefore() 和 isAfter() 用于比较日期。调用 isBefore() 方法时，如果给定日期小于当前日期则返回 true。 12345678LocalDate tomorrow = LocalDate.of(2018,6,20);if(tomorrow.isAfter(now))&#123; System.out.println(\"Tomorrow comes after today\");&#125;LocalDate yesterday = now.minus(1, ChronoUnit.DAYS);if(yesterday.isBefore(now))&#123; System.out.println(\"Yesterday is day before today\");&#125; 在 Java 8 中比较日期非常方便，不需要使用额外的 Calendar 类来做这些基础工作了。 示例 12、在 Java 8 中处理时区Java 8 不仅分离了日期和时间，也把时区分离出来了。现在有一系列单独的类如 ZoneId 来处理特定时区，ZoneDateTime 类来表示某时区下的时间。这在 Java 8 以前都是 GregorianCalendar 类来做的。 1234ZoneId america = ZoneId.of(\"America/New_York\");LocalDateTime localtDateAndTime = LocalDateTime.now();ZonedDateTime dateAndTimeInNewYork = ZonedDateTime.of(localtDateAndTime, america );System.out.println(dateAndTimeInNewYork); 示例 13、如何表示信用卡到期这类固定日期，答案就在 YearMonth与 MonthDay 检查重复事件的例子相似，YearMonth 是另一个组合类，用于表示信用卡到期日、FD 到期日、期货期权到期日等。还可以用这个类得到 当月共有多少天，YearMonth 实例的 lengthOfMonth() 方法可以返回当月的天数，在判断 2 月有 28 天还是 29 天时非常有用。 1234YearMonth currentYearMonth = YearMonth.now();System.out.printf(\"Days in month year %s: %d%n\", currentYearMonth, currentYearMonth.lengthOfMonth());YearMonth creditCardExpiry = YearMonth.of(2018, Month.FEBRUARY);System.out.printf(\"Your credit card expires on %s %n\", creditCardExpiry); 结果： 12Days in month year 2018-06: 30Your credit card expires on 2018-02 示例 14、如何在 Java 8 中检查闰年LocalDate 类有一个很实用的方法 isLeapYear() 判断该实例是否是一个闰年。 示例 15、计算两个日期之间的天数和月数有一个常见日期操作是计算两个日期之间的天数、周数或月数。在 Java 8 中可以用 java.time.Period 类来做计算。下面这个例子中，我们计算了当天和将来某一天之间的月数。 123LocalDate date = LocalDate.of(2019, Month.MARCH, 20);Period period = Period.between(now, date);System.out.println(&quot;离下个时间还有&quot; + period.getMonths() + &quot; 个月&quot;); 示例 16、包含时差信息的日期和时间在 Java 8 中，ZoneOffset 类用来表示时区，举例来说印度与 GMT 或 UTC 标准时区相差 +05:30，可以通过ZoneOffset.of() 静态方法来获取对应的时区。一旦得到了时差就可以通过传入 LocalDateTime 和 ZoneOffset 来创建一个 OffSetDateTime 对象。 1234LocalDateTime datetime = LocalDateTime.of(2014, Month.JANUARY, 14,19,30);ZoneOffset offset = ZoneOffset.of(\"+05:30\");OffsetDateTime date = OffsetDateTime.of(datetime, offset); System.out.println(\"Date and Time with timezone offset in Java : \" + date); 示例 17、在 Java 8 中获取当前的时间戳如果你还记得 Java 8 以前是如何获得当前时间戳，那么现在你终于解脱了。Instant 类有一个静态工厂方法 now() 会返回当前的时间戳，如下所示： 12Instant timestamp = Instant.now();System.out.println(timestamp); 结果： 12018-06-20T06:35:24.881Z 时间戳信息里同时包含了日期和时间，这和 java.util.Date 很像。实际上 Instant 类确实等同于 Java 8 之前的 Date类，你可以使用 Date 类和 Instant 类各自的转换方法互相转换，例如：Date.from(Instant) 将 Instant 转换成java.util.Date，Date.toInstant() 则是将 Date 类转换成 Instant 类。 示例 18、在 Java 8 中如何使用预定义的格式化工具去解析或格式化日期在 Java 8 以前的世界里，日期和时间的格式化非常诡异，唯一的帮助类 SimpleDateFormat 也是非线程安全的，而且用作局部变量解析和格式化日期时显得很笨重。幸好线程局部变量能使它在多线程环境中变得可用，不过这都是过去时了。Java 8 引入了全新的日期时间格式工具，线程安全而且使用方便。它自带了一些常用的内置格式化工具。 参见我上一篇文章： 《SimpleDateFormat 如何安全的使用？》 示例 19、如何在 Java 中使用自定义格式化工具解析日期尽管内置格式化工具很好用，有时还是需要定义特定的日期格式。可以调用 DateTimeFormatter 的 ofPattern() 静态方法并传入任意格式返回其实例，格式中的字符和以前代表的一样，M 代表月，m 代表分。如果格式不规范会抛出 DateTimeParseException 异常，不过如果只是把 M 写成 m 这种逻辑错误是不会抛异常的。 参见我上一篇文章： 《SimpleDateFormat 如何安全的使用？》 示例 20、在 Java 8 中如何把日期转换成字符串上两个主要是从字符串解析日期。现在我们反过来，把 LocalDateTime 日期实例转换成特定格式的字符串。这是迄今为止 Java 日期转字符串最为简单的方式了。下面的例子将返回一个代表日期的格式化字符串。和前面类似，还是需要创建 DateTimeFormatter 实例并传入格式，但这回调用的是 format() 方法，而非 parse() 方法。这个方法会把传入的日期转化成指定格式的字符串。 123456789LocalDateTime arrivalDate = LocalDateTime.now();try &#123; DateTimeFormatter format = DateTimeFormatter.ofPattern(\"MMMdd yyyy hh:mm a\"); String landing = arrivalDate.format(format); System.out.printf(\"Arriving at : %s %n\", landing);&#125;catch (DateTimeException ex) &#123; System.out.printf(\"%s can't be formatted!%n\", arrivalDate); ex.printStackTrace();&#125; Java 8 日期时间 API 的重点通过这些例子，你肯定已经掌握了 Java 8 日期时间 API 的新知识点。现在来回顾一下这个优雅 API 的使用要点： 1）提供了 javax.time.ZoneId 获取时区。 2）提供了 LocalDate 和 LocalTime 类。 3）Java 8 的所有日期和时间 API 都是不可变类并且线程安全，而现有的 Date 和 Calendar API 中的 java.util.Date 和SimpleDateFormat 是非线程安全的。 4）主包是 java.time, 包含了表示日期、时间、时间间隔的一些类。里面有两个子包 java.time.format 用于格式化， java.time.temporal 用于更底层的操作。 5）时区代表了地球上某个区域内普遍使用的标准时间。每个时区都有一个代号，格式通常由区域/城市构成（Asia/Tokyo），在加上与格林威治或 UTC 的时差。例如：东京的时差是 +09:00。 6）OffsetDateTime 类实际上组合了 LocalDateTime 类和 ZoneOffset 类。用来表示包含和格林威治或 UTC 时差的完整日期（年、月、日）和时间（时、分、秒、纳秒）信息。 7）DateTimeFormatter 类用来格式化和解析时间。与 SimpleDateFormat 不同，这个类不可变并且线程安全，需要时可以给静态常量赋值。 DateTimeFormatter 类提供了大量的内置格式化工具，同时也允许你自定义。在转换方面也提供了 parse() 将字符串解析成日期，如果解析出错会抛出 DateTimeParseException。DateTimeFormatter 类同时还有format() 用来格式化日期，如果出错会抛出 DateTimeException异常。 8）再补充一点，日期格式“MMM d yyyy”和“MMM dd yyyy”有一些微妙的不同，第一个格式可以解析“Jan 2 2014”和“Jan 14 2014”，而第二个在解析“Jan 2 2014”就会抛异常，因为第二个格式里要求日必须是两位的。如果想修正，你必须在日期只有个位数时在前面补零，就是说“Jan 2 2014”应该写成 “Jan 02 2014”。 相关文章SimpleDateFormat 如何安全的使用？","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"}]},{"title":"SimpleDateFormat 如何安全的使用？","date":"2018-06-18T16:00:00.000Z","path":"2018/06/19/SimpleDateFormat/","text":"前言为什么会写这篇文章？因为这些天在看《阿里巴巴开发手册详尽版》，没看过的可以关注微信公众号：zhisheng，回复关键字：阿里巴巴开发手册详尽版 就可以获得。 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/06/19/SimpleDateFormat/ 在看的过程中有这么一条： 【强制】SimpleDateFormat 是线程不安全的类，一般不要定义为 static 变量，如果定义为 static，必须加锁，或者使用 DateUtils 工具类。 看到这条我立马就想起了我实习的时候有个项目里面就犯了这个错误，记得当时是这样写的： 1private static final SimpleDateFormat df = new SimpleDateFormat(\"yyyyMMddHHmmss\"); 所以才认真的去研究下这个 SimpleDateFormat，所以才有了这篇文章。 它是谁？想必大家对 SimpleDateFormat 并不陌生。SimpleDateFormat 是 Java 中一个非常常用的类，他是以区域敏感的方式格式化和解析日期的具体类。 它允许格式化 (date -&gt; text)、语法分析 (text -&gt; date)和标准化。 SimpleDateFormat 允许以任何用户指定的日期-时间格式方式启动。 但是，建议使用 DateFormat 中的 getTimeInstance、 getDateInstance 或 getDateTimeInstance 方法来创建一个日期-时间格式。 这几个方法会返回一个默认的日期／时间格式。 你可以根据需要用 applyPattern 方法修改格式方式。 日期时间格式日期和时间格式由 日期和时间模式字符串 指定。在 日期和时间模式字符串 中，未加引号的字母 ‘A’ 到 ‘Z’ 和 ‘a’ 到 ‘z’ 被解释为模式字母，用来表示日期或时间字符串元素。文本可以使用单引号 (‘) 引起来，以免进行解释。所有其他字符均不解释，只是在格式化时将它们简单复制到输出字符串。 简单的讲：这些 A ——Z，a —— z 这些字母(不被单引号包围的)会被特殊处理替换为对应的日期时间，其他的字符串还是原样输出。 日期和时间模式(注意大小写，代表的含义是不同的)如下： 怎么使用？日期／时间格式模版样例：（给的时间是：2001-07-04 12:08:56 U.S. Pacific Time time zone） 使用方法： 1234567891011121314151617181920212223import java.text.SimpleDateFormat;import java.util.Date;/** * Created by zhisheng_tian on 2018/6/19 */public class FormatDateTime &#123; public static void main(String[] args) &#123; SimpleDateFormat myFmt = new SimpleDateFormat(\"yyyy年MM月dd日 HH时mm分ss秒\"); SimpleDateFormat myFmt1 = new SimpleDateFormat(\"yy/MM/dd HH:mm\"); SimpleDateFormat myFmt2 = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");//等价于now.toLocaleString() SimpleDateFormat myFmt3 = new SimpleDateFormat(\"yyyy年MM月dd日 HH时mm分ss秒 E \"); SimpleDateFormat myFmt4 = new SimpleDateFormat(\"一年中的第 D 天 一年中第w个星期 一月中第W个星期 在一天中k时 z时区\"); Date now = new Date(); System.out.println(myFmt.format(now)); System.out.println(myFmt1.format(now)); System.out.println(myFmt2.format(now)); System.out.println(myFmt3.format(now)); System.out.println(myFmt4.format(now)); System.out.println(now.toGMTString()); System.out.println(now.toLocaleString()); System.out.println(now.toString()); &#125;&#125; 结果是： 123456782018年06月19日 23时10分05秒18/06/19 23:102018-06-19 23:10:052018年06月19日 23时10分05秒 星期二一年中的第 170 天 一年中第25个星期 一月中第4个星期 在一天中23时 CST时区19 Jun 2018 15:10:05 GMT2018-6-19 23:10:05Tue Jun 19 23:10:05 CST 2018 使用方法很简单，就是先自己定义好时间／日期模版，然后调用 format 方法（传入一个时间 Date 参数）。 上面的是日期转换成自己想要的字符串格式。下面反过来，将字符串类型装换成日期类型： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Date;/** * Created by zhisheng_tian on 2018/6/19 */public class StringFormatDate &#123; public static void main(String[] args) &#123; String time1 = \"2018年06月19日 23时10分05秒\"; String time2 = \"18/06/19 23:10\"; String time3 = \"2018-06-19 23:10:05\"; String time4 = \"2018年06月19日 23时10分05秒 星期二\"; SimpleDateFormat myFmt = new SimpleDateFormat(\"yyyy年MM月dd日 HH时mm分ss秒\"); SimpleDateFormat myFmt1 = new SimpleDateFormat(\"yy/MM/dd HH:mm\"); SimpleDateFormat myFmt2 = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");//等价于now.toLocaleString() SimpleDateFormat myFmt3 = new SimpleDateFormat(\"yyyy年MM月dd日 HH时mm分ss秒 E\"); Date date1 = null; try &#123; date1 = myFmt.parse(time1); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; System.out.println(date1); Date date2 = null; try &#123; date2 = myFmt1.parse(time2); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; System.out.println(date2); Date date3 = null; try &#123; date3 = myFmt2.parse(time3); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; System.out.println(date3); Date date4 = null; try &#123; date4 = myFmt3.parse(time4); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; System.out.println(date4); &#125;&#125; 结果是： 1234Tue Jun 19 23:10:05 CST 2018Tue Jun 19 23:10:00 CST 2018Tue Jun 19 23:10:05 CST 2018Tue Jun 19 23:10:05 CST 2018 这个转换方法也很简单。但是不要高兴的太早，主角不在这。 线程不安全 在 SimpleDateFormat 类的 JavaDoc 中，描述了该类不能够保证线程安全，建议为每个线程创建单独的日期／时间格式实例，如果多个线程同时访问一个日期／时间格式，它必须在外部进行同步。那么在多线程环境下调用 format() 和 parse() 方法应该使用同步代码来避免问题。下面我们通过一个具体的场景来一步步的深入学习和理解SimpleDateFormat 类。 1、每个线程创建单独的日期／时间格式实例 大量的创建 SimpleDateFormat 实例对象，然后再丢弃这个对象，占用大量的内存和 JVM 空间。 2、创建一个静态的 SimpleDateFormat 实例，在使用时直接使用这个实例进行操作（我当时就是这么干的😄） 123private static final SimpleDateFormat df = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\");Date date = new Date();df.format(date); 当然，这个方法的确很不错，在大部分的时间里面都会工作得很好，但一旦在生产环境中一定负载情况下时，这个问题就出来了。他会出现各种不同的情况，比如转化的时间不正确，比如报错，比如线程被挂死等等。我们看下面的测试用例，拿事实说话： 1234567891011121314151617import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Date;/** * Created by zhisheng_tian on 2018/6/20 */public class DateUtils &#123; private static final SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"); public static String formatDate(Date date) throws ParseException &#123; return sdf.format(date); &#125; public static Date parse(String strDate) throws ParseException &#123; return sdf.parse(strDate); &#125;&#125; 12345678910111213141516171819202122232425262728import java.text.ParseException;/** * Created by zhisheng_tian on 2018/6/20 */public class DateUtilsTest &#123; public static class TestSimpleDateFormatThreadSafe extends Thread &#123; @Override public void run() &#123; while (true) &#123; try &#123; this.join(2000); &#125; catch (InterruptedException e1) &#123; e1.printStackTrace(); &#125; try &#123; System.out.println(this.getName() + \":\" + DateUtils.parse(\"2018-06-20 01:18:20\")); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; public static void main(String[] args) &#123; for (int i = 0; i &lt; 3; i++) &#123; new TestSimpleDateFormatThreadSafe().start(); &#125; &#125;&#125; 运行结果如下： 1234567891011121314151617181920212223242526Exception in thread \"Thread-0\" Exception in thread \"Thread-1\" java.lang.NumberFormatException: For input string: \"\" at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) at java.lang.Long.parseLong(Long.java:601) at java.lang.Long.parseLong(Long.java:631) at java.text.DigitList.getLong(DigitList.java:195) at java.text.DecimalFormat.parse(DecimalFormat.java:2051) at java.text.SimpleDateFormat.subParse(SimpleDateFormat.java:1869) at java.text.SimpleDateFormat.parse(SimpleDateFormat.java:1514) at java.text.DateFormat.parse(DateFormat.java:364) at com.zhisheng.demo.date.DateUtils.parse(DateUtils.java:19) at com.zhisheng.demo.date.DateUtilsTest$TestSimpleDateFormatThreadSafe.run(DateUtilsTest.java:19)java.lang.NumberFormatException: For input string: \".1818\" at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) at java.lang.Long.parseLong(Long.java:578) at java.lang.Long.parseLong(Long.java:631) at java.text.DigitList.getLong(DigitList.java:195) at java.text.DecimalFormat.parse(DecimalFormat.java:2051) at java.text.SimpleDateFormat.subParse(SimpleDateFormat.java:2162) at java.text.SimpleDateFormat.parse(SimpleDateFormat.java:1514) at java.text.DateFormat.parse(DateFormat.java:364) at com.zhisheng.demo.date.DateUtils.parse(DateUtils.java:19) at com.zhisheng.demo.date.DateUtilsTest$TestSimpleDateFormatThreadSafe.run(DateUtilsTest.java:19)Thread-2:Sat Jun 20 01:18:20 CST 2201Thread-2:Wed Jun 20 01:18:20 CST 2018Thread-2:Wed Jun 20 01:18:20 CST 2018Thread-2:Wed Jun 20 01:18:20 CST 2018 说明：Thread-1和Thread-0报java.lang.NumberFormatException: multiple points错误，直接挂死，没起来；Thread-2 虽然没有挂死，但输出的时间是有错误的，比如我们输入的时间是：2018-06-20 01:18:20 ，当会输出：Sat Jun 20 01:18:20 CST 2201 这样的灵异事件。 Why?为什么会出现线程不安全的问题呢？ 下面我们通过看 JDK 源码来看看为什么 SimpleDateFormat 和 DateFormat 类不是线程安全的真正原因： SimpleDateFormat 继承了 DateFormat，在 DateFormat 中定义了一个 protected 属性的 Calendar 类的对象：calendar。只是因为 Calendar 类的概念复杂，牵扯到时区与本地化等等，JDK 的实现中使用了成员变量来传递参数，这就造成在多线程的时候会出现错误。 在 SimpleDateFormat 中的 format 方法源码中： 123456789101112131415161718192021222324252627282930313233@Overridepublic StringBuffer format(Date date, StringBuffer toAppendTo,FieldPosition pos) &#123; pos.beginIndex = pos.endIndex = 0; return format(date, toAppendTo, pos.getFieldDelegate());&#125;// Called from Format after creating a FieldDelegateprivate StringBuffer format(Date date, StringBuffer toAppendTo,FieldDelegate delegate) &#123; // Convert input date to time field list calendar.setTime(date); boolean useDateFormatSymbols = useDateFormatSymbols(); for (int i = 0; i &lt; compiledPattern.length; ) &#123; int tag = compiledPattern[i] &gt;&gt;&gt; 8; int count = compiledPattern[i++] &amp; 0xff; if (count == 255) &#123; count = compiledPattern[i++] &lt;&lt; 16; count |= compiledPattern[i++]; &#125; switch (tag) &#123; case TAG_QUOTE_ASCII_CHAR: toAppendTo.append((char)count); break; case TAG_QUOTE_CHARS: toAppendTo.append(compiledPattern, i, count); i += count; break; default: subFormat(tag, count, delegate, toAppendTo, useDateFormatSymbols); break; &#125; &#125; return toAppendTo;&#125; calendar.setTime(date) 这条语句改变了 calendar，稍后，calendar 还会用到（在 subFormat 方法里），而这就是引发问题的根源。想象一下，在一个多线程环境下，有两个线程持有了同一个 SimpleDateFormat 的实例，分别调用format 方法： 12345线程 1 调用 format 方法，改变了 calendar 这个字段。线程 1 中断了。线程 2 开始执行，它也改变了 calendar。线程 2 中断了。线程 1 回来了 此时，calendar 已然不是它所设的值，而是走上了线程 2 设计的道路。如果多个线程同时争抢 calendar 对象，则会出现各种问题，时间不对，线程挂死等等。 分析一下 format 的实现，我们不难发现，用到成员变量 calendar，唯一的好处，就是在调用 subFormat 时，少了一个参数，却带来了许多的问题。其实，只要在这里用一个局部变量，一路传递下去，所有问题都将迎刃而解。 这个问题背后隐藏着一个更为重要的问题–无状态：无状态方法的好处之一，就是它在各种环境下，都可以安全的调用。衡量一个方法是否是有状态的，就看它是否改动了其它的东西，比如全局变量，比如实例的字段。format 方法在运行过程中改动了 SimpleDateFormat 的 calendar 字段，所以，它是有状态的。 这也同时提醒我们在开发和设计系统的时候注意下一下三点: 1.自己写公用类的时候，要对多线程调用情况下的后果在注释里进行明确说明 2.多线程环境下，对每一个共享的可变变量都要注意其线程安全性 3.我们的类和方法在做设计的时候，要尽量设计成无状态的 解决方法1、需要的时候创建新实例 说明：在需要用到 SimpleDateFormat 的地方新建一个实例，不管什么时候，将有线程安全问题的对象由共享变为局部私有都能避免多线程问题，不过也加重了创建对象的负担。在一般情况下，这样其实对性能影响比不是很明显的。 2、使用同步：同步 SimpleDateFormat 对象 1234567891011121314151617181920import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Date;public class DateSyncUtil &#123; private static SimpleDateFormat sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\"); public static String formatDate(Date date) throws ParseException &#123; synchronized(sdf) &#123; return sdf.format(date); &#125; &#125; public static Date parse(String strDate) throws ParseException &#123; synchronized(sdf) &#123; return sdf.parse(strDate); &#125; &#125;&#125; 说明：当线程较多时，当一个线程调用该方法时，其他想要调用此方法的线程就要 block 等待，多线程并发量大的时候会对性能有一定的影响。 3、使用 ThreadLocal 12345678910111213141516171819202122import java.text.DateFormat;import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Date;public class ConcurrentDateUtil &#123; private static ThreadLocal&lt;DateFormat&gt; threadLocal = new ThreadLocal&lt;DateFormat&gt;() &#123; @Override protected DateFormat initialValue() &#123; return new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); &#125; &#125;; public static Date parse(String dateStr) throws ParseException &#123; return threadLocal.get().parse(dateStr); &#125; public static String format(Date date) &#123; return threadLocal.get().format(date); &#125;&#125; 说明：使用 ThreadLocal, 也是将共享变量变为独享，线程独享肯定能比方法独享在并发环境中能减少不少创建对象的开销。如果对性能要求比较高的情况下，一般推荐使用这种方法。 Java 8 中的解决办法Java 8 提供了新的日期时间 API，其中包括用于日期时间格式化的 DateTimeFormatter，它与 SimpleDateFormat 最大的区别在于：DateTimeFormatter 是线程安全的，而 SimpleDateFormat 并不是线程安全。 DateTimeFormatter 如何使用： 解析日期 123String dateStr= \"2018年06月20日\";DateTimeFormatter formatter = DateTimeFormatter.ofPattern(\"yyyy年MM月dd日\"); LocalDate date= LocalDate.parse(dateStr, formatter); 日期转换为字符串 123LocalDateTime now = LocalDateTime.now(); DateTimeFormatter format = DateTimeFormatter.ofPattern(\"yyyy年MM月dd日 hh:mm a\");String nowStr = now .format(format); 由 DateTimeFormatter 的静态方法 ofPattern() 构建日期格式，LocalDateTime 和 LocalDate 等一些表示日期或时间的类使用 parse 和 format 方法把日期和字符串做转换。 使用新的 API，整个转换过程都不需要考虑线程安全的问题。 总结SimpleDateFormat 是线程不安全的类，多线程环境下注意线程安全问题，如果是 Java 8 ，建议使用 DateTimeFormatter 代替 SimpleDateFormat。 参考资料http://www.cnblogs.com/peida/archive/2013/05/31/3070790.html 相关文章20 个案例教你在 Java 8 中如何处理日期和时间?","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"}]},{"title":"苦逼的毕业论文经历","date":"2018-05-25T16:00:00.000Z","path":"2018/05/26/paper/","text":"背景最近一直在学校忙毕业论文的事，抱歉了，很长一段时间没更新文章了，今天星期六，昨天星期五，幸好所有资料都在周末前交齐了，昨晚还到参加班上的聚会，也喝了不少酒，但幸好没醉，不然今天肯定不会写下这篇文章的。这篇文章就把从接触到毕业论文指导老师到现在的这半年时间有关毕业论文的事都讲讲，希望能够留下点回忆！ 毕设小分队成立学校会给每个毕业导师安排带几个学生（7～8 人左右），但是在学校发出表格之前，谁也不知道自己将会由哪个导师带，甚至你会发现就算学校发出表格后看到自己导师名字后，自己都不知道这导师是谁？是男是女？（当某个导师的名字比较中性的时候）到底好不好相处？性格咋样？他的联系方式（手机 或者 QQ），WTF，一个都不知道？苦逼了，后面也是通过问辅导员才知道这些情况。后面我们几个人中就有一个带头建了个 QQ 群（毕设小分队），并把指导老师拉进群了。 就这样，我们终于开始了毕设小分队之旅！ 实习时的毕设状态2017.12月那时是还在公司实习的，到了毕业设计选题的时候，我们导师还好，给我们每个人很自由的选择，你可以自拟题目，也可以从导师那里挑选题目。不过我看了下老师给的题目，大都是深度学习、机器学习做的相关图像识别、推荐系统、搜索系统、人脸识别、网络流量监控、文本情感分析等高大上的课题。臣妾做不到，毕竟这都是研究生才会研究点方向，老师会发出来这些题目可能也是和她自己在研究学习的相关知识有关（因为我后来给导师检查我的系统的时候就是导师带的一个研究生检查的，期间那个学长和我说他最近就在做文本情感分析相关的一个项目，因为看到我项目中的中文分词器，所以还向我请教了下，我于是把我原来写过的一篇中英文分词器找出来给他一个个解释），所以导师希望在我们这里也能够有人去研究下这方面的知识，拓展下视野。但是由于在公司实习，工作之外腾出的时间确实有限，没这么多时间去学习这些特别新的东西，而且还要做出一定的成果出来，这还是有点挑战性的。导师说你可以自拟题目，或者把自己在公司做的项目优化后拿来做毕设项目。于是，我选择的是拿自己以前写的项目来当毕业论文项目了，因为感觉项目也还行，拿来做毕设完全可以了，所以后面自己也就比较轻松了。 毕设流程把自己的毕设题目报给老师后，老师会根据你的题目审核是否可以拿来当毕设。然后再要求你把毕业设计的开题报告书（论文题目、目的、意义、怎么做、做出什么效果、目标之类的）写详细上交上去。要经过导师和学校的审批后才能够正式开题。开题后确定后几乎就可以开始写自己的项目了。机智的我，那时比较轻松，哈哈哈，工作之外都撸代码和博客呢。差不多过了一个月后，导师就开始在群里问我们系统完成的怎么样了？然后要我们上交论文任务书，学校的这种材料有时候要的很急，不得不吐槽下，很头疼，自己有时候都一下子交不了。又过了一段时间就需要把论文初稿交上去，真是苦逼，那时还不知道论文的模版到底是咋样？该怎么写本科论文呢？搞得我自己那段时间花了周末两天和一天的上班的时间才勉强交了个一般的初稿，初稿中的各种图画的我是想哭。真多，还麻烦。幸好在那段时间公司的任务还不忙，所以在有时间在实习上班的时候也稍微完成下自己的初稿。 今年 3 ～ 4 月的时候，导师也在不断的叫我们把学校该交的实习每周报告交给她。这个实习还有实习申请表、实习鉴定表材料（要求盖章），这两个资料也是很重要的，如果后面没有这个材料都不能参加答辩的。 在快五月的时候，导师那时就天天晚上深夜在毕设小分队群里催同学们，你们的论文和系统都完成的咋样了？然后又催了要交论文中期报告（这个应该也蛮重要的，我们班两个同学好像就因为这个导致有个论文中期报告警告，吓得他们后来答辩的时候都怕自己挂了）。 过完劳动节后，再过了几天后，我就辞职了。辞职后在苏州玩了几天，期间也是在不断的修改自己的论文和优化些格式问题。 后来又浪回家了，在家呆了几天，也是改论文改到深夜，那段时间自己已经开始在把自己的论文初稿拿到些第三方免费的查重去查重网站去查重，然后根据查重后的结果，将一些重复的地方东改改，西凑凑，或者用自己的话写一遍。这样就可以减少点重复语句。 再然后就是 18 号赶回来学校了，19，20号刚好周末，又是不眠之夜，那两天不断查重，向别人请教，怎么加字数，自己一开始的字数好像还是不够的，学校要求的字数有点高，其他学校都是 8000 字，我们却要求 1.5 w，真是能扯啊，怎么可以扯出这么多字来。通过某人的经验之谈，我成功的扯了不少字，还不带重复的，😄。 然后 21 号周一拿给老师检查的时候就是看了下格式（老师不在办公室，她带的研究生给检查的），说你全文首行咋都没缩进呢（写博客写习惯了，谁还缩进啊），行间距比较大，图的标号错误，主要还是检查格式。当晚，又好好改了下，并和我同学讨论了下他的指导导师的要求有哪些，并也做了相应的修改。（因为我的答辩导师就有我那同学的指导老师）记得当晚好像改论文改到三点，幸好有人陪着我一边聊天一边改，不然早困了，睡了。 22 号一早就醒来了，然后就去打印店打印论文。这里想给的大家一个建议就是：如果你也要打印论文，最好早点去，因为毕业季，学校打印店几乎都是爆满，打印论文的非常多，还有学弟学妹打印各种考试卷子、资料啥的，反正人很多，估计要排队。指导老师也不是一直在办公室的，他们也是要上课的。所以最好在导师在办公室的时候能赶到，这样就不会错过了。这次老师检查论文就比较仔细了，论文一行行的找内容，看是否通顺？是否有不合适的地方？标点符号是不是多了或者少了？格式是不是还有问题？所以呢，这次又很惨，要改的地方很多，还包括流程图和 E-R 图要改的。苦逼了，下午回去租房宾馆的时候就开始拼命的改。改的差不多了，因为第二天要答辩了，所以就在看看自己的系统是否能够跑起来，有没有什么bug，结果还真发现几个小错误和一个大错误，小错误很快修复完善了，有一个 Redis 存数据再取出来的时候数据变化问题当然debug了很久没找到原因。找到深夜一点多，没解决，放弃了，第二天不演示这块。当晚怕自己明天项目启动的时候其他环境要一个个起，需要耗费不少时间，于是自己简单的写了个脚本，一键将自己项目的环境启动起来，这样就可以直接运行项目展示给答辩老师，节省时间，尤其是关键场合，怕掉链子。 23 号早上很早就去把自己的实习申请表、实习鉴定表、实习 20 周日志、实习终结、论文初稿这些资料打印，然后拿给导师查看和打分。（又找出问题来了，苦逼，记得那个早上现场在导师办公室用电脑改好后，检查完才拿去打印再拿回来给导师检查的，来回跑来跑去的真的很急，出了不少汗）注意：越是关键时刻，千万要顺着导师来，他说改哪里就改哪里，千万别刚，我一个同学的论文改的导师都发火了，差点没让我那同学参加答辩，直接进入二辩的。 答辩的时候先每个人介绍下自己的实习经历，听到不少牛逼的同学，进了不少厉害的公司，有的同学还当上了公司的项目组组长了，真厉害，一年的实习时间就混的这么好了。瞬间发现差距很大。 然后就是每个人的答辩了，我第三个，还好，答辩的过程问了我的问题和演示系统的时候都表现的蛮好的。就现场还问了我有个功能的代码是哪？（估计是想看下是不是自己写的项目）后面也看到有的同学项目竟然起不来的，或者回答不出老师的问题的。答辩过程中，答辩老师又给论文找了不少问题，又要苦逼的修改了。 当天就出了待定名单和要二辩的名单，速度还是很快。还好我没有，开森。本以为答辩好了，就可以松下一口气了，后面改好了论文后拿去给导师检查，她都说 OK 了，结果我就去把论文拿去胶装了，注意胶装顺序别搞错了。第二天把胶装好的论文和各种资料一起交给老师的时候，结果又挑出一个目录的问题，WTF，搞得我后面改好后去打印店重新胶装了遍（封皮从原来的上面扯下来胶装）。昨天签完字了，把论文的初稿、终稿、答辩记录、实习材料等材料一起装进档案袋了，这才安心了。 于是当晚就去参加了班上的毕业聚会。 感受这段时间真的是很累，每天熬夜到很晚，第二天早上很早就自然醒了。瘦了不少，牛仔裤的皮带我感觉都缩了一圈，右手的几个手指天天在电脑的触摸板上滑来滑去的，都脱了一层皮了，现在放上去都有点痛，打印论文好像打印了7份，烧钱啊。发现我的记忆还是可以的，这么多事竟然还能记得，搞得整篇文章有点像在记流水账，哈哈哈。反正也是记录下自己的论文答辩这段时间的经历。论文的格式很重要，不再像写博客那样随意，论文是需要以一种严谨的态度去对待的。 写着写着忘了说论文的查重了。一般你一开始最好先用第三方的免费查重下，然后修改。 我当时用的是：http://xueshu.baidu.com/usercenter/papercheck/ 这个地方有几个是可以第一次免费查重的，如果次数用完了，记得换个百度账号就又有好些次查重的机会。如果你实在是不放心，可以去学校的打印店问下是否有论文查重的，也是查的知网的库，和学校知网查重的区别不大，我比较自信，没花这个钱，毕竟好贵，😯我穷，哈哈哈。学校规定的是不能高于 30% 的查重率，建议还是自己把查重降低到 20% 以下然后再提交到学校知网去，不然超过的话是需要二次查重的。 还有一个感受就是：多和你的导师交流，遇到不会或者拿不准的最好在群里问他，然后他说的错误，你要及时更改好过来，说你问题的时候要学会脸上挂着笑容，嘻嘻嘻就过去了，然后记住该问题的错误，回去立马改好，及时拿材料给老师查阅修改好的论文。","tags":[{"name":"随笔","slug":"随笔","permalink":"http://www.54tianzhisheng.cn/tags/随笔/"}]},{"title":"Spring Boot 2.0系列文章(七)：SpringApplication 深入探索","date":"2018-04-29T16:00:00.000Z","path":"2018/04/30/springboot_SpringApplication/","text":"SpringBoot 系列文章 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/04/30/springboot_SpringApplication/ 前言在 Spring Boot 项目的启动类中常见代码如下： 123456@SpringBootApplicationpublic class SpringbotApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(SpringbotApplication.class, args); &#125;&#125; 其中也就两个比较引人注意的地方： @SpringBootApplication SpringApplication.run() 对于第一个注解 @SpringBootApplication，我已经在博客 Spring Boot 2.0系列文章(六)：Spring Boot 2.0中SpringBootApplication注解详解 中详细的讲解了。接下来就是深入探究第二个了 SpringApplication.run() 。 换个姿势上面的姿势太简单了，只一行代码就完事了。 1SpringApplication.run(SpringbotApplication.class, args); 其实是支持做一些个性化的设置，接下来我们换个姿势瞧瞧： 123456789@SpringBootApplicationpublic class SpringbotApplication &#123; public static void main(String[] args) &#123; SpringApplication app = new SpringApplication(SpringbotApplication.class); // 自定义应用程序的配置 //app.setXxx() app.run(args) &#125;&#125; 没错，就是通过一个构造函数，然后设置相关的属性，从而达到定制化服务。有哪些属性呢？ 属性对应的 get／set 方法 看到没，还很多呢！ 举个例子：你想把 Spring Boot 项目的默认 Banner 换成你自己的，就需要在这里如下： 123456789101112131415161718public static void main(String[] args) &#123;// SpringApplication.run(Springboot2Application.class, args); SpringApplication application = new SpringApplication(Springboot2Application.class); application.setBanner((environment, sourceClass, out) -&gt; &#123; //这里打印一个logo System.out.println(\" _ _ _\\n\" + \" | | (_) | |\\n\" + \" ____| |__ _ ___ | |__ ___ _ __ __ _\\n\" + \"|_ /| '_ \\\\ | |/ __|| '_ \\\\ / _ \\\\| '_ \\\\ / _` |\\n\" + \" / / | | | || |\\\\__ \\\\| | | || __/| | | || (_| |\\n\" + \"/___||_| |_||_||___/|_| |_| \\\\___||_| |_| \\\\__, |\\n\" + \" __/ |\\n\" + \" |___/\\n\"); &#125;); application.setBannerMode(Banner.Mode.CONSOLE); //你还可以干其他的定制化初始设置 application.run(args);&#125; 现在重启项目，你就会发现，控制台的 logo 已经换成你自己的了。 当然了，你可能会觉得这样写有点复杂，嗯嗯，确实，这样硬编码在代码里确实不太友好。你还可以在src/main/resources路径下新建一个banner.txt文件，banner.txt中填写好需要打印的字符串内容即可。 从该类中可以看到在 Spring Boot 2 中引入了个新的 WebApplicationType 和 WebEnvironment。 确实，这也是 Spring Boot 2 中比较大的特性，它是支持响应式编程的。我之前在文章 Spring Boot 2.0系列文章(二)：Spring Boot 2.0 新特性详解 中也介绍过，以后有机会会介绍它的，这里我先卖个关子。 SpringApplication 初始化SpringApplication.run() 的实现才是我们要深入探究的主角，该方法代码如下： 123456789//静态方法，可用于使用默认配置运行 SpringApplicationpublic static ConfigurableApplicationContext run(Class&lt;?&gt; primarySource, String... args) &#123; return run(new Class&lt;?&gt;[] &#123; primarySource &#125;, args);&#125;public static ConfigurableApplicationContext run(Class&lt;?&gt;[] primarySources, String[] args) &#123; return new SpringApplication(primarySources).run(args);&#125; 在这个静态方法中，创建 SpringApplication 对象，并调用该对象的 run 方法。 123456789101112131415public SpringApplication(Class&lt;?&gt;... primarySources) &#123; this(null, primarySources);&#125;//创建一个 SpringApplication 实例，应用上下文会根据指定的主要资源加载 beans ，实例在调用 run 方法之前可以定制化@SuppressWarnings(&#123; \"unchecked\", \"rawtypes\" &#125;)public SpringApplication(ResourceLoader resourceLoader, Class&lt;?&gt;... primarySources) &#123; this.resourceLoader = resourceLoader; Assert.notNull(primarySources, \"PrimarySources must not be null\"); this.primarySources = new LinkedHashSet&lt;&gt;(Arrays.asList(primarySources)); this.webApplicationType = deduceWebApplicationType(); setInitializers((Collection) getSpringFactoriesInstances( ApplicationContextInitializer.class)); setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class)); this.mainApplicationClass = deduceMainApplicationClass();&#125; 首先是进入单个参数的构造方法，然后进入两参数的构造方法（ResourceLoader 为 null），然后进行初始化。 1、deduceWebApplicationType() : 推断应用的类型 ，创建的是一个 SERVLET 应用还是 REACTIVE应用或者是 NONE 1234567891011121314151617private static final String REACTIVE_WEB_ENVIRONMENT_CLASS = \"org.springframework.web.reactive.DispatcherHandler\";private static final String MVC_WEB_ENVIRONMENT_CLASS = \"org.springframework.web.servlet.DispatcherServlet\";private static final String[] WEB_ENVIRONMENT_CLASSES = &#123; \"javax.servlet.Servlet\", \"org.springframework.web.context.ConfigurableWebApplicationContext\" &#125;;private WebApplicationType deduceWebApplicationType() &#123; if (ClassUtils.isPresent(REACTIVE_WEB_ENVIRONMENT_CLASS, null) &amp;&amp; !ClassUtils.isPresent(MVC_WEB_ENVIRONMENT_CLASS, null)) &#123; return WebApplicationType.REACTIVE; //该程序是 REACTIVE 程序 &#125; for (String className : WEB_ENVIRONMENT_CLASSES) &#123; if (!ClassUtils.isPresent(className, null)) &#123; return WebApplicationType.NONE; //该程序为 NONE &#125; &#125; return WebApplicationType.SERVLET; //默认返回是 SERVLET 程序&#125; 2、setInitializers((Collection) getSpringFactoriesInstances(ApplicationContextInitializer.class))：初始化 classpath 下的所有的可用的 ApplicationContextInitializer。 1）、getSpringFactoriesInstances() 123456789101112131415161718192021222324252627282930313233private &lt;T&gt; Collection&lt;T&gt; getSpringFactoriesInstances(Class&lt;T&gt; type) &#123; return getSpringFactoriesInstances(type, new Class&lt;?&gt;[] &#123;&#125;);&#125;//获取所有的 Spring 工厂实例private &lt;T&gt; Collection&lt;T&gt; getSpringFactoriesInstances(Class&lt;T&gt; type,Class&lt;?&gt;[] parameterTypes, Object... args) &#123; ClassLoader classLoader = Thread.currentThread().getContextClassLoader(); // Use names and ensure unique to protect against duplicates Set&lt;String&gt; names = new LinkedHashSet&lt;&gt;(SpringFactoriesLoader.loadFactoryNames(type, classLoader)); //获取所有 Spring Factories 的名字 List&lt;T&gt; instances = createSpringFactoriesInstances(type, parameterTypes, classLoader, args, names); AnnotationAwareOrderComparator.sort(instances); //Spring 工厂实例排序 return instances;&#125;//根据读取到的名字创建对象（Spring 工厂实例）private &lt;T&gt; List&lt;T&gt; createSpringFactoriesInstances(Class&lt;T&gt; type, Class&lt;?&gt;[] parameterTypes, ClassLoader classLoader, Object[] args, Set&lt;String&gt; names) &#123; List&lt;T&gt; instances = new ArrayList&lt;&gt;(names.size()); for (String name : names) &#123; try &#123; Class&lt;?&gt; instanceClass = ClassUtils.forName(name, classLoader); Assert.isAssignable(type, instanceClass); Constructor&lt;?&gt; constructor = instanceClass.getDeclaredConstructor(parameterTypes); T instance = (T) BeanUtils.instantiateClass(constructor, args); instances.add(instance); &#125; catch (Throwable ex) &#123; throw new IllegalArgumentException( \"Cannot instantiate \" + type + \" : \" + name, ex); &#125; &#125; return instances;&#125; 上面的 SpringFactoriesLoader.loadFactoryNames() ，是从 META-INF/spring.factories 的资源文件中，读取 key 为org.springframework.context.ApplicationContextInitializer 的 value。 而 spring.factories 的部分内容如下： 可以看到，最近的得到的，是 ConfigurationWarningsApplicationContextInitializer，ContextIdApplicationContextInitializer，DelegatingApplicationContextInitializer，ServerPortInfoApplicationContextInitializer 这四个类的名字。 2）、setInitializers()： 12345public void setInitializers( Collection&lt;? extends ApplicationContextInitializer&lt;?&gt;&gt; initializers) &#123; this.initializers = new ArrayList&lt;&gt;(); this.initializers.addAll(initializers);&#125; 所以，这里 setInitializers() 所得到的成员变量 initializers 就被初始化为ConfigurationWarningsApplicationContextInitializer，ContextIdApplicationContextInitializer，DelegatingApplicationContextInitializer，ServerPortInfoApplicationContextInitializer 这四个类的对象组成的 list。 3、setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class))：初始化 classpath 下的所有的可用的 ApplicationListener。 1）、getSpringFactoriesInstances() 和上面的类似，但是它是从 META-INF/spring.factories 的资源文件中，获取到 key 为 org.springframework.context.ApplicationListener 的 value。 2）、setListeners()： 1234public void setListeners(Collection&lt;? extends ApplicationListener&lt;?&gt;&gt; listeners) &#123; this.listeners = new ArrayList&lt;&gt;(); this.listeners.addAll(listeners);&#125; 所以，这里 setListeners() 所得到的成员变量 listeners 就被初始化为 ClearCachesApplicationListener，ParentContextCloserApplicationListener，FileEncodingApplicationListener，AnsiOutputApplicationListener ，ConfigFileApplicationListener，DelegatingApplicationListener，ClasspathLoggingApplicationListener，LoggingApplicationListener，LiquibaseServiceLocatorApplicationListener 这九个类的对象组成的 list。 4、deduceMainApplicationClass() ：根据调用栈，推断出 main 方法的类名 1234567891011121314private Class&lt;?&gt; deduceMainApplicationClass() &#123; try &#123; StackTraceElement[] stackTrace = new RuntimeException().getStackTrace(); for (StackTraceElement stackTraceElement : stackTrace) &#123; if (\"main\".equals(stackTraceElement.getMethodName())) &#123; return Class.forName(stackTraceElement.getClassName()); &#125; &#125; &#125; catch (ClassNotFoundException ex) &#123; // Swallow and continue &#125; return null;&#125; run 方法背后的秘密上面看完了构造方法后，已经初始化了一个 SpringApplication 对象，接下来调用其 run 方法，代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445//运行 Spring 应用程序，创建并刷新一个新的 ApplicationContextpublic ConfigurableApplicationContext run(String... args) &#123; StopWatch stopWatch = new StopWatch(); stopWatch.start(); ConfigurableApplicationContext context = null; Collection&lt;SpringBootExceptionReporter&gt; exceptionReporters = new ArrayList&lt;&gt;(); configureHeadlessProperty(); SpringApplicationRunListeners listeners = getRunListeners(args); listeners.starting(); try &#123; ApplicationArguments applicationArguments = new DefaultApplicationArguments( args); ConfigurableEnvironment environment = prepareEnvironment(listeners, applicationArguments); configureIgnoreBeanInfo(environment); Banner printedBanner = printBanner(environment); context = createApplicationContext(); exceptionReporters = getSpringFactoriesInstances( SpringBootExceptionReporter.class, new Class[] &#123; ConfigurableApplicationContext.class &#125;, context); prepareContext(context, environment, listeners, applicationArguments, printedBanner); refreshContext(context); afterRefresh(context, applicationArguments); stopWatch.stop(); if (this.logStartupInfo) &#123; new StartupInfoLogger(this.mainApplicationClass) .logStarted(getApplicationLog(), stopWatch); &#125; listeners.started(context); callRunners(context, applicationArguments); &#125; catch (Throwable ex) &#123; handleRunFailure(context, ex, exceptionReporters, listeners); throw new IllegalStateException(ex); &#125; try &#123; listeners.running(context); &#125; catch (Throwable ex) &#123; handleRunFailure(context, ex, exceptionReporters, null); throw new IllegalStateException(ex); &#125; return context; &#125; 可变个数参数 args 即是我们整个应用程序的入口 main 方法的参数。StopWatch 是来自 org.springframework.util 的工具类，可以用来方便的记录程序的运行时间。 再来看看 1.5.12 与 2.0.1 版本的 run 方法 有什么不一样的地方？ 接下来好好分析上面新版本（2.0.1）的 run 方法的代码并配合比较旧版本（1.5.12）。 1、configureHeadlessProperty()：设置 headless 模式 1234567private static final String SYSTEM_PROPERTY_JAVA_AWT_HEADLESS = \"java.awt.headless\";private boolean headless = true;private void configureHeadlessProperty() &#123; System.setProperty(SYSTEM_PROPERTY_JAVA_AWT_HEADLESS, System.getProperty( SYSTEM_PROPERTY_JAVA_AWT_HEADLESS, Boolean.toString(this.headless)));&#125; 实际上是就是设置系统属性 java.awt.headless，该属性会被设置为 true。 2、getRunListeners()：加载 SpringApplicationRunListener 对象 12345678910111213141516171819 //TODO: xxxSpringApplicationRunListeners listeners = getRunListeners(args);//初始化监听器listeners.starting();try &#123; prepareContext(context, environment, listeners, applicationArguments, printedBanner); refreshContext(context); afterRefresh(context, applicationArguments); listeners.started(context); callRunners(context, applicationArguments);&#125;try &#123; listeners.running(context);&#125;private SpringApplicationRunListeners getRunListeners(String[] args) &#123; Class&lt;?&gt;[] types = new Class&lt;?&gt;[] &#123; SpringApplication.class, String[].class &#125;; return new SpringApplicationRunListeners(logger, getSpringFactoriesInstances( SpringApplicationRunListener.class, types, this, args));&#125; 上面的 getRunListeners() 中也利用 SpringFactoriesLoader 加载 META-INF/spring.factories 中 key 为 SpringApplicationRunListener 的值，然后再将获取到的值作为参数传递到 SpringApplicationRunListeners 的构造方法中去创建对象。 3、new DefaultApplicationArguments(args) ：获取启动时传入参数 args（main 方法传进来的参数） 并初始化为 ApplicationArguments 对象。 12345public DefaultApplicationArguments(String[] args) &#123; Assert.notNull(args, \"Args must not be null\"); this.source = new Source(args); this.args = args;&#125; 4、prepareEnvironment(listeners, applicationArguments)：根据 listeners 和 applicationArguments 配置SpringBoot 应用的环境。 1234567891011121314151617181920212223242526272829303132333435363738private ConfigurableEnvironment prepareEnvironment( SpringApplicationRunListeners listeners, ApplicationArguments applicationArguments) &#123; // Create and configure the environment ConfigurableEnvironment environment = getOrCreateEnvironment(); configureEnvironment(environment, applicationArguments.getSourceArgs()); listeners.environmentPrepared(environment); bindToSpringApplication(environment); if (this.webApplicationType == WebApplicationType.NONE) &#123; environment = new EnvironmentConverter(getClassLoader()) .convertToStandardEnvironmentIfNecessary(environment); &#125; ConfigurationPropertySources.attach(environment); return environment;&#125;//如果 environment 不为空，直接 get 到，否则创建private ConfigurableEnvironment getOrCreateEnvironment() &#123; if (this.environment != null) &#123; return this.environment; &#125; if (this.webApplicationType == WebApplicationType.SERVLET) &#123; return new StandardServletEnvironment(); &#125; return new StandardEnvironment();&#125;//配置环境protected void configureEnvironment(ConfigurableEnvironment environment,String[] args) &#123; configurePropertySources(environment, args);//配置要使用的PropertySources configureProfiles(environment, args);//配置要使用的Profiles&#125;//将环境绑定到 SpringApplicationprotected void bindToSpringApplication(ConfigurableEnvironment environment) &#123; try &#123; Binder.get(environment).bind(\"spring.main\", Bindable.ofInstance(this)); &#125; catch (Exception ex) &#123; throw new IllegalStateException(\"Cannot bind to SpringApplication\", ex); &#125;&#125; 5、configureIgnoreBeanInfo(environment)：根据环境信息配置要忽略的 bean 信息 1234567891011public static final String IGNORE_BEANINFO_PROPERTY_NAME = \"spring.beaninfo.ignore\";private void configureIgnoreBeanInfo(ConfigurableEnvironment environment) &#123; if (System.getProperty( CachedIntrospectionResults.IGNORE_BEANINFO_PROPERTY_NAME) == null) &#123; Boolean ignore = environment.getProperty(\"spring.beaninfo.ignore\", Boolean.class, Boolean.TRUE); System.setProperty(CachedIntrospectionResults.IGNORE_BEANINFO_PROPERTY_NAME, ignore.toString()); &#125;&#125; 6、printBanner(environment)：打印标志，上面我已经说过了。 12345678910111213private Banner printBanner(ConfigurableEnvironment environment) &#123; if (this.bannerMode == Banner.Mode.OFF) &#123; //如果设置为 off，不打印 Banner return null; &#125; ResourceLoader resourceLoader = this.resourceLoader != null ? this.resourceLoader : new DefaultResourceLoader(getClassLoader()); SpringApplicationBannerPrinter bannerPrinter = new SpringApplicationBannerPrinter( resourceLoader, this.banner); if (this.bannerMode == Mode.LOG) &#123; return bannerPrinter.print(environment, this.mainApplicationClass, logger); &#125; return bannerPrinter.print(environment, this.mainApplicationClass, System.out);&#125; 7、createApplicationContext()：根据应用类型来确定该 Spring Boot 项目应该创建什么类型的 ApplicationContext ，默认情况下，如果没有明确设置的应用程序上下文或应用程序上下文类，该方法会在返回合适的默认值。 1234567891011121314151617181920212223242526public static final String DEFAULT_WEB_CONTEXT_CLASS = \"org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext\";public static final String DEFAULT_REACTIVE_WEB_CONTEXT_CLASS = \"org.springframework.boot.web.reactive.context.AnnotationConfigReactiveWebServerApplicationContext\";public static final String DEFAULT_CONTEXT_CLASS = \"org.springframework.context.annotation.AnnotationConfigApplicationContext\";protected ConfigurableApplicationContext createApplicationContext() &#123; Class&lt;?&gt; contextClass = this.applicationContextClass; if (contextClass == null) &#123; try &#123; switch (this.webApplicationType) &#123; //根据应用程序的类型来初始化容器 case SERVLET: //servlet 应用程序 contextClass = Class.forName(DEFAULT_WEB_CONTEXT_CLASS); break; case REACTIVE: //reactive 应用程序 contextClass = Class.forName(DEFAULT_REACTIVE_WEB_CONTEXT_CLASS); break; default: //默认 contextClass = Class.forName(DEFAULT_CONTEXT_CLASS); &#125; &#125; catch (ClassNotFoundException ex) &#123; throw new IllegalStateException( \"Unable create a default ApplicationContext,please specify an ApplicationContextClass\",ex); &#125; &#125; //最后通过Spring的工具类 BeanUtils 初始化容器类 bean return (ConfigurableApplicationContext) BeanUtils.instantiateClass(contextClass);&#125; 来看看在 1.5.12 中是怎么样的？ 8、exceptionReporters = getSpringFactoriesInstances( SpringBootExceptionReporter.class, new Class[] { ConfigurableApplicationContext.class }, context) 1234567891011private &lt;T&gt; Collection&lt;T&gt; getSpringFactoriesInstances(Class&lt;T&gt; type, Class&lt;?&gt;[] parameterTypes, Object... args) &#123; ClassLoader classLoader = Thread.currentThread().getContextClassLoader(); // Use names and ensure unique to protect against duplicates Set&lt;String&gt; names = new LinkedHashSet&lt;&gt;( SpringFactoriesLoader.loadFactoryNames(type, classLoader)); List&lt;T&gt; instances = createSpringFactoriesInstances(type, parameterTypes, classLoader, args, names);//根据类型 key 为 SpringBootExceptionReporter 去加载 AnnotationAwareOrderComparator.sort(instances);//对实例排序 return instances;&#125; 这里也是通过 SpringFactoriesLoader 加载 META-INF/spring.factories 中 key 为 SpringBootExceptionReporter 的全类名的 value 值。 9、prepareContext(context, environment, listeners, applicationArguments, printedBanner)：完成整个容器的创建与启动以及 bean 的注入功能。 12345678910111213141516171819202122232425262728//装配 Contextprivate void prepareContext(ConfigurableApplicationContext context, ConfigurableEnvironment environment, SpringApplicationRunListeners listeners, ApplicationArguments applicationArguments, Banner printedBanner) &#123; //将之前准备好的 environment 设置给创建好的 ApplicationContext 使用 context.setEnvironment(environment); //1、 postProcessApplicationContext(context); //2、 applyInitializers(context); listeners.contextPrepared(context); if (this.logStartupInfo) &#123;//启动日志 logStartupInfo(context.getParent() == null); logStartupProfileInfo(context); &#125; // Add boot specific singleton beans context.getBeanFactory().registerSingleton(\"springApplicationArguments\", applicationArguments); if (printedBanner != null) &#123; context.getBeanFactory().registerSingleton(\"springBootBanner\", printedBanner); &#125; // Load the sources Set&lt;Object&gt; sources = getAllSources(); Assert.notEmpty(sources, \"Sources must not be empty\"); //3、 load(context, sources.toArray(new Object[0])); listeners.contextLoaded(context);&#125; 1）、postProcessApplicationContext(context) 12345678910111213141516171819public static final String CONFIGURATION_BEAN_NAME_GENERATOR = \"org.springframework.context.annotation.internalConfigurationBeanNameGenerator\";protected void postProcessApplicationContext(ConfigurableApplicationContext context) &#123; if (this.beanNameGenerator != null) &#123; context.getBeanFactory().registerSingleton( AnnotationConfigUtils.CONFIGURATION_BEAN_NAME_GENERATOR, this.beanNameGenerator); &#125; if (this.resourceLoader != null) &#123; if (context instanceof GenericApplicationContext) &#123; ((GenericApplicationContext) context) .setResourceLoader(this.resourceLoader); &#125; if (context instanceof DefaultResourceLoader) &#123; ((DefaultResourceLoader) context) .setClassLoader(this.resourceLoader.getClassLoader()); &#125; &#125;&#125; 该方法对 context 进行了预设置，设置了 ResourceLoader 和 ClassLoader，并向 bean 工厂中添加了一个beanNameGenerator 。 2）、applyInitializers(context) 12345678protected void applyInitializers(ConfigurableApplicationContext context) &#123; for (ApplicationContextInitializer initializer : getInitializers()) &#123; Class&lt;?&gt; requiredType = GenericTypeResolver.resolveTypeArgument( initializer.getClass(), ApplicationContextInitializer.class); Assert.isInstanceOf(requiredType, context, \"Unable to call initializer.\"); initializer.initialize(context); &#125;&#125; 在刷新之前将任何 ApplicationContextInitializer 应用于上下文 3)、load(context, sources.toArray(new Object[0])) 主要是加载各种 beans 到 ApplicationContext 对象中。 1234567891011121314protected void load(ApplicationContext context, Object[] sources) &#123; BeanDefinitionLoader loader = createBeanDefinitionLoader( //2 getBeanDefinitionRegistry(context), sources);// 1 if (this.beanNameGenerator != null) &#123; loader.setBeanNameGenerator(this.beanNameGenerator); &#125; if (this.resourceLoader != null) &#123; loader.setResourceLoader(this.resourceLoader); &#125; if (this.environment != null) &#123; loader.setEnvironment(this.environment); &#125; loader.load();//3&#125; (1)、getBeanDefinitionRegistry(context) 获取 bean 定义注册表 12345678910private BeanDefinitionRegistry getBeanDefinitionRegistry(ApplicationContext context) &#123; if (context instanceof BeanDefinitionRegistry) &#123; return (BeanDefinitionRegistry) context; &#125; if (context instanceof AbstractApplicationContext) &#123; return (BeanDefinitionRegistry) ((AbstractApplicationContext) context) .getBeanFactory(); &#125; throw new IllegalStateException(\"Could not locate BeanDefinitionRegistry\");&#125; (2)、createBeanDefinitionLoader() 通过 BeanDefinitionLoader 的构造方法把参数（注册表、资源）传进去，然后创建 BeanDefinitionLoader。 (3)、load() 把资源全部加载。 10、refreshContext(context) 12345678910111213141516private void refreshContext(ConfigurableApplicationContext context) &#123; refresh(context);//1 if (this.registerShutdownHook) &#123; try &#123; context.registerShutdownHook(); &#125; catch (AccessControlException ex) &#123; // Not allowed in some environments. &#125; &#125;&#125;//刷新底层的 ApplicationContextprotected void refresh(ApplicationContext applicationContext) &#123; Assert.isInstanceOf(AbstractApplicationContext.class, applicationContext); ((AbstractApplicationContext) applicationContext).refresh();&#125; refreshContext(context) 方法又调用了 refresh(context)。在调用了 refresh(context) 方法之后，调用了 registerShutdownHook 方法。继续看它的 refresh 方法： 123456789101112131415161718192021222324252627282930313233343536373839404142public void refresh() throws BeansException, IllegalStateException &#123; synchronized (this.startupShutdownMonitor) &#123; // Prepare this context for refreshing. prepareRefresh(); // Tell the subclass to refresh the internal bean factory. ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // Prepare the bean factory for use in this context. prepareBeanFactory(beanFactory); try &#123; // Allows post-processing of the bean factory in context subclasses. postProcessBeanFactory(beanFactory); // Invoke factory processors registered as beans in the context. invokeBeanFactoryPostProcessors(beanFactory); // Register bean processors that intercept bean creation. registerBeanPostProcessors(beanFactory); // Initialize message source for this context. initMessageSource(); // Initialize event multicaster for this context. initApplicationEventMulticaster(); // Initialize other special beans in specific context subclasses. onRefresh(); // Check for listener beans and register them. registerListeners(); // Instantiate all remaining (non-lazy-init) singletons. finishBeanFactoryInitialization(beanFactory); //1 // Last step: publish corresponding event. finishRefresh(); &#125; catch (BeansException ex) &#123; 。。。 // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset 'active' flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; &#125; finally &#123; // Reset common introspection caches in Spring's core, since we // might not ever need metadata for singleton beans anymore... resetCommonCaches(); &#125; &#125;&#125; 到这里，我们就看见重点了，仔细看上的注释，正在做各种初始化工作，而今天我们关注的重点就是方法 finishBeanFactoryInitialization(beanFactory)。该方法进行了非懒加载 beans 的初始化工作。现在我们进入该方法内部，一探究竟。 看上图方法中的最后一步，调用了 beanFactory 的 preInstantiateSingletons() 方法。此处的 beanFactory 是哪个类的实例对象呢？ 可以看到 ConfigurableListableBeanFactory 接口的实现类只有 DefaultListableBeanFactory，我们看下实现类中的 preInstantiateSingletons 方法是怎么做的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public void preInstantiateSingletons() throws BeansException &#123; // Iterate over a copy to allow for init methods which in turn register new bean definitions. // While this may not be part of the regular factory bootstrap, it does otherwise work fine. List&lt;String&gt; beanNames = new ArrayList&lt;&gt;(this.beanDefinitionNames); // Trigger initialization of all non-lazy singleton beans... for (String beanName : beanNames) &#123; RootBeanDefinition bd = getMergedLocalBeanDefinition(beanName); if (!bd.isAbstract() &amp;&amp; bd.isSingleton() &amp;&amp; !bd.isLazyInit()) &#123; if (isFactoryBean(beanName)) &#123; Object bean = getBean(FACTORY_BEAN_PREFIX + beanName); if (bean instanceof FactoryBean) &#123; final FactoryBean&lt;?&gt; factory = (FactoryBean&lt;?&gt;) bean; boolean isEagerInit; if (System.getSecurityManager() != null &amp;&amp; factory instanceof SmartFactoryBean)&#123; isEagerInit = AccessController.doPrivileged((PrivilegedAction&lt;Boolean&gt;) ((SmartFactoryBean&lt;?&gt;) factory)::isEagerInit, getAccessControlContext()); &#125; else &#123; isEagerInit = (factory instanceof SmartFactoryBean &amp;&amp; ((SmartFactoryBean&lt;?&gt;) factory).isEagerInit()); &#125; if (isEagerInit) &#123; getBean(beanName); &#125; &#125; &#125; else &#123; getBean(beanName); &#125; &#125; &#125; // Trigger post-initialization callback for all applicable beans... for (String beanName : beanNames) &#123; Object singletonInstance = getSingleton(beanName); if (singletonInstance instanceof SmartInitializingSingleton) &#123; final SmartInitializingSingleton smartSingleton = (SmartInitializingSingleton) singletonInstance; if (System.getSecurityManager() != null) &#123; AccessController.doPrivileged((PrivilegedAction&lt;Object&gt;) () -&gt; &#123; smartSingleton.afterSingletonsInstantiated(); return null; &#125;, getAccessControlContext()); &#125; else &#123; smartSingleton.afterSingletonsInstantiated(); &#125; &#125; &#125;&#125; 从上面的代码中可以看到很多调用了 getBean(beanName) 方法，跟踪此方法进去后，最终发现 getBean 调用了AbstractBeanFactory 类的 doGetBean(xxx) 方法，doGetBean(xxx) 方法中有这么一段代码： 但是 createBean() 方法并没有得到实现，实现类在 AbstractAutowireCapableBeanFactory 中。这才是创建 bean 的核心方法。 不知不觉，代码看的越来越深，感觉思维都差点回不去 run 方法了，切回大脑的上下文线程到 run 方法去。 11、afterRefresh(context, applicationArguments)：在上下文刷新后调用该方法，其内部没有做任何操作。 发现没做任何操作了之后，就觉得有点奇怪，所以把当前版本和 1.5.12 对比了下，发现： 在 1.5.12 中的 afterRefresh() 方法中调用了 callRunners() 方法，但是在 2.0.1 版本中的 run 方法中调用了 callRunners () 方法: 这里不得不说 SpringApplicationRunListeners 在 2.0.1 中的改变： 可以发现在 run 方法中，SpringApplicationRunListeners 监听器的状态花生了变化，这也是通过对比不同版本的代码才知道的区别，所以说我们看源码需要多对比着看。 so，我们来看下这个 SpringApplicationRunListener 这个接口： started 状态：The context has been refreshed and the application has started but CommandLineRunner and ApplicationRunner have not been called running 状态：Called immediately before the run method finishes, when the application context has been refreshed and all CommandLineRunner and ApplicationRunners have been called. 相关文章1、Spring Boot 2.0系列文章(一)：Spring Boot 2.0 迁移指南 2、Spring Boot 2.0系列文章(二)：Spring Boot 2.0 新特性详解 3、Spring Boot 2.0系列文章(三)：Spring Boot 2.0 配置改变 4、Spring Boot 2.0系列文章(四)：Spring Boot 2.0 源码阅读环境搭建 5、Spring Boot 2.0系列文章(五)：Spring Boot 2.0 项目源码结构预览 6、Spring Boot 2.0系列文章(六)：Spring boot 2.0 中 SpringBootApplication 注解详解 7、Spring Boot 2.0系列文章(七)：SpringApplication 深入探索 总结本文从源码级别分析了 Spring Boot 应用程序的启动过程，着重看了 SpringApplication 类中的构造函数的初始化和其 run 方法内部实现，并把涉及到的流程代码都过了一遍。 感悟：有时候跟代码跟着跟着，发现越陷越深，好难跳出来！后面还需多向别人请教阅读源码的技巧！ 最后虽然源码很难，但随着不断的探索，源码在你面前将会一览无遗，享受这种探索后的成就感！加油！骚年！ 自己本人能力有限，源码看的不多，上面如有不对的还请留言交流。","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://www.54tianzhisheng.cn/tags/SpringBoot/"}]},{"title":"分布式锁看这篇就够了","date":"2018-04-23T16:00:00.000Z","path":"2018/04/24/Distributed_lock/","text":"关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/04/24/Distributed_lock/ 什么是锁？ 在单进程的系统中，当存在多个线程可以同时改变某个变量（可变共享变量）时，就需要对变量或代码块做同步，使其在修改这种变量时能够线性执行消除并发修改变量。 而同步的本质是通过锁来实现的。为了实现多个线程在一个时刻同一个代码块只能有一个线程可执行，那么需要在某个地方做个标记，这个标记必须每个线程都能看到，当标记不存在时可以设置该标记，其余后续线程发现已经有标记了则等待拥有标记的线程结束同步代码块取消标记后再去尝试设置标记。这个标记可以理解为锁。 不同地方实现锁的方式也不一样，只要能满足所有线程都能看得到标记即可。如 Java 中 synchronize 是在对象头设置标记，Lock 接口的实现类基本上都只是某一个 volitile 修饰的 int 型变量其保证每个线程都能拥有对该 int 的可见性和原子修改，linux 内核中也是利用互斥量或信号量等内存数据做标记。 除了利用内存数据做锁其实任何互斥的都能做锁（只考虑互斥情况），如流水表中流水号与时间结合做幂等校验可以看作是一个不会释放的锁，或者使用某个文件是否存在作为锁等。只需要满足在对标记进行修改能保证原子性和内存可见性即可。 什么是分布式？分布式的 CAP 理论告诉我们: 任何一个分布式系统都无法同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance），最多只能同时满足两项。 目前很多大型网站及应用都是分布式部署的，分布式场景中的数据一致性问题一直是一个比较重要的话题。基于 CAP理论，很多系统在设计之初就要对这三者做出取舍。在互联网领域的绝大多数的场景中，都需要牺牲强一致性来换取系统的高可用性，系统往往只需要保证最终一致性。 分布式场景 此处主要指集群模式下，多个相同服务同时开启. 在许多的场景中，我们为了保证数据的最终一致性，需要很多的技术方案来支持，比如分布式事务、分布式锁等。很多时候我们需要保证一个方法在同一时间内只能被同一个线程执行。在单机环境中，通过 Java 提供的并发 API 我们可以解决，但是在分布式环境下，就没有那么简单啦。 分布式与单机情况下最大的不同在于其不是多线程而是多进程。 多线程由于可以共享堆内存，因此可以简单的采取内存作为标记存储位置。而进程之间甚至可能都不在同一台物理机上，因此需要将标记存储在一个所有进程都能看到的地方。 什么是分布式锁？ 当在分布式模型下，数据只有一份（或有限制），此时需要利用锁的技术控制某一时刻修改数据的进程数。 与单机模式下的锁不仅需要保证进程可见，还需要考虑进程与锁之间的网络问题。（我觉得分布式情况下之所以问题变得复杂，主要就是需要考虑到网络的延时和不可靠。。。一个大坑） 分布式锁还是可以将标记存在内存，只是该内存不是某个进程分配的内存而是公共内存如 Redis、Memcache。至于利用数据库、文件等做锁与单机的实现是一样的，只要保证标记能互斥就行。 我们需要怎样的分布式锁？ 可以保证在分布式部署的应用集群中，同一个方法在同一时间只能被一台机器上的一个线程执行。 这把锁要是一把可重入锁（避免死锁） 这把锁最好是一把阻塞锁（根据业务需求考虑要不要这条） 这把锁最好是一把公平锁（根据业务需求考虑要不要这条） 有高可用的获取锁和释放锁功能 获取锁和释放锁的性能要好 基于数据库做分布式锁 基于乐观锁 基于表主键唯一做分布式锁利用主键唯一的特性，如果有多个请求同时提交到数据库的话，数据库会保证只有一个操作可以成功，那么我们就可以认为操作成功的那个线程获得了该方法的锁，当方法执行完毕之后，想要释放锁的话，删除这条数据库记录即可。 上面这种简单的实现有以下几个问题： 这把锁强依赖数据库的可用性，数据库是一个单点，一旦数据库挂掉，会导致业务系统不可用。 这把锁没有失效时间，一旦解锁操作失败，就会导致锁记录一直在数据库中，其他线程无法再获得到锁。 这把锁只能是非阻塞的，因为数据的 insert 操作，一旦插入失败就会直接报错。没有获得锁的线程并不会进入排队队列，要想再次获得锁就要再次触发获得锁操作。 这把锁是非重入的，同一个线程在没有释放锁之前无法再次获得该锁。因为数据中数据已经存在了。 这把锁是非公平锁，所有等待锁的线程凭运气去争夺锁。 在 MySQL 数据库中采用主键冲突防重，在大并发情况下有可能会造成锁表现象。 当然，我们也可以有其他方式解决上面的问题。 数据库是单点？搞两个数据库，数据之前双向同步，一旦挂掉快速切换到备库上。 没有失效时间？只要做一个定时任务，每隔一定时间把数据库中的超时数据清理一遍。 非阻塞的？搞一个 while 循环，直到 insert 成功再返回成功。 非重入的？在数据库表中加个字段，记录当前获得锁的机器的主机信息和线程信息，那么下次再获取锁的时候先查询数据库，如果当前机器的主机信息和线程信息在数据库可以查到的话，直接把锁分配给他就可以了。 非公平的？再建一张中间表，将等待锁的线程全记录下来，并根据创建时间排序，只有最先创建的允许获取锁。 比较好的办法是在程序中生产主键进行防重。 基于表字段版本号做分布式锁这个策略源于 mysql 的 mvcc 机制，使用这个策略其实本身没有什么问题，唯一的问题就是对数据表侵入较大，我们要为每个表设计一个版本号字段，然后写一条判断 sql 每次进行判断，增加了数据库操作的次数，在高并发的要求下，对数据库连接的开销也是无法忍受的。 基于悲观锁 基于数据库排他锁做分布式锁在查询语句后面增加for update，数据库会在查询过程中给数据库表增加排他锁 (注意： InnoDB 引擎在加锁的时候，只有通过索引进行检索的时候才会使用行级锁，否则会使用表级锁。这里我们希望使用行级锁，就要给要执行的方法字段名添加索引，值得注意的是，这个索引一定要创建成唯一索引，否则会出现多个重载方法之间无法同时被访问的问题。重载方法的话建议把参数类型也加上。)。当某条记录被加上排他锁之后，其他线程无法再在该行记录上增加排他锁。 我们可以认为获得排他锁的线程即可获得分布式锁，当获取到锁之后，可以执行方法的业务逻辑，执行完方法之后，通过connection.commit()操作来释放锁。 这种方法可以有效的解决上面提到的无法释放锁和阻塞锁的问题。 阻塞锁？ for update语句会在执行成功后立即返回，在执行失败时一直处于阻塞状态，直到成功。 锁定之后服务宕机，无法释放？使用这种方式，服务宕机之后数据库会自己把锁释放掉。 但是还是无法直接解决数据库单点和可重入问题。 这里还可能存在另外一个问题，虽然我们对方法字段名使用了唯一索引，并且显示使用 for update 来使用行级锁。但是，MySQL 会对查询进行优化，即便在条件中使用了索引字段，但是否使用索引来检索数据是由 MySQL 通过判断不同执行计划的代价来决定的，如果 MySQL 认为全表扫效率更高，比如对一些很小的表，它就不会使用索引，这种情况下 InnoDB 将使用表锁，而不是行锁。如果发生这种情况就悲剧了。。。 还有一个问题，就是我们要使用排他锁来进行分布式锁的 lock，那么一个排他锁长时间不提交，就会占用数据库连接。一旦类似的连接变得多了，就可能把数据库连接池撑爆。 优缺点优点：简单，易于理解 缺点：会有各种各样的问题（操作数据库需要一定的开销，使用数据库的行级锁并不一定靠谱，性能不靠谱） 基于 Redis 做分布式锁基于 redis 的 setnx()、expire() 方法做分布式锁setnx()setnx 的含义就是 SET if Not Exists，其主要有两个参数 setnx(key, value)。该方法是原子的，如果 key 不存在，则设置当前 key 成功，返回 1；如果当前 key 已经存在，则设置当前 key 失败，返回 0。 expire()expire 设置过期时间，要注意的是 setnx 命令不能设置 key 的超时时间，只能通过 expire() 来对 key 设置。 使用步骤1、setnx(lockkey, 1) 如果返回 0，则说明占位失败；如果返回 1，则说明占位成功 2、expire() 命令对 lockkey 设置超时时间，为的是避免死锁问题。 3、执行完业务代码后，可以通过 delete 命令删除 key。 这个方案其实是可以解决日常工作中的需求的，但从技术方案的探讨上来说，可能还有一些可以完善的地方。比如，如果在第一步 setnx 执行成功后，在 expire() 命令执行成功前，发生了宕机的现象，那么就依然会出现死锁的问题，所以如果要对其进行完善的话，可以使用 redis 的 setnx()、get() 和 getset() 方法来实现分布式锁。 基于 redis 的 setnx()、get()、getset()方法做分布式锁这个方案的背景主要是在 setnx() 和 expire() 的方案上针对可能存在的死锁问题，做了一些优化。 getset()这个命令主要有两个参数 getset(key，newValue)。该方法是原子的，对 key 设置 newValue 这个值，并且返回 key 原来的旧值。假设 key 原来是不存在的，那么多次执行这个命令，会出现下边的效果： getset(key, “value1”) 返回 null 此时 key 的值会被设置为 value1 getset(key, “value2”) 返回 value1 此时 key 的值会被设置为 value2 依次类推！ 使用步骤 setnx(lockkey, 当前时间+过期超时时间)，如果返回 1，则获取锁成功；如果返回 0 则没有获取到锁，转向 2。 get(lockkey) 获取值 oldExpireTime ，并将这个 value 值与当前的系统时间进行比较，如果小于当前系统时间，则认为这个锁已经超时，可以允许别的请求重新获取，转向 3。 计算 newExpireTime = 当前时间+过期超时时间，然后 getset(lockkey, newExpireTime) 会返回当前 lockkey 的值currentExpireTime。 判断 currentExpireTime 与 oldExpireTime 是否相等，如果相等，说明当前 getset 设置成功，获取到了锁。如果不相等，说明这个锁又被别的请求获取走了，那么当前请求可以直接返回失败，或者继续重试。 在获取到锁之后，当前线程可以开始自己的业务处理，当处理完毕后，比较自己的处理时间和对于锁设置的超时时间，如果小于锁设置的超时时间，则直接执行 delete 释放锁；如果大于锁设置的超时时间，则不需要再锁进行处理。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import cn.com.tpig.cache.redis.RedisService;import cn.com.tpig.utils.SpringUtils;//redis分布式锁public final class RedisLockUtil &#123; private static final int defaultExpire = 60; private RedisLockUtil() &#123; // &#125; /** * 加锁 * @param key redis key * @param expire 过期时间，单位秒 * @return true:加锁成功，false，加锁失败 */ public static boolean lock(String key, int expire) &#123; RedisService redisService = SpringUtils.getBean(RedisService.class); long status = redisService.setnx(key, \"1\"); if(status == 1) &#123; redisService.expire(key, expire); return true; &#125; return false; &#125; public static boolean lock(String key) &#123; return lock2(key, defaultExpire); &#125; /** * 加锁 * @param key redis key * @param expire 过期时间，单位秒 * @return true:加锁成功，false，加锁失败 */ public static boolean lock2(String key, int expire) &#123; RedisService redisService = SpringUtils.getBean(RedisService.class); long value = System.currentTimeMillis() + expire; long status = redisService.setnx(key, String.valueOf(value)); if(status == 1) &#123; return true; &#125; long oldExpireTime = Long.parseLong(redisService.get(key, \"0\")); if(oldExpireTime &lt; System.currentTimeMillis()) &#123; //超时 long newExpireTime = System.currentTimeMillis() + expire; long currentExpireTime = Long.parseLong(redisService.getSet(key, String.valueOf(newExpireTime))); if(currentExpireTime == oldExpireTime) &#123; return true; &#125; &#125; return false; &#125; public static void unLock1(String key) &#123; RedisService redisService = SpringUtils.getBean(RedisService.class); redisService.del(key); &#125; public static void unLock2(String key) &#123; RedisService redisService = SpringUtils.getBean(RedisService.class); long oldExpireTime = Long.parseLong(redisService.get(key, \"0\")); if(oldExpireTime &gt; System.currentTimeMillis()) &#123; redisService.del(key); &#125; &#125;&#125; 123456789101112131415public void drawRedPacket(long userId) &#123; String key = \"draw.redpacket.userid:\" + userId; boolean lock = RedisLockUtil.lock2(key, 60); if(lock) &#123; try &#123; //领取操作 &#125; finally &#123; //释放锁 RedisLockUtil.unLock(key); &#125; &#125; else &#123; new RuntimeException(\"重复领取奖励\"); &#125;&#125; 基于 Redlock 做分布式锁Redlock 是 Redis 的作者 antirez 给出的集群模式的 Redis 分布式锁，它基于 N 个完全独立的 Redis 节点（通常情况下 N 可以设置成 5）。 算法的步骤如下： 1、客户端获取当前时间，以毫秒为单位。 2、客户端尝试获取 N 个节点的锁，（每个节点获取锁的方式和前面说的缓存锁一样），N 个节点以相同的 key 和 value 获取锁。客户端需要设置接口访问超时，接口超时时间需要远远小于锁超时时间，比如锁自动释放的时间是 10s，那么接口超时大概设置 5-50ms。这样可以在有 redis 节点宕机后，访问该节点时能尽快超时，而减小锁的正常使用。 3、客户端计算在获得锁的时候花费了多少时间，方法是用当前时间减去在步骤一获取的时间，只有客户端获得了超过 3 个节点的锁，而且获取锁的时间小于锁的超时时间，客户端才获得了分布式锁。 4、客户端获取的锁的时间为设置的锁超时时间减去步骤三计算出的获取锁花费时间。 5、如果客户端获取锁失败了，客户端会依次删除所有的锁。使用 Redlock 算法，可以保证在挂掉最多 2 个节点的时候，分布式锁服务仍然能工作，这相比之前的数据库锁和缓存锁大大提高了可用性，由于 redis 的高效性能，分布式缓存锁性能并不比数据库锁差。 但是，有一位分布式的专家写了一篇文章《How to do distributed locking》，质疑 Redlock 的正确性。 https://mp.weixin.qq.com/s/1bPLk_VZhZ0QYNZS8LkviA https://blog.csdn.net/jek123456/article/details/72954106 优缺点优点： 性能高 缺点： 失效时间设置多长时间为好？如何设置的失效时间太短，方法没等执行完，锁就自动释放了，那么就会产生并发问题。如果设置的时间太长，其他获取锁的线程就可能要平白的多等一段时间。 基于 redisson 做分布式锁redisson 是 redis 官方的分布式锁组件。GitHub 地址：https://github.com/redisson/redisson 上面的这个问题 ——&gt; 失效时间设置多长时间为好？这个问题在 redisson 的做法是：每获得一个锁时，只设置一个很短的超时时间，同时起一个线程在每次快要到超时时间时去刷新锁的超时时间。在释放锁的同时结束这个线程。 基于 ZooKeeper 做分布式锁zookeeper 锁相关基础知识 zk 一般由多个节点构成（单数），采用 zab 一致性协议。因此可以将 zk 看成一个单点结构，对其修改数据其内部自动将所有节点数据进行修改而后才提供查询服务。 zk 的数据以目录树的形式，每个目录称为 znode， znode 中可存储数据（一般不超过 1M），还可以在其中增加子节点。 子节点有三种类型。序列化节点，每在该节点下增加一个节点自动给该节点的名称上自增。临时节点，一旦创建这个 znode 的客户端与服务器失去联系，这个 znode 也将自动删除。最后就是普通节点。 Watch 机制，client 可以监控每个节点的变化，当产生变化会给 client 产生一个事件。 zk 基本锁 原理：利用临时节点与 watch 机制。每个锁占用一个普通节点 /lock，当需要获取锁时在 /lock 目录下创建一个临时节点，创建成功则表示获取锁成功，失败则 watch/lock 节点，有删除操作后再去争锁。临时节点好处在于当进程挂掉后能自动上锁的节点自动删除即取消锁。 缺点：所有取锁失败的进程都监听父节点，很容易发生羊群效应，即当释放锁后所有等待进程一起来创建节点，并发量很大。 zk 锁优化 原理：上锁改为创建临时有序节点，每个上锁的节点均能创建节点成功，只是其序号不同。只有序号最小的可以拥有锁，如果这个节点序号不是最小的则 watch 序号比本身小的前一个节点 (公平锁)。 步骤： 在 /lock 节点下创建一个有序临时节点 (EPHEMERAL_SEQUENTIAL)。 判断创建的节点序号是否最小，如果是最小则获取锁成功。不是则取锁失败，然后 watch 序号比本身小的前一个节点。 当取锁失败，设置 watch 后则等待 watch 事件到来后，再次判断是否序号最小。 取锁成功则执行代码，最后释放锁（删除该节点）。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169import java.io.IOException;import java.util.ArrayList;import java.util.Collections;import java.util.List;import java.util.concurrent.CountDownLatch;import java.util.concurrent.TimeUnit;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooDefs;import org.apache.zookeeper.ZooKeeper;import org.apache.zookeeper.data.Stat;public class DistributedLock implements Lock, Watcher&#123; private ZooKeeper zk; private String root = \"/locks\";//根 private String lockName;//竞争资源的标志 private String waitNode;//等待前一个锁 private String myZnode;//当前锁 private CountDownLatch latch;//计数器 private int sessionTimeout = 30000; private List&lt;Exception&gt; exception = new ArrayList&lt;Exception&gt;(); /** * 创建分布式锁,使用前请确认config配置的zookeeper服务可用 * @param config 127.0.0.1:2181 * @param lockName 竞争资源标志,lockName中不能包含单词lock */ public DistributedLock(String config, String lockName)&#123; this.lockName = lockName; // 创建一个与服务器的连接 try &#123; zk = new ZooKeeper(config, sessionTimeout, this); Stat stat = zk.exists(root, false); if(stat == null)&#123; // 创建根节点 zk.create(root, new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE,CreateMode.PERSISTENT); &#125; &#125; catch (IOException e) &#123; exception.add(e); &#125; catch (KeeperException e) &#123; exception.add(e); &#125; catch (InterruptedException e) &#123; exception.add(e); &#125; &#125; /** * zookeeper节点的监视器 */ public void process(WatchedEvent event) &#123; if(this.latch != null) &#123; this.latch.countDown(); &#125; &#125; public void lock() &#123; if(exception.size() &gt; 0)&#123; throw new LockException(exception.get(0)); &#125; try &#123; if(this.tryLock())&#123; System.out.println(\"Thread \" + Thread.currentThread().getId() + \" \" +myZnode + \" get lock true\"); return; &#125; else&#123; waitForLock(waitNode, sessionTimeout);//等待锁 &#125; &#125; catch (KeeperException e) &#123; throw new LockException(e); &#125; catch (InterruptedException e) &#123; throw new LockException(e); &#125; &#125; public boolean tryLock() &#123; try &#123; String splitStr = \"_lock_\"; if(lockName.contains(splitStr)) throw new LockException(\"lockName can not contains \\\\u000B\"); //创建临时子节点 myZnode = zk.create(root + \"/\" + lockName + splitStr, new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE,CreateMode.EPHEMERAL_SEQUENTIAL); System.out.println(myZnode + \" is created \"); //取出所有子节点 List&lt;String&gt; subNodes = zk.getChildren(root, false); //取出所有lockName的锁 List&lt;String&gt; lockObjNodes = new ArrayList&lt;String&gt;(); for (String node : subNodes) &#123; String _node = node.split(splitStr)[0]; if(_node.equals(lockName))&#123; lockObjNodes.add(node); &#125; &#125; Collections.sort(lockObjNodes); System.out.println(myZnode + \"==\" + lockObjNodes.get(0)); if(myZnode.equals(root+\"/\"+lockObjNodes.get(0)))&#123; //如果是最小的节点,则表示取得锁 return true; &#125; //如果不是最小的节点，找到比自己小1的节点 String subMyZnode = myZnode.substring(myZnode.lastIndexOf(\"/\") + 1); waitNode = lockObjNodes.get(Collections.binarySearch(lockObjNodes, subMyZnode) - 1); &#125; catch (KeeperException e) &#123; throw new LockException(e); &#125; catch (InterruptedException e) &#123; throw new LockException(e); &#125; return false; &#125; public boolean tryLock(long time, TimeUnit unit) &#123; try &#123; if(this.tryLock())&#123; return true; &#125; return waitForLock(waitNode,time); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return false; &#125; private boolean waitForLock(String lower, long waitTime) throws InterruptedException, KeeperException &#123; Stat stat = zk.exists(root + \"/\" + lower,true); //判断比自己小一个数的节点是否存在,如果不存在则无需等待锁,同时注册监听 if(stat != null)&#123; System.out.println(\"Thread \" + Thread.currentThread().getId() + \" waiting for \" + root + \"/\" + lower); this.latch = new CountDownLatch(1); this.latch.await(waitTime, TimeUnit.MILLISECONDS); this.latch = null; &#125; return true; &#125; public void unlock() &#123; try &#123; System.out.println(\"unlock \" + myZnode); zk.delete(myZnode,-1); myZnode = null; zk.close(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (KeeperException e) &#123; e.printStackTrace(); &#125; &#125; public void lockInterruptibly() throws InterruptedException &#123; this.lock(); &#125; public Condition newCondition() &#123; return null; &#125; public class LockException extends RuntimeException &#123; private static final long serialVersionUID = 1L; public LockException(String e)&#123; super(e); &#125; public LockException(Exception e)&#123; super(e); &#125; &#125;&#125; 优缺点优点： 有效的解决单点问题，不可重入问题，非阻塞问题以及锁无法释放的问题。实现起来较为简单。 缺点： 性能上可能并没有缓存服务那么高，因为每次在创建锁和释放锁的过程中，都要动态创建、销毁临时节点来实现锁功能。ZK 中创建和删除节点只能通过 Leader 服务器来执行，然后将数据同步到所有的 Follower 机器上。还需要对 ZK的原理有所了解。 基于 Consul 做分布式锁DD 写过类似文章，其实主要利用 Consul 的 Key / Value 存储 API 中的 acquire 和 release 操作来实现。 文章地址：http://blog.didispace.com/spring-cloud-consul-lock-and-semphore/ 使用分布式锁的注意事项1、注意分布式锁的开销 2、注意加锁的粒度 3、加锁的方式 总结无论你身处一个什么样的公司，最开始的工作可能都需要从最简单的做起。不要提阿里和腾讯的业务场景 qps 如何大，因为在这样的大场景中你未必能亲自参与项目，亲自参与项目未必能是核心的设计者，是核心的设计者未必能独自设计。希望大家能根据自己公司业务场景，选择适合自己项目的方案。 参考资料http://www.hollischuang.com/archives/1716 http://www.spring4all.com/question/158 https://www.cnblogs.com/PurpleDream/p/5559352.html http://www.cnblogs.com/PurpleDream/p/5573040.html https://www.cnblogs.com/suolu/p/6588902.html","tags":[{"name":"数据库","slug":"数据库","permalink":"http://www.54tianzhisheng.cn/tags/数据库/"},{"name":"Redis","slug":"Redis","permalink":"http://www.54tianzhisheng.cn/tags/Redis/"},{"name":"分布式锁","slug":"分布式锁","permalink":"http://www.54tianzhisheng.cn/tags/分布式锁/"},{"name":"Zookeeper","slug":"Zookeeper","permalink":"http://www.54tianzhisheng.cn/tags/Zookeeper/"}]},{"title":"Spring Boot 2.0系列文章(六)：Spring Boot 2.0中SpringBootApplication注解详解","date":"2018-04-18T16:00:00.000Z","path":"2018/04/19/SpringBootApplication-annotation/","text":"SpringBoot 系列文章 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/04/19/SpringBootApplication-annotation/ 概述许多 Spring Boot 开发者喜欢他们的应用程序使用自动配置、组件扫描、并能够在他们的 “Application” 类上定义额外的配置。 可以使用一个 @SpringBootApplication 注解来启用这些功能。 123456789import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; @SpringBootApplication跟进去 @SpringBootApplication 注解可以发现下图： 其中标注的三个注解正能解决我们上面所说的三种功能，它们是： @SpringBootConfiguration @EnableAutoConfiguration @ComponentScan 该接口除了上面三个注解外，还有四个方法如下： Class&lt;?&gt;[] exclude() default {}:根据 class 来排除，排除特定的类加入 spring 容器，传入参数 value 类型是 class 类型。 String[] excludeName() default {}:根据 class name 来排除，排除特定的类加入 spring 容器，传入参数 value 类型是 class 的全类名字符串数组。 String[] scanBasePackages() default {}:指定扫描包，参数是包名的字符串数组。 Class&lt;?&gt;[] scanBasePackageClasses() default {}:扫描特定的包，参数类似是 Class 类型数组。 就拿 scanBasePackages 来举个例子： 1@SpringBootApplication(scanBasePackages = &#123;\"com.zhisheng.controller\",\"com.zhisheng.model\"&#125;) 将不需要的 bean 排除在 spring 容器中，如何操作？看看官方的代码怎么用的： @SpringBootConfiguration @SpringBootConfiguration继承自@Configuration，二者功能也一致，标注当前类是配置类，并会将当前类内声明的一个或多个以@Bean注解标记的方法的实例纳入到srping容器中，并且实例名就是方法名。 虽说现在已经推荐使用 Spring Boot 里面的 @SpringBootConfiguration 注解，为了探个究竟，我们还是继续研究下 @Configuration 注解。 @Configuration@Configuration 标注在类上，相当于把该类作为 spring 的 xml 配置文件中的 &lt;beans&gt;，作用为：配置 spring 容器(应用上下文) 123&lt;beans&gt;&lt;/beans&gt; @Bean@Bean 标注在方法上(返回某个实例的方法)，等价于 spring 的 xml 配置文件中的&lt;bean&gt;，作用为：注册 bean 对象 可以看看这篇文章：https://www.ibm.com/developerworks/cn/webservices/ws-springjava/index.html @ComponentScan可以通过该注解指定扫描某些包下包含如下注解的均自动注册为 spring beans： @Component、@Service、 @Repository、 @Controller、@Entity 等等 例如： 1@ComponentScan(basePackages = &#123;\"com.zhisheng.controller\",\"com.zhisheng.model\"&#125;) 以前是在 xml 配置文件中设置如下标签：&lt;context:component-scan&gt;（用来扫描包配置） 除了可以使用 @ComponentScan 注解来加载我们的 bean，还可以在 Application 类中使用 @Import 指定该类。 例如： 1@Import(&#123;ConsulConfig.class, Log4jEndPointConfiguration.class&#125;) //直接 imoport 要引入的类 @EnableAutoConfiguration@EnableAutoConfiguration的作用启动自动的配置，@EnableAutoConfiguration注解的意思就是Springboot根据你添加的 jar 包来配置你项目的默认配置，比如根据spring-boot-starter-web ，来判断你的项目是否需要添加了webmvc和tomcat，就会自动的帮你配置 web 项目中所需要的默认配置。简单点说就是它会根据定义在 classpath 下的类，自动的给你生成一些 Bean，并加载到 Spring 的 Context 中。 可以看到 import 引入了 AutoConfigurationImportSelector 类。该类使用了 Spring Core 包的 SpringFactoriesLoader 类的 loadFactoryNamesof() 方法。 AutoConfigurationImportSelector 类实现了 DeferredImportSelector 接口，并实现了 selectImports 方法，用来导出Configuration 类。 1List&lt;String&gt; configurations = getCandidateConfigurations(annotationMetadata, attributes); 导出的类是通过 SpringFactoriesLoader.loadFactoryNames() 读取了 ClassPath 下面的 META-INF/spring.factories 文件。 这个文件内容大致如下。 后面继续会写自动配置方面的博客，请继续关注！ 如果你发现自动装配的 Bean 不是你想要的，你也可以 disable 它。比如说，我不想要自动装配 Database 的那些Bean： 1@EnableAutoConfiguration(exclude = &#123;DataSourceAutoConfiguration.class&#125;) 相关文章1、Spring Boot 2.0系列文章(一)：Spring Boot 2.0 迁移指南 2、Spring Boot 2.0系列文章(二)：Spring Boot 2.0 新特性详解 3、Spring Boot 2.0系列文章(三)：Spring Boot 2.0 配置改变 4、Spring Boot 2.0系列文章(四)：Spring Boot 2.0 源码阅读环境搭建 5、Spring Boot 2.0系列文章(五)：Spring Boot 2.0 项目源码结构预览 6、Spring Boot 2.0系列文章(六)：Spring boot 2.0 中 SpringBootApplication 注解详解 7、Spring Boot 2.0系列文章(七)：SpringApplication 深入探索 总结本文主要讲了 SpringBootApplication 注解，然后展开写了其包含的三个注解 SpringBootConfiguration、ComponentScan、EnableAutoConfiguration 最后虽然源码很难，但随着不断的探索，源码在你面前将会一览无遗，享受这种探索后的成就感！加油！骚年！ 自己本人能力有限，源码看的不多，上面如有不对的还请留言交流。","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://www.54tianzhisheng.cn/tags/SpringBoot/"}]},{"title":"Spring Boot 2.0系列文章(五)：Spring Boot 2.0 项目源码结构预览","date":"2018-04-17T16:00:00.000Z","path":"2018/04/18/spring_boot2_project/","text":"SpringBoot 系列文章 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/04/15/springboot2_code/ 项目结构 结构分析： Spring-boot-project 核心代码，代码量很多（197508 行） Spring-boot-samples 一些样例 demo，代码量不多（9685 行），蛮有用的 Spring-boot-samples-invoker 里面无代码 Spring-boot-tests 测试代码（1640 行） spring-boot-projectSpring-boot-project 下面有很多模块，如下： Spirng-boot 该模块 47760 行代码（含测试代码），Spring boot 主要的库，提供了支持 Spring Boot 其他部分的功能，其中包括了： 在SpringApplication类，提供静态便捷方法，可以很容易写一个独立的 Spring 应用程序。它唯一的工作就是创造并更新一个合适的 SpringApplicationContext 带有可选容器的嵌入式 Web 应用程序（Tomcat，Jetty 或 Undertow） 一流的外部配置支持 便捷ApplicationContext初始化程序，包括对敏感日志记录默认值的支持 spring-boot-actuator 该模块 18398 行代码（含测试代码），spring-boot-actuator 模块它完全是一个用于暴露自身信息的模块，提供了一个监控和管理生产环境的模块，可以使用 http、jmx、ssh、telnet 等管理和监控应用。审计（Auditing）、 健康（health）、数据采集（metrics gathering）会自动加入到应用里面。 spring-boot-actuator-autoconfigure 该模块 16721 行代码（含测试代码），Spring Boot Actuator 提供了额外的自动配置功能，可以在生产环境中实现可即时部署和支持的功能，从而装饰你的应用。例如，如果您正在编写 JSON Web 服务，那么它将提供服务器，安全性，日志记录，外部配置，管理端点，审计抽象等等功能。如果您想关闭内置功能，或者扩展或替换它们，它也会变得非常简单。 spring-boot-autoconfigure 该模块 51100 行代码（含测试代码）， Spring Boot 可以根据类路径的内容配置大部分常用应用程序。单个@EnableAutoConfiguration注释会触发 Spring上下文的自动配置。 自动配置尝试推断用户可能需要哪些 bean。例如，如果 HSQLDB在类路径中，并且用户尚未配置任何数据库连接，则他们可能需要定义内存数据库。当用户开始定义他们自己的 bean 时，自动配置将永远远离。 spring-boot-cli 该模块 9346 行代码（含测试代码），Spring 命令行应用程序编译并运行 Groovy 源代码，使得可以编写少量代码就能运行应用程序。Spring CLI 也可以监视文件，当它们改变时自动重新编译并重新启动。 spring-boot-dependencies 该模块里面没有源码，只有所有依赖和插件的版本号信息。 spring-boot-devtools 该模块 9418 行代码（含测试代码），spring-boot-devtools 模块来使 Spring Boot 应用支持热部署，提高开发者的开发效率，无需手动重启 Spring Boot 应用。 spring-boot-docs 该模块 671 行代码，springboot 参考文件。 spring-boot-parent 该模块是其他项目的 parent，该模块的父模块是 spring-boot-dependencies。 spring-boot-properties-migrator 该模块有 495 行代码，在 Spring Boot 2.0 中，许多配置属性被重新命名/删除，开发人员需要更新application.properties/ application.yml相应的配置。为了帮助你解决这一问题，Spring Boot 发布了一个新spring-boot-properties-migrator模块。一旦作为该模块作为依赖被添加到你的项目中，它不仅会分析应用程序的环境，而且还会在启动时打印诊断信息，而且还会在运行时为您暂时迁移属性。在您的应用程序迁移期间，这个模块是必备的，完成迁移后，请确保从项目的依赖关系中删除此模块。 spring-boot-starters Starter POMs 是由很多方便的依赖集合组成，如果你需要使用某种技术，通过添加少量的jar就可以把相关的依赖加入到项目中去。 虽然你看得到有这么多 starter，但是却没有一行 Java 代码，意不意外？ 这确实是 Spring Boot 自动配置的关键之处，后面我可以讲讲。 spring-boot-test测试代码！有 10980 行代码。 spring-boot-test-autoconfigure自动配置的测试代码，有 6063 行代码。 spring-boot-tools spring-boot-antlib Spring Boot AntLib 模块为 Apache Ant 提供了基本的 Spring Boot 支持。 您可以使用该模块创建可执行文件夹。 要使用该模块，您需要在 build.xml 中声明一个额外的 spring-boot 命名空间，如以下示例所示： 12345&lt;project xmlns:ivy=\"antlib:org.apache.ivy.ant\" xmlns:spring-boot=\"antlib:org.springframework.boot.ant\" name=\"myapp\" default=\"build\"&gt; ...&lt;/project&gt; 您需要记住使用 -lib 选项启动 Ant，如以下示例所示： 1ant -lib &lt;folder containing spring-boot-antlib-2.1.0.BUILD-SNAPSHOT.jar&gt; Spring-boot-autoconfigure-processor spring boot 自动配置的核心类 Spring-boot-configuration-metadata Spring boot 配置元数据 Spring-boot-configuration-processor spring boot 配置的核心 Spring-boot-gradle-plugin Spring Boot Gradle 插件在 Gradle 中提供了 Spring Boot 支持，可以打包成可执行 jar 或 war ，运行 Spring Boot 应用程序，并使用 spring-boot-dependencies 提供的依赖关系管理。 它需要 Gradle 4.0 或更高版本。 Spring-boot-maven-plugin Spring Boot Maven Plugin 在 Maven 中提供了 Spring Boot 支持，让您可以打包成可执行 jar 或 war 应用，并“就地”运行应用程序。 要使用它，你必须使用 Maven 3.2（或更高版本）。 Spring-boot-loader spring-boot-load 模块通过自定义 jar 包结构，自定义类加载器，优雅的实现了嵌套 jar 资源的加载，通过打包时候重新设置启动类和组织 jar 结构，通过运行时设置自定义加载器来实现嵌套 jar 资源加载。 Spring-boot-loader-tools spring-boot-load 模块的工具模块 Spring-boot-test-support 测试 spring-boot-samples 样例 demo 比较多，大家看源码的时候可以拿这些现成 demo 测试。 spring-boot-tests 相关文章1、Spring Boot 2.0系列文章(一)：Spring Boot 2.0 迁移指南 2、Spring Boot 2.0系列文章(二)：Spring Boot 2.0 新特性详解 3、Spring Boot 2.0系列文章(三)：Spring Boot 2.0 配置改变 4、Spring Boot 2.0系列文章(四)：Spring Boot 2.0 源码阅读环境搭建 5、Spring Boot 2.0系列文章(五)：Spring Boot 2.0 项目源码结构预览 6、Spring Boot 2.0系列文章(六)：Spring boot 2.0 中 SpringBootApplication 注解详解 7、Spring Boot 2.0系列文章(七)：SpringApplication 深入探索 总结本文主要分析了下 Spring boot 项目源码结构。包含 Spring boot 核心源码、样例 demo、测试。分析了项目的整体结构后，后面才能够有的放矢的去读源码。 最后虽然源码很难，但随着不断的探索，源码在你面前将会一览无遗，享受这种探索后的成就感！加油！骚年！","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://www.54tianzhisheng.cn/tags/SpringBoot/"}]},{"title":"Spring Boot 2.0系列文章(四)：Spring Boot 2.0 源码阅读环境搭建","date":"2018-04-14T16:00:00.000Z","path":"2018/04/15/springboot2_code/","text":"SpringBoot 系列文章 前提前几天面试的时候，被问过 Spring Boot 的自动配置源码怎么实现的，没看过源码的我只能投降👦了。 这不，赶紧来补补了，所以才有了这篇文章的出现，Spring Boot 2. 0 源码阅读环境的搭建中还遇到点问题，被坑死了，还好解决了，感谢群里的小伙伴！ 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/04/15/springboot2_code/ 项目下载从 https://github.com/spring-projects/spring-boot/releases 可以看到所有版本的下载地址，我这里选择的是 Spring Boot 2 中最新的 v2.0.1.RELEASE 版本，下载后，然后解压。获取代码之前，请先确保你的 JDK 版本是 1.8 以上哦。 项目编译进入 spring-boot-2.0.1.RELEASE 的目录下，执行下面的命令。 跳过测试用例编译1sudo mvn clean install -DskipTests -Pfast //跳过测试用例 跳过测试用例可以加快编译的速度。 先看下运行成功的效果： 只花了 6 分多钟就好了。 全量编译1sudo mvn -f spring-boot-project -Pfull clean install 全量编译竟然报错，一波未平，一波又起！ 看网上的解决方法是：在项目的 pom.xml 文件中的 &lt;properties&gt; 添加 &lt;javadocExecutable&gt; 123&lt;properties&gt; &lt;javadocExecutable&gt;$&#123;java.home&#125;/../bin/javadoc&lt;/javadocExecutable&gt;&lt;/properties&gt; 此方法虽然管用，但是只是临时的，需要对每个项目都进行添加。 问题产生的原因应该是，mvn 拿到的 JAVA_HOME 位置应该是 ${JAVA_HOME}/jre 而不是 jdk 位置。 后面又看官方的 README 上面写的执行命令： 1sudo mvn clean install 执行后也是有各种报错，尝试了很久解决，最后花了好几个小时才到下面这图： 太折腾人了，太麻烦了！ 暂时就不全量编译了，我们就直接把现在 跳过测试用例编译 后的项目导入到 IDEA 中去。 导入项目工程 导入后将那些测试的 module 标记为 maven 项目，然后后面自己再根据测试用例去跟源码吧。 导入后项目没出现报错，美滋滋，后面源码可以看起来。 遇到的坑在这之前，我自己创建项目 Spring Boot 2 项目都是失败的，maven 运行项目（mvn clean install）报错如下： 通过上图可以发现报错的罪魁祸首是由于找不到 org.yaml.snakeyaml 1.19 的包，这个依赖死活下不下来，苦逼了😢。 一开始以为是公司配的 maven setting.xml 文件有问题（公司私服有问题），导致我这个 org.yaml.snakeyaml 1.19 的包一直下载不来。后来我叫群里的好友帮忙测试下能不能创建 Spring Boot 2 项目，结果他们都行的。我就换成了他们阿里云镜像的 setting 文件，结果在我这还是不行的。真是醉了，我干脆直接叫他把 maven 本地仓库中的 org.yaml.snakeyaml 1.19 整个包都发给我，结果再次创建 Spring Boot 2 项目就能成功了。美滋滋😄！ 然后就蹭着现在环境 OK，开始搭建我的 Spring Boot 2 源码阅读环境！ 相关文章1、Spring Boot 2.0系列文章(一)：Spring Boot 2.0 迁移指南 2、Spring Boot 2.0系列文章(二)：Spring Boot 2.0 新特性详解 3、Spring Boot 2.0系列文章(三)：Spring Boot 2.0 配置改变 4、Spring Boot 2.0系列文章(四)：Spring Boot 2.0 源码阅读环境搭建 5、Spring Boot 2.0系列文章(五)：Spring Boot 2.0 项目源码结构预览 6、Spring Boot 2.0系列文章(六)：Spring boot 2.0 中 SpringBootApplication 注解详解 7、Spring Boot 2.0系列文章(七)：SpringApplication 深入探索 最后源码不骗人，多看看！","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://www.54tianzhisheng.cn/tags/SpringBoot/"}]},{"title":"Spring Boot 2.0系列文章(三)：Spring Boot 2.0 配置改变","date":"2018-04-12T16:00:00.000Z","path":"2018/04/13/Spring_Boot_2.0_Configuration_Changelog/","text":"SpringBoot 系列文章 前提好久没更新文章了，本来打算在毕业之前不更新了，这里，对不住了，我又更新了。😝😝 之前翻译了两篇 Spring Boot 2.0 的文章，Spring Boot 2.0系列文章(一)：Spring Boot 2.0 迁移指南 和 Spring Boot 2.0系列文章(二)：Spring Boot 2.0 新特性详解 今天就继续详细探究 Spring Boot 2.0 里面的改变。 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/04/13/Spring_Boot_2.0_Configuration_Changelog/ 配置改变配置属性在 1.5.10.RELEASE 和 2.0.0.RELEASE 两个版本之间的改变： 启用键下面的表是 2.0.0.RELEASE 版本中的弃用键： Key Replacement（替代） 原因 spring.datasource.hikari.initialization-fail-fast spring.main.web-environment spring.main.web-application-type 新键下面的表是 2.0.0.RELEASE 版本中新的键： Key Default value（默认值） 描述 logging.file.max-history 0 要保存的归档日志文件的最大值 logging.file.max-size 10MB 日志文件最大容量 logging.pattern.dateformat yyyy-MM-dd HH:mm:ss.SSS 日志的日期格式 management.endpoint.auditevents.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.auditevents.enabled true 是否启用 auditevents 端点 management.endpoint.beans.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.beans.enabled true 是否启用 bean 端点 management.endpoint.conditions.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.conditions.enabled true 是否启用 conditions 端点 management.endpoint.configprops.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.configprops.enabled true 是否启用 configprops 端点 management.endpoint.configprops.keys-to-sanitize password,secret,key,token, .credentials.,vcap_services Keys that should be sanitized management.endpoint.env.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.env.enabled true 是否启用 env 端点 management.endpoint.env.keys-to-sanitize password,secret,key,token, .credentials.,vcap_services Keys that should be sanitized. management.endpoint.flyway.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.flyway.enabled true 是否启用 flyway 端点 management.endpoint.health.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.health.enabled true 是否启用 health 端点 management.endpoint.health.roles 角色用于确定用户是否有权显示详细信息 management.endpoint.health.show-details never 何时显示完整的健康详情 management.endpoint.heapdump.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.heapdump.enabled true 是否启用 heapdump 端点 management.endpoint.httptrace.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.httptrace.enabled true 是否启用 httptrace 端点 management.endpoint.info.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.info.enabled true 是否启用 info 端点 management.endpoint.jolokia.config Jolokia 设置 management.endpoint.jolokia.enabled true 是否启用 jolokia 端点 management.endpoint.liquibase.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.liquibase.enabled true 是否启用 liquibase 端点 management.endpoint.logfile.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.logfile.enabled true 是否启用 logfile 端点 management.endpoint.logfile.external-file 要访问的外部日志文件 management.endpoint.loggers.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.loggers.enabled true 是否启用 loggers 端点 management.endpoint.mappings.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.mappings.enabled true 是否启用 mappings 端点 management.endpoint.metrics.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.metrics.enabled true 是否启用 metrics 端点 management.endpoint.prometheus.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.prometheus.enabled true 是否启用 prometheus 端点 management.endpoint.scheduledtasks.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.scheduledtasks.enabled true 是否启用 scheduledtasks 端点 management.endpoint.sessions.enabled true 是否启用 sessions 端点 management.endpoint.shutdown.enabled false 是否启用 shutdown 端点 management.endpoint.threaddump.cache.time-to-live 0ms 可以缓存响应的最长时间 management.endpoint.threaddump.enabled true 是否启用 threaddump 端点 management.endpoints.enabled-by-default 是否启用或者关闭所有的端点 management.endpoints.jmx.domain org.springframework.boot 端点 JMX 域名 management.endpoints.jmx.exposure.exclude 应排除的端点 ID management.endpoints.jmx.exposure.include * 应包含的端点 ID 或全部 * management.endpoints.jmx.static-names 追加到所有表示端点的 MBean 的ObjectName 的静态属性. management.endpoints.jmx.unique-names false 是否确保 ObjectNames 在发生冲突时被修改 management.endpoints.web.base-path /actuator Web 端点的基本路径 management.endpoints.web.cors.allow-credentials 是否支持凭证 management.endpoints.web.cors.allowed-headers Comma-separated list of headers to allow in a request. ‘*’ allows all headers. management.endpoints.web.cors.allowed-methods Comma-separated list of methods to allow. ‘*’ allows all methods. management.endpoints.web.cors.allowed-origins Comma-separated list of origins to allow. ‘*’ allows all origins. management.endpoints.web.cors.exposed-headers Comma-separated list of headers to include in a response. management.endpoints.web.cors.max-age 1800s How long the response from a pre-flight request can be cached by clients. management.endpoints.web.exposure.exclude Endpoint IDs that should be excluded. management.endpoints.web.exposure.include health,info Endpoint IDs that should be included or ‘*’ for all. management.endpoints.web.path-mapping Mapping between endpoint IDs and the path that should expose them. management.health.influxdb.enabled true Whether to enable InfluxDB health check. management.health.neo4j.enabled true Whether to enable Neo4j health check. management.health.status.http-mapping Mapping of health statuses to HTTP status codes. management.metrics.binders.files.enabled true Whether to enable files metrics. management.metrics.binders.integration.enabled true Whether to enable Spring Integration metrics. management.metrics.binders.jvm.enabled true Whether to enable JVM metrics. management.metrics.binders.logback.enabled true Whether to enable Logback metrics. management.metrics.binders.processor.enabled true Whether to enable processor metrics. management.metrics.binders.uptime.enabled true Whether to enable uptime metrics. management.metrics.distribution.percentiles Specific computed non-aggregable percentiles to ship to the backend for meter IDs starting-with the specified name. management.metrics.distribution.percentiles-histogram Whether meter IDs starting-with the specified name should be publish percentile histograms. management.metrics.distribution.sla Specific SLA boundaries for meter IDs starting-with the specified name. management.metrics.enable Whether meter IDs starting-with the specified name should be enabled. management.metrics.export.atlas.batch-size 10000 Number of measurements per request to use for this backend. management.metrics.export.atlas.config-refresh-frequency 10s Frequency for refreshing config settings from the LWC service. management.metrics.export.atlas.config-time-to-live 150s Time to live for subscriptions from the LWC service. management.metrics.export.atlas.config-uri http://localhost:7101/ lwc/api/v1/expressions/local-dev URI for the Atlas LWC endpoint to retrieve current subscriptions. management.metrics.export.atlas.connect-timeout 1s Connection timeout for requests to this backend. management.metrics.export.atlas.enabled true Whether exporting of metrics to this backend is enabled. management.metrics.export.atlas.eval-uri http://localhost:7101/ lwc/api/v1/evaluate URI for the Atlas LWC endpoint to evaluate the data for a subscription. management.metrics.export.atlas.lwc-enabled false Whether to enable streaming to Atlas LWC. management.metrics.export.atlas.meter-time-to-live 15m Time to live for meters that do not have any activity. management.metrics.export.atlas.num-threads 2 Number of threads to use with the metrics publishing scheduler. management.metrics.export.atlas.read-timeout 10s Read timeout for requests to this backend. management.metrics.export.atlas.step 1m Step size (i.e. reporting frequency) to use. management.metrics.export.atlas.uri http://localhost:7101/ api/v1/publish URI of the Atlas server. management.metrics.export.datadog.api-key Datadog API key. management.metrics.export.datadog.application-key Datadog application key. management.metrics.export.datadog.batch-size 10000 Number of measurements per request to use for this backend. management.metrics.export.datadog.connect-timeout 1s Connection timeout for requests to this backend. management.metrics.export.datadog.descriptions true Whether to publish descriptions metadata to Datadog. management.metrics.export.datadog.enabled true Whether exporting of metrics to this backend is enabled. management.metrics.export.datadog.host-tag instance Tag that will be mapped to “host” when shipping metrics to Datadog. management.metrics.export.datadog.num-threads 2 Number of threads to use with the metrics publishing scheduler. management.metrics.export.datadog.read-timeout 10s Read timeout for requests to this backend. management.metrics.export.datadog.step 1m Step size (i.e. reporting frequency) to use. management.metrics.export.datadog.uri https://app.datadoghq.com URI to ship metrics to. management.metrics.export.ganglia.addressing-mode multicast UDP addressing mode, either unicast or multicast. management.metrics.export.ganglia.duration-units milliseconds Base time unit used to report durations. management.metrics.export.ganglia.enabled true Whether exporting of metrics to Ganglia is enabled. management.metrics.export.ganglia.host localhost Host of the Ganglia server to receive exported metrics. management.metrics.export.ganglia.port 8649 Port of the Ganglia server to receive exported metrics. management.metrics.export.ganglia.protocol-version 3.1 Ganglia protocol version. management.metrics.export.ganglia.rate-units seconds Base time unit used to report rates. management.metrics.export.ganglia.step 1m Step size (i.e. reporting frequency) to use. management.metrics.export.ganglia.time-to-live 1 Time to live for metrics on Ganglia. management.metrics.export.graphite.duration-units milliseconds Base time unit used to report durations. management.metrics.export.graphite.enabled true Whether exporting of metrics to Graphite is enabled. management.metrics.export.graphite.host localhost Host of the Graphite server to receive exported metrics. management.metrics.export.graphite.port 2004 Port of the Graphite server to receive exported metrics. management.metrics.export.graphite.protocol pickled Protocol to use while shipping data to Graphite. management.metrics.export.graphite.rate-units seconds Base time unit used to report rates. management.metrics.export.graphite.step 1m Step size (i.e. reporting frequency) to use. management.metrics.export.graphite.tags-as-prefix `` For the default naming convention, turn the specified tag keys into part of the metric prefix. management.metrics.export.influx.auto-create-db true Whether to create the Influx database if it does not exist before attempting to publish metrics to it. management.metrics.export.influx.batch-size 10000 Number of measurements per request to use for this backend. management.metrics.export.influx.compressed true Whether to enable GZIP compression of metrics batches published to Influx. management.metrics.export.influx.connect-timeout 1s Connection timeout for requests to this backend. management.metrics.export.influx.consistency one Write consistency for each point. management.metrics.export.influx.db mydb Tag that will be mapped to “host” when shipping metrics to Influx. management.metrics.export.influx.enabled true Whether exporting of metrics to this backend is enabled. management.metrics.export.influx.num-threads 2 Number of threads to use with the metrics publishing scheduler. management.metrics.export.influx.password Login password of the Influx server. management.metrics.export.influx.read-timeout 10s Read timeout for requests to this backend. management.metrics.export.influx.retention-policy Retention policy to use (Influx writes to the DEFAULT retention policy if one is not specified). management.metrics.export.influx.step 1m Step size (i.e. reporting frequency) to use. management.metrics.export.influx.uri http://localhost:8086 URI of the Influx server. management.metrics.export.influx.user-name Login user of the Influx server. management.metrics.export.jmx.enabled true Whether exporting of metrics to JMX is enabled. management.metrics.export.jmx.step 1m Step size (i.e. reporting frequency) to use. management.metrics.export.newrelic.account-id New Relic account ID. management.metrics.export.newrelic.api-key New Relic API key. management.metrics.export.newrelic.batch-size 10000 Number of measurements per request to use for this backend. management.metrics.export.newrelic.connect-timeout 1s Connection timeout for requests to this backend. management.metrics.export.newrelic.enabled true Whether exporting of metrics to this backend is enabled. management.metrics.export.newrelic.num-threads 2 Number of threads to use with the metrics publishing scheduler. management.metrics.export.newrelic.read-timeout 10s Read timeout for requests to this backend. management.metrics.export.newrelic.step 1m Step size (i.e. reporting frequency) to use. management.metrics.export.newrelic.uri https://insights-collector .newrelic.com URI to ship metrics to. management.metrics.export.prometheus.descriptions true Whether to enable publishing descriptions as part of the scrape payload to Prometheus. management.metrics.export.prometheus.enabled true Whether exporting of metrics to Prometheus is enabled. management.metrics.export.prometheus.step 1m Step size (i.e. reporting frequency) to use. management.metrics.export.signalfx.access-token SignalFX access token. management.metrics.export.signalfx.batch-size 10000 Number of measurements per request to use for this backend. management.metrics.export.signalfx.connect-timeout 1s Connection timeout for requests to this backend. management.metrics.export.signalfx.enabled true Whether exporting of metrics to this backend is enabled. management.metrics.export.signalfx.num-threads 2 Number of threads to use with the metrics publishing scheduler. management.metrics.export.signalfx.read-timeout 10s Read timeout for requests to this backend. management.metrics.export.signalfx.source Uniquely identifies the app instance that is publishing metrics to SignalFx. management.metrics.export.signalfx.step 10s Step size (i.e. reporting frequency) to use. management.metrics.export.signalfx.uri https://ingest.signalfx.com URI to ship metrics to. management.metrics.export.simple.enabled true Whether, in the absence of any other exporter, exporting of metrics to an in-memory backend is enabled. management.metrics.export.simple.mode cumulative Counting mode. management.metrics.export.simple.step 1m Step size (i.e. reporting frequency) to use. management.metrics.export.statsd.enabled true Whether exporting of metrics to StatsD is enabled. management.metrics.export.statsd.flavor datadog StatsD line protocol to use. management.metrics.export.statsd.host localhost Host of the StatsD server to receive exported metrics. management.metrics.export.statsd.max-packet-length 1400 Total length of a single payload should be kept within your network’s MTU. management.metrics.export.statsd.polling-frequency 10s How often gauges will be polled. management.metrics.export.statsd.port 8125 Port of the StatsD server to receive exported metrics. management.metrics.export.statsd.publish-unchanged-meters true Whether to send unchanged meters to the StatsD server. management.metrics.export.statsd.queue-size 2147483647 Maximum size of the queue of items waiting to be sent to the StatsD server. management.metrics.export.wavefront.api-token API token used when publishing metrics directly to the Wavefront API host. management.metrics.export.wavefront.batch-size 10000 Number of measurements per request to use for this backend. management.metrics.export.wavefront.connect-timeout 1s Connection timeout for requests to this backend. management.metrics.export.wavefront.enabled true Whether exporting of metrics to this backend is enabled. management.metrics.export.wavefront.global-prefix Global prefix to separate metrics originating from this app’s white box instrumentation from those originating from other Wavefront integrations when viewed in the Wavefront UI. management.metrics.export.wavefront.num-threads 2 Number of threads to use with the metrics publishing scheduler. management.metrics.export.wavefront.read-timeout 10s Read timeout for requests to this backend. management.metrics.export.wavefront.source Unique identifier for the app instance that is the source of metrics being published to Wavefront. management.metrics.export.wavefront.step 10s Step size (i.e. reporting frequency) to use. management.metrics.export.wavefront.uri https://longboard.wavefront.com URI to ship metrics to. management.metrics.use-global-registry true Whether auto-configured MeterRegistry implementations should be bound to the global static registry on Metrics. management.metrics.web.client.max-uri-tags 100 Maximum number of unique URI tag values allowed. management.metrics.web.client.requests-metric-name http.client.requests Name of the metric for sent requests. management.metrics.web.server.auto-time-requests true Whether requests handled by Spring MVC or WebFlux should be automatically timed. management.metrics.web.server.requests-metric-name http.server.requests Name of the metric for received requests. management.server.add-application-context-header false Add the “X-Application-Context” HTTP header in each response. management.server.address Network address to which the management endpoints should bind. management.server.port Management endpoint HTTP port (uses the same port as the application by default). management.server.servlet.context-path Management endpoint context-path (for instance,/management). management.server.ssl.ciphers management.server.ssl.client-auth management.server.ssl.enabled management.server.ssl.enabled-protocols management.server.ssl.key-alias management.server.ssl.key-password management.server.ssl.key-store management.server.ssl.key-store-password management.server.ssl.key-store-provider management.server.ssl.key-store-type management.server.ssl.protocol management.server.ssl.trust-store management.server.ssl.trust-store-password management.server.ssl.trust-store-provider management.server.ssl.trust-store-type management.trace.http.enabled true Whether to enable HTTP request-response tracing. management.trace.http.include request-headers,response-headers, cookies,errors Items to be included in the trace. server.error.include-exception false Include the “exception” attribute. server.http2.enabled server.jetty.accesslog.append false Append to log. server.jetty.accesslog.date-format dd/MMM/yyyy:HH:mm:ss Z Timestamp format of the request log. server.jetty.accesslog.enabled false Enable access log. server.jetty.accesslog.extended-format false Enable extended NCSA format. server.jetty.accesslog.file-date-format Date format to place in log file name. server.jetty.accesslog.filename Log filename. server.jetty.accesslog.locale Locale of the request log. server.jetty.accesslog.log-cookies false Enable logging of the request cookies. server.jetty.accesslog.log-latency false Enable logging of request processing time. server.jetty.accesslog.log-server false Enable logging of the request hostname. server.jetty.accesslog.retention-period 31 Number of days before rotated log files are deleted. server.jetty.accesslog.time-zone GMT Timezone of the request log. server.servlet.application-display-name application Display name of the application. server.servlet.context-parameters Servlet context init parameters. server.servlet.context-path Context path of the application. server.servlet.jsp.class-name server.servlet.jsp.init-parameters server.servlet.jsp.registered server.servlet.path / Path of the main dispatcher servlet. server.servlet.session.cookie.comment server.servlet.session.cookie.domain server.servlet.session.cookie.http-only server.servlet.session.cookie.max-age server.servlet.session.cookie.name server.servlet.session.cookie.path server.servlet.session.cookie.secure server.servlet.session.persistent server.servlet.session.store-dir server.servlet.session.timeout server.servlet.session.tracking-modes server.tomcat.max-http-header-size 0 Maximum size, in bytes, of the HTTP message header. server.tomcat.resource.cache-ttl Time-to-live of the static resource cache. server.tomcat.use-relative-redirects Whether HTTP 1.1 and later location headers generated by a call to sendRedirect will use relative or absolute redirects. server.undertow.eager-filter-init true Whether servlet filters should be initialized on startup. spring.banner.charset UTF-8 Banner file encoding. spring.banner.image.height Height of the banner image in chars (default based on image height). spring.banner.image.invert false Whether images should be inverted for dark terminal themes. spring.banner.image.location classpath:banner.gif Banner image file location (jpg or png can also be used). spring.banner.image.margin 2 Left hand image margin in chars. spring.banner.image.width 76 Width of the banner image in chars. spring.banner.location classpath:banner.txt Banner text resource location. spring.batch.initialize-schema embedded Database schema initialization mode. spring.cache.redis.cache-null-values true Allow caching null values. spring.cache.redis.key-prefix Key prefix. spring.cache.redis.time-to-live Entry expiration. spring.cache.redis.use-key-prefix true Whether to use the key prefix when writing to Redis. spring.config.additional-location Config file locations used in addition to the defaults. spring.data.cassandra.connect-timeout Socket option: connection time out. spring.data.cassandra.pool.heartbeat-interval 30s Heartbeat interval after which a message is sent on an idle connection to make sure it’s still alive. spring.data.cassandra.pool.idle-timeout 120s Idle timeout before an idle connection is removed. spring.data.cassandra.pool.max-queue-size 256 Maximum number of requests that get queued if no connection is available. spring.data.cassandra.pool.pool-timeout 5000ms Pool timeout when trying to acquire a connection from a host’s pool. spring.data.cassandra.read-timeout Socket option: read time out. spring.data.cassandra.repositories.type auto Type of Cassandra repositories to enable. spring.data.couchbase.repositories.type auto Type of Couchbase repositories to enable. spring.data.mongodb.repositories.type auto Type of Mongo repositories to enable. spring.data.neo4j.auto-index none Auto index mode. spring.data.web.pageable.default-page-size 20 Default page size. spring.data.web.pageable.max-page-size 2000 Maximum page size to be accepted. spring.data.web.pageable.one-indexed-parameters false Whether to expose and assume 1-based page number indexes. spring.data.web.pageable.page-parameter page Page index parameter name. spring.data.web.pageable.prefix `` General prefix to be prepended to the page number and page size parameters. spring.data.web.pageable.qualifier-delimiter _ Delimiter to be used between the qualifier and the actual page number and size properties. spring.data.web.pageable.size-parameter size Page size parameter name. spring.data.web.sort.sort-parameter sort Sort parameter name. spring.datasource.hikari.initialization-fail-timeout spring.datasource.hikari.metrics-tracker-factory spring.datasource.hikari.scheduled-executor spring.datasource.hikari.scheduled-executor-service spring.datasource.hikari.schema spring.datasource.initialization-mode embedded Initialize the datasource with available DDL and DML scripts. spring.devtools.restart.log-condition-evaluation-delta true Whether to log the condition evaluation delta upon restart. spring.flyway.baseline-description spring.flyway.baseline-on-migrate spring.flyway.baseline-version spring.flyway.check-location true Whether to check that migration scripts location exists. spring.flyway.clean-disabled spring.flyway.clean-on-validation-error spring.flyway.dry-run-output spring.flyway.enabled true 是否启用 flyway. spring.flyway.encoding spring.flyway.error-handlers spring.flyway.group spring.flyway.ignore-future-migrations spring.flyway.ignore-missing-migrations spring.flyway.init-sqls SQL statements to execute to initialize a connection immediately after obtaining it. spring.flyway.installed-by spring.flyway.locations spring.flyway.mixed spring.flyway.out-of-order spring.flyway.password 如果您想让 Flyway 创建自己的DataSource，可以使用 JDBC 密码 spring.flyway.placeholder-prefix spring.flyway.placeholder-replacement spring.flyway.placeholder-suffix spring.flyway.placeholders spring.flyway.repeatable-sql-migration-prefix spring.flyway.schemas spring.flyway.skip-default-callbacks spring.flyway.skip-default-resolvers spring.flyway.sql-migration-prefix spring.flyway.sql-migration-separator spring.flyway.sql-migration-suffix spring.flyway.sql-migration-suffixes spring.flyway.table spring.flyway.target spring.flyway.undo-sql-migration-prefix spring.flyway.url 要迁移的数据库的 JDBC URL spring.flyway.user 登录要迁移数据库的用户名 spring.flyway.validate-on-migrate spring.gson.date-format 序列化 Date 对象时使用的格式 spring.gson.disable-html-escaping Whether to disable the escaping of HTML characters such as ‘&lt;’, ‘&gt;’, etc. spring.gson.disable-inner-class-serialization Whether to exclude inner classes during serialization. spring.gson.enable-complex-map-key-serialization Whether to enable serialization of complex map keys (i.e. non-primitives). spring.gson.exclude-fields-without-expose-annotation Whether to exclude all fields from consideration for serialization or deserialization that do not have the “Expose” annotation. spring.gson.field-naming-policy Naming policy that should be applied to an object’s field during serialization and deserialization. spring.gson.generate-non-executable-json Whether to generate non executable JSON by prefixing the output with some special text. spring.gson.lenient Whether to be lenient about parsing JSON that doesn’t conform to RFC 4627. spring.gson.long-serialization-policy Serialization policy for Long and long types. spring.gson.pretty-printing Whether to output serialized JSON that fits in a page for pretty printing. spring.gson.serialize-nulls Whether to serialize null fields. spring.influx.password Influx 登录用户名密码 spring.influx.url InfluxDB 数据库 URL spring.influx.user Influx 登录用户名 spring.integration.jdbc.initialize-schema embedded Database schema initialization mode. spring.integration.jdbc.schema classpath:org/springframework/ integration/jdbc/schema-@@platform@@.sql Path to the SQL file to use to initialize the database schema. spring.jdbc.template.fetch-size -1 Number of rows that should be fetched from the database when more rows are needed. spring.jdbc.template.max-rows -1 Maximum number of rows. spring.jdbc.template.query-timeout Query timeout. spring.jpa.mapping-resources Mapping resources (equivalent to “mapping-file” entries in persistence.xml). spring.jta.atomikos.datasource.concurrent-connection-validation spring.jta.atomikos.properties.allow-sub-transactions true Specify whether sub-transactions are allowed. spring.jta.atomikos.properties.default-max-wait-time-on-shutdown How long should normal shutdown (no-force) wait for transactions to complete. spring.jta.atomikos.properties.recovery.delay 10000ms Delay between two recovery scans. spring.jta.atomikos.properties.recovery.forget-orphaned-log-entries-delay 86400000ms Delay after which recovery can cleanup pending (‘orphaned’) log entries. spring.jta.atomikos.properties.recovery.max-retries 5 Number of retry attempts to commit the transaction before throwing an exception. spring.jta.atomikos.properties.recovery.retry-interval 10000ms Delay between retry attempts. spring.kafka.admin.client-id ID to pass to the server when making requests. spring.kafka.admin.fail-fast false Whether to fail fast if the broker is not available on startup. spring.kafka.admin.properties Additional admin-specific properties used to configure the client. spring.kafka.admin.ssl.key-password Password of the private key in the key store file. spring.kafka.admin.ssl.keystore-location Location of the key store file. spring.kafka.admin.ssl.keystore-password Store password for the key store file. spring.kafka.admin.ssl.truststore-location Location of the trust store file. spring.kafka.admin.ssl.truststore-password Store password for the trust store file. spring.kafka.consumer.properties Additional consumer-specific properties used to configure the client. spring.kafka.consumer.ssl.key-password Password of the private key in the key store file. spring.kafka.consumer.ssl.keystore-location Location of the key store file. spring.kafka.consumer.ssl.keystore-password Store password for the key store file. spring.kafka.consumer.ssl.truststore-location Location of the trust store file. spring.kafka.consumer.ssl.truststore-password Store password for the trust store file. spring.kafka.jaas.control-flag required Control flag for login configuration. spring.kafka.jaas.enabled false Whether to enable JAAS configuration. spring.kafka.jaas.login-module com.sun.security.auth .module.Krb5LoginModule Login module. spring.kafka.jaas.options Additional JAAS options. spring.kafka.listener.client-id Prefix for the listener’s consumer client.id property. spring.kafka.listener.idle-event-interval Time between publishing idle consumer events (no data received). spring.kafka.listener.log-container-config Whether to log the container configuration during initialization (INFO level). spring.kafka.listener.monitor-interval Time between checks for non-responsive consumers. spring.kafka.listener.no-poll-threshold Multiplier applied to “pollTimeout” to determine if a consumer is non-responsive. spring.kafka.listener.type single Listener type. spring.kafka.producer.properties Additional producer-specific properties used to configure the client. spring.kafka.producer.ssl.key-password Password of the private key in the key store file. spring.kafka.producer.ssl.keystore-location Location of the key store file. spring.kafka.producer.ssl.keystore-password Store password for the key store file. spring.kafka.producer.ssl.truststore-location Location of the trust store file. spring.kafka.producer.ssl.truststore-password Store password for the trust store file. spring.kafka.producer.transaction-id-prefix When non empty, enables transaction support for producer. spring.ldap.anonymous-read-only false Whether read-only operations should use an anonymous environment. spring.liquibase.change-log classpath:/db/changelog/ db.changelog-master.yaml Change log configuration path. spring.liquibase.check-change-log-location true Whether to check that the change log location exists. spring.liquibase.contexts Comma-separated list of runtime contexts to use. spring.liquibase.default-schema Default database schema. spring.liquibase.drop-first false Whether to first drop the database schema. spring.liquibase.enabled true Whether to enable Liquibase support. spring.liquibase.labels Comma-separated list of runtime labels to use. spring.liquibase.parameters Change log parameters. spring.liquibase.password Login password of the database to migrate. spring.liquibase.rollback-file File to which rollback SQL is written when an update is performed. spring.liquibase.url JDBC URL of the database to migrate. spring.liquibase.user Login user of the database to migrate. spring.main.web-application-type Flag to explicitly request a specific type of web application. spring.messages.cache-duration Loaded resource bundle files cache duration. spring.messages.use-code-as-default-message false Whether to use the message code as the default message instead of throwing a “NoSuchMessageException”. spring.mvc.contentnegotiation.favor-parameter false Whether a request parameter (“format” by default) should be used to determine the requested media type. spring.mvc.contentnegotiation.favor-path-extension false Whether the path extension in the URL path should be used to determine the requested media type. spring.mvc.contentnegotiation.media-types Map file extensions to media types for content negotiation. spring.mvc.contentnegotiation.parameter-name Query parameter name to use when “favor-parameter” is enabled. spring.mvc.pathmatch.use-registered-suffix-pattern false Whether suffix pattern matching should work only against extensions registered with “spring.mvc.contentnegotiation.media-types.*”. spring.mvc.pathmatch.use-suffix-pattern false Whether to use suffix pattern match (“.*”) when matching patterns to requests. spring.quartz.jdbc.initialize-schema embedded Database schema initialization mode. spring.quartz.jdbc.schema classpath:org/quartz/impl/ jdbcjobstore/ tables_@@platform@@.sql Path to the SQL file to use to initialize the database schema. spring.quartz.job-store-type memory Quartz job store type. spring.quartz.properties Additional Quartz Scheduler properties. spring.rabbitmq.listener.direct.acknowledge-mode Acknowledge mode of container. spring.rabbitmq.listener.direct.auto-startup true Whether to start the container automatically on startup. spring.rabbitmq.listener.direct.consumers-per-queue Number of consumers per queue. spring.rabbitmq.listener.direct.default-requeue-rejected Whether rejected deliveries are re-queued by default. spring.rabbitmq.listener.direct.idle-event-interval How often idle container events should be published. spring.rabbitmq.listener.direct.prefetch Number of messages to be handled in a single request. spring.rabbitmq.listener.direct.retry.enabled false Whether publishing retries are enabled. spring.rabbitmq.listener.direct.retry.initial-interval 1000ms Duration between the first and second attempt to deliver a message. spring.rabbitmq.listener.direct.retry.max-attempts 3 Maximum number of attempts to deliver a message. spring.rabbitmq.listener.direct.retry.max-interval 10000ms Maximum duration between attempts. spring.rabbitmq.listener.direct.retry.multiplier 1 Multiplier to apply to the previous retry interval. spring.rabbitmq.listener.direct.retry.stateless true Whether retries are stateless or stateful. spring.rabbitmq.listener.type simple Listener container type. spring.rabbitmq.ssl.key-store-type PKCS12 Key store type. spring.rabbitmq.ssl.trust-store-type JKS Trust store type. spring.rabbitmq.template.exchange `` Name of the default exchange to use for send operations. spring.rabbitmq.template.routing-key `` Value of a default routing key to use for send operations. spring.reactor.stacktrace-mode.enabled false Whether Reactor should collect stacktrace information at runtime. spring.redis.jedis.pool.max-active 8 Maximum number of connections that can be allocated by the pool at a given time. spring.redis.jedis.pool.max-idle 8 Maximum number of “idle” connections in the pool. spring.redis.jedis.pool.max-wait -1ms Maximum amount of time a connection allocation should block before throwing an exception when the pool is exhausted. spring.redis.jedis.pool.min-idle 0 Target for the minimum number of idle connections to maintain in the pool. spring.redis.lettuce.pool.max-active 8 Maximum number of connections that can be allocated by the pool at a given time. spring.redis.lettuce.pool.max-idle 8 Maximum number of “idle” connections in the pool. spring.redis.lettuce.pool.max-wait -1ms Maximum amount of time a connection allocation should block before throwing an exception when the pool is exhausted. spring.redis.lettuce.pool.min-idle 0 Target for the minimum number of idle connections to maintain in the pool. spring.redis.lettuce.shutdown-timeout 100ms Shutdown timeout. spring.resources.cache.cachecontrol.cache-private Indicate that the response message is intended for a single user and must not be stored by a shared cache. spring.resources.cache.cachecontrol.cache-public Indicate that any cache may store the response. spring.resources.cache.cachecontrol.max-age Maximum time the response should be cached, in seconds if no duration suffix is not specified. spring.resources.cache.cachecontrol.must-revalidate Indicate that once it has become stale, a cache must not use the response without re-validating it with the server. spring.resources.cache.cachecontrol.no-cache Indicate that the cached response can be reused only if re-validated with the server. spring.resources.cache.cachecontrol.no-store Indicate to not cache the response in any case. spring.resources.cache.cachecontrol.no-transform Indicate intermediaries (caches and others) that they should not transform the response content. spring.resources.cache.cachecontrol.proxy-revalidate Same meaning as the “must-revalidate” directive, except that it does not apply to private caches. spring.resources.cache.cachecontrol.s-max-age Maximum time the response should be cached by shared caches, in seconds if no duration suffix is not specified. spring.resources.cache.cachecontrol.stale-if-error Maximum time the response may be used when errors are encountered, in seconds if no duration suffix is not specified. spring.resources.cache.cachecontrol.stale-while-revalidate Maximum time the response can be served after it becomes stale, in seconds if no duration suffix is not specified. spring.resources.cache.period Cache period for the resources served by the resource handler. spring.security.filter.dispatcher-types async,error,request Security filter chain dispatcher types. spring.security.filter.order -100 Security filter chain order. spring.security.oauth2.client.provider OAuth provider details. spring.security.oauth2.client.registration OAuth client registrations. spring.security.user.name user Default user name. spring.security.user.password Password for the default user name. spring.security.user.roles Granted roles for the default user name. spring.servlet.multipart.enabled true Whether to enable support of multipart uploads. spring.servlet.multipart.file-size-threshold 0 Threshold after which files are written to disk. spring.servlet.multipart.location Intermediate location of uploaded files. spring.servlet.multipart.max-file-size 1MB Max file size. spring.servlet.multipart.max-request-size 10MB Max request size. spring.servlet.multipart.resolve-lazily false Whether to resolve the multipart request lazily at the time of file or parameter access. spring.session.jdbc.cleanup-cron 0 * * * * * Cron expression for expired session cleanup job. spring.session.jdbc.initialize-schema embedded Database schema initialization mode. spring.session.mongodb.collection-name sessions Collection name used to store sessions. spring.session.redis.cleanup-cron 0 * * * * * Cron expression for expired session cleanup job. spring.session.servlet.filter-dispatcher-types async,error,request Session repository filter dispatcher types. spring.session.servlet.filter-order Session repository filter order. spring.thymeleaf.enable-spring-el-compiler false Enable the SpringEL compiler in SpringEL expressions. spring.thymeleaf.reactive.chunked-mode-view-names Comma-separated list of view names (patterns allowed) that should be the only ones executed in CHUNKED mode when a max chunk size is set. spring.thymeleaf.reactive.full-mode-view-names Comma-separated list of view names (patterns allowed) that should be executed in FULL mode even if a max chunk size is set. spring.thymeleaf.reactive.max-chunk-size 0 Maximum size of data buffers used for writing to the response, in bytes. spring.thymeleaf.reactive.media-types Media types supported by the view technology. spring.thymeleaf.servlet.content-type text/html Content-Type value written to HTTP responses. spring.webflux.date-format Date format to use. spring.webflux.static-path-pattern /** Path pattern used for static resources. spring.webservices.wsdl-locations Comma-separated list of locations of WSDLs and accompanying XSDs to be exposed as beans. Key Replacement（替代） 原因 banner.charset spring.banner.charset banner.image.height spring.banner.image.height banner.image.invert spring.banner.image.invert banner.image.location spring.banner.image.location banner.image.margin spring.banner.image.margin banner.image.width spring.banner.image.width banner.location spring.banner.location endpoints.actuator.enabled actuator 端点不再可用 endpoints.actuator.path actuator 端点不再可用 endpoints.actuator.sensitive actuator 端点不再可用 endpoints.auditevents.enabled management.endpoint. auditevents.enabled endpoints.auditevents.path management.endpoints.web.path-mapping.auditevents endpoints.auditevents.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.autoconfig.enabled management.endpoint. conditions.enabled endpoints.autoconfig.id 端点标识符不再可定制 endpoints.autoconfig.path management.endpoints.web.path-mapping.conditions endpoints.autoconfig.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.beans.enabled management.endpoint.beans.enabled endpoints.beans.id 端点标识符不再可定制 endpoints.beans.path management.endpoints.web.path-mapping.beans endpoints.beans.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.configprops.enabled management.endpoint. configprops.enabled endpoints.configprops.id 端点标识符不再可定制 endpoints.configprops.keys-to-sanitize management.endpoint. configprops.keys-to-sanitize endpoints.configprops.path management.endpoints.web.path-mapping.configprops endpoints.configprops.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.cors.allow-credentials management.endpoints. web.cors.allow-credentials endpoints.cors.allowed-headers management.endpoints. web.cors.allowed-headers endpoints.cors.allowed-methods management.endpoints. web.cors.allowed-methods endpoints.cors.allowed-origins management.endpoints. web.cors.allowed-origins endpoints.cors.exposed-headers management.endpoints. web.cors.exposed-headers endpoints.cors.max-age management.endpoints. web.cors.max-age endpoints.docs.curies.enabled docs 端点不再可用 endpoints.docs.enabled docs 端点不再可用 endpoints.docs.path docs 端点不再可用 endpoints.docs.sensitive docs 端点不再可用 endpoints.dump.enabled management.endpoint. threaddump.enabled endpoints.dump.id 端点标识符不再可定制 endpoints.dump.path management.endpoints.web.path-mapping.dump endpoints.dump.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.enabled management.endpoints. enabled-by-default endpoints.env.enabled management.endpoint.env.enabled endpoints.env.id 端点标识符不再可定制 endpoints.env.keys-to-sanitize management.endpoint. env.keys-to-sanitize endpoints.env.path management.endpoints. web.path-mapping.env endpoints.env.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.flyway.enabled management.endpoint.flyway.enabled endpoints.flyway.id 端点标识符不再可定制 endpoints.flyway.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.health.enabled management.endpoint.health.enabled endpoints.health.id 端点标识符不再可定制 endpoints.health.mapping management.health.status.http-mapping endpoints.health.path management.endpoints.web.path-mapping.health endpoints.health.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.health.time-to-live management.endpoint. health.cache.time-to-live endpoints.heapdump.enabled management.endpoint.heapdump.enabled endpoints.heapdump.path management.endpoints.web.path-mapping.heapdump endpoints.heapdump.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.hypermedia.enabled Actuator 中的 Hypermedia 不再可用 endpoints.info.enabled management.endpoint.info.enabled endpoints.info.id 端点标识符不再可定制 endpoints.info.path management.endpoints.web.path-mapping.info endpoints.info.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.jmx.domain management.endpoints.jmx.domain endpoints.jmx.enabled management.endpoints. jmx.exposure.exclude endpoints.jmx.static-names management.endpoints. jmx.static-names endpoints.jmx.unique-names management.endpoints. jmx.unique-names endpoints.jolokia.enabled management.endpoint. jolokia.enabled endpoints.jolokia.path management.endpoints.web.path-mapping.jolokia endpoints.jolokia.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.liquibase.enabled management.endpoint. liquibase.enabled endpoints.liquibase.id 端点标识符不再可定制 endpoints.liquibase.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.logfile.enabled management.endpoint. logfile.enabled endpoints.logfile.external-file management.endpoint. logfile.external-file endpoints.logfile.path management.endpoints.web.path-mapping.logfile endpoints.logfile.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.loggers.enabled management.endpoint. loggers.enabled endpoints.loggers.id 端点标识符不再可定制 endpoints.loggers.path management.endpoints.web.path-mapping.loggers endpoints.loggers.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.mappings.enabled management.endpoint. mappings.enabled endpoints.mappings.id 端点标识符不再可定制 endpoints.mappings.path management.endpoints.web.path-mapping.mappings endpoints.mappings.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.metrics.enabled management.endpoint.metrics.enabled endpoints.metrics.filter.counter-submissions Metrics support 现在使用千分尺 endpoints.metrics.filter.enabled Metrics support 现在使用千分尺 endpoints.metrics.filter.gauge-submissions Metrics support 现在使用千分尺 endpoints.metrics.id 端点标识符不再可定制 endpoints.metrics.path management.endpoints.web.path-mapping.metrics endpoints.metrics.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.shutdown.enabled management.endpoint. shutdown.enabled endpoints.shutdown.id 端点标识符不再可定制 endpoints.shutdown.path management.endpoints.web.path-mapping.shutdown endpoints.shutdown.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 endpoints.trace.enabled management.endpoint. httptrace.enabled endpoints.trace.filter.enabled management.trace.http.enabled endpoints.trace.id 端点标识符不再可定制 endpoints.trace.path management.endpoints.web.path-mapping.httptrace endpoints.trace.sensitive 终端敏感标志不再可定制，因为Spring Boot 不再提供可自定义的安全自动配置 error.path Path of the error controller. flyway.baseline-description spring.flyway.baseline-description flyway.baseline-on-migrate spring.flyway.baseline-on-migrate flyway.baseline-version spring.flyway.baseline-version flyway.check-location spring.flyway.check-location flyway.clean-on-validation-error spring.flyway. clean-on-validation-error flyway.enabled spring.flyway.enabled flyway.encoding spring.flyway.encoding flyway.ignore-failed-future-migration flyway.init-sqls spring.flyway.init-sqls flyway.locations spring.flyway.locations flyway.out-of-order spring.flyway.out-of-order flyway.password spring.flyway.password flyway.placeholder-prefix spring.flyway.placeholder-prefix flyway.placeholder-replacement spring.flyway.placeholder-replacement flyway.placeholder-suffix spring.flyway.placeholder-suffix flyway.placeholders spring.flyway.placeholders flyway.schemas spring.flyway.schemas flyway.sql-migration-prefix spring.flyway.sql-migration-prefix flyway.sql-migration-separator spring.flyway.sql-migration-separator flyway.sql-migration-suffix spring.flyway.sql-migration-suffixes flyway.table spring.flyway.table flyway.target spring.flyway.target flyway.url spring.flyway.url flyway.user spring.flyway.user flyway.validate-on-migrate spring.flyway.validate-on-migrate jolokia.config management.endpoint.jolokia.config liquibase.change-log spring.liquibase.change-log liquibase.check-change-log-location spring.liquibase.check-change-log-location liquibase.contexts spring.liquibase.contexts liquibase.default-schema spring.liquibase.default-schema liquibase.drop-first spring.liquibase.drop-first liquibase.enabled spring.liquibase.enabled liquibase.labels spring.liquibase.labels liquibase.parameters spring.liquibase.parameters liquibase.password spring.liquibase.password liquibase.rollback-file spring.liquibase.rollback-file liquibase.url spring.liquibase.url liquibase.user spring.liquibase.user management.add-application-context-header management.server.add-application-context-header management.address management.server.address management.context-path management.server. servlet.context-path management.port management.server.port management.security.enabled 现在提供全局 security 自动配置。 management.security.roles security 自动配置不再可定制 management.security.sessions security 自动配置不再可定制 management.shell.auth.jaas.domain CRaSH 支持不再可用 management.shell.auth.key.path CRaSH 支持不再可用 management.shell.auth.simple.user.name CRaSH 支持不再可用 management.shell.auth.simple.user.password CRaSH 支持不再可用 management.shell.auth.spring.roles CRaSH 支持不再可用 management.shell.auth.type CRaSH 支持不再可用 management.shell.ssh.auth-timeout CRaSH 支持不再可用 management.shell.ssh.enabled CRaSH 支持不再可用 management.shell.ssh.idle-timeout CRaSH 支持不再可用 management.shell.ssh.key-path CRaSH 支持不再可用 management.shell.ssh.port CRaSH 支持不再可用 management.shell.telnet.enabled CRaSH 支持不再可用 management.shell.telnet.port CRaSH 支持不再可用 management.ssl.ciphers management.server.ssl.ciphers management.ssl.client-auth management.server.ssl.client-auth management.ssl.enabled management.server.ssl.enabled management.ssl.enabled-protocols management.server.ssl.enabled-protocols management.ssl.key-alias management.server.ssl.key-alias management.ssl.key-password management.server.ssl.key-password management.ssl.key-store management.server.ssl.key-store management.ssl.key-store-password management.server.ssl.key-store-password management.ssl.key-store-provider management.server.ssl.key-store-provider management.ssl.key-store-type management.server.ssl.key-store-type management.ssl.protocol management.server.ssl.protocol management.ssl.trust-store management.server.ssl.trust-store management.ssl.trust-store-password management.server.ssl.trust-store-password management.ssl.trust-store-provider management.server.ssl.trust-store-provider management.ssl.trust-store-type management.server.ssl.trust-store-type management.trace.include management.trace.http.include security.basic.authorize-mode security 自动配置不再可定制 security.basic.enabled security 自动配置不再可定制 security.basic.path security 自动配置不再可定制 security.basic.realm security 自动配置不再可定制 security.enable-csrf security 自动配置不再可定制 security.filter-dispatcher-types spring.security. filter.dispatcher-types security.filter-order spring.security.filter.order security.headers.cache security 自动配置不再可定制 security.headers.content-security-policy security 自动配置不再可定制 security.headers.content-security-policy-mode security 自动配置不再可定制 security.headers.content-type security 自动配置不再可定制 security.headers.frame security 自动配置不再可定制 security.headers.hsts security 自动配置不再可定制 security.headers.xss security 自动配置不再可定制 security.ignored security 自动配置不再可定制 security.oauth2. authorization.check-token-access Spring Security 访问规则用于检查令牌端点（例如，SpEL表达式，如“isAuthenticated（）”） security.oauth2.authorization.realm 客户端身份验证的领域名称 security.oauth2.authorization.token-key-access Spring Security访问规则用于检查令牌端点（例如，SpEL表达式，如“isAuthenticated（）”） security.oauth2.client.access-token-uri security.oauth2.client.access-token-validity-seconds security.oauth2.client.additional-information security.oauth2.client.authentication-scheme security.oauth2.client.authorities security.oauth2.client.authorized-grant-types security.oauth2.client.auto-approve-scopes security.oauth2.client.client-authentication-scheme security.oauth2.client.client-id security.oauth2.client.client-secret security.oauth2.client.grant-type security.oauth2.client.id security.oauth2.client.pre- established-redirect-uri security.oauth2.client.refresh-token-validity-seconds security.oauth2.client.registered-redirect-uri security.oauth2.client.resource-ids security.oauth2.client.scope security.oauth2.client.token-name security.oauth2.client.use-current-uri security.oauth2.client.user-authorization-uri security.oauth2.resource.filter-order 0 用于验证令牌的过滤器链的顺序。 security.oauth2.resource.id 资源的标识符 security.oauth2.resource.jwk.key-set-uri 获取验证密钥以验证 JWT 令牌的 URI security.oauth2.resource.jwt.key-uri JWT 令牌的 URI security.oauth2.resource.jwt.key-value JWT 令牌的验证密钥 security.oauth2.resource.prefer-token-info true 使用令牌信息，可以设置为 false以使用用户信息 security.oauth2.resource.service-id resource security.oauth2.resource.token-info-uri token decoding 端点的 URI security.oauth2.resource.token-type 使用 userInfoUri 时要发送的令牌类型。 security.oauth2.resource.user-info-uri 用户端点的 URI security.oauth2.sso.filter-order 如果不提供显式的WebSecurityConfigurerAdapter，则应用过滤器顺序（在这种情况下，可以改为提供顺序）。 security.oauth2.sso.login-path /login 登录页面的路径，即触发重定向到 OAuth2 授权服务器的页面。 security.require-ssl security 自动配置已不再可定制 security.sessions security 自动配置已不再可定制 security.user.name spring.security.user.name security.user.password spring.security.user.password security.user.role spring.security.user.roles server.context-parameters server.servlet.context-parameters server.context-path server.servlet.context-path server.display-name server.servlet. application-display-name server.jsp-servlet.class-name server.servlet.jsp.class-name server.jsp-servlet.init-parameters server.servlet.jsp.init-parameters server.jsp-servlet.registered server.servlet.jsp.registered server.servlet-path server.servlet.path server.session.cookie.comment server.servlet.session.cookie.comment server.session.cookie.domain server.servlet.session.cookie.domain server.session.cookie.http-only server.servlet. session.cookie.http-only server.session.cookie.max-age server.servlet.session.cookie.max-age server.session.cookie.name server.servlet.session.cookie.name server.session.cookie.path server.servlet.session.cookie.path server.session.cookie.secure server.servlet.session.cookie.secure server.session.persistent server.servlet.session.persistent server.session.store-dir server.servlet.session.store-dir server.session.timeout server.servlet.session.timeout server.session.tracking-modes server.servlet.session.tracking-modes spring.activemq.pool.configuration.block-if-session-pool-is-full spring.activemq.pool.configuration.block-if-session-pool-is-full-timeout spring.activemq.pool.configuration.connection-factory spring.activemq.pool.configuration.create-connection-on-startup spring.activemq.pool.configuration.expiry-timeout spring.activemq.pool.configuration.idle-timeout spring.activemq.pool.configuration.max-connections spring.activemq.pool.configuration.maximum-active-session-per-connection spring.activemq.pool.configuration.properties spring.activemq.pool.configuration.reconnect-on-exception spring.activemq.pool.configuration.time-between-expiration-check-millis spring.activemq.pool.configuration.use-anonymous-producers spring.application.index 应用程序上下文 ID 默认情况下是唯一的 spring.batch.initializer.enabled spring.batch.initialize-schema spring.cache.guava.spec 用于创建缓存的规范 spring.cache.hazelcast.config 用于初始化 Hazelcast 的配置文件的位置 spring.data.cassandra.connect-timeout-millis spring.data. cassandra.connect-timeout spring.data.cassandra.read-timeout-millis spring.data.cassandra.read-timeout spring.data.cassandra.repositories.enabled spring.data.cassandra. repositories.type spring.data.couchbase.repositories.enabled spring.data.couchbase. repositories.type spring.data.mongodb.repositories.enabled spring.data.mongodb. repositories.type spring.data.neo4j.compiler 从 Neo4j 3 开始不再支持 spring.datasource.dbcp.access-to-underlying-connection-allowed spring.datasource.dbcp.connection-init-sqls spring.datasource.dbcp.default-auto-commit spring.datasource.dbcp.default-catalog spring.datasource.dbcp.default-read-only spring.datasource.dbcp.default-transaction-isolation spring.datasource.dbcp.driver-class-name spring.datasource.dbcp.initial-size spring.datasource.dbcp.log-abandoned spring.datasource.dbcp.login-timeout spring.datasource.dbcp.max-active spring.datasource.dbcp.max-idle spring.datasource.dbcp.max-open-prepared-statements spring.datasource.dbcp.max-wait spring.datasource.dbcp.min-evictable-idle-time-millis spring.datasource.dbcp.min-idle spring.datasource.dbcp.num-tests-per-eviction-run spring.datasource.dbcp.password spring.datasource.dbcp.pool-prepared-statements spring.datasource.dbcp.remove-abandoned spring.datasource.dbcp.remove-abandoned-timeout spring.datasource.dbcp.test-on-borrow spring.datasource.dbcp.test-on-return spring.datasource.dbcp.test-while-idle spring.datasource.dbcp.time-between-eviction-runs-millis spring.datasource.dbcp.url spring.datasource.dbcp.username spring.datasource.dbcp.validation-query spring.datasource.dbcp.validation-query-timeout spring.datasource.hikari.connection-customizer-class-name spring.datasource.initialize spring.datasource. initialization-mode spring.devtools.remote.debug.enabled 远程 debug 已不再支持 spring.devtools.remote.debug.local-port 远程 debug 已不再支持 spring.http.multipart.enabled spring.servlet.multipart.enabled spring.http.multipart.file-size-threshold spring.servlet.multipart.file-size-threshold spring.http.multipart.location spring.servlet.multipart.location spring.http.multipart.max-file-size spring.servlet.multipart.max-file-size spring.http.multipart.max-request-size spring.servlet. multipart.max-request-size spring.http.multipart.resolve-lazily spring.servlet.multipart.resolve-lazily spring.jpa.hibernate.naming.strategy Hibernate 4 的自动配置已不再提供 spring.jta.atomikos.properties.console-log-level warn spring.messages.cache-seconds spring.messages.cache-duration spring.metrics.export.aggregate.key-pattern Metrics support 现在使用千分尺 spring.metrics.export.aggregate.prefix Metrics support 现在使用千分尺 spring.metrics.export.delay-millis Metrics support 现在使用千分尺 spring.metrics.export.enabled Metrics support 现在使用千分尺 spring.metrics.export.excludes Metrics support 现在使用千分尺 spring.metrics.export.includes Metrics support 现在使用千分尺 spring.metrics.export.redis.key Metrics support 现在使用千分尺 spring.metrics.export.redis.prefix Metrics support 现在使用千分尺 spring.metrics.export.send-latest Metrics support 现在使用千分尺 spring.metrics.export.statsd.host management.metrics. export.statsd.host spring.metrics.export.statsd.port management.metrics. export.statsd.port spring.metrics.export.statsd.prefix Metrics support 现在使用千分尺 spring.metrics.export.triggers Metrics support 现在使用千分尺 spring.mobile.devicedelegatingviewresolver.enable-fallback false 启用对回退解决方案的支持 spring.mobile.devicedelegatingviewresolver.enabled false 启用 device 视图解析器 spring.mobile.devicedelegatingviewresolver.mobile-prefix mobile/ 用于查看移动设备名称的前缀 spring.mobile.devicedelegatingviewresolver.mobile-suffix `` 附加后缀以查看移动设备的名称 spring.mobile.devicedelegatingviewresolver.normal-prefix `` 用于查看普通设备名称的前缀. spring.mobile.devicedelegatingviewresolver.normal-suffix `` 附加后缀以查看普通设备的名称 spring.mobile.devicedelegatingviewresolver.tablet-prefix tablet/ 前缀预设为查看平板电脑设备的名称 spring.mobile.devicedelegatingviewresolver.tablet-suffix `` 附加后缀以查看平板电脑设备的名称 spring.mobile.sitepreference.enabled true 启用 SitePreferenceHandler. spring.mvc.media-types spring.mvc. contentnegotiation.media-types spring.rabbitmq.listener.acknowledge-mode spring.rabbitmq.listener.auto-startup spring.rabbitmq.listener.concurrency spring.rabbitmq.listener.default-requeue-rejected spring.rabbitmq.listener.idle-event-interval spring.rabbitmq.listener.max-concurrency spring.rabbitmq.listener.prefetch spring.rabbitmq.listener.retry.enabled false 是否启用发布重试 spring.rabbitmq.listener.retry.initial-interval 1000 第一次和第二次尝试发布或传递讯息的时间间隔 spring.rabbitmq.listener.retry.max-attempts 3 尝试发布或传递邮件的最大次数 spring.rabbitmq.listener.retry.max-interval 10000 尝试之间的最大间隔 spring.rabbitmq.listener.retry.multiplier 1 于先前重试间隔的倍数 spring.rabbitmq.listener.retry.stateless true 无论重试是无状态还是有状态 spring.rabbitmq.listener.transaction-size spring.redis.pool.max-active spring.redis.jedis.pool.max-idle spring.redis.pool.max-idle spring.redis.jedis.pool.max-idle spring.redis.pool.max-wait spring.redis.jedis.pool.max-wait spring.redis.pool.min-idle spring.redis.jedis.pool.min-idle spring.resources.cache-period spring.resources.cache.period spring.sendgrid.password 不再支持使用用户名和密码 ( 使用 spring.sendgrid.api-key 代替 ) spring.sendgrid.username 不再支持使用用户名和密码 ( 使用 spring.sendgrid.api-key 代替 ) spring.session.jdbc.initializer.enabled spring.session. jdbc.initialize-schema spring.session.mongo.collection-name spring.session.mongodb.collection-name spring.social.auto-connection-views false 为支持的生产者启用连接状态视图 spring.social.facebook.app-id Application id. spring.social.facebook.app-secret Application secret. spring.social.linkedin.app-id Application id. spring.social.linkedin.app-secret Application secret. spring.social.twitter.app-id Application id. spring.social.twitter.app-secret Application secret. spring.thymeleaf.content-type spring.thymeleaf. servlet.content-type 相关文章相关文章1、Spring Boot 2.0系列文章(一)：Spring Boot 2.0 迁移指南 2、Spring Boot 2.0系列文章(二)：Spring Boot 2.0 新特性详解 3、Spring Boot 2.0系列文章(三)：Spring Boot 2.0 配置改变 4、Spring Boot 2.0系列文章(四)：Spring Boot 2.0 源码阅读环境搭建 5、Spring Boot 2.0系列文章(五)：Spring Boot 2.0 项目源码结构预览 6、Spring Boot 2.0系列文章(六)：Spring boot 2.0 中 SpringBootApplication 注解详解 最后英文参考：https://github.com/spring-projects/spring-boot/wiki/Spring-Boot-2.0-Configuration-Changelog","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://www.54tianzhisheng.cn/tags/SpringBoot/"}]},{"title":"写这么多系列博客，怪不得找不到女朋友","date":"2018-03-26T16:00:00.000Z","path":"2018/03/27/blogs/","text":"前提好几周没更新博客了，对不断支持我博客的童鞋们说声：“抱歉了！”。自己这段时间确实比较忙，而且还在抽空完成学校的毕业设计。今天晚上抽空把大学期间写过的博客弄一个系列文章合集，算是对大学这四年的一个总结，证明自己没白过。 熟悉我的人都知道我写博客的时间比较早，而且坚持的时间也比较久，一直到现在也是一直保持着更新状态。最早最早开始写博客是在 CSDN 上写的，然后在简书也写过一段时间，后来放弃了简书转战了掘金，以下图片是自己在掘金这一年的成果，快 1.5 万关注了，哈哈哈。 去年过年前还收到掘金送来的专属礼物，真是激动，感谢掘金，希望越办越好！ 细数文章后，发现自己在实习的这段时间写的博客也挺多的，而且质量还比较高，经常上开发者头条、掘金等平台的首页推荐。在此，感谢实习期间组内大佬们的各种帮助！ 这里再次说下写博客的好处： 很好的用来总结自己所学的知识 遇到那么一群也写博客的大佬，有共同话题聊了 面试加分（在简历上放上自己的个人网站链接，面试官就可以更好的了解你，知道你所学知识的深度和广度） 不要小看你的每一篇不起眼博客，用一个蚂蚁金服大佬跟我说的话叫做：厚积薄发！ 不多说了，如果想和我交流的可以加我 qq 群：528776268 和我的微信：zhisheng_tian 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/03/27/blogs/ 系列文章合集Spring Boot 系列文章1、Spring Boot系列文章（一）：SpringBoot Kafka 整合使用 2、Spring Boot系列文章（二）：SpringBoot Admin 使用指南 3、Spring Boot系列文章（三）：SpringBoot RabbitMQ 整合使用 4、Spring Boot系列文章（四）：SpringBoot ActiveMQ 整合使用 5、Spring Boot系列文章（五）：SpringBoot RabbitMQ 整合进阶版 6、Spring Boot系列文章（六）：SpringBoot RocketMQ 整合使用和监控 7、更多请期待 Spring Boot 2.0 系列文章1、Spring Boot 2.0系列文章(一)：Spring Boot 2.0 迁移指南 2、Spring Boot 2.0系列文章(二)：Spring Boot 2.0 新特性详解 3、后面绝对有更多文章出现的 Docker 系列文章1、Docker系列文章（一）：基于 Harbor 搭建 Docker 私有镜像仓库 2、Docker系列文章（二）：Mac 安装 Docker 及常用命令 3、同样，后面也会持续更新 ElasticSearch 系列文章1、Elasticsearch 系列文章（一）：Elasticsearch 默认分词器和中分分词器之间的比较及使用方法 2、 Elasticsearch 系列文章（二）：全文搜索引擎 Elasticsearch 集群搭建入门教程 3、Elasticsearch 系列文章（三）：ElasticSearch 集群监控 4、Elasticsearch 系列文章（四）：ElasticSearch 单个节点监控 5、Elasticsearch 系列文章（五）：ELK 实时日志分析平台环境搭建 6、也有更多深入的文章 搭建博客系列文章1、利用Github Page 搭建个人博客网站 2、Github pages + Hexo 博客 yilia 主题使用畅言评论系统 3、Hexo + yilia 搭建博客可能会遇到的所有疑问 4、Hexo + yilia 主题实现文章目录 5、这个系列看情况，可能还会有 Java 系列文章1、关于String s = new String(“xyz”); 创建几个对象的问题 2、《Java 多线程编程核心技术》学习笔记及总结 3、从对象深入分析 Java 中实例变量和类变量的区别 4、深度探究Java 中 finally 语句块 5、解决jdk1.8中发送邮件失败（handshake_failure）问题 6、深入分析 Java Web 中的中文编码问题 7、奇怪的Java题：为什么128 == 128返回为False，而127 == 127会返回为True? 8、Java读取文件 9、HashMap、Hashtable、HashSet 和 ConcurrentHashMap 的比较 10、Java连接Oracle数据库的三种连接方式 11、Java NIO 系列教程 12、《疯狂 Java 突破程序员基本功的 16 课》读书笔记 13、详细深入分析 Java ClassLoader 工作机制 14、详解 Filter 过滤器 15、Java IO流学习超详细总结（图文并茂） 16、通过源码详解 Servlet 17、Java 性能调优需要格外注意的细节 18、Java 线程池艺术探索 19、JVM性能调优监控工具jps、jstack、jmap、jhat、jstat等使用详解 20、这个必须的持续更新下去 Maven 系列文章1、Centos7 搭建最新 Nexus3 Maven 私服 2、Maven 中 dependencies 与 dependencyManagement 的区别 Kafka 系列文章1、Kafka 安装及快速入门 2、Spring Boot系列文章（一）：SpringBoot Kafka 整合使用 Mybatis 系列文章1、通过项目逐步深入了解Mybatis&lt;一&gt; 2、通过项目逐步深入了解Mybatis&lt;二&gt; 3、通过项目逐步深入了解Mybatis&lt;三&gt; 4、通过项目逐步深入了解Mybatis（四)/) 5、MyBatis的foreach语句详解 6、期待它的源码解析文章吗？ Nginx 系列文章1、Ubuntu16.10 安装 Nginx 2、Nginx 基本知识快速入门 Python 爬虫系列文章1、Python爬虫实战之爬取百度贴吧帖子 2、Pyspider框架 —— Python爬虫实战之爬取 V2EX 网站帖子 3、Python爬虫实战之爬取糗事百科段子 4、这个估计得等有机会再次学 Python 时再写 RocketMQ 系列文章1、RocketMQ系列文章（一）：RocketMQ 初探 2、RocketMQ系列文章（二）：RocketMQ 安装及快速入门 3、RocketMQ系列文章（三）：RocketMQ 简单的消息示例 4、Spring Boot系列文章（六）：SpringBoot RocketMQ 整合使用和监控 Spring MVC 系列文章1、Spring MVC系列文章（一）：Spring MVC + Hibernate JPA + Bootstrap 搭建的博客系统 Demo 2、Spring MVC系列文章（二）：Spring MVC+Hibernate JPA搭建的博客系统项目中所遇到的坑 3、Spring MVC系列文章（三）：看透 Spring MVC 源代码分析与实践 —— 网站基础知识 4、Spring MVC系列文章（四）：看透 Spring MVC 源代码分析与实践 —— 俯视 Spring MVC 5、Spring MVC系列文章（五）：看透 Spring MVC 源代码分析与实践 —— Spring MVC 组件分析 6、通过项目逐步深入了解Spring MVC（一） Netty 系列文章1、Netty系列文章（一）：Netty 源码阅读之初始环境搭建 2、这个系列迟早会更新的。 前端系列文章1、Bootstrap入门需掌握的知识点（一） 2、Bootstrap入门需掌握的知识点（二） 3、使用 CodeMirror 打造属于自己的在线代码编辑器 4、AJAX 学习 5、前端渣渣这个也要慢慢学习这块 面试经验系列1、秋招第一站 —— 亚信科技 2、秋招第二站 —— 内推爱奇艺（一面二面） 3、秋招第三站 —— 内推阿里（一面） 4、 面试过阿里等互联网大公司，我知道了这些套路 5、那些年我看过的书 —— 致敬我的大学生活 —— Say Good Bye ！ 其他还有一些其他方面的技术文章，算不是系列文章，比较零散，还有就是一些随笔文章，就不把它们放在合集里了。 总结用自己的一句话：坑要一个个填，路要一步步走！前人栽树，后人乘凉，学会感恩！ 建了个不错的微信群，如果有感兴趣的可以加我微信，对我回复 加群 ，然后会拉你进群交流。","tags":[{"name":"博客合集","slug":"博客合集","permalink":"http://www.54tianzhisheng.cn/tags/博客合集/"}]},{"title":"Spring Boot 2.0系列文章(二)：Spring Boot 2.0 新特性详解","date":"2018-03-05T16:00:00.000Z","path":"2018/03/06/SpringBoot2-new-features/","text":"背景在 3 月 1 号，Spring Boot2.0.0.RELEASE正式发布，这是 Spring Boot1.0 发布 4 年之后第一次重大修订，因此有多的新功能和特性值得大家期待！下面带大家了解下 Spring Boot 2.0 中的新特性。 SpringBoot 系列文章 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/03/06/SpringBoot2-new-features/ 从 Spring Boot 1.5 升级由于 Spring Boot 2.0 的改变幅度有点大，所以升级现有的程序可能会比平常更大一些。 如果你还在考虑是否要升级，这里推荐 DD 的博客文章：Spring Boot 2.0 正式发布，升还是不升呢？ 如果要升级可以参考我的另外一篇文章：Spring Boot 2.0 迁移指南 如果您目前正在运行较早版本的 Spring Boot，我们强烈建议您在迁移到 Spring Boot 2.0 之前先升级到 Spring Boot 1.5。 新的和值得注意的特性小技巧：检查 配置更改日志 来获取配置更改的完整描述。 起码 JDK 8 和支持 JDK 9Spring Boot 2.0 要求 Java 8 作为最低版本。许多现有的 API 已更新，以利用 Java 8 的特性，例如：接口上的默认方法，函数回调以及新的 API，如javax.time。如果您当前正在使用 Java 7 或更早版本，则在开发 Spring Boot 2.0 应用程序之前，您需要升级您的 JDK。 Spring Boot 2.0 通过了在 JDK 9 下的测试，可以在 JDK 9 下正常运行，。我们所有的 jar 包都在模块系统兼容性的清单中附带了自动模块名称条目。 第三方库的升级Spring Boot 2.0 建立在 Spring Framework 5 之上，并且需要 Spring Framework 5 。你可以通过 What’s New in Spring Framework 5.x 了解 Spring 5 的新特性。并在继续之前查看其升级指南 Upgrading to Spring Framework 5.x 。 我们已尽可能升级到其他第三方库的最新稳定版本。 本版本中一些显着的依赖性升级包括： Tomcat 8.5 Flyway 5 Hibernate 5.2 Thymeleaf 3 Reactive SpringSpring 产品组合中的许多项目现在都为开发反应式应用程序提供一流的支持。反应性应用程序是完全异步和非阻塞的。它们旨在用于事件循环执行模型（而不是更传统的每个请求线程执行模型）。Spring 框架参考文档中的“Web 反应堆栈”部分为这个主题提供了一个很好的入门。 Spring Boot 2.0 通过自动配置和启动器 POM 完全支持反应式应用。Spring Boot 的内部本身也在必要时进行了更新，以提供反应性的反应（最明显的是我们的嵌入式服务器支持）。 Spring WebFlux＆WebFlux.fnSpring WebFlux 是 Spring MVC 的完全非阻塞反应式替代方案。Spring Boot 为基于注释的 Spring WebFlux 应用程序以及 WebFlux.fn 提供了自动配置，WebFlux.fn 提供了更实用的样式 API。 要开始，请添加 spring-boot-starter-webflux 到 POM，它将提供由嵌入式 Netty 服务器支持的 Spring WebFlux。 Reactive Spring Data在底层技术支持的情况下，Spring Data 还为反应式应用程序提供支持。目前 Cassandra，MongoDB，Couchbase 和 Redis 都有反应式 API 支持。 Spring Boot 包含针对这些技术的特殊 starter-POMs，可为您提供启动所需的一切。例如，spring-boot-starter-data-mongodb-reactive包括对反应性 mongo 驱动程序和项目反应堆的依赖性。 Reactive Spring SecuritySpring Boot 2.0 可以充分利用 Spring Security 5.0 来保护您的反应式应用程序。当 Spring Security 位于类路径中时，会为 WebFlux 应用程序提供自动配置。 使用 WebFlux 的 Spring Security 访问规则可以通过SecurityWebFilterChain。如果你之前整合过 Spring MVC 和 Spring Security，应该会感到非常熟悉。有关更多详细信息，请参阅 Spring Boot 参考文档和 Spring Security 文档。 嵌入式 Netty 服务器由于 WebFlux 不依赖于 Servlet API，我们现在可以首次为 Netty 作为嵌入式服务器提供支持。该spring-boot-starter-webflux 启动 POM 将拉取 Netty 4.1 和 Ractor Netty 。 注意：您只能将 Netty 用作反应式服务器。不提供阻止 servlet API 支持。 HTTP/2 支持为 Tomcat，Undertow 和 Jetty 提供 HTTP / 2 支持。支持取决于所选的 Web 服务器和应用程序环境（因为 JDK 8 不支持该协议）。 如何配置 HTTP／2，请参考 官方文档 。 配置属性的绑定在 Spring Boot 2.0 中，用于绑定Environment属性的机制@ConfigurationProperties已经完全彻底修改。我们借此机会收紧了松散绑定的规则，并修复了 Spring Boot 1.x 中的许多不一致之处。 新的BinderAPI 也可以@ConfigurationProperties直接在你自己的代码之外使用。例如，下面将结合到List的PersonName对象： 123List&lt;PersonName&gt; people = Binder.get(environment) .bind(\"my.property\", Bindable.listOf(PersonName.class)) .orElseThrow(IllegalStateException::new); 配置源可以像这样在 YAML 中表示： 123456my: property: - first-name: zhisheng last-name: tian - first-name: zhisheng last-name: tian 有关更新绑定规则的更多信息，请参阅此Wiki页面。 配置起源YAML 文件和被 Spring Boot 加载的 Properties 文件现在包含Origin信息，可帮助您跟踪项目从何处加载的信息。有些 Spring Boot 特性利用了这个信息可以在适当的时候展示出来。 例如，BindException绑定失败时抛出的类是一个OriginProvider。这意味着原始信息可以很好地从故障分析器中显示出来。 另一个例子是env执行器端点，当它有可用时包含了原始信息。下面的代码片断显示该spring.security.user.name属性来自 jar 包中的 application.properties 文件的第 1行，第 27 列。 123456789&#123; \"name\": \"applicationConfig: [classpath:/application.properties]\", \"properties\": &#123; \"spring.security.user.name\": &#123; \"value\": \"user\", \"origin\": \"class path resource [application.properties]:1:27\" &#125; &#125;&#125; 转换器支持Binding 利用了一个新的 ApplicationConversionService 类，它提供了一些对属性绑定特别有用的额外转换器。最引人注目的是转换器的Duration类型和分隔字符串。 该Duration转换器允许在任一 ISO-8601 格式中指定的持续时间，或作为一个简单的字符串（例如10m，10 分钟）。现有的属性已更改为始终使用Duration。该@DurationUnit注释通过设置如果没有指定所使用的单元确保向后兼容性。例如，Spring Boot 1.5 中需要秒数的属性现在必须@DurationUnit(ChronoUnit.SECONDS)确保一个简单的值，例如10实际使用的值10s。 分隔字符串转换允许您将简单绑定String到Collection或Array不必分割逗号。例如，LDAP base-dn 属性用 @Delimiter(Delimiter.NONE)，所以 LDAP DN（通常包含逗号）不会被错误解释。 Gradle 插件Spring Boot 的 Gradle 插件已在很大程度上进行了重新编写，以实现许多重大改进。您可以在其参考文献和 API 文档中阅读关于插件功能的更多信息。 Spring Boot 现在需要 Gradle 4.x. 如果您要升级使用 Gradle 的项目，请查看迁移指南。 KotlinSpring Boot 2.0 现在包含对 Kotlin 1.2.x 的支持，并提供了runApplication ，一个使用 Kotlin 运行 Spring Boot 应用程序的方法。我们还公开和利用了 Kotlin 对其他 Spring 项目（如Spring Framework，Spring Data 和 Reactor）已添加到其最近版本中的支持。 有关更多信息，请参阅参考文档的Kotlin支持部分。 Actuator 改进在 Spring Boot 2.0 中 Actuator endpoints 有很大的改进。所有 HTTP Actuator endpoints 现在都在该/actuator路径下公开，并且生成的 JSON 有效负载得到了改进。 我们现在也不会在默认情况下暴露很多端点。如果您要升级现有的 Spring Boot 1.5 应用程序，请务必查看迁移指南并特别注意该management.endpoints.web.exposure.include属性。 Actuator JSONSpring Boot 2.0 改进了从许多端点返回的 JSON 有效负载。 现在许多端点都具有更精确地反映底层数据的 JSON。例如，/actuator/conditions终端（/autoconfig在Spring Boot 1.5中）现在有一个顶级contexts密钥来将结果分组ApplicationContext。 现在还使用 Spring REST Docs 生成了广泛的 REST API 文档，并随每个版本发布。 Jersey and WebFlux 支持除了支持 Spring MVC 和 JMX，您现在可以在开发 Jersey 或 WebFlux 应用程序时访问执行器端点。Jersey 支持通过自定义 Jersey 提供Resource，WebFlux 使用自定义HandlerMapping。 Hypermedia links该/actuator端点现在提供了一个 HAL 格式的响应提供链接到所有活动端点（即使你没有 Spring HATEOAS 在classpath）。 Actuator @Endpoints为了支持 Spring MVC，JMX，WebFlux 和 Jersey，我们为 Actuator @Endpoints 开发了一种新的编程模型。该@Endpoint注解可以与@ReadOperation，@WriteOperation 和 @DeleteOperation 组合使用开发 endpoints。 您还可以使用@EndpointWebExtension或@EndpointJmxExtension编写技术特定的增强功能到 endpoints。详细信息请参阅更新的参考文档。 MicrometerSpring Boot 2.0 不再提供自己的指标 API。相反，我们依靠 micrometer.io 来满足所有应用程序监视需求。 Micrometer 包括尺寸指标的支持，当与尺寸监测系统配对时，尺寸指标可以有效访问特定的指定度量标准，并且可以在其尺寸范围内向下钻取。 指标可以输出到各种系统和开箱即用的 Spring Boot 2.0，为 Atlas，Datadog，Ganglia，Graphite，Influx，JMX，New Relic，Prometheus，SignalFx，StatsD 和 Wavefront 提供支持。另外还可以使用简单的内存中度量标准。 集成随 JVM 指标（包括 CPU，内存，线程和 GC），Logback，Tomcat，Spring MVC＆提供RestTemplate。 有关更多详细信息，请参阅参考文档的更新“指标”部分。 数据支持除了上面提到的 Reactive Spring Data 支持外，在数据领域还进行了其他一些更新和改进。 HikariCPSpring Boot 2.0 中的默认数据库池技术已从 Tomcat Pool 切换到 HikariCP。我们发现 Hakari 提供了卓越的性能，我们的许多用户更喜欢 Tomcat Pool。 初始化数据库初始化逻辑在 Spring Boot 2.0 中已经合理化。Spring Batch，Spring Integration，Spring Session 和 Quartz的初始化现在仅在使用嵌入式数据库时才会默认发生。该enabled属性已被替换为更具表现力枚举。例如，如果你想一直执行 Spring Batch 的初始化，您可以设置spring.batch.initialize-schema=always。 如果 Flyway 或 Liquibase 正在管理您的 DataSource 的模式，并且您正在使用嵌入式数据库，Spring Boot 现在会自动关闭 Hibernate 的自动 DDL 功能。 JOOQSpring Boot 2.0 现在基于 DataSource 自动检测 JOOQ 方言（类似于为 JPA 方言所做的）。@JooqTest是新引入的注解用来简化那些只有 JOOQ 必须被使用的测试。 JdbcTemplateSpring Boot 自动配置的 JdbcTemplate 现在可以通过 spring.jdbc.template 属性进行自定义。此外，NamedParameterJdbcTemplate自动配置的内容会重用JdbcTemplate。 Spring Data Web 配置Spring Boot 公开了一个新的spring.data.web配置名称空间，可以轻松配置分页和排序。 InfluxDBSpring Boot 现在自动配置开源时间序列数据库 InfluxDB。要启用 InfluxDB 支持，您需要设置一个spring.influx.url属性，并将其包含influxdb-java在您的类路径中。 Flyway/Liquibase 灵活配置如果仅提供自定义url或user属性，则 Flyway 和 Liquibase 的自动配置现在将重用标准数据源属性，而不是忽略它们。这使您可以创建一个自定义的数据源，仅用于所需信息的迁移。 Hibernate现在支持自定义 Hibernate 命名策略。对于高级场景，现在可以在上下文中定义ImplicitNamingStrategy或PhysicalNamingStrategy使用常规 bean。 现在也可以通过公开HibernatePropertiesCustomizerbean 来更加细致地定制 Hibernate 使用的属性。 MongoDB 客户端自定义现在可以通过定义一个类型的 bean 来为 Spring Boot 自动配置的 Mongo 客户端应用高级定制MongoClientSettingsBuilderCustomizer。 Redis现在可以使用spring.cache.redis.*属性配置 Redis 的缓存默认值。 Web除了上面提到的 WebFlux 和 WebFlux.fn 支持之外，还在开发 Web 应用程序时进行了以下改进。 上下文路径记录当使用嵌入式容器时，当您的应用程序启动时，上下文路径将与 HTTP 端口一起记录。例如，嵌入式 Tomcat 现在看起来像这样： 1Tomcat 在端口上启动：8080（http），其上下文路径为 &apos;/foo&apos; Web过滤器初始化Web 过滤器现在在所有支持的容器上急切地初始化。 ThymeleafThymeleaf 初始化现在包括thymeleaf-extras-java8time，提供javax.time类型支持。 JSON 支持新的spring-boot-starter-json起始者收集必要的位以读取和写入 JSON。它不仅提供了jackson-databind与Java8 工作时，也是有用的模块：jackson-datatype-jdk8，jackson-datatype-jsr310和jackson-module-parameter-names。这个新的起动器现在被用于jackson-databind之前定义的地方。 如果您更喜欢 Jackson 之外的其他产品，我们对 GSON 的支持在 Spring Boot 2.0 已经大大提高。我们还引入了对 JSON-B 的支持（包括 JSON-B 测试支持）。 Quartz自动配置支持目前包含了 Quartz Scheduler。我们还添加了新的spring-boot-starter-quartz 初始化 POM。 您可以使用内存JobStores中或完整的基于 JDBC 的存储。所有JobDetail，Calendar并Trigger从你的 Spring应用程序上下文豆将自动注册Scheduler。 有关更多详细信息，请阅读参考文档的新“Quartz Scheduler”部分。 测试对 Spring Boot 2.0 中提供的测试支持进行了一些补充和调整： @WebFluxTest已添加新注释以支持 WebFlux 应用程序的“切片”测试。 Converter和GenericConverter豆类现在自动扫描@WebMvcTest和@WebFluxTest。 @AutoConfigureWebTestClient已经添加了一个注释来提供一个WebTestClientbean 供测试使用。注释会自动应用于@WebFluxTest测试。 增加了一个新的ApplicationContextRunner测试实用程序，可以很容易地测试您的自动配置。我们已将大部分内部测试套件移至此新模型。详细信息请参阅更新的文档。 其它除了上面列出的变化外，还有很多小的调整和改进，包括： @ConditionalOnBean现在在确定是否满足条件时使用逻辑AND而不是逻辑OR。 无条件类现在包含在自动配置报告中。 该springCLI 应用程序现在包括encodepassword可用于创建 Spring Security 的兼容散列密码命令。 计划任务（即 @EnableScheduling）可以使用scheduledtasks执行器端点进行审查。 该loggers驱动器终端现在允许你重新设置一个记录器级别为它的默认。 Spring Session 用户现在可以通过sessions执行器端点查找和删除会话。 使用spring-boot-starter-parent现在基于 Maven 的应用程序-parameters默认使用标志。 我们的构建现在使用 concourse 的 CI 和我们的项目 POM 文件已被重构，使它们更简单的。 动画 ASCII 艺术最后，为了好玩，Spring Boot 2.0 现在支持动画 GIF 横幅。 参考资料https://github.com/spring-projects/spring-boot/wiki/Spring-Boot-2.0-Release-Notes 相关文章1、Spring Boot 2.0系列文章(一)：Spring Boot 2.0 迁移指南 2、Spring Boot 2.0系列文章(二)：Spring Boot 2.0 新特性详解 3、Spring Boot 2.0系列文章(三)：Spring Boot 2.0 配置改变 4、Spring Boot 2.0系列文章(四)：Spring Boot 2.0 源码阅读环境搭建 5、Spring Boot 2.0系列文章(五)：Spring Boot 2.0 项目源码结构预览 6、Spring Boot 2.0系列文章(六)：Spring boot 2.0 中 SpringBootApplication 注解详解 7、Spring Boot 2.0系列文章(七)：SpringApplication 深入探索","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://www.54tianzhisheng.cn/tags/SpringBoot/"}]},{"title":"Spring Boot 2.0系列文章(一)：Spring Boot 2.0 迁移指南","date":"2018-03-05T16:00:00.000Z","path":"2018/03/06/SpringBoot2-Migration-Guide/","text":"前提希望本文档将帮助您把应用程序迁移到 Spring Boot 2.0。 SpringBoot 系列文章 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/03/06/SpringBoot2-Migration-Guide/ 在你开始之前首先，Spring Boot 2.0 需要 Java 8 或更高版本。不再支持 Java 6 和 7 了。 在 Spring Boot 2.0 中，许多配置属性被重新命名/删除，开发人员需要更新application.properties/ application.yml相应的配置。为了帮助你解决这一问题，Spring Boot 发布了一个新spring-boot-properties-migrator模块。一旦作为该模块作为依赖被添加到你的项目中，它不仅会分析应用程序的环境，而且还会在启动时打印诊断信息，而且还会在运行时为您暂时迁移属性。在您的应用程序迁移期间，这个模块是必备的： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-properties-migrator&lt;/artifactId&gt;&lt;/dependency&gt; 注意：完成迁移后，请确保从项目的依赖关系中删除此模块。 构建您的 Spring Boot 应用程序Spring Boot Maven 插件为了保持了一致性，并且避免与其他插件发生冲突，现在暴露的插件配置属性都以一个spring-boot前缀开始。 例如，以下命令prod使用命令行启用配置文件 1mvn spring-boot:run -Dspring-boot.run.profiles=prod Surefire 默认值以前的 include/exclude 模式已与最新的 Surefire 默认设置保持一致。如果依赖于此插件，需要相应地更新插件配置。之前对应的配置如下： 12345678910111213&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;includes&gt; &lt;include&gt;**/*Tests.java&lt;/include&gt; &lt;include&gt;**/*Test.java&lt;/include&gt; &lt;/includes&gt; &lt;excludes&gt; &lt;exclude&gt;**/Abstract*.java&lt;/exclude&gt; &lt;/excludes&gt; &lt;/configuration&gt;&lt;/plugin&gt; PS: 如果您使用 JUnit 5，则应将 Surefire 降级到 2.19.1。该**/*Tests.java版本不包含此模式，因此如果您依赖该模式，请确保将其添加到您的配置中。 Spring Boot Gradle 插件Spring Boot 的 Gradle 插件在很大程度上已被重写，有了重大的改进。您可以在其参考文献和API文档中阅读关于插件功能的更多信息。 依赖管理Spring Boot 的 Gradle 插件不再自动应用依赖管理插件。相反，Spring Boot 的插件现在可以通过导入正确版本的spring-boot-dependencies BOM 来应用依赖管理插件。当依赖管理被配置的时候，这一点会让你有更多的控制权。 对于大多数应用程序，使用应用依赖管理插件就足够了： 12apply plugin: &apos;org.springframework.boot&apos;apply plugin: &apos;io.spring.dependency-management&apos; // &lt;-- add this to your build.gradle 注意：依赖管理插件仍然是 spring-boot-gradle-plugin 的传递依赖项，所以不需要在 buildscript 配置中将其列为类路径依赖项。 建立可执行的 Jars 和 Wars bootRepackage 任务已经被替换成 bootJar 和 bootWar 任务，分别用于构建可执行的 jar 包和 war包。 配置更新BootRun，BootJar和BootWar任务现在都使用mainClassName的属性来配置主类的名称。这使得三个特定于引导的任务相互一致，并将其与 Gradle 自己的应用程序插件进行对齐。 Spring Boot 特性默认动态代理策略Spring Boot 默认使用 CGLIB 做动态代理代理(基于类的动态代理)，包括对 AOP 的支持。如果你需要基于接口的动态代理，你需要将spring.aop.proxy-target-class 设置为false。 SpringApplicationWeb 环境Spring Boot 应用程序现在可以在更多模式下运行，因此spring.main.web-environment现在不推荐使用，spring.main.web-application-type属性可以提供更多的支持。 如果您想确保应用程序不启动 Web 服务器，则必须将该属性更改为： 1spring.main.web-application-type=none 注意：可以通过 SpringApplication 的 setWebApplicationType 方法实现。 Spring Boot 应用程序事件更改我们已经添加了一个新事件ApplicationStartedEvent。 ApplicationStartedEvent在上下文刷新之后但在任何应用程序和命令行参数被调用之前发送。 ApplicationReadyEvent在任何应用程序和命令行参数被调用后发送。它表示应用程序已准备好为请求提供服务。 请参阅更新的参考文档。 Banner在我们限制 Spring Boot 使用的根名称空间的数量的过程中，与标志相关的属性已被重定位到spring.banner。 外部化配置轻松的绑定有关宽松绑定的规则已经收紧。我们假设一个现有的acme.my-project.my-name属性： 所有前缀必须是 kebab格式（小写，连字符分隔）acme.myProject或acme.my_project无效 - 您必须acme.my-project在此处使用。 属性名称可以使用 kebab-case（my-name），camel-case（myName）或 snake-case（my_name）。 环境属性（来自操作系统环境变量）必须使用通常的大写下划线格式，下划线只能用于分隔键的各个部分ACME_MYPROJECT_MYNAME。 这种新的放松绑定具有以下几个优点： 无需担心密钥的结构@ConditionalOnProperty：只要密钥是以规范格式定义的，支持的松散变体就可以透明地工作。如果您正在使用该prefix属性，则现在只需使用name或value属性即可放置完整密钥。 RelaxedPropertyResolver不再可以Environment自动处理：env.getProperty(&quot;com.foo.my-bar&quot;)将找到一个com.foo.myBar属性。 该org.springframework.boot.bind软件包不再可用，并被新的宽松绑定规则所取代。特别是，RelaxedDataBinder朋友已被新的BinderAPI 取代。以下样品MyProperties从app.acme前缀中进行绑定。 123MyProperties target = Binder.get(environment) .bind(\"app.acme\", MyProperties.class) .orElse(null); 由于现在内置了轻松绑定，因此只要使用其中一种支持的格式，就可以请求任何属性而不必关心案例： 123FlagType flagType = Binder.get(environment) .bind(\"acme.app.my-flag\", FlagType.class) .orElse(FlagType.DEFAULT); @ConfigurationProperties 验证如果您想打开验证，现在必须为您的@ConfigurationProperties对象添加注释@Validated。 配置位置spring.config.location配置的方式已被修复; 它提前将一个位置添加到默认位置列表中，现在它将替换默认位置。如果你是按照以前的方式进行处理，现在应该使用它spring.config.additional-location进行替换。 开发 Web 应用程序嵌入式容器包装结构为了支持响应式用例，嵌入式容器包结构已经被大幅度的重构。 EmbeddedServletContainer已被重新命名为，WebServer并且该org.springframework.boot.context.embedded包已被重新定位到org.springframework.boot.web.embedded。例如，如果您使用TomcatEmbeddedServletContainerFactory回调接口定制嵌入式 Tomcat 容器，则应该使用TomcatServletWebServerFactory。 特定于 Servlet 的服务器属性许多server.* 属性 ( Servlet 特有的) 已经转移到server.servlet： 旧的属性 新的属性 server.context-parameters.* server.servlet.context-parameters.* server.context-path server.servlet.context-path server.jsp.class-name server.servlet.jsp.class-name server.jsp.init-parameters.* server.servlet.jsp.init-parameters.* server.jsp.registered server.servlet.jsp.registered server.servlet-path server.servlet.path Web Starter 作为传递依赖以前有几个 Spring Boot starter 是依赖于 Spring MVC 而传递的spring-boot-starter-web。在 Spring WebFlux 新的支持下，spring-boot-starter-mustache，spring-boot-starter-freemarker并spring-boot-starter-thymeleaf不再依赖它。开发者有责任选择和添加spring-boot-starter-web或spring-boot-starter-webflux。 模板引擎Mustache 模板曾经的文件扩展名是.html，现在的扩展名为 .mustache ，与官方规范和大多数 IDE 插件一致。您可以通过更改spring.mustache.suffix配置键来覆盖此新的默认值。 Jackson / JSON 支持在 2.0 中，我们改变了 Jackson 配置的默认值，将 ISO-8601 字符串 写为 JSR-310 日期 。如果你想回到以前的行为，你可以添加spring.jackson.serialization.write-dates-as-timestamps=true到你的配置。 新的spring-boot-starter-json starter 收集了必要的位去读写 JSON。它不仅提供了jackson-databind，而且提供了和 Java8 一起运作的时候相当有用的组件：jackson-datatype-jdk8, jackson-datatype-jsr310 和 jackson-module-parameter-names。如果你曾经手动地依赖这些组件，现在可以依赖这个新的 starter 取代。 Spring MVC 路径匹配默认行为更改我们已决定在 Spring MVC 应用程序中更改后缀路径匹配的默认值（请参阅＃11105）。按照 Spring Framework 中记录的最佳实践，此功能不再默认启用。 如果您的应用程序希望将请求&quot;GET /projects/spring-boot.json&quot;映射到@GetMapping(&quot;/projects/spring-boot&quot;)映射，则此更改会影响您。 有关此更多信息以及如何减轻此更改，请查阅Spring Boot中有关路径匹配和内容协商的参考文档。 Servlet 过滤器Servlet 过滤器的默认调度程序类型现在是DipatcherType.REQUEST; 这使 Spring Boot 的默认值与 Servlet 规范的默认值一致。如果您希望将过滤器映射到其他调度程序类型，请使用FilterRegistrationBean注册您的过滤器。 注意：Spring Security 和 Spring Session 过滤器配置 ASYNC, ERROR以及 REQUEST 调度类型。 RestTemplateBuilder该requestFactory(ClientHttpRequestFactory)方法已被新requestFactory(Supplier&lt;ClientHttpRequestFactory&gt; requestFactorySupplier)方法所取代。Supplier允许构建器生成的每个模板使用它自己的请求工厂，从而避免共享工厂可能导致的副作用。见＃11255。 WebJars 定位器Spring Boot 1.x 使用并提供依赖关系管理org.webjars:webjars-locator。webjars-locator是一个“命名不佳的库……包装webjars-locator-core项目”。org.webjars:webjars-locator应该更新依赖项来org.webjars:webjars-locator-core代替使用。 SecuritySpring Boot 2 极大地简化了默认的安全配置，并使添加定制安全变得简单。Spring Boot 现在具有一种行为，只要您添加自己的 WebSecurityConfigurerAdapter 就会退出，而不是进行多种与安全性相关的自动配置。 如果您使用以下任何属性，则会受到影响： 123456789101112131415security.basic.authorize-modesecurity.basic.enabledsecurity.basic.pathsecurity.basic.realmsecurity.enable-csrfsecurity.headers.cachesecurity.headers.content-security-policysecurity.headers.content-security-policy-modesecurity.headers.content-typesecurity.headers.framesecurity.headers.hstssecurity.headers.xsssecurity.ignoredsecurity.require-sslsecurity.sessions 默认安全安全自动配置不再公开选项，并尽可能使用 Spring Security 默认值。一个明显的副作用是使用 Spring Security 的内容协商进行授权（表单登录）。 默认用户默认情况下，Spring Boot 使用生成的密码配置单个用户。用户可以使用 spring.security.user.* 属性进行配置。要进一步定制用户或添加其他用户，您将不得不公开一个UserDetailsServicebean。 AuthenticationManager Bean如果您想将 Spring Security AuthenticationManager作为 bean 公开，请覆盖authenticationManagerBean您的方法WebSecurityConfigurerAdapter并为其添加注释@Bean。 OAuth2从功能的 Spring Security OAuth 项目 迁移到核心 Spring Security。不再为依赖关系提供依赖管理，Spring Boot 2 通过 Spring Security 5 提供 OAuth 2.0 客户端支持。 如果您依赖尚未迁移的 Spring Security OAuth 功能，则需要在其他 jar 上添加依赖项，请查看文档以获取更多详细信息。我们还继续支持 Spring Boot 1.5，以便旧版应用程序可以继续使用它，直到提供升级路径。 执行器安全执行器不再有单独的安全自动配置（management.security.*属性消失）。sensitive每个端点的标志也没有在安全配置中变得更加明确。如果您依赖于此行为，则需要创建或调整您的安全配置，以保护您选择角色的端点。 例如，假设以下配置： 123endpoints.flyway.sensitive=falseendpoints.info.sensitive=truemanagement.security.roles=MY_ADMIN 12345http .authorizeRequests() .requestMatchers(EndpointRequest.to(\"health\", \"flyway\")).permitAll() .requestMatchers(EndpointRequest.toAnyEndpoint()).hasRole(\"MY_ADMIN\") ... 需要注意的是在2.x，health和info在默认情况下启用（与health默认情况下不显示其细节）。为了与这些新的默认值一致，health已被添加到第一个匹配器。 使用 SQL 数据库配置数据源默认连接池已从 Tomcat 切换到 HikariCP。如果您过去spring.datasource.type在基于 Tomcat 的应用程序中强制使用 Hikari，现在可以删除重写。 特别是，如果你有这样的设置： 123456789101112131415&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.apache.tomcat&lt;/groupId&gt; &lt;artifactId&gt;tomcat-jdbc&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.zaxxer&lt;/groupId&gt; &lt;artifactId&gt;HikariCP&lt;/artifactId&gt;&lt;/dependency&gt; 现在可以这样修改： 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;&lt;/dependency&gt; WARN 消息隐含的’打开在视图’从现在起，未明确启用的应用程序spring.jpa.open-in-view将在启动过程中收到警告消息。虽然这种行为是一种友好的默认行为，但如果您没有完全意识到为您做了什么，这可能会导致问题。此消息可确保您了解可在查看呈现期间执行数据库查询。如果你没有问题，你可以明确地配置这个属性来消除警告信息。 JPA 和 Spring Data在 Spring Boot 1.x 中，一些用户正在扩展HibernateJpaAutoConfiguration以将高级自定义应用于自动配置EntityManagerFactory。为了防止发生这种错误的用例，Spring Boot 2 中不再可能扩展它。 为了支持这些用例，现在可以定义一个HibernatePropertiesCustomizerbean，它可以完全控制 Hibernate 属性，包括注册在上下文中声明为 bean 的 Hibernate 拦截器的能力。 FlywayFlyway 配置键被移动到spring命名空间（即spring.flyway） 升级到 Spring Boot 2 将会将 Flyway 升级3.x到5.x。为确保模式升级顺利进行，请按照以下说明操作： 首先将您的1.5.xSpring Boot 应用程序升级到 Flyway 4，请参阅Maven和Gradle的说明。 一旦您的架构升级到了 Flyway 4，升级到 Spring Boot 2 并再次运行迁移以将您的应用程序移植到 Flyway 5。 LiquibaseLiquibase 配置键被移动到spring命名空间（即spring.liquibase） 数据库初始化基本DataSource初始化现在仅针对嵌入式数据源启用，并将在您使用生产数据库时立即关闭。新的spring.datasource.initialization-mode（替换spring.datasource.initialize）提供更多的控制。 更新默认的’创建 - 删除’处理spring.jpa.hibernate.ddl-auto 属性默认为只有在没有使用 Liquibase 或 Flyway 等模式管理器时才使用嵌入式数据库进行创建。一旦检测到模式管理器，默认更改为 none。 整合 NoSQLRedis现在使用的是 Lettuce 而不是 Jedis 作为 Redis 驱动程序spring-boot-starter-redis。如果您使用更高级别的Spring Data 构造，则应该发现变化是透明的。我们仍然支持 Jedis，如果您愿意，通过排除 io.lettuce：lettuce-core并添加 redis.clients：jedis，则可以自由切换依赖项。 ElasticsearchElasticsearch 已经升级到 6.0+。与 Elastic 宣布嵌入式 Elasticsearch 不再受支持一致，自动配置NodeClient已被删除。TransportClient可以通过使用spring.data.elasticsearch.cluster-nodes提供要连接的一个或多个节点的地址来自动配置。 高速缓存用于缓存的专用 Hazelcast 自动配置。 无法自动配置常规HazelcastInstance和专用HazelcastInstance缓存。因此，该spring.cache.hazelcast.config属性已不再可用。 批量在启动时执行批处理作业的 CommandLineRunner 的顺序为 0。 测试Mockito 1.xMockito 1.x 不再支持@MockBean和@SpyBean。如果你不用spring-boot-starter-test来管理你的依赖关系，你应该升级到 Mockito 2.x. Spring Boot ActuatorSpring Boot 2 为 Actuator 带来了重要变化，无论是内部还是面向用户，请查阅参考指南中的更新部分和新的Actuator API文档。 您应该期望编程模型，配置密钥和某些端点的响应格式发生变化。Actuator 现在在 Spring MVC，Spring WebFlux 和Jersey 上得到本地支持。 构建Actuator 的代码分为两个模块：现有的spring-boot-actuator和新的spring-boot-actuator-autoconfigure。如果您使用原始模块（spring-boot-actuator）导入 actuator，请考虑使用spring-boot-starter-actuator启动器替代它。 Keys 的配置结构Endpoints 基础配置 key 已经统一： 旧的属性 新的属性 endpoints.&lt;id&gt;.* management.endpoint.&lt;id&gt;.* endpoints.cors.* management.endpoints.web.cors.* endpoints.jmx.* management.endpoints.jmx.* management.address management.server.address management.context-path management.server.servlet.context-path management.ssl.* management.server.ssl.* management.port management.server.port 基本路径所有 endpoints 默认情况下都已移至 /actuator。 我们修改了 management.server.servlet.context-path 的含义：它现在是 server.servlet.context-path 的端点管理的等价替代（只有在设置了 management.server.port 时才有效）。另外，您还可以使用新的单独属性 management.endpoints.web.base-path 为管理端点设置基本路径。 例如，如果你设置management.server.servlet.context-path=/management和management.endpoints.web.base-path=/application，你就可以在下面的路径到达终点健康：/management/application/health。 如果你想恢复 1.x 的行为（即具有/health代替/actuator/health），设置以下属性： 1management.endpoints.web.base-path=/ 审计事件 API 更改AuditEventRepository 现在有一个包含所有可选参数的单一方法。 Endpoints要通过 HTTP 使执行器端点可用，它需要同时启用和公开。默认： 无论您的应用程序中是否存在和配置 Spring Security，只有端点/health和/info端点都是暴露的。 所有端点，但/shutdown已启用。 您可以按如下方式公开所有端点： 1management.endpoints.web.exposure.include=* 您可以通过以下方式显式启用/shutdown端点： 1management.endpoint.shutdown.enabled=true 要公开所有（已启用）网络端点除env端点之外： 12management.endpoints.web.exposure.include=*management.endpoints.web.exposure.exclude=env Endpoint changes 1.x 端点 2.0 端点（改变） /actuator 不再可用。 但是，在 management.endpoints.web.base-path 的根目录中有一个映射，它提供了到所有暴露端点的链接。 /auditevents 该after参数不再需要 /autoconfig 重命名为 /conditions /docs 不再可用 /health 现在有一个 management.endpoint.health.show-details 选项 never, always, when-authenticated，而不是依靠 sensitive 标志来确定 health 端点是否必须显示全部细节。 默认情况下，/actuator/health公开并且不显示细节。 /trace 重命名为 /httptrace 端点属性已更改如下： endpoints.&lt;id&gt;.enabled 已经转移到了 management.endpoint.&lt;id&gt;.enabled endpoints.&lt;id&gt;.id 没有替换（端点的 ID 不再可配置） endpoints.&lt;id&gt;.sensitive没有替代品（请参见执行器安全） endpoints.&lt;id&gt;.path 已经转移到了 management.endpoints.web.path-mapping.&lt;id&gt; 端点格式/actuator/mappings 端点大改变JSON 格式已经更改为现在正确地包含有关上下文层次结构，多个DispatcherServlets，部署的 Servlet 和 Servlet 过滤器的信息。详情请参阅＃9979。 Actuator API 文档的相关部分提供了一个示例文档。 /actuator/httptrace 端点大改变响应的结构已经过改进，以反映端点关注跟踪 HTTP 请求 - 响应交换的情况。 迁移自定义端点如果您有自定义执行器端点，请查看专用博客文章。该团队还撰写了一个 wiki 页面，介绍如何将现有的执行器端点迁移到新的基础架构。 MetricsSpring Boot 自己的指标已被支持取代，包括自动配置，用于 icrometer 和 dimensional 指标。 设置 icrometer如果您的 Spring Boot 2.0 应用程序已依赖于 Actuator，则 icrometer 已在此处并自动配置。如果您希望将度量标准导出到 Prometheus，Atlas 或 Datadog 等外部注册表，Micrometer 将为许多注册表提供依赖关系; 您可以使用spring.metrics.*属性配置您的应用程序以导出到特定的注册表。 迁移定制计数器/量表您可以通过以下方式创建各种指标，而不是在应用程序代码中注入CounterService或GaugeService的实例： 注入MeterRegistry和调用方法。 直接调用静态方法Counter featureCounter = Metrics.counter(&quot;feature&quot;);。 开发者工具热拔插由于 Spring Loaded 项目被搁置，它在 Spring Boot 的支持已被删除。我们建议使用 Devtools。 Devtools 远程调试隧道已经从 Devtools 中删除了对通过 HTTP 进行隧道远程调试的支持。 已删除的功能以下功能不再可用： CRaSH 支持 Spring Mobile 的自动配置和依赖关系管理。 Spring Social 的自动配置和依赖关系管理。 依赖关系管理commons-digester。 依赖版本以下库的最低支持版本已更改： Elasticsearch 5.6 Gradle 4 Hibernate 5.2 Jetty 9.4 Spring Framework 5 Spring Security 5 Tomcat 8.5 参考资料https://github.com/spring-projects/spring-boot/wiki/Spring-Boot-2.0-Migration-Guide 相关文章1、Spring Boot 2.0系列文章(一)：Spring Boot 2.0 迁移指南 2、Spring Boot 2.0系列文章(二)：Spring Boot 2.0 新特性详解 3、Spring Boot 2.0系列文章(三)：Spring Boot 2.0 配置改变 4、Spring Boot 2.0系列文章(四)：Spring Boot 2.0 源码阅读环境搭建 5、Spring Boot 2.0系列文章(五)：Spring Boot 2.0 项目源码结构预览 6、Spring Boot 2.0系列文章(六)：Spring boot 2.0 中 SpringBootApplication 注解详解 7、Spring Boot 2.0系列文章(七)：SpringApplication 深入探索","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://www.54tianzhisheng.cn/tags/SpringBoot/"}]},{"title":"小马哥 《Java 微服务实践 - Spring Boot 系列》强烈推荐","date":"2018-03-03T16:00:00.000Z","path":"2018/03/04/springboot-vedio/","text":"视频内容Java 微服务实践 - Spring Boot 为系列讲座，二十节专题直播，时长高达50个小时，包括目前最流行技术，深入源码分析，授人以渔的方式，帮助初学者深入浅出地掌握，为高阶从业人员抛砖引玉。 SpringBoot 系列文章 讲师介绍 小马哥，一线互联网公司技术专家，十余年 Java EE 从业经验，架构师、微服务布道师。目前主要负责微服务技术实施、架构衍进、基础设施构建等。重点关注云计算、微服务以及软件架构等领域。通过 SUN Java（SCJP、SCWCD、SCBCD）以及 Oracle OCA 等的认证。 视频列表1、Java 微服务实践 - Spring Boot 系列（一）初体验 2、Java 微服务实践 - Spring Boot 系列（二） Web篇（上） 3、Java 微服务实践 - Spring Boot 系列（三）Web篇（中） 4、 Java 微服务实践 - Spring Boot 系列（四）Web篇（下） 5、Java 微服务实践 - Spring Boot 系列（五）嵌入式Web容器 6、Java 微服务实践 - Spring Boot 系列（六）数据库 JDBC 7、 Java 微服务实践 - Spring Boot 系列（七）MyBatis 8、 Java 微服务实践 - Spring Boot 系列（八）JPA 9、 Java 微服务实践 - Spring Boot 系列（九）NoSQL 10、 Java 微服务实践 - Spring Boot 系列（十）缓存 11、 Java 微服务实践 - Spring Boot 系列（十一）消息 12、 Java 微服务实践 - Spring Boot 系列（十二）验证 13、Java 微服务实践 - Spring Boot 系列（十三）WebSocket 14、 Java 微服务实践- Spring Boot 系列（十四）WebService 15、 Java 微服务实践 - Spring Boot 系列（十五）安全 16、Java 微服务实践 - Spring Boot 系列（十六）日志 17、Java 微服务实践 - Spring Boot 系列（十七）监管 18、Java 微服务实践 - Spring Boot 系列（十八）配置 19、 Java 微服务实践 - Spring Boot 系列（十九）测试 20、Java 微服务实践 - Spring Boot 系列（二十）自定义启动器 购买链接你可以使用下面链接购买（有优惠的哦） https://segmentfault.com/ls/1650000011063780?r=bPrFW3 或者你也可以扫描下面的二维码购买（同样也有优惠的哦） 课程评价 1、小马哥讲的好详细，以前似是而非的东东，现在都懂了2、学到了怎么使用，也学到了怎么自定义校验，知其然也知其所以然3、6666 起飞了 websocket 之前看了写 这次更加深入和标准化 感谢小马哥带来的干货4、深入浅出，思路清晰。5、深入浅出啊。非常好，而且如果有问题，不管简单还是难的，都从不同的角度和层次来解答了。学到的东西非常多。谢谢。 以上评价摘自 segmentfault 相关课程Java 微服务实践 - Spring Cloud 系列 优惠地址：https://segmentfault.com/ls/1650000011386794?r=bPrFW3 另外：还有这个 Java 微服务实践 - Spring Boot / Spring Cloud，算是两个的合集了，合买的话优惠更大。 优惠地址：https://segmentfault.com/ls/1650000011387052?r=bPrFW3 关注我 建了个不错的微信群，如果有感兴趣的可以加我微信，然后拉你进群交流。","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://www.54tianzhisheng.cn/tags/SpringBoot/"},{"name":"微服务","slug":"微服务","permalink":"http://www.54tianzhisheng.cn/tags/微服务/"}]},{"title":"小马哥 《Java 微服务实践 - Spring Cloud 系列》强烈推荐","date":"2018-03-03T16:00:00.000Z","path":"2018/03/04/springcloud-vedio/","text":"视频内容Spring Cloud 系列课程致力于以实战的方式覆盖所有功能特性，结合小马哥十余年的学习方法和工作经验，体会作者设计意图。结合源码加深理解，最终达到形成系统性的知识和技术体系的目的。 SpringBoot 系列文章 讲师介绍 小马哥，一线互联网公司技术专家，十余年 Java EE 从业经验，架构师、微服务布道师。目前主要负责微服务技术实施、架构衍进、基础设施构建等。重点关注云计算、微服务以及软件架构等领域。通过 SUN Java（SCJP、SCWCD、SCBCD）以及 Oracle OCA 等的认证。 视频列表1、Java 微服务实践 - Spring Cloud 系列（一）云原生应用 2、Java 微服务实践 - Spring Cloud 系列（二）配置客户端 3、Java 微服务实践 - Spring Cloud 系列（三）配置服务器 4、Java 微服务实践 - Spring Cloud 系列（四）服务发现/注册 5、Java 微服务实践 - Spring Cloud 系列（五）高可用服务治理 6、Java 微服务实践 - Spring Cloud 系列（六）负载均衡 7、Java 微服务实践 - Spring Cloud 系列（七）Ribbon 源码 8、Java 微服务实践 - Spring Cloud 系列（八）服务短路 9、Java 微服务实践 - Spring Cloud 系列（九）Hystrix源码 10、Java 微服务实践 - Spring Cloud 系列（十）服务调用 11、Java 微服务实践 - Spring Cloud 系列（十一）服务网关 12、Java 微服务实践 - Spring Cloud 系列（十二）消息驱动整合 13、Java 微服务实践 - Spring Cloud 系列（十三）Binder实现 14、Java 微服务实践 - Spring Cloud 系列（十四）消息总线 15、Java 微服务实践 - Spring Cloud 系列（十五）分布式应用跟踪 16、Java 微服务实践 - Spring Cloud 系列（十六）系列回顾 购买链接你可以使用下面链接购买（有优惠的哦） https://segmentfault.com/ls/1650000011386794?r=bPrFW3 或者你也可以扫描下面的二维码购买（同样也有优惠的哦） 课程评价 1、小马哥讲的很好，实践和原理一起结合讲，理解的很深刻。期待小马哥的下一个高并发的系列讲座，还有那本万众瞩目的新书2、这些系列课程怎么说呢，就我来说其实学的不是知识，是学习方法。小马哥厉害的，给个赞。3、满满的赞，社会套路太多，须加强学习，期待下期与小马哥相逢4、基本是都是守者直播时间，跟着 Spring Boot 系列和 Spring Cloud 系列的课程，小马哥讲得很有启发性。讲一种技术的从哪里来，解决了什么问题，以及优缺点。讲源码又不仅限制于源码。总的来说值得花时间来学习和讨论的。值得推荐，五星好评。5、可以的，解答问题很耐心。从你这学到了浏览JSR等之类的规范，在用一些框架的时候发现国外的，很多都实现了JSR，比如spring batch，而国内的话，个人觉得这方面稍微差了点，这方面我们还要努力，加油！ 以上评价摘自 segmentfault 相关课程Java 微服务实践 - Spring Boot 系列 优惠地址：https://segmentfault.com/ls/1650000011063780?r=bPrFW3 另外：还有这个 Java 微服务实践 - Spring Boot / Spring Cloud，算是两个的合集了，合买的话优惠更大。 优惠地址：https://segmentfault.com/ls/1650000011387052?r=bPrFW3 关注我 建了个不错的微信群，如果有感兴趣的可以加我微信，然后拉你进群交流。","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"微服务","slug":"微服务","permalink":"http://www.54tianzhisheng.cn/tags/微服务/"},{"name":"SpringCloud","slug":"SpringCloud","permalink":"http://www.54tianzhisheng.cn/tags/SpringCloud/"}]},{"title":"《深入理解 Java 内存模型》读书笔记","date":"2018-02-27T16:00:00.000Z","path":"2018/02/28/Java-Memory-Model/","text":"前提《深入理解 Java 内存模型》程晓明著，该书在以前看过一遍，现在学的东西越多，感觉那块越重要，于是又再细看一遍，于是便有了下面的读书笔记总结。全书页数虽不多，内容讲得挺深的。细看的话，也是挺花时间的，看完收获绝对挺大的。也建议 Java 开发者都去看看。里面主要有 Java 内存模型的基础、重排序、顺序一致性、Volatile 关键字、锁、final。本文参考书中内容。 关注我如果你想查看这本书可以关注我的公众号: zhisheng ，然后里面回复关键字 JMM 可以查看我分享的百度云链接。 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/02/28/Java-Memory-Model/ 基础并发编程的模型分类在并发编程需要处理的两个关键问题是：线程之间如何通信 和 线程之间如何同步。 通信通信 是指线程之间以何种机制来交换信息。在命令式编程中，线程之间的通信机制有两种：共享内存 和 消息传递。 在共享内存的并发模型里，线程之间共享程序的公共状态，线程之间通过写-读内存中的公共状态来隐式进行通信。 在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过明确的发送消息来显式进行通信。 同步同步 是指程序用于控制不同线程之间操作发生相对顺序的机制。 在共享内存的并发模型里，同步是显式进行的。程序员必须显式指定某个方法或某段代码需要在线程之间互斥执行。 在消息传递的并发模型里，由于消息的发送必须在消息的接收之前，因此同步是隐式进行的。 Java 的并发采用的是共享内存模型，Java 线程之间的通信总是隐式进行，整个通信过程对程序员完全透明。 Java 内存模型的抽象在 Java 中，所有实例域、静态域 和 数组元素存储在堆内存中，堆内存在线程之间共享。局部变量、方法定义参数 和 异常处理器参数 不会在线程之间共享，它们不会有内存可见性问题，也不受内存模型的影响。 Java 线程之间的通信由 Java 内存模型（JMM）控制。JMM 决定了一个线程对共享变量的写入何时对另一个线程可见。从抽象的角度来看，JMM 定义了线程与主内存之间的抽象关系：线程之间的共享变量存储在主内存中，每一个线程都有一个自己私有的本地内存，本地内存中存储了该变量以读／写共享变量的副本。本地内存是 JMM 的一个抽象概念，并不真实存在。 JMM 抽象示意图： 从上图来看，如果线程 A 和线程 B 要通信的话，要如下两个步骤： 1、线程 A 需要将本地内存 A 中的共享变量副本刷新到主内存去 2、线程 B 去主内存读取线程 A 之前已更新过的共享变量 步骤示意图： 举个例子： 本地内存 A 和 B 有主内存共享变量 X 的副本。假设一开始时，这三个内存中 X 的值都是 0。线程 A 正执行时，把更新后的 X 值（假设为 1）临时存放在自己的本地内存 A 中。当线程 A 和 B 需要通信时，线程 A 首先会把自己本地内存 A 中修改后的 X 值刷新到主内存去，此时主内存中的 X 值变为了 1。随后，线程 B 到主内存中读取线程 A 更新后的共享变量 X 的值，此时线程 B 的本地内存的 X 值也变成了 1。 整体来看，这两个步骤实质上是线程 A 再向线程 B 发送消息，而这个通信过程必须经过主内存。JMM 通过控制主内存与每个线程的本地内存之间的交互，来为 Java 程序员提供内存可见性保证。 重排序在执行程序时为了提高性能，编译器和处理器常常会对指令做重排序。重排序分三类： 1、编译器优化的重排序。编译器在不改变单线程程序语义的前提下，可以重新安排语句的执行顺序。 2、指令级并行的重排序。现代处理器采用了指令级并行技术来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 3、内存系统的重排序。由于处理器使用缓存和读／写缓冲区，这使得加载和存储操作看上去可能是在乱序执行。 从 Java 源代码到最终实际执行的指令序列，会分别经历下面三种重排序： 上面的这些重排序都可能导致多线程程序出现内存可见性问题。对于编译器，JMM 的编译器重排序规则会禁止特定类型的编译器重排序（不是所有的编译器重排序都要禁止）。对于处理器重排序，JMM 的处理器重排序规则会要求 Java 编译器在生成指令序列时，插入特定类型的内存屏障指令，通过内存屏障指令来禁止特定类型的处理器重排序（不是所有的处理器重排序都要禁止）。 JMM 属于语言级的内存模型，它确保在不同的编译器和不同的处理器平台之上，通过禁止特定类型的编译器重排序和处理器重排序，为程序员提供一致的内存可见性保证。 处理器重排序现代的处理器使用写缓冲区来临时保存向内存写入的数据。写缓冲区可以保证指令流水线持续运行，它可以避免由于处理器停顿下来等待向内存写入数据而产生的延迟。同时，通过以批处理的方式刷新写缓冲区，以及合并写缓冲区中对同一内存地址的多次写，可以减少对内存总线的占用。虽然写缓冲区有这么多好处，但每个处理器上的写缓冲区，仅仅对它所在的处理器可见。这个特性会对内存操作的执行顺序产生重要的影响：处理器对内存的读/写操作的执行顺序，不一定与内存实际发生的读/写操作顺序一致！ 举个例子： 假设处理器A和处理器B按程序的顺序并行执行内存访问，最终却可能得到 x = y = 0。具体的原因如下图所示： 处理器 A 和 B 同时把共享变量写入在写缓冲区中（A1、B1），然后再从内存中读取另一个共享变量（A2、B2），最后才把自己写缓冲区中保存的脏数据刷新到内存中（A3、B3）。当以这种时序执行时，程序就可以得到 x = y = 0 的结果。 从内存操作实际发生的顺序来看，直到处理器 A 执行 A3 来刷新自己的写缓存区，写操作 A1 才算真正执行了。虽然处理器 A 执行内存操作的顺序为：A1 -&gt; A2，但内存操作实际发生的顺序却是：A2 -&gt; A1。此时，处理器 A 的内存操作顺序被重排序了。 这里的关键是，由于写缓冲区仅对自己的处理器可见，它会导致处理器执行内存操作的顺序可能会与内存实际的操作执行顺序不一致。由于现代的处理器都会使用写缓冲区，因此现代的处理器都会允许对写-读操作重排序。 内存屏障指令为了保证内存可见性，Java 编译器在生成指令序列的适当位置会插入内存屏障指令来禁止特定类型的处理器重排序。JMM 把内存屏障指令分为下列四类： 屏障类型 指令示例 说明 LoadLoad Barriers Load1; LoadLoad; Load2 确保 Load1 数据的装载，之前于 Load2 及所有后续装载指令的装载。 StoreStore Barriers Store1; StoreStore; Store2 确保 Store1 数据对其他处理器可见（刷新到内存），之前于 Store2 及所有后续存储指令的存储。 LoadStore Barriers Load1; LoadStore; Store2 确保 Load1 数据装载，之前于 Store2 及所有后续的存储指令刷新到内存。 StoreLoad Barriers Store1; StoreLoad; Load2 确保 Store1 数据对其他处理器变得可见（指刷新到内存），之前于 Load2 及所有后续装载指令的装载。StoreLoadBarriers 会使该屏障之前的所有内存访问指令（存储和装载指令）完成之后，才执行该屏障之后的内存访问指令。 happens-beforeJSR-133 内存模型使用 happens-before 的概念来阐述操作之间的内存可见性。在 JMM 中，如果一个操作执行的结果需要对另一个操作可见，那么这两个操作之间必须要存在 happens-before 关系。这里提到的两个操作既可以是在一个线程之内，也可以是在不同线程之间。 与程序员密切相关的 happens-before 规则如下： 程序顺序规则：一个线程中的每个操作，happens-before 于该线程中的任意后续操作。 监视器锁规则：对一个监视器的解锁，happens-before 于随后对这个监视器的加锁。 volatile 变量规则：对一个 volatile 域的写，happens-before 于任意后续对这个 volatile 域的读。 传递性：如果 A happens-before B，且 B happens-before C，那么 A happens-before C。 注意，两个操作之间具有 happens-before 关系，并不意味着前一个操作必须要在后一个操作之前执行！happens-before 仅仅要求前一个操作（执行的结果）对后一个操作可见，且前一个操作按顺序排在第二个操作之前（the first is visible to and ordered before the second）。 happens-before 与 JMM 的关系如下图所示： 如上图所示，一个 happens-before 规则对应于一个或多个编译器和处理器重排序规则。 数据依赖性如果两个操作访问同一个变量，且这两个操作中有一个为写操作，此时这两个操作之间就存在数据依赖性。数据依赖分下列三种类型： 名称 代码示例 说明 写后读 a = 1; b = a; 写一个变量之后，再读这个位置。 写后写 a = 1; a = 2; 写一个变量之后，再写这个变量。 读后写 a = b; b = 1; 读一个变量之后，再写这个变量。 上面三种情况，只要重排序两个操作的执行顺序，程序的执行结果将会被改变。 前面提到过，编译器和处理器可能会对操作做重排序。编译器和处理器在重排序时，会遵守数据依赖性，编译器和处理器不会改变存在数据依赖关系的两个操作的执行顺序。 注意，这里所说的数据依赖性仅针对单个处理器中执行的指令序列和单个线程中执行的操作，不同处理器之间和不同线程之间的数据依赖性不被编译器和处理器考虑。 as-if-serial 语义as-if-serial 语义的意思指：不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不能被改变。编译器，runtime 和处理器都必须遵守 as-if-serial 语义。 为了遵守 as-if-serial 编译器和处理器不会对存在数据依赖关系的操作做重排序，因为这种重排序会改变执行结果。但是如果操作之间没有数据依赖关系，这些操作就可能被编译器和处理器重排序。 举个例子： 123double pi = 3.14; //Adouble r = 1.0; //Bdouble area = pi * r * r; //C 上面三个操作的数据依赖关系如下图所示： 如上图所示，A 和 C 之间存在数据依赖关系，同时 B 和 C 之间也存在数据依赖关系。因此在最终执行的指令序列中，C 不能被重排序到 A 和 B 的前面（C 排到 A 和 B 的前面，程序的结果将会被改变）。但 A 和 B 之间没有数据依赖关系，编译器和处理器可以重排序 A 和 B 之间的执行顺序。下图是该程序的两种执行顺序： 在计算机中，软件技术和硬件技术有一个共同的目标：在不改变程序执行结果的前提下，尽可能的开发并行度。编译器和处理器遵从这一目标，从 happens-before 的定义我们可以看出，JMM 同样遵从这一目标。 重排序对多线程的影响举例： 123456789101112131415class Demo &#123; int a = 0; boolean flag = false; public void write() &#123; a = 1; //1 flag = true; //2 &#125; public void read() &#123; if(flag) &#123; //3 int i = a * a; //4 &#125; &#125;&#125; 由于操作 1 和 2 没有数据依赖关系，编译器和处理器可以对这两个操作重排序；操作 3 和操作 4 没有数据依赖关系，编译器和处理器也可以对这两个操作重排序。 1、当操作 1 和操作 2 重排序时，可能会产生什么效果？ 如上图所示，操作 1 和操作 2 做了重排序。程序执行时，线程 A 首先写标记变量 flag，随后线程 B 读这个变量。由于条件判断为真，线程 B 将读取变量 a。此时，变量 a 还根本没有被线程 A 写入，在这里多线程程序的语义被重排序破坏了！ 2、当操作 3 和操作 4 重排序时会产生什么效果（借助这个重排序，可以顺便说明控制依赖性）。 在程序中，操作 3 和操作 4 存在控制依赖关系。当代码中存在控制依赖性时，会影响指令序列执行的并行度。为此，编译器和处理器会采用猜测（Speculation）执行来克服控制相关性对并行度的影响。以处理器的猜测执行为例，执行线程 B 的处理器可以提前读取并计算 a * a，然后把计算结果临时保存到一个名为重排序缓冲（reorder buffer ROB）的硬件缓存中。当接下来操作 3 的条件判断为真时，就把该计算结果写入变量 i 中。 从图中我们可以看出，猜测执行实质上对操作3和4做了重排序。重排序在这里破坏了多线程程序的语义！ 在单线程程序中，对存在控制依赖的操作重排序，不会改变执行结果（这也是 as-if-serial 语义允许对存在控制依赖的操作做重排序的原因）；但在多线程程序中，对存在控制依赖的操作重排序，可能会改变程序的执行结果。 顺序一致性顺序一致性内存模型顺序一致性内存模型有两大特性： 一个线程中的所有操作必须按照程序的顺序来执行。 （不管程序是否同步）所有线程都只能看到一个单一的操作执行顺序。在顺序一致性内存模型中，每个操作都必须原子执行且立刻对所有线程可见。 顺序一致性内存模型为程序员提供的视图如下： 在概念上，顺序一致性模型有一个单一的全局内存，这个内存通过一个左右摆动的开关可以连接到任意一个线程，同时每一个线程必须按照程序的顺序来执行内存读/写操作。从上面的示意图我们可以看出，在任意时间点最多只能有一个线程可以连接到内存。当多个线程并发执行时，图中的开关装置能把所有线程的所有内存读/写操作串行化。 举个例子： 假设有两个线程 A 和 B 并发执行。其中 A 线程有三个操作，它们在程序中的顺序是：A1 -&gt; A2 -&gt; A3。B 线程也有三个操作，它们在程序中的顺序是：B1 -&gt; B2 -&gt; B3。 假设这两个线程使用监视器锁来正确同步：A 线程的三个操作执行后释放监视器锁，随后 B 线程获取同一个监视器锁。那么程序在顺序一致性模型中的执行效果将如下图所示： 现在我们再假设这两个线程没有做同步，下面是这个未同步程序在顺序一致性模型中的执行示意图： 未同步程序在顺序一致性模型中虽然整体执行顺序是无序的，但所有线程都只能看到一个一致的整体执行顺序。以上图为例，线程 A 和 B 看到的执行顺序都是：B1 -&gt; A1 -&gt; A2 -&gt; B2 -&gt; A3 -&gt; B3。之所以能得到这个保证是因为顺序一致性内存模型中的每个操作必须立即对任意线程可见。 但是，在 JMM 中就没有这个保证。未同步程序在 JMM 中不但整体的执行顺序是无序的，而且所有线程看到的操作执行顺序也可能不一致。比如，在当前线程把写过的数据缓存在本地内存中，在还没有刷新到主内存之前，这个写操作仅对当前线程可见；从其他线程的角度来观察，会认为这个写操作根本还没有被当前线程执行。只有当前线程把本地内存中写过的数据刷新到主内存之后，这个写操作才能对其他线程可见。在这种情况下，当前线程和其它线程看到的操作执行顺序将不一致。 同步程序的顺序一致性效果下面我们对前面的示例程序用锁来同步，看看正确同步的程序如何具有顺序一致性。 请看下面的示例代码： 123456789101112131415class demo &#123; int a = 0; boolean flag = false; public synchronized void write() &#123; //获取锁 a = 1; flag = true; &#125; //释放锁 public synchronized void read() &#123; //获取锁 if(flag) &#123; int i = a; &#125; &#125; //释放锁&#125; 上面示例代码中，假设 A 线程执行 write() 方法后，B 线程执行 reade() 方法。这是一个正确同步的多线程程序。根据JMM规范，该程序的执行结果将与该程序在顺序一致性模型中的执行结果相同。下面是该程序在两个内存模型中的执行时序对比图： 在顺序一致性模型中，所有操作完全按程序的顺序执行。而在 JMM 中，临界区内的代码可以重排序（但 JMM 不允许临界区内的代码“逸出”到临界区之外，那样会破坏监视器的语义）。JMM 会在退出临界区和进入临界区这两个关键时间点做一些特别处理，使得线程在这两个时间点具有与顺序一致性模型相同的内存视图。虽然线程 A 在临界区内做了重排序，但由于监视器的互斥执行的特性，这里的线程 B 根本无法“观察”到线程 A 在临界区内的重排序。这种重排序既提高了执行效率，又没有改变程序的执行结果。 从这里我们可以看到 JMM 在具体实现上的基本方针：在不改变（正确同步的）程序执行结果的前提下，尽可能的为编译器和处理器的优化打开方便之门。 未同步程序的执行特性未同步程序在 JMM 中的执行时，整体上是无序的，其执行结果无法预知。未同步程序在两个模型中的执行特性有下面几个差异： 顺序一致性模型保证单线程内的操作会按程序的顺序执行，而 JMM 不保证单线程内的操作会按程序的顺序执行（比如上面正确同步的多线程程序在临界区内的重排序）。 顺序一致性模型保证所有线程只能看到一致的操作执行顺序，而 JMM 不保证所有线程能看到一致的操作执行顺序。 JMM 不保证对 64 位的 long 型和 double 型变量的读/写操作具有原子性，而顺序一致性模型保证对所有的内存读/写操作都具有原子 。 第三个差异与处理器总线的工作机制密切相关。在计算机中，数据通过总线在处理器和内存之间传递。每次处理器和内存之间的数据传递都是通过总线事务来完成的。总线事务包括读事务和写事务。读事务从内存传送数据到处理器，写事务从处理器传递数据到内存，每个事务会读／写内存中一个或多个物理上连续的字。总线会同步试图并发使用总线的事务。在一个处理器执行总线事务期间，总线会禁止其它所有的处理器和 I／O 设备执行内存的读／写。 总线的工作机制： 如上图所示，假设处理器 A、B、和 C 同时向总线发起总线事务，这时总线仲裁会对竞争作出裁决，假设总线在仲裁后判定处理器 A 在竞争中获胜（总线仲裁会确保所有处理器都能公平的访问内存）。此时处理器 A 继续它的总线事务，而其它两个处理器则要等待处理器 A 的总线事务完成后才能开始再次执行内存访问。假设在处理器 A 执行总线事务期间（不管这个总线事务是读事务还是写事务），处理器 D 向总线发起了总线事务，此时处理器 D 的这个请求会被总线禁止。 总线的这些工作机制可以把所有处理器对内存的访问以串行化的方式来执行；在任意时间点，最多只能有一个处理器能访问内存。这个特性确保了单个总线事务之中的内存读/写操作具有原子性。 在一些 32 位的处理器上，如果要求对 64 位数据的写操作具有原子性，会有比较大的开销。为了照顾这种处理器，Java 语言规范鼓励但不强求 JVM 对 64 位的 long 型变量和 double 型变量的写具有原子性。当 JVM 在这种处理器上运行时，会把一个 64 位 long/ double 型变量的写操作拆分为两个 32 位的写操作来执行。这两个 32 位的写操作可能会被分配到不同的总线事务中执行，此时对这个 64 位变量的写将不具有原子性。 当单个内存操作不具有原子性，将可能会产生意想不到后果。请看下面示意图： 如上图所示，假设处理器 A 写一个 long 型变量，同时处理器 B 要读这个 long 型变量。处理器 A 中 64 位的写操作被拆分为两个 32 位的写操作，且这两个 32 位的写操作被分配到不同的写事务中执行。同时处理器 B 中 64 位的读操作被分配到单个的读事务中执行。当处理器 A 和 B 按上图的时序来执行时，处理器 B 将看到仅仅被处理器 A “写了一半“的无效值。 注意，在 JSR -133 之前的旧内存模型中，一个 64 位 long/ double 型变量的读/写操作可以被拆分为两个 32 位的读/写操作来执行。从 JSR -133 内存模型开始（即从JDK5开始），仅仅只允许把一个 64 位 long/ double 型变量的写操作拆分为两个 32 位的写操作来执行，任意的读操作在JSR -133中都必须具有原子性（即任意读操作必须要在单个读事务中执行）。 VolatileVolatile 特性举个例子： 123456789101112131415public class VolatileTest &#123; volatile long a = 1L; // 使用 volatile 声明 64 位的 long 型 public void set(long l) &#123; a = l; //单个 volatile 变量的写 &#125; public long get() &#123; return a; //单个 volatile 变量的读 &#125; public void getAndIncreament() &#123; a++; // 复合（多个） volatile 变量的读 /写 &#125;&#125; 假设有多个线程分别调用上面程序的三个方法，这个程序在语义上和下面程序等价： 1234567891011121314151617public class VolatileTest &#123; long a = 1L; // 64 位的 long 型普通变量 public synchronized void set(long l) &#123; //对单个普通变量的写用同一个锁同步 a = l; &#125; public synchronized long get() &#123; //对单个普通变量的读用同一个锁同步 return a; &#125; public void getAndIncreament() &#123; //普通方法调用 long temp = get(); //调用已同步的读方法 temp += 1L; //普通写操作 set(temp); //调用已同步的写方法 &#125;&#125; 如上面示例程序所示，对一个 volatile 变量的单个读/写操作，与对一个普通变量的读/写操作使用同一个锁来同步，它们之间的执行效果相同。 锁的 happens-before 规则保证释放锁和获取锁的两个线程之间的内存可见性，这意味着对一个 volatile 变量的读，总是能看到（任意线程）对这个 volatile 变量最后的写入。 锁的语义决定了临界区代码的执行具有原子性。这意味着即使是 64 位的 long 型和 double 型变量，只要它是 volatile变量，对该变量的读写就将具有原子性。如果是多个 volatile 操作或类似于 volatile++ 这种复合操作，这些操作整体上不具有原子性。 简而言之，volatile 变量自身具有下列特性： 可见性。对一个 volatile 变量的读，总是能看到（任意线程）对这个 volatile 变量最后的写入。 原子性：对任意单个 volatile 变量的读/写具有原子性，但类似于 volatile++ 这种复合操作不具有原子性。 volatile 写-读的内存定义 当写一个 volatile 变量时，JMM 会把该线程对应的本地内存中的共享变量值刷新到主内存。 当读一个 volatile 变量时，JMM 会把该线程对应的本地内存置为无效。线程接下来将从主内存中读取共享变量。 假设上面的程序 flag 变量用 volatile 修饰 volatile 内存语义的实现下面是 JMM 针对编译器制定的 volatile 重排序规则表： 为了实现 volatile 的内存语义，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。 下面是基于保守策略的 JMM 内存屏障插入策略： 在每个 volatile 写操作的前面插入一个 StoreStore 屏障。 在每个 volatile 写操作的后面插入一个 StoreLoad 屏障。 在每个 volatile 读操作的后面插入一个 LoadLoad 屏障。 在每个 volatile 读操作的后面插入一个 LoadStore 屏障。 下面是保守策略下，volatile 写操作 插入内存屏障后生成的指令序列示意图： 下面是在保守策略下，volatile 读操作 插入内存屏障后生成的指令序列示意图： 上述 volatile 写操作和 volatile 读操作的内存屏障插入策略非常保守。在实际执行时，只要不改变 volatile 写-读的内存语义，编译器可以根据具体情况省略不必要的屏障。 锁锁释放和获取的内存语义当线程释放锁时，JMM 会把该线程对应的本地内存中的共享变量刷新到主内存中。 当线程获取锁时，JMM 会把该线程对应的本地内存置为无效。从而使得被监视器保护的临界区代码必须要从主内存中去读取共享变量。 锁内存语义的实现借助 ReentrantLock 来讲解，PS： 后面专门讲下这块（ReentrantLock、Synchronized、公平锁、非公平锁、AQS等），可以看看大明哥的博客：http://cmsblogs.com/?p=2210 concurrent 包的实现如果我们仔细分析 concurrent 包的源代码实现，会发现一个通用化的实现模式： 首先，声明共享变量为 volatile； 然后，使用 CAS 的原子条件更新来实现线程之间的同步； 同时，配合以 volatile 的读/写和 CAS 所具有的 volatile 读和写的内存语义来实现线程之间的通信。 AQS，非阻塞数据结构和原子变量类（java.util.concurrent.atomic 包中的类），这些 concurrent 包中的基础类都是使用这种模式来实现的，而 concurrent 包中的高层类又是依赖于这些基础类来实现的。从整体来看，concurrent 包的实现示意图如下： final对于 final 域，编译器和处理器要遵守两个重排序规则： 在构造函数内对一个 final 域的写入，与随后把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序。 初次读一个包含 final 域的对象的引用，与随后初次读这个 final 域，这两个操作之间不能重排序。 写 final 域的重排序规则写 final 域的重排序规则禁止把 final 域的写重排序到构造函数之外。这个规则的实现包含下面2个方面： JMM 禁止编译器把 final 域的写重排序到构造函数之外。 编译器会在 final 域的写之后，构造函数 return 之前，插入一个 StoreStore 屏障。这个屏障禁止处理器把 final 域的写重排序到构造函数之外。 读 final 域的重排序规则在一个线程中，初次读对象引用与初次读该对象包含的 final 域，JMM 禁止处理器重排序这两个操作（注意，这个规则仅仅针对处理器）。编译器会在读 final 域操作的前面插入一个 LoadLoad 屏障。 final 域是引用类型对于引用类型，写 final 域的重排序规则对编译器和处理器增加了如下约束： 在构造函数内对一个 final 引用的对象的成员域的写入，与随后在构造函数外把这个被构造对象的引用赋值给一个引用变量，这两个操作之间不能重排序。 总结JMM，处理器内存模型与顺序一致性内存模型之间的关系JMM 是一个语言级的内存模型，处理器内存模型是硬件级的内存模型，顺序一致性内存模型是一个理论参考模型。下面是语言内存模型，处理器内存模型和顺序一致性内存模型的强弱对比示意图： JMM 的设计示意图 JMM 的内存可见性保证Java 程序的内存可见性保证按程序类型可以分为下列三类： 1.单线程程序。单线程程序不会出现内存可见性问题。编译器，runtime 和处理器会共同确保单线程程序的执行结果与该程序在顺序一致性模型中的执行结果相同。 2.正确同步的多线程程序。正确同步的多线程程序的执行将具有顺序一致性（程序的执行结果与该程序在顺序一致性内存模型中的执行结果相同）。这是 JMM 关注的重点，JMM通过限制编译器和处理器的重排序来为程序员提供内存可见性保证。 3.未同步/未正确同步的多线程程序。JMM 为它们提供了最小安全性保障：线程执行时读取到的值，要么是之前某个线程写入的值，要么是默认值（0，null，false）。 下图展示了这三类程序在 JMM 中与在顺序一致性内存模型中的执行结果的异同：","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"JMM","slug":"JMM","permalink":"http://www.54tianzhisheng.cn/tags/JMM/"}]},{"title":"RocketMQ系列文章（三）：RocketMQ 简单的消息示例","date":"2018-02-06T16:00:00.000Z","path":"2018/02/07/rocketmq-example/","text":"使用 RocketMQ 以三种方式发送消息：可靠的同步，可靠的异步和单向传输。 SpringBoot 系列文章 可靠的同步传输应用：可靠的同步传输广泛应用于重要通知消息，短信通知，短信营销系统等。 12345678910111213141516171819202122public class SyncProducer &#123; public static void main(String[] args) throws Exception &#123; //Instantiate with a producer group name. DefaultMQProducer producer = new DefaultMQProducer(\"please_rename_unique_group_name\"); //Launch the instance. producer.start(); for (int i = 0; i &lt; 100; i++) &#123; //Create a message instance, specifying topic, tag and message body. Message msg = new Message(\"TopicTest\" /* Topic */, \"TagA\" /* Tag */, (\"Hello RocketMQ \" + i).getBytes(RemotingHelper.DEFAULT_CHARSET) /* Message body */ ); //Call send message to deliver message to one of brokers. SendResult sendResult = producer.send(msg); System.out.printf(\"%s%n\", sendResult); &#125; //Shut down once the producer instance is not longer in use. producer.shutdown(); &#125;&#125; 可靠的异步传输应用：异步传输一般用于响应时间敏感的业务场景。 12345678910111213141516171819202122232425262728293031public class AsyncProducer &#123; public static void main(String[] args) throws Exception &#123; //Instantiate with a producer group name. DefaultMQProducer producer = new DefaultMQProducer(\"ExampleProducerGroup\"); //Launch the instance. producer.start(); producer.setRetryTimesWhenSendAsyncFailed(0); for (int i = 0; i &lt; 100; i++) &#123; final int index = i; //Create a message instance, specifying topic, tag and message body. Message msg = new Message(\"TopicTest\", \"TagA\", \"OrderID188\", \"Hello world\".getBytes(RemotingHelper.DEFAULT_CHARSET)); producer.send(msg, new SendCallback() &#123; @Override public void onSuccess(SendResult sendResult) &#123; System.out.printf(\"%-10d OK %s %n\", index, sendResult.getMsgId()); &#125; @Override public void onException(Throwable e) &#123; System.out.printf(\"%-10d Exception %s %n\", index, e); e.printStackTrace(); &#125; &#125;); &#125; //Shut down once the producer instance is not longer in use. producer.shutdown(); &#125;&#125; 单向传输应用：单向传输用于需要中等可靠性的情况，例如日志收集。 123456789101112131415161718192021public class OnewayProducer &#123; public static void main(String[] args) throws Exception&#123; //Instantiate with a producer group name. DefaultMQProducer producer = new DefaultMQProducer(\"ExampleProducerGroup\"); //Launch the instance. producer.start(); for (int i = 0; i &lt; 100; i++) &#123; //Create a message instance, specifying topic, tag and message body. Message msg = new Message(\"TopicTest\" /* Topic */, \"TagA\" /* Tag */, (\"Hello RocketMQ \" + i).getBytes(RemotingHelper.DEFAULT_CHARSET) /* Message body */ ); //Call send message to deliver message to one of brokers. producer.sendOneway(msg); &#125; //Shut down once the producer instance is not longer in use. producer.shutdown(); &#125;&#125; 关注我 总结本文是 RocketMQ 的三种发送消息的方式。 转发请注明地址：http://www.54tianzhisheng.cn/2018/02/07/rocketmq-example/","tags":[{"name":"RocketMQ","slug":"RocketMQ","permalink":"http://www.54tianzhisheng.cn/tags/RocketMQ/"}]},{"title":"Spring Boot系列文章（六）：SpringBoot RocketMQ 整合使用和监控","date":"2018-02-06T16:00:00.000Z","path":"2018/02/07/SpringBoot-RocketMQ/","text":"前提通过前面两篇文章可以简单的了解 RocketMQ 和 安装 RocketMQ ，今天就将 SpringBoot 和 RocketMQ 整合起来使用。 SpringBoot 系列文章 相关文章1、SpringBoot Kafka 整合使用 2、SpringBoot RabbitMQ 整合使用 3、SpringBoot ActiveMQ 整合使用 4、Kafka 安装及快速入门 5、SpringBoot RabbitMQ 整合进阶版 6、RocketMQ 初探 7、RocketMQ 安装及快速入门 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/02/07/SpringBoot-RocketMQ/ 创建项目在 IDEA 创建一个 SpringBoot 项目，项目结构如下： pom 文件引入 RocketMQ 的一些相关依赖，最后的 pom 文件如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.zhisheng&lt;/groupId&gt; &lt;artifactId&gt;rocketmq&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;rocketmq&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot RocketMQ&lt;/description&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.rocketmq&lt;/groupId&gt; &lt;artifactId&gt;rocketmq-common&lt;/artifactId&gt; &lt;version&gt;4.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.rocketmq&lt;/groupId&gt; &lt;artifactId&gt;rocketmq-client&lt;/artifactId&gt; &lt;version&gt;4.2.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 配置文件application.properties 中如下： 123456# 消费者的组名apache.rocketmq.consumer.PushConsumer=PushConsumer# 生产者的组名apache.rocketmq.producer.producerGroup=Producer# NameServer地址apache.rocketmq.namesrvAddr=localhost:9876 生产者1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package com.zhisheng.rocketmq.client;import org.apache.rocketmq.client.producer.DefaultMQProducer;import org.apache.rocketmq.common.message.Message;import org.apache.rocketmq.remoting.common.RemotingHelper;import org.springframework.beans.factory.annotation.Value;import org.springframework.stereotype.Component;import org.springframework.util.StopWatch;import javax.annotation.PostConstruct;/** * Created by zhisheng_tian on 2018/2/6 */@Componentpublic class RocketMQClient &#123; /** * 生产者的组名 */ @Value(\"$&#123;apache.rocketmq.producer.producerGroup&#125;\") private String producerGroup; /** * NameServer 地址 */ @Value(\"$&#123;apache.rocketmq.namesrvAddr&#125;\") private String namesrvAddr; @PostConstruct public void defaultMQProducer() &#123; //生产者的组名 DefaultMQProducer producer = new DefaultMQProducer(producerGroup); //指定NameServer地址，多个地址以 ; 隔开 producer.setNamesrvAddr(namesrvAddr); try &#123; /** * Producer对象在使用之前必须要调用start初始化，初始化一次即可 * 注意：切记不可以在每次发送消息时，都调用start方法 */ producer.start(); //创建一个消息实例，包含 topic、tag 和 消息体 //如下：topic 为 \"TopicTest\"，tag 为 \"push\" Message message = new Message(\"TopicTest\", \"push\", \"发送消息----zhisheng-----\".getBytes(RemotingHelper.DEFAULT_CHARSET)); StopWatch stop = new StopWatch(); stop.start(); for (int i = 0; i &lt; 10000; i++) &#123; SendResult result = producer.send(message); System.out.println(\"发送响应：MsgId:\" + result.getMsgId() + \"，发送状态:\" + result.getSendStatus()); &#125; stop.stop(); System.out.println(\"----------------发送一万条消息耗时：\" + stop.getTotalTimeMillis()); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; producer.shutdown(); &#125; &#125;&#125; 消费者123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566package com.zhisheng.rocketmq.server;import org.apache.rocketmq.client.consumer.DefaultMQPushConsumer;import org.apache.rocketmq.client.consumer.listener.ConsumeConcurrentlyStatus;import org.apache.rocketmq.client.consumer.listener.MessageListenerConcurrently;import org.apache.rocketmq.common.consumer.ConsumeFromWhere;import org.apache.rocketmq.common.message.MessageExt;import org.apache.rocketmq.remoting.common.RemotingHelper;import org.springframework.beans.factory.annotation.Value;import org.springframework.stereotype.Component;import javax.annotation.PostConstruct;/** * Created by zhisheng_tian on 2018/2/6 */@Componentpublic class RocketMQServer &#123; /** * 消费者的组名 */ @Value(\"$&#123;apache.rocketmq.consumer.PushConsumer&#125;\") private String consumerGroup; /** * NameServer 地址 */ @Value(\"$&#123;apache.rocketmq.namesrvAddr&#125;\") private String namesrvAddr; @PostConstruct public void defaultMQPushConsumer() &#123; //消费者的组名 DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(consumerGroup); //指定NameServer地址，多个地址以 ; 隔开 consumer.setNamesrvAddr(namesrvAddr); try &#123; //订阅PushTopic下Tag为push的消息 consumer.subscribe(\"TopicTest\", \"push\"); //设置Consumer第一次启动是从队列头部开始消费还是队列尾部开始消费 //如果非第一次启动，那么按照上次消费的位置继续消费 consumer.setConsumeFromWhere(ConsumeFromWhere.CONSUME_FROM_FIRST_OFFSET); consumer.registerMessageListener((MessageListenerConcurrently) (list, context) -&gt; &#123; try &#123; for (MessageExt messageExt : list) &#123; System.out.println(\"messageExt: \" + messageExt);//输出消息内容 String messageBody = new String(messageExt.getBody(), RemotingHelper.DEFAULT_CHARSET); System.out.println(\"消费响应：msgId : \" + messageExt.getMsgId() + \", msgBody : \" + messageBody);//输出消息内容 &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); return ConsumeConcurrentlyStatus.RECONSUME_LATER; //稍后再试 &#125; return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; //消费成功 &#125;); consumer.start(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 启动类123456789101112package com.zhisheng.rocketmq;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;@SpringBootApplicationpublic class RocketmqApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(RocketmqApplication.class, args); &#125;&#125; RocketMQ代码已经都写好了，接下来我们需要将与 RocketMQ 有关的启动起来。 启动 Name Server在前面文章中已经写过怎么启动，http://www.54tianzhisheng.cn/2018/02/06/RocketMQ-install/#%E5%90%AF%E5%8A%A8-NameServer 进入到目录 ： 1cd distribution/target/apache-rocketmq 启动： 123nohup sh bin/mqnamesrv &amp;tail -f ~/logs/rocketmqlogs/namesrv.log //通过日志查看是否启动成功 启动 Broker123nohup sh bin/mqbroker -n localhost:9876 &amp;tail -f ~/logs/rocketmqlogs/broker.log //通过日志查看是否启动成功 然后运行启动类，运行效果如下： 监控RocketMQ有一个对其扩展的开源项目 ocketmq-console ，如今也提交给了 Apache ，地址在：https://github.com/apache/rocketmq-externals/tree/master/rocketmq-console ，官方也给出了其支持的功能的中文文档：https://github.com/apache/rocketmq-externals/blob/master/rocketmq-console/doc/1_0_0/UserGuide_CN.md ， 那么该如何安装？ Docker 安装1、获取 Docker 镜像 1docker pull styletang/rocketmq-console-ng 2、运行，注意将你自己的 NameServer 地址替换下面的 127.0.0.1 1docker run -e &quot;JAVA_OPTS=-Drocketmq.namesrv.addr=127.0.0.1:9876 -Dcom.rocketmq.sendMessageWithVIPChannel=false&quot; -p 8080:8080 -t styletang/rocketmq-console-ng 非 Docker 安装我们 git clone 一份代码到本地： 123git clone https://github.com/apache/rocketmq-externals.gitcd rocketmq-externals/rocketmq-console/ 需要 jdk 1.7 以上。 执行以下命令： 1mvn spring-boot:run 或者 123mvn clean package -Dmaven.test.skip=truejava -jar target/rocketmq-console-ng-1.0.0.jar 注意： 1、如果你下载依赖缓慢，你可以重新设置 maven 的 mirror 为阿里云的镜像 12345678&lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt;&lt;/mirrors&gt; 2、如果你使用的 RocketMQ 版本小于 3.5.8，如果您使用 rocketmq &lt; 3.5.8，请在启动 rocketmq-console-ng 时添加 -Dcom.rocketmq.sendMessageWithVIPChannel = false（或者您可以在 ops 页面中更改它） 3、更改 resource / application.properties 中的 rocketmq.config.namesrvAddr（或者可以在ops页面中更改它） 错误解决方法1、Docker 启动项目报错 org.apache.rocketmq.remoting.exception.RemotingConnectException: connect to &lt;null&gt; failed 将 Docker 启动命令改成如下以后： 1docker run -e &quot;JAVA_OPTS=-Drocketmq.config.namesrvAddr=127.0.0.1:9876 -Drocketmq.config.isVIPChannel=false&quot; -p 8080:8080 -t styletang/rocketmq-console-ng 报错信息改变了，新的报错信息如下： 123ERROR op=global_exception_handler_print_errororg.apache.rocketmq.console.exception.ServiceException: This date have&apos;t data! 看到网上有人也遇到这个问题，他们都通过自己的方式解决了，但是方法我都试了，不适合我。不得不说，阿里，你能再用心点吗？既然把 RocketMQ 捐给 Apache 了，这些文档啥的都必须更新啊，不要还滞后着呢，不然少不了被吐槽！ 搞了很久这种方法没成功，暂时放弃！mmp 2、非 Docker 安装，只好把源码编译打包了。 1) 注意需要修改如下图中的配置： 1234rocketmq.config.namesrvAddr=localhost:9876 //注意替换你自己的ip#如果你 rocketmq 版本小于 3.5.8 才需设置 `rocketmq.config.isVIPChannel` 为 false，默认是 true, 这个可以在源码中可以看到的rocketmq.config.isVIPChannel= 2) 执行以下命令： 1mvn clean package -Dmaven.test.skip=true 编译成功： 可以看到已经打好了 jar 包： 运行： 1java -jar rocketmq-console-ng-1.0.0.jar 成功，不报错了，开心😄，访问 http://localhost:8080/ 整个监控大概就是这些了。 然后我运行之前的 SpringBoot 整合项目，查看监控信息如下： 总结整篇文章讲述了 SpringBoot 与 RocketMQ 整合和 RocketMQ 监控平台的搭建。 参考文章1、http://www.ymq.io/2018/02/02/spring-boot-rocketmq-example/#%E6%96%B0%E5%8A%A0%E9%A1%B9%E7%9B%AE 2、GitHub 官方 README","tags":[{"name":"RocketMQ","slug":"RocketMQ","permalink":"http://www.54tianzhisheng.cn/tags/RocketMQ/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://www.54tianzhisheng.cn/tags/SpringBoot/"}]},{"title":"RocketMQ系列文章（二）：RocketMQ 安装及快速入门","date":"2018-02-05T16:00:00.000Z","path":"2018/02/06/RocketMQ-install/","text":"如果你对 RocketMQ 还没了解，建议先看下上一篇文章：RocketMQ 初探 SpringBoot 系列文章 安装条件 64位操作系统，建议使用 Linux / Unix / Mac; 64位JDK 1.8+; Maven 3.2.x 下载和构建从 https://www.apache.org/dyn/closer.cgi?path=rocketmq/4.2.0/rocketmq-all-4.2.0-source-release.zip 下载 4.2.0 的源码版本，执行以下命令来解压4.2.0源码版本并构建二进制文件。 12345unzip rocketmq-all-4.2.0-source-release.zipcd rocketmq-all-4.2.0/mvn -Prelease-all -DskipTests clean install -U 构建成功如下： 进入到目录 ： 1cd distribution/target/apache-rocketmq 启动 NameServer123nohup sh bin/mqnamesrv &amp;tail -f ~/logs/rocketmqlogs/namesrv.log 结果如下就代表启动成功了： 启动 Broker123nohup sh bin/mqbroker -n localhost:9876 &amp;tail -f ~/logs/rocketmqlogs/broker.log 结果如下就代表启动成功了：从日志中可以看到 broker 注册到了 nameserver 上了（localhost:9876） 发送和接收消息在发送/接收消息之前，我们需要告诉客户名称服务器的位置。RocketMQ 提供了多种方法来实现这一点。为了简单起见，我们使用环境变量NAMESRV_ADDR 发送消息123export NAMESRV_ADDR=localhost:9876sh bin/tools.sh org.apache.rocketmq.example.quickstart.Producer 接收消息1sh bin/tools.sh org.apache.rocketmq.example.quickstart.Consumer 关闭服务器123sh bin/mqshutdown broker //停止 brokersh bin/mqshutdown namesrv //停止 nameserver 关闭成功后如下： 常用命令上面几个启动和关闭 name server 和 broker 的就不再说了， 查看集群情况 ./mqadmin clusterList -n 127.0.0.1:9876 查看 broker 状态 ./mqadmin brokerStatus -n 127.0.0.1:9876 -b 172.20.1.138:10911 (注意换成你的 broker 地址) 查看 topic 列表 ./mqadmin topicList -n 127.0.0.1:9876 查看 topic 状态 ./mqadmin topicStatus -n 127.0.0.1:9876 -t MyTopic (换成你想查询的 topic) 查看 topic 路由 ./mqadmin topicRoute -n 127.0.0.1:9876 -t MyTopic 关注我 总结本文是 RocketMQ 的安装及快速入门案例。 转发请注明地址：http://www.54tianzhisheng.cn/2018/02/06/RocketMQ-install/","tags":[{"name":"RocketMQ","slug":"RocketMQ","permalink":"http://www.54tianzhisheng.cn/tags/RocketMQ/"}]},{"title":"RocketMQ系列文章（一）：RocketMQ 初探","date":"2018-02-04T16:00:00.000Z","path":"2018/02/05/RocketMQ/","text":"介绍RocketMQ 是阿里开源的消息中间件，前不久捐献给了 Apache 。正如官网介绍如下：它是一个开源的分布式消息传递和流式数据平台。 SpringBoot 系列文章 特点如下： 产品发展历史大约经历了三个主要版本迭代 一、Metaq（Metamorphosis）1.x 由开源社区killme2008维护，开源社区非常活跃。 二、Metaq 2.x 于2012年10月份上线，在淘宝内部被广泛使用。 三、RocketMQ 3.x 基于公司内部开源共建原则，RocketMQ 项目只维护核心功能，且去除了所有其他运行时依赖，核心功能最简化。每个 BU 的个性化需求都在 RocketMQ 项目之上进行深度定制。RocketMQ 向其他 BU 提供的仅仅是 Jar 包，例如要定制一个 Broker，那么只需要依赖 rocketmq-broker 这个 jar 包即可，可通过 API 进行交互，如果定制 client，则依赖 rocketmq-client 这个 jar 包，对其提供的 api 进行再封装。 在 RocketMQ 项目基础上衍生的项目如下 com.taobao.metaq v3.0 = RocketMQ + 淘宝个性化需求 为淘宝应用提供消息服务 com.alipay.zpullmsg v1.0 =RocketMQ + 支付宝个性化需求 为支付宝应用提供消息服务 com.alibaba.commonmq v1.0 = Notify + RocketMQ + B2B个性化需求 为 B2B 应用提供消息服务 四、RocketMQ 3.x 目前它的最新版本是 4.2 版本。 概念专业术语Producer 消息生产者，负责产生消息，一般由业务系统负责产生消息。 Consumer 消息消费者，负责消费消息，一般是后台系统负责异步消费。 Push Consumer Consumer 的一种，应用通常向 Consumer 对象注册一个 Listener 接口，一旦收到消息，Consumer 对象立刻回调 Listener 接口方法。 Pull Consumer Consumer 的一种，应用通常主动调用 Consumer 的拉消息方法从 Broker 拉消息，主动权由应用控制。 Producer Group 一类 Producer 的集合名称，这类 Producer 通常发送一类消息，且发送逻辑一致。 Consumer Group 一类 Consumer 的集合名称，这类 Consumer 通常消费一类消息，且消费逻辑一致。 Broker 消息中转角色，负责存储消息，转发消息，一般也称为 Server。在 JMS 规范中称为 Provider。 架构 从这架构图中可以看到它主要由四部分组成：Producer（生产者）、NameServer、Broker、Consumer（消费者）。 Producer生产者支持分布式部署。分布式生产者通过多种负载均衡模式向 Broker 集群发送消息。发送过程支持快速失败并具有低延迟。 NameServer它提供轻量级服务发现和路由，每个 Name Server 记录完整的路由信息，提供相应的读写服务，支持快速存储扩展。主要包括两个功能： 代理管理， NameServer 接受来自 Broker 集群的注册，并提供检测代理是否存在的心跳机制。 路由管理，每个 NameServer 将保存有关代理群集的全部路由信息以及客户端查询的队列信息。 我们知道，RocketMQ客户端（生产者/消费者）将从NameServer查询队列路由信息，但客户端如何找到NameServer地址？ 将NameServer地址列表提供给客户端有四种方法： 编程方式，就像producer.setNamesrvAddr(&quot;ip:port&quot;)。 Java选项，使用rocketmq.namesrv.addr。 环境变量，使用NAMESRV_ADDR。 HTTP 端点。 BrokerBroker 通过提供轻量级的 Topic 和 Queue 机制来照顾消息存储。它们支持 Push 和 Pull 模式，包含容错机制（2个拷贝或者3个拷贝），并且提供了强大的峰值填充和以原始时间顺序累计数千亿条消息的能力。此外，broker 还提供灾难恢复，丰富的指标统计数据和警报机制，而传统的消息传递系统都缺乏这些机制。 如上图：Broker 服务器重要的子模块： 远程处理模块是 broker 的入口，处理来自客户的请求。 Client manager，管理客户（生产者/消费者）并维护消费者的主题订阅。 Store Service，提供简单的 API 来存储或查询物理磁盘中的消息。 HA 服务，提供主代理和从代理之间的数据同步功能。 索引服务，通过指定键为消息建立索引，并提供快速的消息查询。 Consumer消费者也支持 Push 和 Pull 模型中的分布式部署。它还支持群集消费和消息广播。它提供了实时的消息订阅机制，可以满足大多数消费者的需求。 关注我 总结本文是对 RocketMQ 的简单点了解，参考了官网介绍。 转载请注明地址：http://www.54tianzhisheng.cn/2018/02/05/RocketMQ/","tags":[{"name":"RocketMQ","slug":"RocketMQ","permalink":"http://www.54tianzhisheng.cn/tags/RocketMQ/"}]},{"title":"Spring Boot系列文章（五）：SpringBoot RabbitMQ 整合进阶版","date":"2018-01-27T16:00:00.000Z","path":"2018/01/28/RabbitMQ/","text":"消息中间件RabbitMQ 是消息中间件的一种, 消息中间件即分布式系统中完成消息的发送和接收的基础软件. 这些软件有很多, 包括 ActiveMQ ( apache 公司的), RocketMQ (阿里巴巴公司的, 现已经转让给 apache), 还有性能极高的 Kafka。 SpringBoot 系列文章 消息中间件的工作过程可以用生产者消费者模型来表示. 即生产者不断的向消息队列发送信息, 而消费者从消息队列中消费信息. 具体过程如下: 从上图可看出, 对于消息队列来说, 生产者,消息队列,消费者 是最重要的三个概念。生产者发消息到消息队列中去, 消费者监听指定的消息队列, 并且当消息队列收到消息之后, 接收消息队列传来的消息, 并且给予相应的处理. 消息队列常用于分布式系统之间互相信息的传递. RabbitMQ 工作原理对于 RabbitMQ 来说, 除了这三个基本模块以外, 还添加了一个模块, 即交换机(Exchange). 它使得生产者和消息队列之间产生了隔离, 生产者将消息发送给交换机,而交换机则根据调度策略把相应的消息转发给对应的消息队列. 那么 RabitMQ 的工作流程如下所示: 说一下交换机: 交换机的主要作用是接收相应的消息并且绑定到指定的队列. 交换机有四种类型, 分别为Direct, topic, headers, Fanout. Direct 是 RabbitMQ 默认的交换机模式,也是最简单的模式.即创建消息队列的时候,指定一个 BindingKey. 当发送者发送消息的时候, 指定对应的 Key. 当 Key 和消息队列的 BindingKey 一致的时候,消息将会被发送到该消息队列中. topic 转发信息主要是依据通配符, 队列和交换机的绑定主要是依据一种模式(通配符+字符串), 而当发送消息的时候, 只有指定的 Key 和该模式相匹配的时候, 消息才会被发送到该消息队列中. headers 也是根据一个规则进行匹配, 在消息队列和交换机绑定的时候会指定一组键值对规则, 而发送消息的时候也会指定一组键值对规则, 当两组键值对规则相匹配的时候, 消息会被发送到匹配的消息队列中. Fanout 是路由广播的形式, 将会把消息发给绑定它的全部队列, 即便设置了 key, 也会被忽略. 关注我 转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/01/28/RabbitMQ/ SpringBoot 整合 RabbitMQ（Topic 转发模式）在上一篇文章中，我们也将 SpringBoot 和 RabbitMQ 整合过，不过那是使用 Direct 模式，文章地址是：SpringBoot RabbitMQ 整合使用 相关文章1、SpringBoot Kafka 整合使用 2、SpringBoot RabbitMQ 整合使用 3、SpringBoot ActiveMQ 整合使用 4、Kafka 安装及快速入门 整合接下来，我要带大家继续整合（Topic 转发模式）： 1、配置文件和 pom.xml 这些还都是一样的，我们不用再修改 2、启动类中创建 Queue 和 Exchange，并把 Queue 按照相应的规则绑定到交换机Queue 上。代码如下图： 1234567891011121314@Beanpublic Queue queue() &#123; return new Queue(\"rpc-queue-zhisheng\");&#125;@Beanpublic TopicExchange exchange() &#123; return new TopicExchange(\"rpc-exchange-zhisheng\");&#125;@Beanpublic Binding binding(Queue queue, TopicExchange exchange) &#123; return BindingBuilder.bind(queue).to(exchange).with(\"rpc-zhisheng\");&#125; 这里创建一个 Queue 和 Exchange ，然后绑定。 注意：上面代码中的 with(“rpc-zhisheng”) 这个 “zhisheng” 是 routingkey，RabbitMQ 将会根据这个参数去寻找有没有匹配此规则的队列，如果有，则会把消息发送给它，如果不止有一个，则会把消息分发给所有匹配的队列。 3、消息发送类 1234567891011121314151617181920212223package com.zhisheng.rabbitmq.rpc.client;import org.springframework.amqp.core.TopicExchange;import org.springframework.amqp.rabbit.core.RabbitTemplate;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;/** * Created by zhisheng_tian on 2018/1/25 */@Componentpublic class RabbitMQClient &#123; @Autowired private RabbitTemplate rabbitTemplate; @Autowired private TopicExchange exchange; public void send(String message) &#123; rabbitTemplate.convertAndSend(exchange.getName(), \"rpc-zhisheng\", message); &#125;&#125; 这里是发送消息的代码，“rpc-zhisheng” 就是上面我们设置的 routingkey。 4、消息接收端 12345678910111213141516package com.zhisheng.rabbitmq.rpc.server;import org.springframework.amqp.rabbit.annotation.RabbitListener;import org.springframework.stereotype.Component;/** * Created by zhisheng_tian on 2018/1/25 */@Componentpublic class RabbitMQServer &#123; @RabbitListener(queues = \"rpc-queue-zhisheng\") public void receive(String message) &#123; System.out.println(\"--------receive ------- \" + message); &#125;&#125; 5、启动类中注入 发送消息类，然后调用 send 方法 12345678910111213@Autowiredprivate RabbitMQClient client;@PostConstructpublic void init() &#123; StopWatch stopWatch = new StopWatch(); stopWatch.start(); for (int i = 0; i &lt; 1000; i++) &#123; client.send(\" zhisheng, --------- send \" + i); &#125; stopWatch.stop(); System.out.println(\"总共耗时：\" + stopWatch.getTotalTimeMillis());&#125; 运行此 SpringBoot 项目，则可以发现结果如下： 这里测试的是匹配一个消息队列的情况，感兴趣的可以测试下匹配多个消息队列的。 SpringBoot 整合 RabbitMQ( Fanout Exchange 形式)Fanout Exchange 形式又叫广播形式。 任何发送到 Fanout Exchange 的消息都会被转发到与该 Exchange 绑定(Binding)的所有 Queue 上。 这种模式需要提前将 Exchange 与 Queue 进行绑定，一个 Exchange 可以绑定多个 Queue，一个 Queue 可以同多个 Exchange 进行绑定 这种模式不需要 RoutingKey 如果接受到消息的 Exchange 没有与任何 Queue 绑定，则消息会被抛弃。 1、消息发送类 12345678910111213141516171819package com.zhisheng.rabbitmq.rpc.client;import org.springframework.amqp.rabbit.core.RabbitTemplate;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;/** * Created by zhisheng_tian on 2018/1/25 */@Componentpublic class RabbitMQClient &#123; @Autowired private RabbitTemplate rabbitTemplate; public void send2(String message) &#123; rabbitTemplate.convertAndSend(\"fanout-exchange\", \"\", message); &#125;&#125; 这里可以不设置 routingkey 了。 2、启动类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071package com.zhisheng.rabbitmq.rpc;import com.zhisheng.rabbitmq.rpc.client.RabbitMQClient;import org.springframework.amqp.core.Binding;import org.springframework.amqp.core.BindingBuilder;import org.springframework.amqp.core.FanoutExchange;import org.springframework.amqp.core.Queue;import org.springframework.amqp.support.converter.Jackson2JsonMessageConverter;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.annotation.Qualifier;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.context.annotation.Bean;import javax.annotation.PostConstruct;@SpringBootApplicationpublic class RabbitmqRpcApplication &#123; @Autowired private RabbitMQClient client; @PostConstruct public void init() &#123; client.send2(\"zhisheng ++++++++++ send2 \"); &#125; public static void main(String[] args) &#123; SpringApplication.run(RabbitmqRpcApplication.class, args); &#125; @Bean(name = \"queue\") public Queue queue() &#123; return new Queue(\"rpc.queue\"); &#125; @Bean(name = \"queue2\") public Queue queue2() &#123; return new Queue(\"rpc.queue2\"); &#125; @Bean(name = \"queue3\") public Queue queue3() &#123; return new Queue(\"rpc.queue3\"); &#125; @Bean public FanoutExchange exchange() &#123; return new FanoutExchange(\"fanout-exchange\"); &#125; @Bean public Binding binding(@Qualifier(\"queue\") Queue queue, FanoutExchange exchange) &#123; return BindingBuilder.bind(queue).to(exchange); &#125; @Bean public Binding binding2(@Qualifier(\"queue2\") Queue queue, FanoutExchange exchange) &#123; return BindingBuilder.bind(queue).to(exchange); &#125; @Bean public Binding binding3(@Qualifier(\"queue3\") Queue queue, FanoutExchange exchange) &#123; return BindingBuilder.bind(queue).to(exchange); &#125; @Bean public Jackson2JsonMessageConverter messageConverter() &#123; return new Jackson2JsonMessageConverter(); &#125;&#125; 在启动类中我创建三个 Queue： rpc.queue, rpc.queue2 , rpc.queue3 也创建一个 FanoutExchange，并把这三个 Queue 绑定在同一个交换机 fanout-exchange 上面 注意：这个 fanout-exchange 交换机不知为啥，我自己在应用程序里创建，运行程序会出错，下面讲讲我是怎么解决的。 我是从 RabbitMQ 管理界面直接添加个 exchange 的。 3、消息接收类 123456789101112131415161718192021222324252627package com.zhisheng.rabbitmq.rpc.server;import org.springframework.amqp.rabbit.annotation.RabbitListener;import org.springframework.stereotype.Component;/** * Created by zhisheng_tian on 2018/1/25 */@Componentpublic class RabbitMQServer &#123; @RabbitListener(queues = \"rpc.queue\") public void receive(String message) &#123; System.out.println(\"--------receive ------- \" + message); &#125; @RabbitListener(queues = \"rpc.queue2\") public void receive2(String message) &#123; System.out.println(\"--------receive2 ------- \" + message); &#125; @RabbitListener(queues = \"rpc.queue3\") public void receive3(String message) &#123; System.out.println(\"--------receive3 ------- \" + message); &#125;&#125; 监听每个 Queue，并有一个方法输出对应接收到的消息。 4、运行项目 结果如上，每个队列都打印出自己收到的结果，同时我们看看这三个 Queue 是不是绑定到 Exchange 上呢？ 可以看到三个 Queue 都绑定在 Exchange 上了。 总结RabbitMQ 与 SpringBoot 整合就到这里为止了，后面如果有时间会深度研究 RabbitMQ 的。 还请继续关注我的博客：http://www.54tianzhisheng.cn/","tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://www.54tianzhisheng.cn/tags/SpringBoot/"},{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://www.54tianzhisheng.cn/tags/RabbitMQ/"}]},{"title":"Spring Boot系列文章（四）：SpringBoot ActiveMQ 整合使用","date":"2018-01-26T16:00:00.000Z","path":"2018/01/27/SpringBoot-ActiveMQ/","text":"介绍 ActiveMQ它是 Apache 出品，最流行的，能力强劲的开源消息总线。ActiveMQ 是一个完全支持 JMS1.1 和 J2EE 1.4 规范的 JMS Provider 实现，尽管 JMS 规范出台已经是很久的事情了，但是 JMS 在当今的J2EE应用中间仍然扮演着特殊的地位。—— 摘自百度百科，偷了个懒。 SpringBoot 系列文章 相关文章1、SpringBoot Kafka 整合使用 2、SpringBoot RabbitMQ 整合使用 安装 ActiveMQ同之前一样，直接在 Docker 里面玩吧。命令也是一行解决： 1docker run -d -p 8161:8161 -p 61616:61616 -e ACTIVEMQ_ADMIN_LOGIN=admin -e ACTIVEMQ_ADMIN_PASSWORD=admin --name activemq webcenter/activemq 简单解释下： 8186: 表示 ActiveMQ 控制台端口号，它和 RabbitMQ 一样都是有控制台的，可以登陆控制台进行操作的 61616 ： 表示 ActiveMQ 所监听的 TCP 端口号，应用程序可通过该端口号与 ActiveMQ 建立 TCP 连接 CTIVEMQ_ADMIN_LOGIN ：登陆控制台的用户名 ACTIVEMQ_ADMIN_PASSWORD ：登陆控制台的密码 执行后，可在浏览器输入 http://localhost:8161/ 查看控制台， 解释下上面图片中控制台这些按钮的基本信息： Home：查看 ActiveMQ 的常见信息 Queues：查看 ActiveMQ 的队列信息 Topics：查看 ActiveMQ 的主题信息 Subscribers：查看主题的订阅者信息 Connections：查看 ActiveMQ 客户端的连接信息 Network：查看 ActiveMQ 的网络信息 Scheduled：查看 ActiveMQ 的定时任务 Send：用于通过表单方式向队列或者主题发送具体的消息 整合IDEA 创建 SpringBoot 项目，因为 SpringBoot 已经内置了对 ActiveMQ 的支持，所以直接引入依赖 spring-boot-starter-activemq 就行。整体项目结构如下： 1、pom.xml 文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.zhisheng&lt;/groupId&gt; &lt;artifactId&gt;activemq&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;activemq&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot ActiveMQ&lt;/description&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-activemq&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 2、配置文件 application.properties 123spring.activemq.broker-url=tcp://localhost:61616spring.activemq.user=adminspring.activemq.password=admin 3、发送消息类 12345678910111213141516package com.zhisheng.activemq.client;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.jms.core.JmsTemplate;import org.springframework.stereotype.Component;@Componentpublic class ActiveMQClient &#123; @Autowired private JmsTemplate jmsTemplate; public void send(String message) &#123; jmsTemplate.convertAndSend(\"zhisheng\", message); &#125;&#125; 同样，和 RabbitMQ 类似，不多说了。 4、消息接收类 123456789101112package com.zhisheng.activemq.server;import org.springframework.jms.annotation.JmsListener;import org.springframework.stereotype.Component;@Componentpublic class ActiveMQServer &#123; @JmsListener(destination = \"zhisheng\") public void receive(String message) &#123; System.out.println(\"收到的 message 是：\" + message); &#125;&#125; 5、注意 这个队列是不需要我们提前定义好的，它和 RabbitMQ 不一样，它会在我们需要的时候动态的创建。 运行12345678910111213141516171819202122232425262728293031package com.zhisheng.activemq;import com.zhisheng.activemq.client.ActiveMQClient;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.util.StopWatch;import javax.annotation.PostConstruct;@SpringBootApplicationpublic class ActivemqApplication &#123; @Autowired ActiveMQClient client; @PostConstruct public void init() &#123; StopWatch stopWatch = new StopWatch(); stopWatch.start(); for (int i = 0; i &lt; 10000; i++) &#123; client.send(\"发送消息----zhisheng-----\"); &#125; stopWatch.stop(); System.out.println(\"发送消息耗时: \" + stopWatch.getTotalTimeMillis()); &#125; public static void main(String[] args) &#123; SpringApplication.run(ActivemqApplication.class, args); &#125;&#125; 发送一万条消息运行后需要的时间挺久的：73180 ms 比 RabbitMQ 发送 10000 条消息耗时 215 ms 不知道高出多少倍了，可见其性能并不高的。 关注我 最后转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/01/27/SpringBoot-ActiveMQ/","tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://www.54tianzhisheng.cn/tags/SpringBoot/"},{"name":"ActiveMQ","slug":"ActiveMQ","permalink":"http://www.54tianzhisheng.cn/tags/ActiveMQ/"}]},{"title":"Spring Boot系列文章（三）：SpringBoot  RabbitMQ 整合使用","date":"2018-01-25T16:00:00.000Z","path":"2018/01/26/SpringBoot-RabbitMQ/","text":"SpringBoot 系列文章 前提上次写了篇文章，《SpringBoot Kafka 整合使用》，阅读量还挺高的，于是想想还是把其他几种 MQ 也和 SpringBoot 整合使用下。 下面是四种比较流行的 MQ ： 后面都写写和 SpringBoot 整合的文章。 安装 RabbitMQ由于换 Mac 了，所以一些环境就直接在 Mac 搞，但是像安装 RabbitMQ 这些又会把自己电脑系统给搞的太乱，所以能在 Docker 里面安装就安装在 Docker，这次 RabbitMQ 我也直接在 Docker 里安装。 启动 Docker for Mac，如果没安装过的请看我上一篇文章：http://www.54tianzhisheng.cn/2018/01/25/Docker-install/ 当然你也可以在自己的 Linux 服务器或者虚拟机里启动安装 RabbitMQ 。 Docker 安装的话很简单，因为 RabbitMQ 官方已经提供了自己的 Docker 容器，只需要一行命令： 1docker run -d -p 15672:15672 -p 5672:5672 -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=admin --name rabbitmq rabbitmq:3-management 该镜像拥有一个基于 web 的控制台和 Http API。Http API 可以在地址看到如何使用：http://localhost:15672/api/ 讲解下上面命令行： 15672 ：表示 RabbitMQ 控制台端口号，可以在浏览器中通过控制台来执行 RabbitMQ 的相关操作。 5672 : 表示 RabbitMQ 所监听的 TCP 端口号，应用程序可通过该端口与 RabbitMQ 建立 TCP 连接，并完成后续的异步消息通信 RABBITMQ_DEFAULT_USER：用于设置登陆控制台的用户名，这里我设置 admin RABBITMQ_DEFAULT_PASS：用于设置登陆控制台的密码，这里我设置 admin 容器启动成功后，可以在浏览器输入地址：http://localhost:15672/ 访问控制台 登陆后： 简单描述下上图中中控制台的列表的作用： Overview ：用于查看 RabbitMQ 的一些基本信息（消息队列、消息发送速率、节点、端口和上下文信息等） Connections：用于查看 RabbitMQ 客户端的连接信息 Channels：用户查看 RabbitMQ 的通道信息 Exchange：用于查看 RabbitMQ 交换机 Queues：用于查看 RabbitMQ 的队列 Admin：用于管理用户，可增加用户 创建项目在 IDEA 中创建一个 SpringBoot 项目结构： SpringBoot 框架中已经内置了对 RabbitMQ 的支持，如果你看过官方文档的话，就可以看到的，我们需要把依赖 spring-boot-starter-amqp 引入就行。 1、 pom.xml 引入依赖后如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.zhisheng&lt;/groupId&gt; &lt;artifactId&gt;rabbitmq&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;rabbitmq&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot RabbitMQ&lt;/description&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-amqp&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 2、application.properties 配置修改如下： 123spring.rabbitmq.addresses=localhost:5672spring.rabbitmq.username=adminspring.rabbitmq.password=admin 3、消息发送类 RabbitMQClient.java 12345678910111213141516171819package com.zhisheng.rabbitmq.client;import org.springframework.amqp.rabbit.core.RabbitTemplate;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;/** * Created by zhisheng_tian on 2018/1/23 */@Componentpublic class RabbitMQClient &#123; @Autowired private RabbitTemplate rabbitTemplate; public void send(String message) &#123; rabbitTemplate.convertAndSend(\"zhisheng\", message); &#125;&#125; 就这样，发送消息代码就实现了。 这里关键的代码为 rabbitTemplate.convertAndSend() 方法，zhisheng 这个是路由规则（routingKey），它的值表明将消息发送到指定的队列 zhisheng 中去，这里跟了下源码，发现 convertAndSend() 方法最后调用的方法其实是一个 doSend() 方法。 4、消息接收类 12345678910111213141516package com.zhisheng.rabbitmq.server;import org.springframework.amqp.rabbit.annotation.RabbitListener;import org.springframework.stereotype.Component;/** * Created by zhisheng_tian on 2018/1/23 */@Componentpublic class RabbitMQServer &#123; @RabbitListener(queues = \"zhisheng\") public void receive(String message) &#123; System.out.println(\"收到的 message 是：\" + message); &#125;&#125; 你看，这里就有个 RabbitListener 一直在监听着队列 zhisheng 。 当然这个队列是必须要我们自己在应用程序中创建好，它不会像我之前写的文章 《SpringBoot Kafka 整合使用》 中的 Kafka 一样，Kafka 它会在用到队列的时候动态的创建，不需要我们提前创建好。 那么在 RabbitMQ 中该如何创建队列呢？ 如上图所示：这样我们就创建好了一个 zhisheng 的队列，当程序开始运行时，消息接收类会持续监听队列 zhisheng 中即将到来的消息。 5、运行项目 需要在启动类中注入发送消息的类，并且提供 init 方法，在 init 方法中调用发送消息类的 send() 方法 1234@PostConstructpublic void init() &#123; rabbitMQClient.send(\"发送消息----zhisheng-----\");&#125; 需要注意的是：init() 方法带有 @PostConstruct 注解，被 @PostConstruct 修饰的方法会在构造函数之后执行。 启动项目就可以发现控制台已经接收到消息了。 6、单线程测试性能 看到上面图片中注释掉的代码没？那就是用来测试消息发送的性能的，我发送 10000 条消息看看总共耗时多少。 10000 条消息发送耗时：215ms。 这是在单线程下，下次可以和其他的 MQ 测试对比下，并且也可以在多线程的环境下测试性能。 同时从控制台可以看到发送的速率： 7、多线程测试性能 开了10 个线程，每个线程发送 10000 条消息。 init 方法代码如下： 1234567891011121314151617181920212223242526272829303132333435363738@PostConstruct public void init() &#123; StopWatch stopWatch = new StopWatch(); stopWatch.start(); int threads = 10; ExecutorService executorService = Executors.newFixedThreadPool(threads); final CountDownLatch start = new CountDownLatch(1); final CountDownLatch end = new CountDownLatch(threads); for (int i = 0; i &lt; threads; i++) &#123; executorService.execute(() -&gt; &#123; try &#123; start.await(); for (int i1 = 0; i1 &lt; 10000; i1++) &#123; rabbitMQClient.send(\"发送消息----zhisheng-----\"); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; end.countDown(); &#125; &#125;); &#125; start.countDown(); try &#123; end.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; finally &#123; executorService.shutdown(); &#125; stopWatch.stop(); System.out.println(\"发送消息耗时：\" + stopWatch.getTotalTimeMillis()); &#125; 耗时：4063ms 控制台显示如下图： 8、注意 这里测试发送的消息直接是 String 类型的，你也可以测试下 Bean 类，这需要注意需要序列化。 关注我 最后转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/01/26/SpringBoot-RabbitMQ/","tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://www.54tianzhisheng.cn/tags/SpringBoot/"},{"name":"RabbitMQ","slug":"RabbitMQ","permalink":"http://www.54tianzhisheng.cn/tags/RabbitMQ/"}]},{"title":"Docker系列文章（二）：Mac 安装 Docker 及常用命令","date":"2018-01-24T16:00:00.000Z","path":"2018/01/25/Docker-install/","text":"背景微服务 + 容器，完美的一对！必须得好好学习学习。 安装步骤Mac 下 Docker 的安装真心建议跟着官方的文档走一遍，官网已经讲的很详细了。 https://docs.docker.com/docker-for-mac/install/#what-to-know-before-you-install 使用 Docker for Machttps://docs.docker.com/docker-for-mac/#check-versions-of-docker-engine-compose-and-machine 配置 Docker 加速器Docker 加速器是什么，我需要使用吗？ 使用 Docker 的时候，需要经常从官方获取镜像，但是由于显而易见的网络原因，拉取镜像的过程非常耗时，严重影响使用 Docker 的体验。因此 DaoCloud 推出了加速器工具解决这个难题，通过智能路由和缓存机制，极大提升了国内网络访问 Docker Hub 的速度，目前已经拥有了广泛的用户群体，并得到了 Docker 官方的大力推荐。如果您是在国内的网络环境使用 Docker，那么 Docker 加速器一定能帮助到您。 注册 daocloud，然后在 mac 标签页复制加速器 url。 入门案例跟着下面的文章进行敲一遍，熟悉下 Docker 整个的使用。 https://www.jianshu.com/p/cf6e7248b6c7 Docker 常用命令下面列出些自己常用的命令，目的就是记录下来，以后忘记了，再拿来跟着敲就行！ 12345678910111213141516171819202122232425262728293031323334353637383940414243docker run -i -t &lt;image_name/continar_id&gt; /bin/bash 启动容器并启动bash（交互方式）docker run -d -it image_name 启动容器以后台方式运行(更通用的方式）docker ps 列出当前所有正在运行的containerdocker ps -a 列出所有的containerdocker ps -l 列出最近一次启动的containerdocker images 列出本地所有的镜像docker rmi imagesID 删除指定的镜像iddocker rm CONTAINER ID 删除指定的CONTAINER iddocker diff 镜像名 查看容器的修改部分docker kill CONTAINER ID 杀掉正在运行的容器docker logs 容器ID/name 可以查看到容器主程序的输出docker pull image_name 下载imagedocker push image_name 发布docker镜像docker version 查看docker版本docker info 查看docker系统的信息docker inspect 容器的id 可以查看更详细的关于某一个容器的信息docker run -d image-name 后台运行镜像docker search 镜像名 查找公共的可用镜像docker stop 容器名/容器 ID 终止运行的容器docker restart 容器名/容器 ID 重启容器docker commit 提交，创建个新镜像docker build [OPTIONS] PATH | URL | - 利用 Dockerfile 创建新镜像 关注我 最后转载请注明地址：http://www.54tianzhisheng.cn/2018/01/25/Docker-install/","tags":[{"name":"Docker","slug":"Docker","permalink":"http://www.54tianzhisheng.cn/tags/Docker/"}]},{"title":"MacBook Pro 初体验","date":"2018-01-23T16:00:00.000Z","path":"2018/01/24/mac/","text":"背景 在 Mac 到手之前就在各种群里看到人说 Mac 多好用，也有很多人鼓吹过 Mac 的好处，最后也坚定我的年前目标了 —— 就是买台 Mac，之前请原谅我这个穷鬼，买不起，现在买了 Mac 后更加得体谅我这个穷鬼了，毕竟在上海这个城市，靠着实习工资买这种奢侈品，不容易啊😄 。废话不多说，如果愿意支持我的，请在文章底部扫描二维码，在此先谢谢了。 如何挑选？MacBook 主要分两系列：MacBook Air 和 MacBook Pro。 Air 的话个人感觉配置不高，如果是开发还是建议买 Pro 系列的。如今买的话，可能还会分 2015 款、2016 款、2017 款。每款中又分 内存大小（8/16g）、硬盘大小（128/256/512g）、CPU、处理器（i5/i7）、是否有TouchBar、显卡等。不同配置对应电脑的型号也是不一样的。下面直接上一张在我的特殊渠道里的报价表截图吧。（想了解的可以找我） 光这型号，不懂的人还真不会挑。 不得不说，苹果电脑真尼玛难挑啊，如果你是土豪，那不用挑了，直接上最高价钱的吧。 然后可以从配置中发现 2016 款和 2017 款变化真心不大，在同等配置下，2017 款几乎比 2016 款价格高个 3000 来块。 然后就是 512 G 硬盘比 256 G 也几乎贵个 2000 多。 16G 那是必须的上啊，标配了，8G 就不说了，太小了。 含 TouchBar，虽然确实用处不大，不过调音两还是不错的。高配都有 TouchBar 的。 出于 qiong ，我买了 2016 款，配置是： 不过现在 2016 款好像停产了。 到手2018.01.11 下午六点快递送到的，很开心。晚上拿回家拆箱，第一件事情就是检查序列号啊，上面的图片打码掉的就是序列号，这个序列号在电脑机身、系统、外牛皮癣盒都有的，可以在官网查询这个序列号，获得电脑的激活日期和剩余保修时间的。再就是查询电脑的电池循环次数了，我的是一次，一般好像是几次之内都是符合的。证明之前没被别人用过，这个数字我也忘记了。 熟悉系统Mac 系统是类 Unix 系统，其实我觉得到和 Ubuntu 系统挺像的，既有图形化界面，也可以命令行操作。熟悉过 Linux 的应该上手很快的。 安装软件可以在 Appstore 里面下载，也可以在一些软件的官网直接下载 mac 版的。安装也挺简单的。如果你不知道有什么软件可以安装，那么我这里给你份 Mac 软件参考列表：https://github.com/jaywcjlove/awesome-mac/blob/master/README-zh.md 当然了，上面的不一定全，具体用到其他的还是的自己去找对应的软件。 还有就是好多软件是收费的，在 Mac 上如果要下载的话，还的费点心思去破解，比如 Office、IDEA、Adobe 系列等，当然也不是鼓吹大家去破解，我们自己用用就行，虽说现在没钱支持，但是有钱的话还是支持下。我一个写博客的知道写博客的不容易，那写软件的更不容易了，能支持一两块也挺好的。 然后就是美化下我们的一些软件，比如我们的终端之类的、尽量使用 Homebrew 安装软件。当然这篇文章不会写这些的，改天专门写篇文章写这个话题。 还有就是在 Mac 上从新打造一个适合自己的新写作环境（软件、Hexo 写博客环境）。 最后体验了 Mac 也有一个多礼拜了，整体效果还是不错的，毕竟花了巨资呢，也算是完成了自己年前的小目标。先 bb 到这里吧。有时间再写点关于 Mac 上的东西，这次写的比较简单，这篇文章也是在 Mac 上写的第一篇文章。","tags":[{"name":"Mac","slug":"Mac","permalink":"http://www.54tianzhisheng.cn/tags/Mac/"}]},{"title":"Spring Boot系列文章（二）：SpringBoot Admin 使用指南","date":"2018-01-16T16:00:00.000Z","path":"2018/01/17/SpringBoot-Admin/","text":"什么是 SpringBoot Admin？Spring Boot Admin 是一个管理和监控你的 Spring Boot 应用程序的应用程序。 这些应用程序通过 Spring Boot Admin Client（通过 HTTP）注册或者使用 Spring Cloud（例如 Eureka）发现。 UI只是 Spring Boot Actuator 端点上的一个 AngularJs 应用程序。 SpringBoot 系列文章 快速开始首先在 IDEA 创建一个 SpringBoot 项目，把它当作 server 端，工程如下： 然后在 pom.xml 中引入依赖： 12345678910&lt;dependency&gt; &lt;groupId&gt;de.codecentric&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-admin-server&lt;/artifactId&gt; &lt;version&gt;1.5.6&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;de.codecentric&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-admin-server-ui&lt;/artifactId&gt; &lt;version&gt;1.5.6&lt;/version&gt;&lt;/dependency&gt; 继续在启动类 SpringbootAdminApplication.java 中引入注解 @EnableAdminServer ，然后运行项目： 访问 http://localhost:8084/ 即可： 此时会发现没有任何应用程序的信息。 接下来我们新建一个 SpringBoot 项目，把它当作客户端程序，工程如下： 在 pom.xml 中添加依赖： 12345&lt;dependency&gt; &lt;groupId&gt;de.codecentric&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-admin-starter-client&lt;/artifactId&gt; &lt;version&gt;1.5.6&lt;/version&gt;&lt;/dependency&gt; 然后在 application.yml 中设置： spring.boot.admin.url=http:localhost:8094 用于将当前应用注册到 Spring Boot Admin。 还可以设置，spring.boot.admin.client.name: （应用程序的名字）不设置的话会有默认的名字 此时把两个项目运行起来： 点击图中的 detail 按钮：可以看到应用程序的健康值、内存、JVM、GC 等信息。 metrics 信息： 环境 信息： log 信息： JMX 信息： 线程 信息： Trace 追踪信息： 还可以下载 Heapdump 文件。 刚才首页的应用列表后面有个红色的 ×，我们可以将注册上去的应用移除，但是只要你不把程序停掉，它立马又会注册上去。 还有就是应用列表的 version 和 info 上面的图中为空，下面看看怎么把它变出来： 123info.groupId: @project.groupId@info.artifactId: @project.artifactId@info.version: @project.version@ 重新运行客户端程序，刷新页面可以发现： 还可以查询应用程序的事件变化： 客户端应用程序JMX bean管理要在管理界面中与JMX-beans进行交互，您必须在客户端应用程序中包含 Jolokia, pom.xml 加入依赖： 1234&lt;dependency&gt; &lt;groupId&gt;org.jolokia&lt;/groupId&gt; &lt;artifactId&gt;jolokia-core&lt;/artifactId&gt;&lt;/dependency&gt; 重启客户端程序后，就可以在这里与 JMX 做交互了： 还有很多 SpringBoot Admin 客户端配置选项： http://codecentric.github.io/spring-boot-admin/1.5.6/#spring-boot-admin-client 服务端程序也有些 SpringBoot Admin 服务端程序配置选项： http://codecentric.github.io/spring-boot-admin/1.5.6/#spring-boot-admin-server 官方文档里面还有些关于服务下线消息通知的知识，想了解的可以查看： http://codecentric.github.io/spring-boot-admin/1.5.6/#_notifications 关注我 参考文章http://codecentric.github.io/spring-boot-admin/1.5.6/ 最后转载请注明文章原始地址为：http://www.54tianzhisheng.cn/2018/01/17/SpringBoot-Admin/","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://www.54tianzhisheng.cn/tags/SpringBoot/"}]},{"title":"Lombok 看这篇就够了","date":"2018-01-08T16:00:00.000Z","path":"2018/01/09/lombok/","text":"前提自从进公司实习后，项目代码中能用 Lombok 的都用了，毕竟这么好的轮子要充分利用好。也可以减少一些 get/set/toString 方法的编写，虽说 IDEA 的插件可以自动生成 get/set/toString 方法，但是使用 Lombok 可以让代码更简洁。下面看看如何在 IDEA 中如何安装 Lombok： 安装打开 IDEA 的 Settings 面板，并选择 Plugins 选项，然后点击 “Browse repositories” 在输入框输入”lombok”，得到搜索结果，点击安装，然后安装提示重启 IDEA，安装成功; 引入依赖在自己的项目里添加 lombok 的编译支持，在 pom 文件里面添加 dependency 123456&lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.16.18&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 怎么使用？在实体类上引入相关的注解就行： 有哪些注解？ @Data @Setter @Getter @Slf4j @AllArgsConstructor @NoArgsConstructor @EqualsAndHashCode @NonNull @Cleanup @ToString @RequiredArgsConstructor @Value @SneakyThrows @Synchronized 注解详解@Data 注解在 类 上；提供类所有属性的 get 和 set 方法，此外还提供了equals、canEqual、hashCode、toString 方法。 @Setter 注解在 属性 上；为单个属性提供 set 方法; 注解在 类 上，为该类所有的属性提供 set 方法， 都提供默认构造方法。 @Getter 注解在 属性 上；为单个属性提供 get 方法; 注解在 类 上，为该类所有的属性提供 get 方法，都提供默认构造方法。 @Slf4j 注解在 类 上；为类提供一个 属性名为 log 的日志对象，提供默认构造方法。 @AllArgsConstructor 注解在 类 上；为类提供一个全参的构造方法，加了这个注解后，类中不提供默认构造方法了。 @NoArgsConstructor 注解在 类 上；为类提供一个无参的构造方法。 @EqualsAndHashCode 注解在 类 上, 可以生成 equals、canEqual、hashCode 方法。 @NonNull 注解在 属性 上，会自动产生一个关于此参数的非空检查，如果参数为空，则抛出一个空指针异常，也会有一个默认的无参构造方法。 @Cleanup 这个注解用在 变量 前面，可以保证此变量代表的资源会被自动关闭，默认是调用资源的 close() 方法，如果该资源有其它关闭方法，可使用 @Cleanup(“methodName”) 来指定要调用的方法，也会生成默认的构造方法 @ToString 这个注解用在 类 上，可以生成所有参数的 toString 方法，还会生成默认的构造方法。 @RequiredArgsConstructor 这个注解用在 类 上，使用类中所有带有 @NonNull 注解的或者带有 final 修饰的成员变量生成对应的构造方法。 @Value 这个注解用在 类 上，会生成含所有参数的构造方法，get 方法，此外还提供了equals、hashCode、toString 方法。 @SneakyThrows 这个注解用在 方法 上，可以将方法中的代码用 try-catch 语句包裹起来，捕获异常并在 catch 中用 Lombok.sneakyThrow(e) 把异常抛出，可以使用 @SneakyThrows(Exception.class) 的形式指定抛出哪种异常，也会生成默认的构造方法。 @Synchronized 这个注解用在 类方法 或者 实例方法 上，效果和 synchronized 关键字相同，区别在于锁对象不同，对于类方法和实例方法，synchronized 关键字的锁对象分别是类的 class 对象和 this 对象，而 @Synchronized 的锁对象分别是 私有静态 final 对象 lock 和 私有 final 对象 lock，当然，也可以自己指定锁对象，此外也提供默认的构造方法。 总结以上注解可根据需要一起搭配使用！ 虽说轮子好，但是我们不仅要知其然，也要知其所以然！ 关注我 最后转载请注明原创地址：http://www.54tianzhisheng.cn/2018/01/07/lombok/","tags":[{"name":"lombok","slug":"lombok","permalink":"http://www.54tianzhisheng.cn/tags/lombok/"}]},{"title":"Spring Boot系列文章（一）：SpringBoot Kafka 整合使用","date":"2018-01-04T16:00:00.000Z","path":"2018/01/05/SpringBoot-Kafka/","text":"前提假设你了解过 SpringBoot 和 Kafka。 SpringBoot 系列文章 1、SpringBoot 如果对 SpringBoot 不了解的话，建议去看看 DD 大佬 的系列博客。 2、Kafka Kafka 的话可以看看我前两天写的博客 ： Kafka 安装及快速入门 学习的话自己开台虚拟机自己手动搭建环境吧，有条件的买服务器。 注意：一定要亲自自己安装实践，接下来我们将这两个进行整合。 创建项目项目整体架构： 使用 IDEA 创建 SpringBoot 项目，这个很简单了，这里不做过多的讲解。 1、pom 文件代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.zhisheng&lt;/groupId&gt; &lt;artifactId&gt;kafka-learning&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;kafka-learning&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot + kafka&lt;/description&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt; &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt; &lt;version&gt;1.1.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.google.code.gson&lt;/groupId&gt; &lt;artifactId&gt;gson&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 主要引入了 spring-kafka 、lombok 、 gson 依赖。 2、消息实体类 Message.java 如下： 123456789@Datapublic class Message &#123; private Long id; //id private String msg; //消息 private Date sendTime; //时间戳&#125; 3、消息发送类 KafkaSender.java 12345678910111213141516171819@Component@Slf4jpublic class KafkaSender &#123; @Autowired private KafkaTemplate&lt;String, String&gt; kafkaTemplate; private Gson gson = new GsonBuilder().create(); //发送消息方法 public void send() &#123; Message message = new Message(); message.setId(System.currentTimeMillis()); message.setMsg(UUID.randomUUID().toString()); message.setSendTime(new Date()); log.info(\"+++++++++++++++++++++ message = &#123;&#125;\", gson.toJson(message)); kafkaTemplate.send(\"zhisheng\", gson.toJson(message)); &#125;&#125; 就这样，发送消息代码就实现了。 这里关键的代码为 kafkaTemplate.send() 方法，zhisheng 是 Kafka 里的 topic ，这个 topic 在 Java 程序中是不需要提前在 Kafka 中设置的，因为它会在发送的时候自动创建你设置的 topic， gson.toJson(message) 是消息内容，这里暂时先说这么多了，不详解了，后面有机会继续把里面源码解读写篇博客出来（因为中途碰到坑，老子跟了几遍源码）。 4、消息接收类 KafkaReceiver.java 12345678910111213141516171819@Component@Slf4jpublic class KafkaReceiver &#123; @KafkaListener(topics = &#123;\"zhisheng\"&#125;) public void listen(ConsumerRecord&lt;?, ?&gt; record) &#123; Optional&lt;?&gt; kafkaMessage = Optional.ofNullable(record.value()); if (kafkaMessage.isPresent()) &#123; Object message = kafkaMessage.get(); log.info(\"----------------- record =\" + record); log.info(\"------------------ message =\" + message); &#125; &#125;&#125; 客户端 consumer 接收消息特别简单，直接用 @KafkaListener 注解即可，并在监听中设置监听的 topic ，topics 是一个数组所以是可以绑定多个主题的，上面的代码中修改为 @KafkaListener(topics = {&quot;zhisheng&quot;,&quot;tian&quot;}) 就可以同时监听两个 topic 的消息了。需要注意的是：这里的 topic 需要和消息发送类 KafkaSender.java 中设置的 topic 一致。 5、启动类 KafkaApplication.java 123456789101112131415161718192021@SpringBootApplicationpublic class KafkaApplication &#123; public static void main(String[] args) &#123; ConfigurableApplicationContext context = SpringApplication.run(KafkaApplication.class, args); KafkaSender sender = context.getBean(KafkaSender.class); for (int i = 0; i &lt; 3; i++) &#123; //调用消息发送类中的消息发送方法 sender.send(); try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 6、配置文件 application.properties 1234567891011121314151617181920212223242526#============== kafka ===================# 指定kafka 代理地址，可以多个spring.kafka.bootstrap-servers=192.168.153.135:9092#=============== provider =======================spring.kafka.producer.retries=0# 每次批量发送消息的数量spring.kafka.producer.batch-size=16384spring.kafka.producer.buffer-memory=33554432# 指定消息key和消息体的编解码方式spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializerspring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer#=============== consumer =======================# 指定默认消费者group idspring.kafka.consumer.group-id=test-consumer-groupspring.kafka.consumer.auto-offset-reset=earliestspring.kafka.consumer.enable-auto-commit=truespring.kafka.consumer.auto-commit-interval=100# 指定消息key和消息体的编解码方式spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializerspring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer spring.kafka.bootstrap-servers 后面设置你安装的 Kafka 的机器 IP 地址和端口号 9092。 如果你只是简单整合下，其他的几个默认就好了。 Kafka 设置在你安装的 Kafka 目录文件下： 启动 zk使用安装包中的脚本启动单节点 Zookeeper 实例： 1bin/zookeeper-server-start.sh -daemon config/zookeeper.properties 启动 Kafka 服务使用 kafka-server-start.sh 启动 kafka 服务： 1bin/kafka-server-start.sh config/server.properties 启动成功后！ 千万注意： 记得将你的虚拟机或者服务器关闭防火墙或者开启 Kafka 的端口 9092。 运行 出现这就代表整合成功了！ 我们看下 Kafka 中的 topic 列表就 1bin/kafka-topics.sh --list --zookeeper localhost:2181 就会发现刚才我们程序中的 zhisheng 已经自己创建了。 关注我 最后转载请务必注明原创地址为：http://www.54tianzhisheng.cn/2018/01/05/SpringBoot-Kafka/","tags":[{"name":"SpringBoot","slug":"SpringBoot","permalink":"http://www.54tianzhisheng.cn/tags/SpringBoot/"},{"name":"Kafka","slug":"Kafka","permalink":"http://www.54tianzhisheng.cn/tags/Kafka/"}]},{"title":"为什么要重新运营以前的公众号呢？","date":"2018-01-03T16:00:00.000Z","path":"2018/01/04/weixin/","text":"前提老读者可能会发现现在我的公众号已经改名了。（由 “猿blog” 变成 “zhisheng” 了，细心的童鞋会发现不仅名字变了， ID 也变了，但是图片还没改，暂时还没想到好的 logo 图片），下面说说为啥吧！ 听我瞎 BB 上图是两年前公众号群发的第一条信息，那时自己还是在学校，如今已经进入了社会，在公司实习了。记得当初开这个公众号的原因是因为几个年轻人有着梦想，打算一起做点东西，当时一腔热血的自己立马就先申请了个公众号，后来 “东西” 倒是没做，反倒是我自己慢慢的在微信公众号分享一些文章，然后那时自己也写博客（算算自己写博客应该快三年了，坚持真不易啊），所以偶尔也把自己的博客分享在微信公众号上。 但是好景不长，那时的微信公众号排版真尼玛难用的一批，作为一个理工科的男生，本来自己做事一般细心和耐心，无奈，把这么好的一个童鞋都给逼坏了。我现在还是得吐槽下，如今的微信公众号后台排版还是那么差。但是可能因为需求比较多了，所以就有人做出了工具（将 markdown 排版后在将整个样式复制粘贴到微信公众号后台），这样一篇排版还算不错的博客就出来了。 自己早就知道了这么个工具，以前看 DD 的博客的时候就发现了这个工具，但是很久没更新的微信公众号，自己也不怎么想再管理。 有人就要问了？那为啥现在又要开始跟新了呢？ 我只想说：“贱人就是矫情！！！又想瞎折腾下。”，反正自己的博客也在不断的更新，偶尔顺带把文章同步到微信公众号其实也是可以的。在学校的时候时间比较多，那时真的是时间比较多，后悔没好好坚持运营下来。现在工作了，自己工作之外的时间较少，除了学习，偶尔写写博客，娱乐时间比较少，都是大学时宅的。 前段时间被人 “忽悠” 说继续更新公众号，那时刚好也快 2018 年了，自己也想给自己定几个目标，在元旦的那天，想想还是继续更新微信公众号吧，所以你也看得到最近我的更新了，可能最近的更新比较有规律，因为这些文章大部分是之前就已经写好了的，已经发过在我的博客里了。估计把这些文章更新完后，就不会每天都更新我自己的文章了。 定位说下微信公众号的定位吧： 1、我的技术博客应该都会同步在这里的。 2、分享自己平时的随笔文章。（比如这篇。。。） 3、除了技术文章，当然还有平时自己的 奇淫技巧 （包括但不限于写作方式、推荐好用的软件等） 4、分享自己觉得不错的文章（别人的，尽量征得同意，一定会备注原创地址的） 5、如果你也写博客，但是阅读量很小的话，可以考虑自荐。（注：文章我可能会审批，必须要觉得不错的文章） 6、分享一些学习视频和书籍 7、后期可能会搞工作内推 。。。 暂时只想到这些了 问题来了因为工作了，所以时间少，运营这微信公众号可能需要花费我不少工作之外的时间。 如果可以的话，我希望能找到一个能帮我分担点的朋友。 注：无偿的，如果介意的话，下面就不用再看了。 说点简单的要求吧： 1、细心、耐心的 boy or girl 都行 2、起码要知道点编程方面的知识 3、能坚持下来 4、对新技术有敏感的嗅觉 5、最后一点就是你要有点时间了，希望不耽误你学习 再说下能给你带来的 好处 吧： 1、肯定能增加你的运营能力（再去互联网公司投运营岗位会有优势的） 2、本人一开始会亲自教授该怎么做，所以没经验的朋友不用担心 3、可以增加和大牛勾搭的机会，你们懂的。。 4、本人可以亲自传授经验（编程和生活点滴经验） 如果你有意愿的话，请加我 QQ ： 1041218129 聊聊吧 关注我","tags":[{"name":"随笔","slug":"随笔","permalink":"http://www.54tianzhisheng.cn/tags/随笔/"}]},{"title":"Kafka 安装及快速入门","date":"2018-01-03T16:00:00.000Z","path":"2018/01/04/Kafka/","text":"介绍官网：http://kafka.apache.org/ Apache Kafka是分布式发布-订阅消息系统。它最初由LinkedIn公司开发，之后成为Apache项目的一部分。Kafka是一种快速、可扩展的、设计内在就是分布式的，分区的和可复制的提交日志服务。Apache Kafka与传统消息系统相比，有以下不同： 它被设计为一个分布式系统，易于向外扩展； 它同时为发布和订阅提供高吞吐量； 它支持多订阅者，当失败时能自动平衡消费者； 它将消息持久化到磁盘，因此可用于批量消费，例如ETL，以及实时应用程序。 安装 kafka下载地址：https://kafka.apache.org/downloads 1wget http://mirrors.shuosc.org/apache/kafka/1.0.0/kafka_2.11-1.0.0.tgz 解压：123tar -zxvf kafka_2.11-1.0.0.tgzcd /usr/local/kafka_2.11-1.0.0/ 修改 kafka-server 的配置文件 1vim /usr/local/kafka/config/server.properties 修改其中的： 12broker.id=1log.dir=/data/kafka/logs-1 功能验证：1、启动 zk使用安装包中的脚本启动单节点 Zookeeper 实例： 1bin/zookeeper-server-start.sh -daemon config/zookeeper.properties 2、启动Kafka 服务使用 kafka-server-start.sh 启动 kafka 服务： 1bin/kafka-server-start.sh config/server.properties 3、创建 topic使用 kafka-topics.sh 创建单分区单副本的 topic test： 1bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test 查看 topic 列表： 1bin/kafka-topics.sh --list --zookeeper localhost:2181 查询创建的 topic 列表报错： 解决方法: 1vim /etc/hosts 将 host 里的 12127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 修改为： 12127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4::1 ip6-localhost ip6-localhost.localdomain localhost6 localhost6.localdomain6 方法参考：zookeeper unable to open socket to localhost/0:0:0:0:0:0:0:1:2181 再次查询就不报错了。 4、产生消息使用 kafka-console-producer.sh 发送消息： 1bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test 5、消费消息使用 kafka-console-consumer.sh 接收消息并在终端打印： 1bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning 打开个新的命令窗口执行上面命令即可查看信息： 6、查看描述 topics 信息1bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test 结果： 12Topic:test PartitionCount:1 ReplicationFactor:1 Configs: Topic: test Partition: 0 Leader: 1 Replicas: 1 Isr: 1 第一行给出了所有分区的摘要，每个附加行给出了关于一个分区的信息。 由于我们只有一个分区，所以只有一行。 “Leader”: 是负责给定分区的所有读取和写入的节点。 每个节点将成为分区随机选择部分的领导者。 “Replicas”: 是复制此分区日志的节点列表，无论它们是否是领导者，或者即使他们当前处于活动状态。 “Isr”: 是一组“同步”副本。这是复制品列表的子集，当前活着并被引导到领导者。 集群配置Kafka 支持两种模式的集群搭建：可以在单机上运行多个 broker 实例来实现集群，也可在多台机器上搭建集群，下面介绍下如何实现单机多 broker 实例集群，其实很简单，只需要如下配置即可。 单机多broker 集群配置利用单节点部署多个 broker。 不同的 broker 设置不同的 id，监听端口及日志目录。 例如： 1234567cp config/server.properties config/server-2.propertiescp config/server.properties config/server-3.propertiesvim config/server-2.propertiesvim config/server-3.properties 修改 ： 12345broker.id=2listeners = PLAINTEXT://your.host.name:9093log.dir=/data/kafka/logs-2 和 12345broker.id=3listeners = PLAINTEXT://your.host.name:9094log.dir=/data/kafka/logs-3 启动Kafka服务： 123bin/kafka-server-start.sh config/server-2.properties &amp;bin/kafka-server-start.sh config/server-3.properties &amp; 至此，单机多broker实例的集群配置完毕。 多机多 broker 集群配置分别在多个节点按上述方式安装 Kafka，配置启动多个 Zookeeper 实例。 假设三台机器 IP 地址是 ： 192.168.153.135， 192.168.153.136， 192.168.153.137 分别配置多个机器上的 Kafka 服务，设置不同的 broker id，zookeeper.connect 设置如下: 1vim config/server.properties 里面的 zookeeper.connect 修改为： 1zookeeper.connect=192.168.153.135:2181,192.168.153.136:2181,192.168.153.137:2181 使用 Kafka Connect 来导入/导出数据从控制台写入数据并将其写回控制台是一个方便的起点，但您可能想要使用其他来源的数据或将数据从 Kafka 导出到其他系统。对于许多系统，您可以使用 Kafka Connect 来导入或导出数据，而不必编写自定义集成代码。 Kafka Connect 是 Kafka 包含的一个工具，可以将数据导入和导出到 Kafka。它是一个可扩展的工具，运行 连接器，实现与外部系统交互的自定义逻辑。在这个快速入门中，我们将看到如何使用简单的连接器运行 Kafka Connect，这些连接器将数据从文件导入到 Kafka topic，并将数据从 Kafka topic 导出到文件。 首先，我们将通过创建一些种子数据开始测试： 1echo -e &quot;zhisheng\\ntian&quot; &gt; test.txt 接下来，我们将启动两个以独立模式运行的连接器，这意味着它们将在单个本地专用进程中运行。我们提供三个配置文件作为参数。首先是 Kafka Connect 过程的配置，包含常见的配置，例如要连接的 Kafka 代理以及数据的序列化格式。其余的配置文件都指定一个要创建的连接器。这些文件包括唯一的连接器名称，要实例化的连接器类以及连接器所需的任何其他配置。 1bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties Kafka 附带的这些示例配置文件使用您之前启动的默认本地群集配置，并创建两个连接器：第一个是源连接器，用于读取输入文件中的行，并将每个连接生成为 Kafka topic，第二个为连接器它从 Kafka topic 读取消息，并在输出文件中产生每行消息。 在启动过程中，您会看到一些日志消息，其中一些指示连接器正在实例化。Kafka Connect 进程启动后，源连接器应该开始读取 test.txt topic connect-test，并将其生成 topic ，并且接收器连接器应该开始读取 topic 中的消息 connect-test 并将其写入文件 test.sink.txt。我们可以通过检查输出文件的内容来验证通过整个管道传输的数据： 数据存储在 Kafka topic 中 connect-test，因此我们也可以运行控制台使用者来查看 topic 中的数据 1bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic connect-test --from-beginning 连接器继续处理数据，所以我们可以将数据添加到文件中，并看到它在管道中移动： 1234echo zhishengtian&gt;&gt; test.txtecho zhishengtian2&gt;&gt; test.txtecho zhishengtian3&gt;&gt; test.txtecho zhishengtian4&gt;&gt; test.txt 使用 Kafka 流来处理数据Kafka Streams 是用于构建关键任务实时应用程序和微服务的客户端库，输入和/或输出数据存储在 Kafka 集群中。Kafka Streams 结合了在客户端编写和部署标准 Java 和 Scala 应用程序的简单性以及 Kafka 服务器端集群技术的优势，使这些应用程序具有高度可伸缩性，弹性，容错性，分布式等特性。 可参考官网入门案例：http://kafka.apache.org/10/documentation/streams/quickstart 参考1、在CentOS 7上安装Kafka 2、http://kafka.apache.org/10/documentation/streams/quickstart 关注我 最后转载请注明原创地址为：http://www.54tianzhisheng.cn/2018/01/04/Kafka/","tags":[{"name":"Kafka","slug":"Kafka","permalink":"http://www.54tianzhisheng.cn/tags/Kafka/"}]},{"title":"Windows 下安装 Consul","date":"2017-12-26T16:00:00.000Z","path":"2017/12/27/consul-install/","text":"前提从刚工作就开始接触 Consul，中途自己也有两个项目和 Consul 有关，后面有机会再讲讲，网上关于这个的资料还比较少。因为明天有 Consul 的技术分享，所以自己今天下午在官网看了下相关的介绍。 介绍Consul 是一个支持多数据中心分布式高可用的服务发现和配置共享的服务软件, 由 HashiCorp 公司用 Go 语言开发, 基于 Mozilla Public License 2.0 的协议进行开源。Consul 支持健康检查, 并允许 HTTP 和 DNS 协议调用 API 存储键值对。命令行超级好用的虚拟机管理软件 vgrant 也是 HashiCorp 公司开发的产品。一致性协议采用 Raft 算法, 用来保证服务的高可用， 使用 GOSSIP 协议管理成员和广播消息, 并且支持 ACL 访问控制。 下载安装去官网下载：https://www.consul.io/downloads.html 得到一个 zip 压缩包 在你想要安装的位置解压就行，只有一个 consul.exe 文件（我的解压位置是：D:\\software） 设置环境变量（在 path 中新增一条）： D:\\software cmd 命令窗口启动： 1consul agent -dev consul 自带 UI 界面，打开网址：http://localhost:8500 ，可以看到当前注册的服务界面。 Consul 优势 使用 Raft 算法来保证一致性, 比复杂的 Paxos 算法更直接. 相比较而言, zookeeper 采用的是 Paxos, 而 etcd 使用的则是 Raft. 支持多数据中心，内外网的服务采用不同的端口进行监听。 多数据中心集群可以避免单数据中心的单点故障,而其部署则需要考虑网络延迟, 分片等情况等. zookeeper 和 etcd 均不提供多数据中心功能的支持. 支持健康检查. etcd 不提供此功能. 支持 http 和 dns 协议接口. zookeeper 的集成较为复杂, etcd 只支持 http 协议. 官方提供web管理界面, etcd 无此功能. 综合比较, Consul 作为服务注册和配置管理的新星, 比较值得关注和研究. 最后 本文首发于：zhisheng的博客 地址为：http://www.54tianzhisheng.cn/2017/12/27/consul-install/ 转载请注明地址！","tags":[{"name":"Consul","slug":"Consul","permalink":"http://www.54tianzhisheng.cn/tags/Consul/"}]},{"title":"Elasticsearch 系列文章（五）：ELK 实时日志分析平台环境搭建","date":"2017-12-24T16:00:00.000Z","path":"2017/12/25/ELK/","text":"简单介绍ELK（ElasticSearch, Logstash, Kibana），三者组合在一起搭建实时的日志分析平台，目前好多公司都是这套！ Elasticsearch 是个开源分布式搜索引擎，它的特点有：分布式，零配置，自动发现，索引自动分片，索引副本机制，restful 风格接口，多数据源，自动搜索负载等。 Logstash 是一个完全开源的工具，他可以对你的日志进行收集、过滤，并将其存储供以后使用（如，搜索）。 Kibana 也是一个开源和免费的工具，它 Kibana 可以为 Logstash 和 ElasticSearch 提供的日志分析友好的 Web 界面，可以帮助您汇总、分析和搜索重要数据日志。 安装 ES。。。这个省略，不 bb 了，以前写过。。。传送门：http://www.54tianzhisheng.cn/2017/09/09/Elasticsearch-install/ 安装 LogstashELK 整套环境搭建版本很关键，最好全统一一个版本，否则出啥问题就不太好找了。这是我见过版本统一最严格的了。而已 ES 版本升了后，其他的都要都要升级，包括其插件。升级代价挺大的，最好一开始就定位好要安装哪个版本！ 在官网下好安装包后传到 Linux 上，这是速度最快的。 1234567891011121314151617181920212223242526在 /usr/local 目录下解压：tar -zxvf logstash-5.5.2.tar.gz进入解压后的目录：cd /usr/local/logstash-5.5.2/bin新增配置文件：vim logstash.conf增加：input&#123; file&#123; path =&gt; [&quot;/var/log/*.log&quot;] &#125;&#125;output&#123; elasticsearch&#123; hosts =&gt; [&quot;192.168.153.135:9200&quot;] index =&gt; &quot;logstash__log&quot; &#125;&#125; Logstash 的启动方式是： 123在 /usr/local/logstash-5.5.2/bin 目录下运行：./logstash -f logstash.conf 安装 Kibana同样，官网下好安装包，上传到 Linux。 1234567891011解压：tar -zxvf kibana-5.5.2-linux-x86_64.tar.gz修改配置文件 kibana-5.5.2/config/kibana.yml 如下：Server.host //配置机器ip/hostnameServer.name //此kibana服务的名称elasticsearch.url //es master节点url Kibana 启动方式： 123在 /usr/local/kibana-5.5.2/bin 目录下运行：./kibana Web界面访问: http://ip:5601 此时需要输入用户名和密码登录,默认分别是 elastic 和 changeme X-PackX-Pack 是一个 Elastic Stack 的扩展，将安全，警报，监控，报告和图形功能包含在一个易于安装的软件包中。 ES 和 Kibana 都可安装。 插件 x-pack-5.5.2.zip 依旧官网下。 ES 安装 X-Pack123cd /usr/local/elasticsearch/bin./elasticsearch-plugin install file:///opt/es/x-pack-5.5.2.zip 如果成功：显示如下 123456789101112131415161718192021222324252627282930[root@node1 bin]# ./elasticsearch-plugin install file:///opt/es/x-pack-5.5.2.zip-&gt; Downloading file:///opt/es/x-pack-5.5.2.zip[=================================================] 100%@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ WARNING: plugin requires additional permissions @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@* java.io.FilePermission \\\\.\\pipe\\* read,write* java.lang.RuntimePermission accessClassInPackage.com.sun.activation.registries* java.lang.RuntimePermission getClassLoader* java.lang.RuntimePermission setContextClassLoader* java.lang.RuntimePermission setFactory* java.security.SecurityPermission createPolicy.JavaPolicy* java.security.SecurityPermission getPolicy* java.security.SecurityPermission putProviderProperty.BC* java.security.SecurityPermission setPolicy* java.util.PropertyPermission * read,write* java.util.PropertyPermission sun.nio.ch.bugLevel write* javax.net.ssl.SSLPermission setHostnameVerifierSee http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.htmlfor descriptions of what these permissions allow and the associated risks.Continue with installation? [y/N]y@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ WARNING: plugin forks a native controller @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@This plugin launches a native controller that is not subject to the Javasecurity manager nor to system call filters.Continue with installation? [y/N]y-&gt; Installed x-pack Kibana 安装 X-Pack123cd /usr/local/kibana-5.5.2/bin./kibana-plugin install file:///opt/es/x-pack-5.5.2.zip 安装成功如下： 123456789[root@node1 bin]# ./kibana-plugin install file:///opt/es/x-pack-5.5.2.zipAttempting to transfer from file:///opt/es/x-pack-5.5.2.zipTransferring 159867054 bytes....................Transfer completeRetrieving metadata from plugin archiveExtracting plugin archiveExtraction completeOptimizing and caching browser bundles...Plugin installation complete 启用 x-pack 安全机制分别在 kibana.yml 和 elasticsearch.yml 中加入下行 1xpack.security.enabled: true 这样后，你再打开 ES 的 head 界面和 Kibana 管理界面就需要输入账号密码了。 上图右边是安装 X-Pack 后的，功能多了几个。 最后环境搭建很简单，后面如果有时间的话可以再讲讲在 Kibana 的 Dev Tools 上构建 ES 的 JSON 串来对 ES 进行操作。 我还写过 ES 相关的文章： 1、Elasticsearch 默认分词器和中分分词器之间的比较及使用方法 2、全文搜索引擎 Elasticsearch 集群搭建入门教程 3、ElasticSearch 集群监控 4、ElasticSearch 单个节点监控 结尾 本文首发于：zhisheng 的博客 转载请注明地址：http://www.54tianzhisheng.cn/2017/12/25/ELK/","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://www.54tianzhisheng.cn/tags/ElasticSearch/"},{"name":"LogStash","slug":"LogStash","permalink":"http://www.54tianzhisheng.cn/tags/LogStash/"},{"name":"Kibana","slug":"Kibana","permalink":"http://www.54tianzhisheng.cn/tags/Kibana/"}]},{"title":"Hexo + yilia 搭建博客可能会遇到的所有疑问","date":"2017-12-17T16:00:00.000Z","path":"2017/12/18/hexo-yilia/","text":"前提为什么会再次写这篇博客？请看下图： 这是我博客搜索引擎的主要关键字。为什么会有这些关键字呢？ 我猜估计是曾经写了几篇关于搭建博客的文章，被搜索引擎收入了，所以搜索引擎才会将这些流量引导至我的博客，文章如下： 1、利用Github Page 搭建个人博客网站 2、Hexo + yilia 主题实现文章目录 3、Github pages + Hexo 博客 yilia 主题使用畅言评论系统 那还有这么多人搜索这些关键字？说明碰到问题的还有不少，所以才有了这篇文章的诞生！ 问题解答1、hexo yilia 文章目录 这个我以前写过一篇文章：Hexo + yilia 主题实现文章目录 那篇文章写了我那个版本的 yilia 怎么添加文章目录的，但是好像新版本的 yilia 已经自带了这个文章目录功能。所以如果你是使用的新版本的 yilia ，请不要做任何修改！但是前几天有人给我发了个图片，又好像有点区别，如果实在有不同的话，请加群 528776268 找我要我那个主题版本的所有配置文件。再次说明，我前端也不是很擅长，我写那篇文章也是参考其他博客的修改，所以无能为力了。有什么问题，建议直接在 yilia 主题的 GitHub 去找作者聊！ 2、Hexo yilia 随笔 随笔如下： 对此想说的就是，“随笔” 其实就是文章的一个 tags(标签)，如果你想把文章作为随笔的话，请在文章的首部写个 tags 为 “随笔” 的标签。如下图： 注意：- 后面有个空格。 3、yilia 主题分类实现 如果要有多个标签，可以如下图所示： 4、hexo yilia 设置文章显示长度，不展开全文 yilia 主题中可以用 &lt;!-- more --&gt; 截取文章的显示长度，如果你想在哪截取文章，就在那行使用该字符。 5、yilia 添加阅读量 我添加的是 “不蒜子” 计数，它可以区分 pv/uv 的统计方式，统计更精准，满足更多需求。有这个需求的可以去查找下博客怎么添加。（网上有很多这方面的博客） 6、yilia 主题使用 “畅言” 评论系统 参见我以前的文章： Github pages + Hexo 博客 yilia 主题使用畅言评论系统 7、hexo yilia 引入音乐 1&lt;iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=330 height=86 src=\"填写音乐链接地址\"&gt;&lt;/iframe&gt; 如下图，可以在网易云音乐里搜到你想要引入的音乐，然后点击如下的 “生成外链播放器” 即可： 8、hexo yilia 引入视频 hexo 支持 html 语法的，所以可以如上图这样引入视频！ 9、hexo yilia 相册 这个抱歉，我自己也没做这方面的功能，暂时不太清楚怎么实现。不过有文章写怎么实现，大家可以搜索下！ 10、hexo yilia 怎么写文章 我一般写文章就是先用本地 markdown 编辑器写好后，然后放在 hexo 的 source/_posts 目录下。 结尾好了，大概就这些问题，我也一一解答了，希望搭建博客的你可以看到这篇文章，让你少走点弯路，如果你也遇到过这些问题，还请你能分享下文章，让更多人避免入坑！ 本文地址是：Hexo + yilia 搭建博客可能会遇到的所有疑问 本文原创，转载请注明原创地址。 最后http://www.54tianzhisheng.cn/2017/12/18/hexo-yilia 这个链接是让推酷爬虫吞掉的，哈哈！","tags":[{"name":"hexo","slug":"hexo","permalink":"http://www.54tianzhisheng.cn/tags/hexo/"},{"name":"yilia","slug":"yilia","permalink":"http://www.54tianzhisheng.cn/tags/yilia/"}]},{"title":"谷歌开发者大会收获满满，不去真 “可惜” 了","date":"2017-12-12T16:00:00.000Z","path":"2017/12/13/Google-Developer-Days/","text":"全文图片较多，请在 WiFi 下阅读，土豪请随意！ 前提今年 Google 开发者大会再度来袭，大会将于 12 月 13 日和 14 日在上海举办，主题涵盖机器学习(Machine Learning)、Android、移动网络(Mobile Web)、TensorFlow、Firebase、云服务(Cloud)、AR/VR、设计(Design)以及更多开发者相关内容。 今天我就到走一遭，收获满满，都是用袋子提回来的，哈哈。下图为袋子： 再秀张图代表我去了： 入场 说下今天我参加的会场吧！ 会场主会场开幕 当然是用来用来做开发者开幕大会主题演讲的。相信不少没到现场的也看了直播。 拍了两张李飞飞演讲时的照片： 还有个妹子是讲 TensorFlow 的，全程中文，还贼 6，佩服！！！ 中途演讲还好几个，没拍照了。。。 中途茶歇：去外面看了下。 主会场演讲主题是：《渐进式网页应用：快速、集成、可靠并且具有吸引力》 这次坐的是前排，还拍了照，演讲人技巧很好，边演讲边带有身体动作的，而且还比较诙谐。 午餐胸牌上有13、14 号的午餐券，可以免费吃、免费拿，福利超好。 下午会场下午会场有点多，略略略。。。 都拍了点照，如果想要，可以加群：528776268 找我要、 晚餐诱惑颇大。。 我还喝了杯葡萄酒。。哈哈 另外除了照片，还拍了三个视频 回家吃饱喝足，回家拍了张照 礼物到家了，整理了下今天的礼物： 贴纸一张 小礼品一个 一个可 DIY 的音箱 一个定制的手提电脑包，质量很好。 AndroidThings 最后全文图片较多，谢谢阅读！自己收获也挺多的，明天还有一天，可惜不打算去了，转载请注明地址：http://www.54tianzhisheng.cn/2017/12/13/Google-Developer-Days/ 结尾这个是为了防爬虫写的，哈哈","tags":[{"name":"随笔","slug":"随笔","permalink":"http://www.54tianzhisheng.cn/tags/随笔/"}]},{"title":"使用 CodeMirror 打造属于自己的在线代码编辑器","date":"2017-12-08T16:00:00.000Z","path":"2017/12/09/CodeMirror/","text":"前提写这个的目的是因为之前项目里用到过 CodeMirror，觉得作为一款在线代码编辑器还是不错，也看到过有些网站用到过在线代码编辑，当然我不知道他们是用什么做的，这里我把公司项目里用到的那部分抽出来，单独写篇博客，并把抽出来的那部分代码提交到 GitHub 去（地址），以防日后可能会再次用到（没准毕业设计里可能用的到）。 简单介绍CodeMirror 是一款在线的支持语法高亮的代码编辑器。官网： http://codemirror.net/ 可能光看官网，第一眼觉得那些在线编辑器有点丑，反正第一眼给我的感觉就是这样子，但是经过自己的细调，也能打造出一款精美的在线代码编辑器。 官网可以把它下载下来。 下载后，解压开得到的文件夹中，lib 下是放的是核心库和核心 css，mode 下放的是各种支持语言的语法定义，theme 目录下是支持的主题样式。一般在开发中，添加 lib 下的引用和 mode 下的引用就够了。 如何使用下面两个是使用 Code Mirror 必须引入的： 12&lt;link rel=\"stylesheet\" href=\"codemirror-5.31.0/lib/codemirror.css\"/&gt;&lt;script src=\"codemirror-5.31.0/lib/codemirror.js\"&gt;&lt;/script&gt; 接下来要引用的就是在 mode 目录下编辑器中要编辑的语言对应的 js 文件，这里以 Groovy 为例： 12&lt;!--groovy代码高亮--&gt;&lt;script src=\"codemirror-5.31.0/mode/groovy/groovy.js\"&gt;&lt;/script&gt; 如果你想让 Java 代码也支持代码高亮，则需要引入我从网上下载下来的 clike.js（我已经放到我的 GitHub 去了） 12&lt;!--Java代码高亮必须引入--&gt;&lt;script src=\"codemirror-5.31.0/clike.js\"&gt;&lt;/script&gt; 引用的文件用于支持对应语言的语法高亮。 然后前面说了第一次进入 Code Mirror 官网，觉得那些编辑器比较丑，那可能是主题比较丑，我这里推荐一款还不错的主题，只需按照如下引入即可： 12&lt;!--引入css文件，用以支持主题--&gt;&lt;link rel=\"stylesheet\" href=\"codemirror-5.31.0/theme/dracula.css\"/&gt; 如果你还想让你的编辑器支持代码行折叠，请按照如下进行操作： 123456&lt;!--支持代码折叠--&gt;&lt;link rel=\"stylesheet\" href=\"codemirror-5.31.0/addon/fold/foldgutter.css\"/&gt;&lt;script src=\"codemirror-5.31.0/addon/fold/foldcode.js\"&gt;&lt;/script&gt;&lt;script src=\"codemirror-5.31.0/addon/fold/foldgutter.js\"&gt;&lt;/script&gt;&lt;script src=\"codemirror-5.31.0/addon/fold/brace-fold.js\"&gt;&lt;/script&gt;&lt;script src=\"codemirror-5.31.0/addon/fold/comment-fold.js\"&gt;&lt;/script&gt; 是不是这样引入就好了呢，当然不是啦 创建编辑器在实际项目中，一般都不会直接把 body 整个内容作为编辑器的容器。而最常用的，是使用 textarea。这里我在 里使用个 textarea， 123&lt;!-- begin code --&gt;&lt;textarea class=\"form-control\" id=\"code\" name=\"code\"&gt;&lt;/textarea&gt;&lt;!-- end code--&gt; 接下来就是创建编辑器了。 123//根据DOM元素的id构造出一个编辑器var editor = CodeMirror.fromTextArea(document.getElementById(\"code\"), &#123;&#125;); 是不是有点单调？ 没错，我还可以在里面给他设置些属性：（充分利用我一开始引入的那些文件） 123456789mode: \"text/groovy\", //实现groovy代码高亮mode: \"text/x-java\", //实现Java代码高亮lineNumbers: true, //显示行号theme: \"dracula\", //设置主题lineWrapping: true, //代码折叠foldGutter: true,gutters: [\"CodeMirror-linenumbers\", \"CodeMirror-foldgutter\"],matchBrackets: true, //括号匹配//readOnly: true, //只读 如果需要查看更多属性，可以去官网查找，目前我只用到这些属性！ 下面也列举些吧： indentUnit: integer缩进单位，值为空格数，默认为2 。 smartIndent: boolean自动缩进，设置是否根据上下文自动缩进（和上一行相同的缩进量）。默认为true。 tabSize: integertab字符的宽度，默认为4 。 indentWithTabs: boolean在缩进时，是否需要把 n*tab宽度个空格替换成n个tab字符，默认为false 。 electricChars: boolean在输入可能改变当前的缩进时，是否重新缩进，默认为true （仅在mode支持缩进时有效）。 specialChars: RegExp需要被占位符(placeholder)替换的特殊字符的正则表达式。最常用的是非打印字符。默认为：/[\\u0000-\\u0019\\u00ad\\u200b-\\u200f\\u2028\\u2029\\ufeff]/。 specialCharPlaceholder: function(char) → Element这是一个接收由specialChars选项指定的字符作为参数的函数，此函数会产生一个用来显示指定字符的DOM节点。默认情况下，显示一个红点（•），这个红点有一个带有前面特殊字符编码的提示框。 rtlMoveVisually: booleanDetermines whether horizontal cursor movement through right-to-left (Arabic, Hebrew) text is visual (pressing the left arrow moves the cursor left) or logical (pressing the left arrow moves to the next lower index in the string, which is visually right in right-to-left text). The default is false on Windows, and true on other platforms.（这段完全不晓得搞啥子鬼） keyMap: string配置快捷键。默认值为default，即 codemorrir.js 内部定义。其它在key map目录下。 extraKeys: object给编辑器绑定与前面keyMap配置不同的快捷键。 lineWrapping: boolean在长行时文字是换行(wrap)还是滚动(scroll)，默认为滚动(scroll)。 lineNumbers: boolean是否在编辑器左侧显示行号。 firstLineNumber: integer行号从哪个数开始计数，默认为1 。 lineNumberFormatter: function(line: integer) → string使用一个函数设置行号。 gutters: array用来添加额外的gutter（在行号gutter前或代替行号gutter）。值应该是CSS名称数组，每一项定义了用于绘制gutter背景的宽度（还有可选的背景）。为了能明确设置行号gutter的位置（默认在所有其它gutter的右边），也可以包含CodeMirror-linenumbers类。类名是用于传给setGutterMarker的键名(keys)。 fixedGutter: boolean设置gutter跟随编辑器内容水平滚动（false）还是固定在左侧（true或默认）。 scrollbarStyle: string设置滚动条。默认为”native”，显示原生的滚动条。核心库还提供了”null”样式，此样式会完全隐藏滚动条。Addons可以设置更多的滚动条模式。 coverGutterNextToScrollbar: boolean当fixedGutter启用，并且存在水平滚动条时，在滚动条最左侧默认会显示gutter，当此项设置为true时，gutter会被带有CodeMirror-gutter-filler类的元素遮挡。inputStyle: string选择CodeMirror处理输入和焦点的方式。核心库定义了textarea和contenteditable输入模式。在移动浏览器上，默认是contenteditable，在桌面浏览器上，默认是textarea。在contenteditable模式下对IME和屏幕阅读器支持更好。 readOnly: boolean|string编辑器是否只读。如果设置为预设的值 “nocursor”，那么除了设置只读外，编辑区域还不能获得焦点。 showCursorWhenSelecting: boolean在选择时是否显示光标，默认为false。 lineWiseCopyCut: boolean启用时，如果在复制或剪切时没有选择文本，那么就会自动操作光标所在的整行。 undoDepth: integer最大撤消次数，默认为200（包括选中内容改变事件） 。 historyEventDelay: integer在输入或删除时引发历史事件前的毫秒数。 tabindex: integer编辑器的tabindex。 autofocus: boolean是否在初始化时自动获取焦点。默认情况是关闭的。但是，在使用textarea并且没有明确指定值的时候会被自动设置为true。 dragDrop: boolean是否允许拖放，默认为true。 allowDropFileTypes: array默认为null。当设置此项时，只接收包含在此数组内的文件类型拖入编辑器。文件类型为MIME名称。 cursorBlinkRate: number光标闪动的间隔，单位为毫秒。默认为530。当设置为0时，会禁用光标闪动。负数会隐藏光标。 cursorScrollMargin: number当光标靠近可视区域边界时，光标距离上方和下方的距离。默认为0 。 cursorHeight: number光标高度。默认为1，也就是撑满行高。对一些字体，设置0.85看起来会更好。 resetSelectionOnContextMenu: boolean设置在选择文本外点击打开上下文菜单时，是否将光标移动到点击处。默认为true。 workTime, workDelay: number通过一个假的后台线程高亮 workTime 时长，然后使用 timeout 休息 workDelay 时长。默认为200和300 。（完全不懂这个功能是在说啥） pollInterval: number指明CodeMirror向对应的textarea滚动（写数据）的速度（获得焦点时）。大多数的输入都是通过事件捕获，但是有的输入法（如IME）在某些浏览器上并不会生成事件，所以使用数据滚动。默认为100毫秒。 flattenSpans: boolean默认情况下，CodeMirror会将使用相同class的两个span合并成一个。通过设置此项为false禁用此功能。 addModeClass: boolean当启用时（默认禁用），会给每个标记添加额外的表示生成标记的mode的以cm-m开头的CSS样式类。例如，XML mode产生的标记，会添加cm-m-xml类。 maxHighlightLength: number当需要高亮很长的行时，为了保持响应性能，当到达某些位置时，编辑器会直接将其他行设置为纯文本(plain text)。默认为10000，可以设置为Infinity来关闭此功能。 viewportMargin: integer指定当前滚动到视图中内容上方和下方要渲染的行数。这会影响到滚动时要更新的行数。通常情况下应该使用默认值10。可以设置值为Infinity始终渲染整个文档。注意：这样设置在处理大文档时会影响性能。 如果你要设置代码框的大小该怎么做呢？ 1editor.setSize('800px', '950px'); //设置代码框的长宽 另外，如果你想给代码框赋值，该怎么办呢？ 12editor.setValue(\"\"); //给代码框赋值editor.getValue(); //获取代码框的值 如果你再想在其他地方设置新的属性，可以像下面这样写： 1editor.setOption(\"readOnly\", true); //类似这种 总结上面就大概讲了下 Code Mirror 怎么使用，那么我们来看看效果吧 我自我感觉还是可以的哈！ 里面所有涉及的代码在 GitHub 里可以下载：https://github.com/zhisheng17/CoderBlog/tree/master/CodeMirror 文章原创，转载务必请注明原创地址：http://www.54tianzhisheng.cn/2017/12/09/CodeMirror/ 最后fuck 无脑的推酷爬虫，竟然把我所有文章最后的原创链接都给去掉了，这是我现在想到的一种对策方法。任何其他形式的转载，也必须把我文章所有内容加上，不得做任何修改，否则请别转载了！","tags":[{"name":"前端","slug":"前端","permalink":"http://www.54tianzhisheng.cn/tags/前端/"}]},{"title":"Netty系列文章（一）：Netty 源码阅读之初始环境搭建","date":"2017-12-07T16:00:00.000Z","path":"2017/12/08/netty-01-env/","text":"Netty 简介Netty 是由 JBOSS 提供的一个开源的 java 网络编程框架，主要是对 java 的 nio 包进行了再次封装。Netty 比 java 原生的nio 包提供了更加强大、稳定的功能和易于使用的 api。 netty 的作者是 Trustin Lee，这是一个韩国人，他还开发了另外一个著名的网络编程框架，mina。二者在很多方面都十分相似，它们的线程模型也是基本一致 。不过 netty 社区的活跃程度要 mina 高得多。版本选择： 3.x 目前企业使用最多的版本，最为稳定。例如dubbo使用的就是3.x版本 4.x 引入了内存池等重大特性，可以有效的降低GC负载，rocketmq使用的就是4.x 5.x 已经被废弃了，具体可参见 https://github.com/netty/netty/issues/4466 所以这里我搭建的源码阅读环境是存在的 4.1 版本。 准备工具 IDEA 2017 环境搭建在 IDEA 中导入项目地址：https://github.com/netty/netty.git ，然后就会自动下载项目所有的依赖，但是请注意： 必须在 IDEA 中将 Profiles 中的所有都勾选上，否则会导致很多 jar 包拉不下来，如下图： 然后就是耐心等待了，一直到所有的 jar 包拉取下来。 中途你可能会遇到如下问题： 这里的是 1.5 版本，导致我们如果想用些高级的语法会完全报错。 如果你把这个版本设置为 8 的版本后， 下面会提示你，项目是从 maven 导过来的，如果 maven 配置改变重新 reimport 后，任何在这里的改变都会丢失。 同时你会看到项目的 Java Compile 版本是 1.5 的，如下图： 同样，你在这里修改，如果 maven 配置改变重新 reimport 后，任何在这里的改变也都会丢失。我估计碰到这种问题的不少。 总结起来原因就是 maven 中的编译版本就是 1.5 的，所以才会导致这里的问题发生，如果想完全修改好（一劳永逸）。请直接对 pom 文件动刀，就是干！ 只需把大项目（netty-parent）的那个 pom.xml 修改个属性，把版本信息提高到 1.8。 在等待它拉取 jar 包吧 搞完了之后发现还有两个模块（netty-bom、netty-dev-tools）不能设置到 版本，只能手动的和上面那种设置 language level 和 Java compile 为 1.8 了。 最后你会发现这里的完全没有报错了，开心不？ 代码行数统计额，看到项目这么多子模块，你都不知道该从哪里下手开始看，那么我就写了个简单的 Java 脚本去大概的统计每个子项目代码的行数。先看看统计结果： 整个项目差不多 23 万。（过滤了空行、各种注释和 @Override 之后的 Java 代码行数），靠这个数字很吓人！ 来看看我的脚本代码吧： 123456789101112131415public static void main(String[] args) throws Exception &#123; long count = Files.walk(Paths.get(\"C:\\\\JetBrains\\\\IDEAProject\\\\netty\\\\transport-udt\")) // 递归获得项目目录下的所有文件 .filter(file -&gt; !Files.isDirectory(file)) // 筛选出文件 .filter(file -&gt; file.toString().endsWith(\".java\")) // 筛选出 java 文件 .flatMap(Try.of(file -&gt; Files.lines(file), Stream.empty())) // 将会抛出受检异常的 Lambda 包装为 抛出非受检异常的 Lambda .filter(line -&gt; !line.trim().isEmpty()) // 过滤掉空行 .filter(line -&gt; !line.trim().startsWith(\"//\")) //过滤掉 //之类的注释 .filter(line -&gt; !(line.trim().startsWith(\"/*\") &amp;&amp; line.trim().endsWith(\"*/\"))) //过滤掉/* */之类的注释 .filter(line -&gt; !(line.trim().startsWith(\"/*\") &amp;&amp; !line.trim().endsWith(\"*/\"))) //过滤掉以 /* 开头的注释（去除空格后的开头） .filter(line -&gt; !(!line.trim().startsWith(\"/*\") &amp;&amp; line.trim().endsWith(\"*/\"))) //过滤掉已 */ 结尾的注释 .filter(line -&gt; !line.trim().startsWith(\"*\")) //过滤掉 javadoc 中的文字注释 .filter(line -&gt; !line.trim().startsWith(\"@Override\")) //过滤掉方法上含 @Override 的 .count(); System.out.println(\"代码行数：\" + count);&#125; 后面我会把我阅读源码的中文注释及解析之类的更新到我的 GitHub 去（欢迎关注、我是来骗 star 的），https://github.com/zhisheng17/netty ，如果你不想去自己设置上面所说的这些（偷懒），那就直接 fork 我的这份吧！ 最后环境搭建就写到这里了，转载请注明地址：http://www.54tianzhisheng.cn/2017/12/08/netty-01-env/","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"Netty","slug":"Netty","permalink":"http://www.54tianzhisheng.cn/tags/Netty/"}]},{"title":"RestTemplate 详解","date":"2017-12-02T16:00:00.000Z","path":"2017/12/03/RestTemplate/","text":"背景这段时间自己做的项目中需要调用服务提供者的服务（接口），具体就是：我这边需要将页面所输入的 Groovy 脚本代码传给别人提供的服务接口，然后那边返回脚本编译的结果给我，我需要将编译结果展示在页面，用的就是 RestTemplate 了，那 RestTemplate 是什么呢？简单说就是：简化了发起 HTTP 请求以及处理响应的过程，并且支持 REST 。下文就稍微总结下。 如何使用先讲讲如何使用吧，我项目是 SpringBoot 项目，可以在启动类中加入： 1234@Beanpublic RestTemplate restTemplate() &#123; return new RestTemplate();&#125; 然后在 Controller 层中引入： 12@Autowiredprivate RestTemplate restTemplate; 接下来就可以在 Controller 中各个方法中使用 restTemplate 了，但是 restTemplate 里面有什么方法呢？ RestTemplate 内部方法 从图中 RestTemplate 可以看到有很多方法，我们可以提取出主要的几种方法是： GET POST PUT DELETE HEAD OPTIONS EXCHANGE EXECUTE 图片中依然可以知道 RestTemplate 类中的方法主要是来自接口 RestOperations，下面我们具体看看这些方法里面的具体实现与该如何使用。 Get 方法在 RestTemplate 中，发送一个 GET 请求，我们可以通过如下两种方式： getForEntity getForEntity 方法的返回值是一个ResponseEntity&lt;T&gt;，ResponseEntity&lt;T&gt;是 Spring 对 HTTP 请求响应的封装，包括了几个重要的元素，如响应码、contentType、contentLength、响应消息体等。比如下面一个例子： 1234567891011121314@RequestMapping(\"/gethello\")public String getHello() &#123; ResponseEntity&lt;String&gt; responseEntity = restTemplate.getForEntity(\"http://HELLO-SERVICE/hello\", String.class); String body = responseEntity.getBody(); HttpStatus statusCode = responseEntity.getStatusCode(); int statusCodeValue = responseEntity.getStatusCodeValue(); HttpHeaders headers = responseEntity.getHeaders(); StringBuffer result = new StringBuffer(); result.append(\"responseEntity.getBody()：\").append(body).append(\"&lt;hr&gt;\") .append(\"responseEntity.getStatusCode()：\").append(statusCode).append(\"&lt;hr&gt;\") .append(\"responseEntity.getStatusCodeValue()：\").append(statusCodeValue).append(\"&lt;hr&gt;\") .append(\"responseEntity.getHeaders()：\").append(headers).append(\"&lt;hr&gt;\"); return result.toString();&#125; 关于这段代码，说如下几点： getForEntity 的第一个参数为我要调用的服务的地址，这里我调用了服务提供者提供的 /hello 接口，注意这里是通过服务名调用而不是服务地址，如果写成服务地址就没法实现客户端负载均衡了。（备注：我项目中需要通过 ConsulClient 去获取服务名，然后在去获取服务的 IP 和 Port，并把它拼接起来组合成我的服务地址，所以就没法实现客户端的负载均衡了，如果要是实现负载均衡，可以在 SpringBoot 启动类的中加入注解 @LoadBalanced, 如下: 12345@Bean@LoadBalancedpublic RestTemplate restTemplate() &#123; return new RestTemplate();&#125; ） getForEntity 第二个参数 String.class 表示我希望返回的 body 类型是 String 拿到返回结果之后，将返回结果遍历打印出来 有时候我在调用服务提供者提供的接口时，可能需要传递参数，有两种不同的方式: 123456789101112@RequestMapping(\"/sayhello\")public String sayHello() &#123; ResponseEntity&lt;String&gt; responseEntity = restTemplate.getForEntity(\"http://HELLO-SERVICE/sayhello?name=&#123;1&#125;\", String.class, \"张三\"); return responseEntity.getBody();&#125;@RequestMapping(\"/sayhello2\")public String sayHello2() &#123; Map&lt;String, String&gt; map = new HashMap&lt;&gt;(); map.put(\"name\", \"李四\"); ResponseEntity&lt;String&gt; responseEntity = restTemplate.getForEntity(\"http://HELLO-SERVICE/sayhello?name=&#123;name&#125;\", String.class, map); return responseEntity.getBody();&#125; 可以用一个数字做占位符，最后是一个可变长度的参数，来一 一替换前面的占位符 也可以前面使用 name={name} 这种形式，最后一个参数是一个 map，map 的 key 即为前边占位符的名字，map的 value 为参数值 第一个调用地址也可以是一个URI而不是字符串，这个时候我们构建一个URI即可，参数神马的都包含在URI中了，如下： 1234567@RequestMapping(\"/sayhello3\")public String sayHello3() &#123; UriComponents uriComponents = UriComponentsBuilder.fromUriString(\"http://HELLO-SERVICE/sayhello?name=&#123;name&#125;\").build().expand(\"王五\").encode(); URI uri = uriComponents.toUri(); ResponseEntity&lt;String&gt; responseEntity = restTemplate.getForEntity(uri, String.class); return responseEntity.getBody();&#125; 通过Spring中提供的UriComponents来构建Uri即可。 当然，服务提供者不仅可以返回String，也可以返回一个自定义类型的对象，比如我的服务提供者中有如下方法： 1234@RequestMapping(value = \"/getbook1\", method = RequestMethod.GET)public Book book1() &#123; return new Book(\"三国演义\", 90, \"罗贯中\", \"花城出版社\");&#125; 对于该方法我可以在服务消费者中通过如下方式来调用： 12345@RequestMapping(\"/book1\")public Book book1() &#123; ResponseEntity&lt;Book&gt; responseEntity = restTemplate.getForEntity(\"http://HELLO-SERVICE/getbook1\", Book.class); return responseEntity.getBody();&#125; 运行结果如下： getForObject ​ getForObject 函数实际上是对 getForEntity 函数的进一步封装，如果你只关注返回的消息体的内容，对其他信息都不关注，此时可以使用 getForObject，举一个简单的例子，如下： 12345@RequestMapping(\"/book2\")public Book book2() &#123; Book book = restTemplate.getForObject(\"http://HELLO-SERVICE/getbook1\", Book.class); return book;&#125; POST 方法在 RestTemplate 中，POST 请求可以通过如下三个方法来发起： postForEntity 该方法和get请求中的getForEntity方法类似，如下例子： 1234567@RequestMapping(\"/book3\")public Book book3() &#123; Book book = new Book(); book.setName(\"红楼梦\"); ResponseEntity&lt;Book&gt; responseEntity = restTemplate.postForEntity(\"http://HELLO-SERVICE/getbook2\", book, Book.class); return responseEntity.getBody();&#125; 方法的第一参数表示要调用的服务的地址 方法的第二个参数表示上传的参数 方法的第三个参数表示返回的消息体的数据类型 我这里创建了一个Book对象，这个Book对象只有name属性有值，将之传递到服务提供者那里去，服务提供者代码如下： 12345678@RequestMapping(value = \"/getbook2\", method = RequestMethod.POST)public Book book2(@RequestBody Book book) &#123; System.out.println(book.getName()); book.setPrice(33); book.setAuthor(\"曹雪芹\"); book.setPublisher(\"人民文学出版社\"); return book;&#125; 服务提供者接收到服务消费者传来的参数book，给其他属性设置上值再返回，调用结果如下： postForObject 如果你只关注，返回的消息体，可以直接使用postForObject。用法和getForObject一致。 postForLocation postForLocation 也是提交新资源，提交成功之后，返回新资源的 URI，postForLocation 的参数和前面两种的参数基本一致，只不过该方法的返回值为 URI ，这个只需要服务提供者返回一个 URI 即可，该 URI 表示新资源的位置。 PUT 方法 在 RestTemplate 中，PUT 请求可以通过 put 方法调用，put 方法的参数和前面介绍的 postForEntity 方法的参数基本一致，只是 put 方法没有返回值而已。举一个简单的例子，如下： 123456@RequestMapping(\"/put\")public void put() &#123; Book book = new Book(); book.setName(\"红楼梦\"); restTemplate.put(\"http://HELLO-SERVICE/getbook3/&#123;1&#125;\", book, 99);&#125; book对象是我要提交的参数，最后的99用来替换前面的占位符{1} DELETE 方法 delete 请求我们可以通过 delete 方法调用来实现，如下例子： 1234@RequestMapping(\"/delete\")public void delete() &#123; restTemplate.delete(\"http://HELLO-SERVICE/getbook4/&#123;1&#125;\", 100);&#125; HEADER 方法 返回资源的所有 HTTP headers。 OPTIONS 问可以执行哪些方法。 EXCHANGE 与其它接口的不同： 允许调用者指定HTTP请求的方法（GET,POST,PUT等） 可以在请求中增加body以及头信息，其内容通过参数 HttpEntity&lt;?&gt;requestEntity 描述 exchange支持‘含参数的类型’（即泛型类）作为返回类型，该特性通过 ParameterizedTypeReferenceresponseType 描述 EXECUTE细心的你，不知道有没有发现上面所有的方法内部返回值都调用了同一个方法 —— execute 方法。 下面我们来看看： 可以看到，Excute方法只是将 String 格式的 URI 转成了 java.net.URI，之后调用了doExecute方法。整个调用过程关键起作用的是 doExecute 方法 doExecute 方法 这里需要了解两个类： RequestCallback 和 ResponseExtractor RestTemplate 类中可以看到他们两的实现类。 RequestCallback ：用于操作请求头和body，在请求发出前执行。 该接口有两个实现类： AcceptHeaderRequestCallback 只处理请求头，用于getXXX()方法。 HttpEntityRequestCallback 继承于AcceptHeaderRequestCallback可以处理请求头和body，用于putXXX()、postXXX()和exchange()方法。 ResponseExtractor：解析HTTP响应的数据，而且不需要担心异常和资源的关闭 上面图纸这个实现类 ResponseEntityResponseExtractor 的作用是：使用 HttpMessageConverterExtractor 提取 body（委托模式），然后将 body 和响应头、状态封装成 ResponseEntity 对象。 最后转载请注明地址：http://www.54tianzhisheng.cn/2017/12/03/RestTemplate/ 参考资料1、https://www.cnblogs.com/caolei1108/p/6169950.html 2、https://segmentfault.com/a/1190000011093597 如果想和我进一步交流请关注：","tags":[{"name":"Spring","slug":"Spring","permalink":"http://www.54tianzhisheng.cn/tags/Spring/"}]},{"title":"实习圈群里提问小记","date":"2017-12-01T16:00:00.000Z","path":"2017/12/02/wx-01/","text":"","tags":[{"name":"实习圈","slug":"实习圈","permalink":"http://www.54tianzhisheng.cn/tags/实习圈/"}]},{"title":"Docker系列文章（一）：基于 Harbor 搭建 Docker 私有镜像仓库","date":"2017-11-25T16:00:00.000Z","path":"2017/11/26/Docker-harbor/","text":"什么是 Harbor？第一次使用这个的时候是刚进公司处理的第一个任务的时候，发现 Harbor 就是一个用于存储和分发 Docker 镜像的企业级Registry 服务器。网上找到一个 Harbor 的架构图： Harbor 是 VMware 公司开源的企业级 DockerRegistry 项目，项目地址为 https://github.com/vmware/harbor。其目标是帮助用户迅速搭建一个企业级的 Docker registry 服务。它以 Docker 公司开源的 registry 为基础，提供了管理UI，基于角色的访问控制(Role Based Access Control)，AD/LDAP集成、以及审计日志(Auditlogging) 等企业用户需求的功能，同时还原生支持中文。Harbor 的每个组件都是以 Docker 容器的形式构建的，使用 Docker Compose 来对它进行部署。 环境准备1、自己在腾讯云买的服务器（CentOS7.3） 2、Docker 版本：17.05.0-ce 3、Docker-compose：1.17.1 4、Harbor：1.1.2 安装 Docker因为系统是 CentOS 7.3 ，内核啥的都已经是 3.10，所以不用担心内核升级的问题，一些操作啥的在 7.x 上操作也很方便。 1234567891011121314151617181920212223yum update //系统版本更新vim /etc/yum.repos.d/docker.repo //添加以下内容[dockerrepo]name=Docker Repositorybaseurl=https://yum.dockerproject.org/repo/main/centos/7/enabled=1gpgcheck=1gpgkey=https://yum.dockerproject.org/gpg//下面安装 Docker 引擎yum install docker-engine -y//安装docker引擎，此步也可作为更新docker版本的操作：先#systemctl stop docker 停止docker服务，再#yum install docker-engine 更新docker版本systemctl enable docker.servicesystemctl start docker //启动docker守护进程docker info //查看docker运行情况docker -v //查看版本信息 修改 Docker 配置文件 /etc/default/docker 如下： 1DOCKER_OPTS=\"--registry-mirror=http://aad0405c.m.daocloud.io\" //换成国内的镜像加速源，不然拉取镜像简直龟速，不想在吐槽了 使用 service docker restart 重启 Docker 服务即可。 或者用官方提供的方式： 1curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://ef017c13.m.daocloud.io 安装 Docker-compose如果是想直接命令安装也行， 1234567891011121314下载指定版本的docker-composesudo curl -L https://github.com/docker/compose/releases/download/1.17.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose对二进制文件赋可执行权限chmod +x /usr/local/bin/docker-compose测试下docker-compose是否安装成功docker-compose --version出现如下docker-compose version 1.17.1, build 6d101fb 但是，这种方法简直龟速，幸好还有种方法， 见这里：https://docs.docker.com/compose/install/#install-compose 这种需要通过 Python 的 pip 安装 安装 pip123456789wget --no-check-certificate https://pypi.python.org/packages/source/s/setuptools/setuptools-1.4.2.tar.gztar -vxf setuptools-1.4.2.tar.gzcd setuptools-1.4.2python2.7 setup.py install //因为服务器自带 Python 2.7easy_install-2.7 pip 安装 docker compose123pip install docker-composedocker-compose --version //测试安装是否成功 安装 Harbor12345wget https://github.com/vmware/harbor/releases/download/v1.1.2/harbor-offline-installer-v1.1.2.tgz离线安装包,也是龟速，把这个下载链接用迅雷下载，速度却贼快，嘿嘿，然后再传到服务器上去，整个过程快很多！tar -zxvf harbor-offline-installer-v1.1.2.tgz 解压缩之后，进入目录下会看到 harbor.cfg 文件，该文件就是 Harbor 的配置文件。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495## Configuration file of Harbor# hostname设置访问地址，可以使用ip、域名，不可以设置为127.0.0.1或localhosthostname = 115.159.227.249 #这里我先配置我的服务器IP地址# 访问协议，默认是http，也可以设置https，如果设置https，则nginx ssl需要设置onui_url_protocol = http# mysql数据库root用户默认密码root123，实际使用时修改下db_password = root123#Maximum number of job workers in job servicemax_job_workers = 3#Determine whether or not to generate certificate for the registry&apos;s token.#If the value is on, the prepare script creates new root cert and private key#for generating token to access the registry. If the value is off the default key/cert will be used.#This flag also controls the creation of the notary signer&apos;s cert.customize_crt = on#The path of cert and key files for nginx, they are applied only the protocol is set to httpsssl_cert = /data/cert/server.crtssl_cert_key = /data/cert/server.key#The path of secretkey storagesecretkey_path = /data#Admiral&apos;s url, comment this attribute, or set its value to NA when Harbor is standaloneadmiral_url = NA#NOTES: The properties between BEGIN INITIAL PROPERTIES and END INITIAL PROPERTIES#only take effect in the first boot, the subsequent changes of these properties#should be performed on web ui#************************BEGIN INITIAL PROPERTIES************************#Email account settings for sending out password resetting emails.#Email server uses the given username and password to authenticate on TLS connections to host and act as identity.#Identity left blank to act as username.email_identity =email_server = smtp.mydomain.comemail_server_port = 25email_username = sample_admin@mydomain.comemail_password = abcemail_from = admin &lt;sample_admin@mydomain.com&gt;email_ssl = false##The initial password of Harbor admin, only works for the first time when Harbor starts.#It has no effect after the first launch of Harbor.# 启动Harbor后，管理员UI登录的密码，默认是Harbor12345harbor_admin_password = Harbor12345# 认证方式，这里支持多种认证方式，如LADP、本次存储、数据库认证。默认是db_auth，mysql数据库认证auth_mode = db_auth#The url for an ldap endpoint.ldap_url = ldaps://ldap.mydomain.com#A user&apos;s DN who has the permission to search the LDAP/AD server.#If your LDAP/AD server does not support anonymous search, you should configure this DN and ldap_search_pwd.#ldap_searchdn = uid=searchuser,ou=people,dc=mydomain,dc=com#the password of the ldap_searchdn#ldap_search_pwd = password#The base DN from which to look up a user in LDAP/ADldap_basedn = ou=people,dc=mydomain,dc=com#Search filter for LDAP/AD, make sure the syntax of the filter is correct.#ldap_filter = (objectClass=person)# The attribute used in a search to match a user, it could be uid, cn, email, sAMAccountName or other attributes depending on your LDAP/AD ldap_uid = uid#the scope to search for users, 1-LDAP_SCOPE_BASE, 2-LDAP_SCOPE_ONELEVEL, 3-LDAP_SCOPE_SUBTREEldap_scope = 3#Timeout (in seconds) when connecting to an LDAP Server. The default value (and most reasonable) is 5 seconds.ldap_timeout = 5# 是否开启自注册self_registration = on# Token有效时间，默认30分钟token_expiration = 30# 用户创建项目权限控制，默认是everyone（所有人），也可以设置为adminonly（只能管理员）project_creation_restriction = everyone#Determine whether the job service should verify the ssl cert when it connects to a remote registry.#Set this flag to off when the remote registry uses a self-signed or untrusted certificate.verify_remote_cert = on#************************END INITIAL PROPERTIES************************ 启动 harbor，修改完配置文件后，在的当前目录执行./install.sh，Harbor服务就会根据当期目录下的docker-compose.yml开始下载依赖的镜像，检测并按照顺序依次启动各个服务。 启动完成后，我们访问刚设置的 hostname 即可，http://115.159.227.249/，默认是80端口，如果端口占用，我们可以去修改docker-compose.yml文件中，对应服务的端口映射。 登录 Web Harbor , 输入用户名 admin，默认密码（或已修改密码）登录系统。 我们可以看到系统各个模块如下： 项目：新增/删除项目，查看镜像仓库，给项目添加成员、查看操作日志、复制项目等 日志：仓库各个镜像create、push、pull等操作日志 系统管理 用户管理：新增/删除用户、设置管理员等 复制管理：新增/删除从库目标、新建/删除/启停复制规则等 配置管理：认证模式、复制、邮箱设置、系统设置等 其他设置 用户设置：修改用户名、邮箱、名称信息 修改密码：修改用户密码 注意：非系统管理员用户登录，只能看到有权限的项目和日志，其他模块不可见。 我们要尝试下能不能把自己 Docker 里面的镜像 push 到 Harbor 的 library 里来（默认这个 library 项目是公开的，所有人都可以有读的权限，都不需要 docker login 进来，就可以拉取里面的镜像）。 注意： 为了后面留坑，我这里先 在自己的 docker.service 中添加仓库：（这是个坑，建议你先按照我说的做，不然下面可能会一直登录不上） 1234vim /usr/lib/systemd/system/docker.service里面的这行修改为：（其实就是添加 --insecure-registry 115.159.227.249 ）ExecStart=/usr/bin/dockerd --insecure-registry 115.159.227.249 添加完了后重新启动 docker： 1systemctl daemon-reload &amp;&amp; systemctl enable docker &amp;&amp; systemctl start docker 启动 docker 服务: 1service docker start 登录：（为了测试下能否登录成功） 12345admin登录$ docker login 115.159.227.249Username: adminPassword:Login Succeeded 打 tag 并 push 12345678910docker tag ubuntu:15.10 115.159.227.249/library/ubuntu:15.10 //给我的镜像打个 tagdocker push 115.159.227.249/library/ubuntuThe push refers to a repository [115.159.227.249/library/ubuntu]98d59071f692: Pushedaf288f00b8a7: Pushed4b955941a4d0: Pushedf121afdbbd5d: Pushed15.10: digest: sha256:ec89c4a90f45f5e103860191890f48d8379e0504a2881ff706aef0768dc0321b size: 1150 上传完毕后，登录Web Harbor，选择项目 library，就可以看到我刚 push 的镜像了。 同理，你也可以测试下从 Harbor pull 镜像到你的 Docker 中去，这里就不继续演示了。 最后转载请注明地址为：http://www.54tianzhisheng.cn/2017/11/26/Docker-harbor/","tags":[{"name":"Docker","slug":"Docker","permalink":"http://www.54tianzhisheng.cn/tags/Docker/"}]},{"title":"谈谈我的理财","date":"2017-11-17T16:00:00.000Z","path":"2017/11/18/Money-management/","text":"背景最开始接触理财是去年的时候，在我的一个群里（几个好朋友），有个朋友他女朋友是学金融的，当时还开玩笑地说叫她带带我们怎么买股票、基金、黄金这些东西呢。后来在群里偶尔聊下这方面的东西！ 黄金那时我第才开始接触黄金，也是自己一个人买了点支付宝里面的存金宝，（不多，就几百块），后来慢慢的加仓和减仓，刚开始的时候，因为是刚上手这些东西，比较对这每天的数字增长和降低很在意，一天打开蚂蚁聚宝的次数很多，老是看着每天的实时参考金价，反正就是心理各种不踏实。就是那种患得患失的感觉，哈哈，我也不知道怎么形容了。。。😳 那时也不懂，老是追涨低抛，没有打算长期持有。附图： 现在看看这图，想想自己以前真傻。可以发现现在已经没买黄金了，对，在今年的时候主要是关注些基金！ 基金通过买黄金，发现，黄金增长下跌确实不怎么那么尽人意，只赚了一点点。然后慢慢在关注着基金，发现有些基金的还是收入效果还是很好的。 什么是基金呢？ 好像有很多种，但是我的理解是，我们散户有点闲钱，打算投资，又没时间去买股票，一是缺乏经验，不知道买哪支，而是没空去整天盯着股票的走势。这时就出现了一个平台，我们散户把钱投给这个平台，平台有专业的人去进行买股票，如买股票有盈利，则大家一起赚钱，如果亏，则一起亏，我对基金就是这样的理解，也不知道对不对？ 再说说我现在主要买的基金吧，看看收益图： 嘿嘿，这几个是我观察很久了，并觉得长期看好的基金了，当然以前也买过一两个基金，有个亏得不少，有涨有跌，现在觉得又要平常心，如果是长期看好的，没必要纠结这一两天的涨跌，等过段时间再来看看效果咋样（心态一定要好），如果遇到被套的话，有时也需要装死心态，哈哈！另外，我还自选了一批基金，正在观察中，等有时间把觉得还行的给统计下！ 股票这个不太懂，不过目前觉得我自己公司的股票也还不错，打算看什么时候有机会买点，这东西都是靠自己慢慢研究出来的，然后就是看看高人指点，我倒是关注了点微信公众号讲这方面的知识，微博也关注了几个，在蚂蚁财富里面也关注些，觉得有些还是很靠谱的，还是一样，有时间继续做个统计，然后发在我的小密圈里面。 友金所投了这个是因为进 stormzhang 的小密圈，通过注册并投资点可以免费进他的小密圈，不然得花 199 元，这算很优惠了。因为是新用户，所以这个收益很高，一个月期限，12% 的收益！ 余额宝支付宝里的，平常钱也一般放这里面，因为平时用支付宝比较多，放这里，可以付款的时候选择直接余额宝支付，另外还有差不多 4% 的收益，可以随时存取，比较方便！ 最后有时间将上面所说的：自选基金列表、微信公众号、微博这几个列表发在我的小密圈里。 最重要的话还是得说三遍： 投资有风险，需谨慎！ 投资有风险，需谨慎！ 投资有风险，需谨慎！","tags":[{"name":"投资理财","slug":"投资理财","permalink":"http://www.54tianzhisheng.cn/tags/投资理财/"}]},{"title":"基于分布式环境下限流系统的设计","date":"2017-11-17T16:00:00.000Z","path":"2017/11/18/flow-control/","text":"前提业务背景就拿前些天的双十一的 “抢券活动” 来说，一般是设置整点开始抢的，你想想，淘宝的用户群体非常大，可以达到亿级别，而服务接口每秒能处理的量是有限的，那么这个时候问题就会出现，我们如何通过程序来控制用户抢券呢，于是就必须加上这个限流功能了。 生产环境1、服务接口所能提供的服务上限（limit）假如是 500次/s 2、用户请求接口的次数未知，QPS可能达到 800次/s，1000次/s，或者更高 3、当服务接口的访问频率超过 500次/s，超过的量将拒绝服务，多出的信息将会丢失 4、线上环境是多节点部署的，但是调用的是同一个服务接口 于是，为了保证服务的可用性，就要对服务接口调用的速率进行限制（接口限流）。 什么是限流？限流是对系统的出入流量进行控制，防止大流量出入，导致资源不足，系统不稳定。 限流系统是对资源访问的控制组件，控制主要的两个功能：限流策略和熔断策略，对于熔断策略，不同的系统有不同的熔断策略诉求，有的系统希望直接拒绝、有的系统希望排队等待、有的系统希望服务降级、有的系统会定制自己的熔断策略，这里只针对限流策略这个功能做详细的设计。 限流算法1、限制瞬时并发数Guava RateLimiter 提供了令牌桶算法实现：平滑突发限流(SmoothBursty)和平滑预热限流(SmoothWarmingUp)实现。 2、限制某个接口的时间窗最大请求数即一个时间窗口内的请求数，如想限制某个接口/服务每秒/每分钟/每天的请求数/调用量。如一些基础服务会被很多其他系统调用，比如商品详情页服务会调用基础商品服务调用，但是怕因为更新量比较大将基础服务打挂，这时我们要对每秒/每分钟的调用量进行限速；一种实现方式如下所示： 12345678910111213141516171819LoadingCache&lt;Long, AtomicLong&gt; counter = CacheBuilder.newBuilder() .expireAfterWrite(2, TimeUnit.SECONDS) .build(new CacheLoader&lt;Long, AtomicLong&gt;() &#123; @Override public AtomicLong load(Long seconds) throws Exception &#123; return new AtomicLong(0); &#125; &#125;);long limit = 1000;while(true) &#123; //得到当前秒 long currentSeconds = System.currentTimeMillis() / 1000; if(counter.get(currentSeconds).incrementAndGet() &gt; limit) &#123; System.out.println(\"限流了:\" + currentSeconds); continue; &#125; //业务处理&#125; 使用Guava的Cache来存储计数器，过期时间设置为2秒（保证1秒内的计数器是有的），然后我们获取当前时间戳然后取秒数来作为KEY进行计数统计和限流，这种方式也是简单粗暴，刚才说的场景够用了。 3、令牌桶 算法描述： 假如用户配置的平均发送速率为r，则每隔1/r秒一个令牌被加入到桶中 假设桶中最多可以存放b个令牌。如果令牌到达时令牌桶已经满了，那么这个令牌会被丢弃 当流量以速率v进入，从桶中以速率v取令牌，拿到令牌的流量通过，拿不到令牌流量不通过，执行熔断逻辑 属性 长期来看，符合流量的速率是受到令牌添加速率的影响，被稳定为：r 因为令牌桶有一定的存储量，可以抵挡一定的流量突发情况 M是以字节/秒为单位的最大可能传输速率。 M&gt;r T max = b/(M-r) 承受最大传输速率的时间 B max = T max * M 承受最大传输速率的时间内传输的流量 优点：流量比较平滑，并且可以抵挡一定的流量突发情况 4、Google guava 提供的工具库中 RateLimiter 类（内部也是采用令牌桶算法实现）最快的方式是使用 RateLimit 类，但是这仅限制在单节点，如果是分布式系统，每个节点的 QPS 是一样的，请求量到服务接口那的话就是 QPS * 节点数 了。所以这种方案在分布式的情况下不适用！ 5、基于 Redis 实现，存储两个 key，一个用于计时，一个用于计数。请求每调用一次，计数器增加 1，若在计时器时间内计数器未超过阈值，则可以处理任务。这种能够很好地解决了分布式环境下多实例所导致的并发问题。因为使用redis设置的计时器和计数器均是全局唯一的，不管多少个节点，它们使用的都是同样的计时器和计数器，因此可以做到非常精准的流控。 代码就不公布了，毕竟涉及公司隐私了。 最后参考文章： 基于Redis的限流系统的设计 感兴趣的可以看看别人的代码是怎么写的：https://github.com/wukq/rate-limiter 转载请注明文章地址为：http://www.54tianzhisheng.cn/2017/11/18/flow-control/","tags":[{"name":"Redis","slug":"Redis","permalink":"http://www.54tianzhisheng.cn/tags/Redis/"},{"name":"流控","slug":"流控","permalink":"http://www.54tianzhisheng.cn/tags/流控/"}]},{"title":"Maven 中 dependencies 与 dependencyManagement 的区别","date":"2017-11-10T16:00:00.000Z","path":"2017/11/11/Maven-dependencies-dependencyManagement/","text":"前提这段时间项目中遇到过了一些 Jar 包冲突的问题，很多是由于我们项目模块很多的时候，用 Maven 管理不当导致的冲突问题，本文就这个问题参考网上的资料，于是总结下 Maven 中 dependencies 与 dependencyManagement 的区别。 假设项目结构如下： parent 为父模块，抽象出来管理子项目的公共依赖，为了项目的正确运行，必须让所有的子项目使用依赖项的统一版本，必须确保应用的各个项目的依赖项和版本一致，才能保证测试的和发布的是相同的结果。 dependencyManagement在项目的 parent 层，可以通过 dependencyManagement 元素来管理 jar 包的版本，让子项目中引用一个依赖而不用显示的列出版本号。 parent 中 pom.xml 123456789101112131415161718&lt;properties&gt; &lt;version.framework&gt;1.0-SNAPSHOT&lt;/version.framework&gt; &lt;javaee-api.version&gt;1.0-SNAPSHOT&lt;/javaee-api.version&gt;&lt;/properties&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.zhisheng&lt;/groupId&gt; &lt;artifactId&gt;framework-cache&lt;/artifactId&gt; &lt;version&gt;$&#123;version.framework&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax&lt;/groupId&gt; &lt;artifactId&gt;javaee-api&lt;/artifactId&gt; &lt;version&gt;$&#123;javaee-api.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt; extendion 中的 pom.xml 123456789101112131415161718192021&lt;parent&gt; &lt;artifactId&gt;parent&lt;/artifactId&gt; &lt;groupId&gt;com.zhisheng&lt;/groupId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../parent/pom.xml&lt;/relativePath&gt;&lt;/parent&gt;&lt;!--依赖关系--&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;javax&lt;/groupId&gt; &lt;artifactId&gt;javaee-api&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.zhisheng&lt;/groupId&gt; &lt;artifactId&gt;framework-cache&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-annotations&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 这样做的好处：统一管理项目的版本号，确保应用的各个项目的依赖和版本一致，才能保证测试的和发布的是相同的成果，因此，在顶层 pom 中定义共同的依赖关系。同时可以避免在每个使用的子项目中都声明一个版本号，这样想升级或者切换到另一个版本时，只需要在父类容器里更新，不需要任何一个子项目的修改；如果某个子项目需要另外一个版本号时，只需要在 dependencies 中声明一个版本号即可。子类就会使用子类声明的版本号，不继承于父类版本号。 我们知道 Maven 的继承和 Java 的继承一样，是无法实现多重继承的，如果10个、20个甚至更多模块继承自同一个模块，那么按照我们之前的做法，这个父模块的 dependencyManagement 会包含大量的依赖。如果你想把这些依赖分类以更清晰的管理，那就不可能了，import scope 依赖能解决这个问题。你可以把 dependencyManagement 放到单独的专门用来管理依赖的 POM 中，然后在需要使用依赖的模块中通过 import scope 依赖，就可以引入dependencyManagement。例如可以写这样一个用于依赖管理的 POM： 12345678910111213141516171819202122&lt;project&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.zhisheng.sample&lt;/groupId&gt; &lt;artifactId&gt;sample-dependency-infrastructure&lt;/artifactId&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactid&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.8.2&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactid&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.16&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt;&lt;/project&gt; 然后就可以通过非继承的方式来引入这段依赖管理配置： 1234567891011121314151617181920&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.zhisheng.sample&lt;/groupId&gt; &lt;artifactid&gt;sample-dependency-infrastructure&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactid&gt;junit&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactid&gt;log4j&lt;/artifactId&gt; &lt;/dependency&gt; 这样，父模块的 POM 就会非常干净，由专门的 packaging 为 pom 的 POM 来管理依赖，也契合的面向对象设计中的单一职责原则。此外，我们还能够创建多个这样的依赖管理 POM，以更细化的方式管理依赖。这种做法与面向对象设计中使用组合而非继承也有点相似的味道。 dependencies相对于 dependencyManagement，所有声明在父项目中 dependencies 里的依赖都会被子项目自动引入，并默认被所有的子项目继承。 区别 dependencies 即使在子项目中不写该依赖项，那么子项目仍然会从父项目中继承该依赖项（全部继承） dependencyManagement 里只是声明依赖，并不实现引入，因此子项目需要显示的声明需要用的依赖。如果不在子项目中声明依赖，是不会从父项目中继承下来的；只有在子项目中写了该依赖项，并且没有指定具体版本，才会从父项目中继承该项，并且 version 和 scope 都读取自父 pom; 另外如果子项目中指定了版本号，那么会使用子项目中指定的jar版本。 消除多模块插件配置重复与 dependencyManagement 类似的，我们也可以使用 pluginManagement 元素管理插件。一个常见的用法就是我们希望项目所有模块的使用 Maven Compiler Plugin 的时候，都使用 Java 1.8，以及指定 Java 源文件编码为 UTF-8，这时可以在父模块的 POM 中如下配置 pluginManagement： 12345678910111213141516&lt;build&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;2.5.1&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt;&lt;/build&gt; 这段配置会被应用到所有子模块的 maven-compiler-plugin 中，由于 Maven 内置了 maven-compiler-plugin 与生命周期的绑定，因此子模块就不再需要任何 maven-compiler-plugin 的配置了。 与依赖配置不同的是，通常所有项目对于任意一个依赖的配置都应该是统一的，但插件却不是这样，例如你可以希望模块 A 运行所有单元测试，模块 B 要跳过一些测试，这时就需要配置 maven-surefire-plugin 来实现，那样两个模块的插件配置就不一致了。这也就是说，简单的把插件配置提取到父 POM 的 pluginManagement 中往往不适合所有情况，那我们在使用的时候就需要注意了，只有那些普适的插件配置才应该使用 pluginManagement 提取到父 POM 中。 关于插件 pluginManagement，Maven 并没有提供与 import scope 依赖类似的方式管理，那我们只能借助继承关系，不过好在一般来说插件配置的数量远没有依赖配置那么多，因此这也不是一个问题。 最后你看到的只是冰山一角，更多请看书籍《Maven 实战》 。 转载请注明地址：http://www.54tianzhisheng.cn/2017/11/11/Maven-dependencies-dependencyManagement/","tags":[{"name":"Maven","slug":"Maven","permalink":"http://www.54tianzhisheng.cn/tags/Maven/"}]},{"title":"送你一份双十一剁手书单【墙裂推荐】","date":"2017-11-10T16:00:00.000Z","path":"2017/11/11/recommended-books/","text":"昨晚和朋友聊天说双十一都买啥，结果 TMD 竟然都是书籍，不愧是标准的程序猿。所以这里想推荐基佬的一份书单，方便大家在双十一剁剁剁，建议先收藏本书单，认真啃完一本再买下一本，扎实走完每一步，希望书单能在你想要进一步打怪升级的路上，给予些许帮助！ 书籍列表《Effective Java 中文版》 豆瓣评分：9.1【1235 人评价】 推荐理由：本书介绍了在Java编程中78条极具实用价值的经验规则，这些经验规则涵盖了大多数开发人员每天所面临的问题的解决方案。 友情提示：同推荐《重构 : 改善既有代码的设计》、《代码整洁之道》、《代码大全》，有一定的内容重叠。 《Java性能权威指南》 豆瓣评分：8.2【44 人评价】 推荐理由：市面上介绍Java的书有很多，但专注于Java性能的并不多，能游刃有余地展示Java性能优化难点的更是凤毛麟角，本书即是其中之一。通过使用JVM和Java平台，以及Java语言和应用程序接口，本书详尽讲解了Java性能调优的相关知识，帮助读者深入理解Java平台性能的各个方面，最终使程序如虎添翼。 《Spring揭秘》 豆瓣评分：9.0 【162 人评价】 推荐理由：Spring 使用者不得不读！ 推荐博客：Spring4All社区 推荐公众号：Spring4All社区 《SpringBoot揭秘》 豆瓣评分：6.8 【44 人评价】 推荐理由：《Spring揭秘》相同作者。SpringBoot 入门书籍。 作者博客：扶墙老师说：一个架构士的思考与沉淀 作者公众号：扶墙老师说 付费教程：《Java 微服务实践 - Spring Boot 系列》 《MyBatis技术内幕》 豆瓣评分：暂无 推荐理由：以MyBatis 3.4为基础，针对MyBatis的架构设计和实现细节进行了详细分析，其中穿插介绍了MyBatis源码中涉及的基础知识、设计模式以及笔者自己在实践中的思考。 作者博客：祖大俊的博客 《有效的单元测试》 豆瓣评分：7.4 【18 人评价】 推荐理由：Java 单元测试入门。 《Java并发编程实战》 豆瓣评分：9.0 【651 人评价】 推荐理由：本书深入浅出地介绍了Java线程和并发，是一本完美的Java并发参考手册。 推荐博客：并发编程网 推荐公众号：并发编程网 《Netty实战》 豆瓣评分：7.5【24 人评价】 豆瓣评分：8.1【83 人评价】 《Netty in Action》英文版 推荐理由：Netty之父”Trustin Lee作序推荐。 推荐公众号：Netty之家 《深入剖析Tomcat》 豆瓣评分：8.3【118 人评价】 豆瓣评分：8.9【73 人评价】 《How Tomcat Works》英文版 推荐理由：本书深入剖析Tomcat 4和Tomcat 5中的每个组件，并揭示其内部工作原理。通过学习本书，你将可以自行开发Tomcat组件，或者扩展已有的组件。 《Nginx 中文官方文档》 豆瓣评分：暂无 推荐理由：暂时未找到大家评价不错的 Nginx 实战相关书籍，先推荐看中文翻译的官方文档。如果你有合适的推荐，烦请告诉我。 《深入理解Nginx》 豆瓣评分：8.5【138 人评价】 推荐理由：书中首先通过介绍官方Nginx的基本用法和配置规则，帮助读者了解一般Nginx模块的用法，然后重点介绍了如何开发HTTP模块(含HTTP过滤模块)来得到定制化的Nginx，其中包括开发—个功能复杂的模块所需要了解的各种知识，并对内存池的实现细节及TCP协议进行了详细介绍；接着，综合Nginx框架代码分析了Nginx架构的设计理念和技巧，此外，还新增了如何在模块中支持HTTP变量，以及与slab共享内存等相关的内容，相信通过完善，可进一步帮助读者更好地开发出功能丰富、性能—流的Nginx模块。 友情提示：相对适用于 Nginx 开发者。Nginx 使用者可以了解。 《深入理解Java虚拟机：JVM高级特性与最佳实践》 豆瓣评分：8.9 【657 人评价】 推荐理由：不去了解 JVM 的工程师，和咸鱼有什么区别？ 推荐公众号：你假笨 推荐博客：你假笨@JVM 推荐小程序：JVMPocket 《Java核心技术系列：Java虚拟机规范（Java SE 8版）》 豆瓣评分：暂无评价 豆瓣评分：8.3 【27 人评价】《Java虚拟机规范(Java SE 7版)》 推荐理由：基于Java SE 8,Oracle官方发布，Java虚拟机技术创建人撰写，国内Java技术专家翻译，是深度了解Java虚拟机和Java语言实现细节的必读之作 推荐博客：占小狼的简书 推荐公众号：占小狼的博客 《Go语言编程》 豆瓣评分：7.1 【444 人评价】 推荐理由：这本书从整体的写作风格来说，会以介绍 Go 语言特性为主，示例则尽量采用作者平常的实践，而不是一个没有太大实际意义的语法示范样例。 友情提示：本书作者背景极强，许式伟为原金山WPS首席架构师、曾是盛大创新院研究员，目前是国内Go语言实践圈子公认的Go语言专家。 《 Go语言学习笔记》 豆瓣评分：8.4 【57 人评价】 推荐理由：基于Go1.6， 解析语言规范，深入剖析Go运行时源码 友情提示：雨痕大大，教科书级人物。 《MySQL技术内幕——InnoDB存储引擎》 豆瓣评分：8.6 【104 人评价】 推荐理由：从源代码的角度深度解析了InnoDB的体系结构、实现原理、工作机制，并给出了大量最佳实践，能帮助你系统而深入地掌握InnoDB，更重要的是，它能为你设计管理高性能、高可用的数据库系统提供绝佳的指导。 推荐公众号：DBAplus社群 《高性能MySQL》 豆瓣评分：9.3 【245 人评价】 推荐理由：对于想要了解MySQL性能提升的人来说，这是一本不可多得的书。书中没有各种提升性能的秘籍，而是深入问题的核心，详细的解释了每种提升性能的原理，从而可以使你四两拨千斤。授之于鱼不如授之于渔，这本书做到了。 推荐公众号：老叶茶馆 《高可用MySQL》 豆瓣评分：8.0 【87 人评价】 推荐理由：《高性能MySQL》的姊妹篇。 《MongoDB权威指南》 豆瓣评分：8.0 【69 人评价】 推荐理由：算是普通的参考书了，没有特别有深度的讲解。其实就是一本正常的介绍mongoDB是怎么用的，也可以作为nosql学习的入门。作为指南书，还是很合格的符合期望。 推荐博客：MongoDB 中文社区 推荐公众号：MongoDB 中文社区 《Redis开发与运维》 豆瓣评分：8.8 【41 人评价】 推荐理由：从开发、运维两个角度总结了Redis实战经验，深入浅出地剖析底层实现，包含大规模集群开发与运维的实际案例、应用技巧。全面覆盖Redis 基本功能及应用，图示丰富，讲解细腻。 推荐博客：Redis 中国用户组 推荐公众号：CRUG 《Redis设计与实现》 豆瓣评分：8.5 【427 人评价】 推荐理由：系统而全面地描述了 Redis 内部运行机制。图示丰富，描述清晰，并给出大量参考信息，是NoSQL数据库开发人员案头必备。 《NoSQL精粹》 豆瓣评分：8.2 【226 人评价】 推荐理由：书中全方位比较了关系型数据库与NoSQL数据库的异同；分别以Riak、MongoDB、Cassandra和Neo4J为代表，详细讲解了键值数据库、文档数据库、列族数据库和图数据库这4大类NoSQL数据库的优劣势、用法和适用场合；深入探讨了实现NoSQL数据库系统的各种细节，以及与关系型数据库的混用。 《ElasticSearch 可扩展的开源弹性搜索解决方案》 豆瓣评分：7.3 【23 人评价】 推荐理由：基于ElasticSearch 的0.2 版本，覆盖了ElasticSearch 各种功能和命令的应用，全面、详细地介绍了开源、分布式、RESTful，具有全文检索功能的搜索引擎ElasticSearch。 友情提示：本书 ElasticSearch 比较旧，不忍推荐。仅适合入门，有其他合适的 ElasticSearch 书籍，烦请告诉我。《Elasticsearch权威指南》中文版，目前正在翻译中。 推荐博客：Elastic 中文社区 《ELK Stack权威指南》 豆瓣评分：7.0 【10 人评价】 推荐理由：ELK stack是以Elasticsearch、Logstash、Kibana三个开源软件为主的数据处理工具链，是目前开源界最流行的实时数据分析解决方案，成为实时日志处理领域开源界的第一选择。 《ZooKeeper：分布式过程协同技术详解》 豆瓣评分：7.6 【49 人评价】 推荐理由：Zookeeper 入门 友情提示：翻译可能略显尴尬。 《从Paxos到Zookeeper分布式一致性原理与实践》 豆瓣评分：8.1 【187 人评价】 推荐理由：从分布式一致性的理论出发，向读者简要介绍几种典型的分布式一致性协议，以及解决分布式一致性问题的思路，其中重点讲解了Paxos和ZAB协议。同时，本书深入介绍了分布式一致性问题的工业解决方案——ZooKeeper，并着重向读者展示这一分布式协调框架的使用方法、内部实现及运维技巧，旨在帮助读者全面了解ZooKeeper，并更好地使用和运维ZooKeeper。 《RabbitMQ实战：高效部署分布式消息队列》 豆瓣评分：6.9 【47 人评价】 推荐理由：本书对RabbitMQ做了全面、翔实的讲解，体现了两位专家的真知灼见。本书首先介绍了有关MQ的历史，然后从基本的消息通信原理讲起，带领读者一路探索RabbitMQ的消息通信世界。 友情提示：本书 RabbitMQ 版本较旧。消息队列中间件 RabbitMQ、ActiveMQ、RocketMQ、Kafka 可以选择了解一下。 《Apache Kafka源码剖析》 豆瓣评分：7.8 【30 人评价】 推荐理由：以Kafka 0.10.0版本源码为基础，针对Kafka的架构设计到实现细节进行详细阐述。 《作业调度系统 Quartz 中文文档》 豆瓣评分：暂无 推荐理由：暂时未找到大家评价不错的 Quartz 实战相关书籍，先推荐看中文翻译的官方文档。如果你有合适的推荐，烦请告诉我。 友情提示：国内开源项目 Elastic-Job，XXL-Job 都可以选择了解。 《微服务设计》 豆瓣评分：8.1 【273 人评价】 推荐理由：通过Netflix等多个业界案例，从微服务架构演进到原理剖析，全面讲解建模集成部署等微服务所涉及的各种主题，微服务架构与实践指南。 《Spring Cloud微服务实战》 豆瓣评分：7.9【20 人评价】 推荐理由：从时下流行的微服务架构概念出发，详细介绍了Spring Cloud针对微服务架构中几大核心要素的解决方案和基础组件。对于各个组件的介绍，主要以示例与源码结合的方式来帮助读者更好地理解这些组件的使用方法以及运行原理。同时，在介绍的过程中，还包含了作者在实践中所遇到的一些问题和解决思路，可供读者在实践中作为参考。 作者博客：http://blog.didispace.com/ 作者公众号：didispace 《亿级流量网站架构核心技术》 豆瓣评分：7.6【57 人评价】 推荐理由：总结并梳理了亿级流量网站高可用和高并发原则，通过实例详细介绍了如何落地这些原则。本书分为四部分：概述、高可用原则、高并发原则、案例实战。 作者博客：开涛的博客 作者公众号：开涛的博客 《架构即未来：现代企业可扩展的Web架构、流程和组织》 豆瓣评分：8.7【77 人评价】 推荐理由：任何一个持续成长的公司最终都需要解决系统、组织和流程的扩展性问题。本书汇聚了作者从eBay、VISA、Salesforce.com到Apple超过30年的丰富经验， 全面阐释了经过验证的信息技术扩展方法，对所需要掌握的产品和服务的平滑扩展做了详尽的论述，并在第1版的基础上更新了扩展的策略、技术和案例。 《Maven 实战》 豆瓣评分：8.1【563 人评价】 推荐理由：国内最权威的Maven专家的力作，唯一一本哦！ 《Jenkins权威指南》 豆瓣评分：暂无评分 推荐理由：Jenkins 唯一实体书。 友情提示：内容相对比较旧，大多是过时的案例。建议，快速过一遍。Jenkins 方面无特别好的选择推荐书籍。可以选择 Google 一些教程。 《鸟哥的Linux私房菜 （基础学习篇）》 豆瓣评分：9.1【2269 人评价】 推荐理由：本书是最具知名度的Linux入门书《鸟哥的Linux私房菜基础学习篇》的最新版，全面而详细地介绍了Linux操作系统。 友情提示：内容非常全面，建议挑选和自己实际工作相关度较高的，其他部分有需要再阅读。 《鸟哥的Linux私房菜 （服务器架设篇）》 豆瓣评分：8.8 【198 人评价】 推荐理由：您已有Linux基础，想要进一步学习服务器架设？还想了解如何维护与管理您的服务器？本书是您最佳的选择。 《Zabbix企业级分布式监控系统》 豆瓣评分：7.6 【39 人评价】 推荐理由：本书从运维（OPS）角度对Zabbix的各项功能进行了详细介绍，以自动化运维视角为出发点，对Zabbix的安装和配置、自动化功能、监控告警、性能调优、Zabbix API、Zabbix协议、RPM安装包定制，结合SaltStack实现自动化配置管理等内容进行了全方位的深入剖析。 《第一本Docker书》 豆瓣评分：8.8 【63 人评价】 推荐理由：本书由Docker公司前服务与支持副总裁James Turnbull编写，是Docker开发指南。本书专注于Docker 1.9及以上版本，指导读者完成Docker的安装、部署、管理和扩展，带领读者经历从测试到生产的整个开发生命周期，让读者了解Docker适用于什么场景。 推荐博客：DockerOne 推荐公众号：DockerOne 《Docker——容器与容器云》 豆瓣评分：8.5 【99 人评价】 推荐理由：本书根据Docker 1.10版和Kubernetes 1.2版对第1版进行了全面更新，从实践者的角度出发，以Docker和Kubernetes为重点，沿着“基本用法介绍”到“核心原理解读”到“高级实践技巧”的思路，一本书讲透当前主流的容器和容器云技术，有助于读者在实际场景中利用Docker容器和容器云解决问题并启发新的思考。 《Kubernetes权威指南》 豆瓣评分：7.7【15 人评价】 推荐理由：Kubernetes重磅开山之作，针对Kubernetes v1.6和本书第2版进行大篇幅内容更新，全方位完美覆盖，可借鉴性极强。 推荐博客：Kubernetes 中文社区 推荐公众号：K8S 技术社区 《用Mesos框架构建分布式应用》 豆瓣评分：暂无评分 推荐理由：超级薄的一本书，看完之后，你会对 Mesos 会非常了解，并且极大可能性学会如何基于 Mesos 框架构建分布式应用。 《数据结构与算法分析：Java语言描述》 豆瓣评分：8.3【183 人评价】 推荐理由：本书是国外数据结构与算法分析方面的经典教材，使用卓越的Java编程语言作为实现工具讨论了数据结构（组织大量数据的方法）和算法分析（对算法运行时间的估计）。 友情提示：算法方法还有其他很好的书籍，例如《算法导论》、《算法（第四版）》，也可以选择阅读。重要的是，保持耐心，享受这个痛并快乐的过程。 《Head First 设计模式》 豆瓣评分：9.2【2394 人评价】 推荐理由：《Head First设计模式》(中文版)共有14章，每章都介绍了几个设计模式，完整地涵盖了四人组版本全部23个设计模式。 《HTTP权威指南》 豆瓣评分：8.7 【1126 人评价】 推荐理由：本书尝试着将HTTP中一些互相关联且常被误解的规则梳理清楚，并编写了一系列基于各种主题的章节，对HTTP各方面的特性进行了介绍。纵观全书，对HTTP“为什么”这样做进行了详细的解释，而不仅仅停留在它是“怎么做”的。 《TCP/IP详解 系列》 豆瓣评分：9.3 【1883 人评价】 推荐理由：完整而详细的TCP/IP协议指南。针对任何希望理解TCP/IP协议是如何实现的读者设计。 《Linux内核设计与实现》 豆瓣评分：8.7【286 人评价】 详细描述了Linux内核的主要子系统和特点，包括Linux内核的设计、实现和接口。从理论到实践涵盖了Linux内核的方方面面，可以满足读者的各种兴趣和需求。 友情提示：Linux内核方面不乏好书。本书篇幅方面较为合适。 《剑指Offer：名企面试官精讲典型编程题》 豆瓣评分：8.5【508 人评价】 推荐理由：剖析了80个典型的编程面试题，系统整理基础知识、代码质量、解题思路、优化效率和综合能力这5个面试要点。 推荐网站：牛客网-专业IT笔试面试备考平台 《程序员代码面试指南：IT名企算法与数据结构题目最优解》 豆瓣评分：8.4【32 人评价】 推荐理由：程序员刷题宝典！编程能力提升秘笈！精选IT名企真实代码面试题，全面覆盖算法与数据结构题型！ 《领域驱动设计》 豆瓣评分：9.0【115 人评价】 推荐理由：是领域驱动设计方面的经典之作。全书围绕着设计和开发实践，结合若干真实的项目案例，向读者阐述如何在真实的软件开发中应用领域驱动设计。 友情提示：理论的书籍往往较为枯燥，勤修内功是必须走的路。 《火球:UML大战需求分析》 豆瓣评分：7.9【115 人评价】 推荐理由：融合UML、非UML、需求分析及需求管理等各方面的知识，帮助读者解决UML业界问题、需求分析及需求管理问题。 友情提示：可能不是最好的 UML 书籍，但从是否能够阅读理解完的角度来说，本书可能是相对合适的。有兴趣的同学也可以看看《UML和模式应用》、《大象：Thinking in UML》。 TODO List待推荐主题书籍 TODO 《大数据日知录 架构与算法》TODO 《大型网站系统与Java中间件实践》TODO 《HotSpot实战》TODO 《垃圾回收的算法与实现》TODO 《彩色UML建模》TODO 《七周七并发模型》TODO 《Go程序设计语言》 [x] Go [ ] Node [x] Linux 内核 [x] 领域 [x] UML [x] Tomcat [x] SpringCloud [x] Java 基础 [x] Netty [x] MyBatis [x] 数据库 [x] MongoDB [x] Maven [x] DevOps [x] Linux 运维 [x] 面试 [x] 消息队列 [x] 设计模式 [x] 算法与数据结构 [x] Zookeeper [x] SpringBoot [x] Nginx [x] 定时任务 [x] 搜索引擎 [x] 协议 [x] 单元测试 [x] 重构 [x] 日志 [x] Docker [x] 监控 最后原文出处 http://www.iocoder.cn/Architecture/books-recommended/ 「芋道源码」欢迎转载，保留摘要，谢谢！","tags":[{"name":"书籍","slug":"书籍","permalink":"http://www.54tianzhisheng.cn/tags/书籍/"}]},{"title":"小白谈数据脱敏","date":"2017-10-27T16:00:00.000Z","path":"2017/10/28/Data-Desensitization/","text":"什么是数据脱敏？百度百科是这样描述的： 数据脱敏是指对某些敏感信息通过脱敏规则进行数据的变形，实现敏感隐私数据的可靠保护。在涉及客户安全数据或者一些商业性敏感数据的情况下，在不违反系统规则条件下，对真实数据进行改造并提供测试使用，如身份证号、手机号、卡号、客户姓名、客户地址、等个人敏感信息都需要通过脱敏规则进行数据的变形，实现敏感隐私数据的可靠保护。这样就可以在开发、测试和其他非生产环境以及外包环境中可以安全的使用脱敏后的真实数据集。 生活中的常见例子1、火车票： 2、淘宝网页上的收获地址信息： 敏感数据梳理在进行数据脱敏之前我们应该要确定公司的哪些数据（哪些表、哪些字段）要作为脱敏的目标，下面从用户、公司、卖家方面分析： 1、用户：名字、手机号码、身份证号码、固定电话、收货地址、电子邮箱、银行卡号、密码等 2、卖家：名字、手机号码、身份证号码、固定电话等 3、公司：交易金额、优惠券码、充值码等 确定脱敏规则确定好了公司的哪些数据要作为脱敏目标后，我们就需要制定脱敏的规则（具体的实施方法）。 常见方法： 1、替换：如统一将女性用户名替换为F，这种方法更像“障眼法”，对内部人员可以完全保持信息完整性，但易破解。 2、重排：序号12345 重排为 54321，按照一定的顺序进行打乱，很像“替换”， 可以在需要时方便还原信息，但同样易破解。 3、加密：编号 12345 加密为 23456，安全程度取决于采用哪种加密算法，一般根据实际情况而定。 4、截断：13811001111 截断为 138，舍弃必要信息来保证数据的模糊性，是比较常用的脱敏方法，但往往对生产不够友好。（丢失字段的长度） 5、掩码: 123456 -&gt; 1xxxx6，保留了部分信息，并且保证了信息的长度不变性，对信息持有者更易辨别， 如火车票上得身份信息。（常用方法） 6、日期偏移取整：20130520 12:30:45 -&gt; 20130520 12:00:00，舍弃精度来保证原始数据的安全性，一般此种方法可以保护数据的时间分布密度。 目前我的脱敏规则想法是： 1、【中文姓名】只显示第一个汉字，其他隐藏为2个星号，比如：李** 2、【身份证号】显示最后四位，其他隐藏。共计18位或者15位，比如：*************1234 3、【固定电话】 显示后四位，其他隐藏，比如：*******3241 4、【手机号码】前三位，后四位，其他隐藏，比如：135****6810 5、【地址】只显示到地区，不显示详细地址，比如：上海徐汇区漕河泾开发区*** 6、【电子邮箱】 邮箱前缀仅显示第一个字母，前缀其他隐藏，用星号代替，@及后面的地址显示，比如：d**@126.com 7、【银行卡号】前六位，后四位，其他用星号隐藏每位1个星号，比如：6222600**********1234 8、【密码】密码的全部字符都用代替，比如：* 根据以上规则进行数据脱敏！ 具体思路目前是这样的： 从原数据源查询到的生产数据 ——&gt; 数据脱敏 ——&gt; 更新到目标数据源。 原数据源、目标数据源、需要脱敏的表、字段等都放在配置文件中，做到可扩展性！ 脱敏工具代码根据以上规则已经写好了一份简单的脱敏规则工具类。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119/** * 数据脱敏工具类 * Created by zhisheng_tian on 2017/10/25. */public class DesensitizedUtils &#123; /** * 【中文姓名】只显示第一个汉字，其他隐藏为2个星号，比如：李** * * @param fullName * @return */ public static String chineseName(String fullName) &#123; if (StringUtils.isBlank(fullName)) &#123; return \"\"; &#125; String name = StringUtils.left(fullName, 1); return StringUtils.rightPad(name, StringUtils.length(fullName), \"*\"); &#125; /** * 【身份证号】显示最后四位，其他隐藏。共计18位或者15位，比如：*************1234 * * @param id * @return */ public static String idCardNum(String id) &#123; if (StringUtils.isBlank(id)) &#123; return \"\"; &#125; String num = StringUtils.right(id, 4); return StringUtils.leftPad(num, StringUtils.length(id), \"*\"); &#125; /** * 【固定电话】 显示后四位，其他隐藏，比如：*******3241 * * @param num * @return */ public static String fixedPhone(String num) &#123; if (StringUtils.isBlank(num)) &#123; return \"\"; &#125; return StringUtils.leftPad(StringUtils.right(num, 4), StringUtils.length(num), \"*\"); &#125; /** * 【手机号码】前三位，后四位，其他隐藏，比如：135****6810 * * @param num * @return */ public static String mobilePhone(String num) &#123; if (StringUtils.isBlank(num)) &#123; return \"\"; &#125; return StringUtils.left(num, 3).concat(StringUtils.removeStart(StringUtils.leftPad(StringUtils.right(num, 4), StringUtils.length(num), \"*\"), \"***\")); &#125; /** * 【地址】只显示到地区，不显示详细地址，比如：上海徐汇区漕河泾开发区*** * * @param address * @param sensitiveSize 敏感信息长度 * @return */ public static String address(String address, int sensitiveSize) &#123; if (StringUtils.isBlank(address)) &#123; return \"\"; &#125; int length = StringUtils.length(address); return StringUtils.rightPad(StringUtils.left(address, length - sensitiveSize), length, \"*\"); &#125; /** * 【电子邮箱】 邮箱前缀仅显示第一个字母，前缀其他隐藏，用星号代替，@及后面的地址显示，比如：d**@126.com * * @param email * @return */ public static String email(String email) &#123; if (StringUtils.isBlank(email)) &#123; return \"\"; &#125; int index = StringUtils.indexOf(email, \"@\"); if (index &lt;= 1) return email; else return StringUtils.rightPad(StringUtils.left(email, 1), index, \"*\").concat(StringUtils.mid(email, index, StringUtils.length(email))); &#125; /** * 【银行卡号】前六位，后四位，其他用星号隐藏每位1个星号，比如：6222600**********1234 * * @param cardNum * @return */ public static String bankCard(String cardNum) &#123; if (StringUtils.isBlank(cardNum)) &#123; return \"\"; &#125; return StringUtils.left(cardNum, 6).concat(StringUtils.removeStart(StringUtils.leftPad(StringUtils.right(cardNum, 4), StringUtils.length(cardNum), \"*\"), \"******\")); &#125; /** * 【密码】密码的全部字符都用*代替，比如：****** * * @param password * @return */ public static String password(String password) &#123; if (StringUtils.isBlank(password)) &#123; return \"\"; &#125; String pwd = StringUtils.left(password, 0); return StringUtils.rightPad(pwd, StringUtils.length(password), \"*\"); &#125;&#125; 最后转载请注明地址：http://www.54tianzhisheng.cn/2017/10/28/Data-Desensitization/","tags":[{"name":"数据库","slug":"数据库","permalink":"http://www.54tianzhisheng.cn/tags/数据库/"}]},{"title":"HBase 集群监控","date":"2017-10-20T16:00:00.000Z","path":"2017/10/21/HBase-metrics/","text":"为什么需要监控？为了保证系统的稳定性，可靠性，可运维性。 掌控集群的核心性能指标，了解集群的性能表现。 集群出现问题时及时报警，便于运维同学及时修复问题。 集群重要指标值异常时进行预警，将问题扼杀在摇篮中，不用等集群真正不可用时才采取行动。 当集群出现问题时，监控系统可以帮助我们更快的定位问题和解决问题 如何构建 HBase 集群监控系统？公司有自己的监控系统，我们所要做的就是将 HBase 中我们关心的指标项发送到监控系统去，问题就转换为我们开发，采集并返回哪些 HBase 集群监控指标项。 HBase 集群监控指标采集的监控数据主要包括以下几个方面：某台机器 OS 层面上的数据，例如 CPU、内存、磁盘、网络、load、网络流量等；某台 regionserver（或master）机器 jvm 的状态，例如关于线程的信息，GC 的次数和时间，内存使用状况，以及 ERROR、WARN、Fatal 事件出现的次数；regionserver（或 master）进程中的统计信息。 可以通过以下地址获取 HBase 提供的 JMX 信息的 web 页面 1http://your_master:60010/jmx //所有的bean JMX web 页面的数据格式是json格式，信息很多！ OS 监控数据HBase 中对于 OS 的监控数据，主要是 OperatingSystem 的对象来进行的，如下就是我提取出来的 JSON 信息， 1234567891011121314151617181920&#123; \"name\" : \"java.lang:type=OperatingSystem\", \"modelerType\" : \"com.sun.management.UnixOperatingSystem\", \"MaxFileDescriptorCount\" : 1000000, \"OpenFileDescriptorCount\" : 413, \"CommittedVirtualMemorySize\" : 1892225024, \"FreePhysicalMemorySize\" : 284946432, \"FreeSwapSpaceSize\" : 535703552, \"ProcessCpuLoad\" : 0.0016732901066722444, \"ProcessCpuTime\" : 59306210000000, \"SystemCpuLoad\" : 0.018197029910060655, \"TotalPhysicalMemorySize\" : 16660848640, \"TotalSwapSpaceSize\" : 536862720, \"AvailableProcessors\" : 8, \"Arch\" : \"amd64\", \"SystemLoadAverage\" : 0.0, \"Name\" : \"Linux\", \"Version\" : \"2.6.32-431.11.7.el6.ucloud.x86_64\", \"ObjectName\" : \"java.lang:type=OperatingSystem\" &#125; 其中比较重要的指标有 OpenFileDescriptorCount , FreePhysicalMemorySize , ProcessCpuLoad , SystemCpuLoad , AvailableProcessors , SystemLoadAverage JVM 监控数据Hbase 中对于 JVM 的监控数据，主要是 JvmMetrics 的对象来进行的，如下就是我提取出来的 JSON 信息， 12345678910111213141516171819202122232425262728293031&#123; \"name\" : \"Hadoop:service=HBase,name=JvmMetrics\", \"modelerType\" : \"JvmMetrics\", \"tag.Context\" : \"jvm\", \"tag.ProcessName\" : \"Master\", \"tag.SessionId\" : \"\", \"tag.Hostname\" : \"uhadoop-qrljqo-master2\", \"MemNonHeapUsedM\" : 53.846107, \"MemNonHeapCommittedM\" : 85.84375, \"MemNonHeapMaxM\" : 130.0, \"MemHeapUsedM\" : 79.05823, \"MemHeapCommittedM\" : 240.125, \"MemHeapMaxM\" : 989.875, \"MemMaxM\" : 989.875, \"GcCountParNew\" : 15190, \"GcTimeMillisParNew\" : 72300, \"GcCountConcurrentMarkSweep\" : 2, \"GcTimeMillisConcurrentMarkSweep\" : 319, \"GcCount\" : 15192, \"GcTimeMillis\" : 72619, \"ThreadsNew\" : 0, \"ThreadsRunnable\" : 21, \"ThreadsBlocked\" : 0, \"ThreadsWaiting\" : 144, \"ThreadsTimedWaiting\" : 18, \"ThreadsTerminated\" : 0, \"LogFatal\" : 0, \"LogError\" : 0, \"LogWarn\" : 0, \"LogInfo\" : 0 &#125; JvmMetrics 主要统计的信息包括：内存的使用状态信息；GC的统计信息；线程的统计信息；以及事件的统计信息。 内存的统计信息主要是：JVM 当前已经使用的 NonHeapMemory 的大小、以及配置的 NonHeapMemory 的大小；JVM 当前已经使用的 HeapMemory 的大小、以及配置的 HeapMemory 的大小； JVM 运行时的可以使用的最大的内存的大小。 GC 的统计较为简单，仅统计了进程在固定间隔内 GC 的次数和花费的总时间。 线程的统计，主要是统计进程内当前线程的处于 NEW 、RUNNABLE、BLOCKED、WAITING、TIMED_WAITING、TERMINATED 这六种状态下的线程数量。 对于事件的统计，主要统计固定时间间隔内的 Fatal、Error、Warn 以及 Info 的数量。(这块好像不怎么重要) Region Servers 健康你也可以通过如下地址： 1http://your_master:60010/jmx?qry=Hadoop:service=HBase,name=Master,sub=Server 获得到 Region Servers 健康值： 123456789101112131415161718&#123; \"name\" : \"Hadoop:service=HBase,name=Master,sub=Server\", \"modelerType\" : \"Master,sub=Server\", \"tag.liveRegionServers\" : \"xxx\", \"tag.deadRegionServers\" : \"\", \"tag.zookeeperQuorum\" : \"xxx\", \"tag.serverName\" : \"xxx2,60000,1495683310213\", \"tag.clusterId\" : \"e5e044a3-ef9f-48f7-ba63-637376f5fa90\", \"tag.isActiveMaster\" : \"true\", \"tag.Context\" : \"master\", \"tag.Hostname\" : \"xxx\", \"masterActiveTime\" : 1495683312239, \"masterStartTime\" : 1495683310213, \"averageLoad\" : 143.66666666666666, \"numRegionServers\" : 3, \"numDeadRegionServers\" : 0, \"clusterRequests\" : 1297834323 &#125; MemoryPool从全部的 JSON 值中你会看到很多种 MemoryPool 值，比如 Par Eden Space 、CMS Perm Gen、Par Survivor Space、CMS Old Gen、Code Cache ，按需获取吧。 总结任何一个服务的监控系统都是一个不断迭代，不断优化的过程，不可能一开始就做到最好。监控总是比问题发生来的更早一些，而每一次出问题，又进一步加强相应方面的监控，我们需要让监控系统从出问题时才报警到可能出现问题时就预警逐渐过渡，最终让监控系统成为我们保证系统稳定性的一个有力工具。 最后监控指标有很多，但请按需获取 ! 转载文章请注明原出处，谢谢支持！ http://www.54tianzhisheng.cn/2017/10/21/HBase-metrics/ 参考资料1、hbase性能监控（一） 2、hbase性能监控（二） 3、hbase性能监控（三） 4、HBase 集群监控系统构建 5、hbase jmx常用监控指标 推荐相关文章1、ElasticSearch 单个节点监控 2、ElasticSearch 集群监控","tags":[{"name":"HBase","slug":"HBase","permalink":"http://www.54tianzhisheng.cn/tags/HBase/"}]},{"title":"Elasticsearch 系列文章（四）：ElasticSearch 单个节点监控","date":"2017-10-17T16:00:00.000Z","path":"2017/10/18/ElasticSearch-nodes-metrics/","text":"集群健康监控是对集群信息进行高度的概括，节点统计值 API 提供了集群中每个节点的统计值。节点统计值很多，在监控的时候仍需要我们清楚哪些指标是最值得关注的。 集群健康监控可以参考这篇文章：ElasticSearch 集群监控 节点信息 Node Info :1curl -XGET &apos;http://localhost:9200/_nodes&apos; 执行上述命令可以获取所有 node 的信息 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071_nodes: &#123; total: 2, successful: 2, failed: 0&#125;,cluster_name: \"elasticsearch\",nodes: &#123; MSQ_CZ7mTNyOSlYIfrvHag: &#123; name: \"node0\", transport_address: \"192.168.180.110:9300\", host: \"192.168.180.110\", ip: \"192.168.180.110\", version: \"5.5.0\", build_hash: \"260387d\", total_indexing_buffer: 103887667, roles:&#123;...&#125;, settings: &#123;...&#125;, os: &#123; refresh_interval_in_millis: 1000, name: \"Linux\", arch: \"amd64\", version: \"3.10.0-229.el7.x86_64\", available_processors: 4, allocated_processors: 4 &#125;, process: &#123; refresh_interval_in_millis: 1000, id: 3022, mlockall: false &#125;, jvm: &#123; pid: 3022, version: \"1.8.0_121\", vm_name: \"Java HotSpot(TM) 64-Bit Server VM\", vm_version: \"25.121-b13\", vm_vendor: \"Oracle Corporation\", start_time_in_millis: 1507515225302, mem: &#123; heap_init_in_bytes: 1073741824, heap_max_in_bytes: 1038876672, non_heap_init_in_bytes: 2555904, non_heap_max_in_bytes: 0, direct_max_in_bytes: 1038876672 &#125;, gc_collectors: [], memory_pools: [], using_compressed_ordinary_object_pointers: \"true\", input_arguments:&#123;&#125; &#125; thread_pool:&#123; force_merge: &#123;&#125;, fetch_shard_started: &#123;&#125;, listener: &#123;&#125;, index: &#123;&#125;, refresh: &#123;&#125;, generic: &#123;&#125;, warmer: &#123;&#125;, search: &#123;&#125;, flush: &#123;&#125;, fetch_shard_store: &#123;&#125;, management: &#123;&#125;, get: &#123;&#125;, bulk: &#123;&#125;, snapshot: &#123;&#125; &#125; transport: &#123;...&#125;, http: &#123;...&#125;, plugins: [], modules: [], ingest: &#123;...&#125; &#125; 上面是我已经简写了很多数据之后的返回值，但是指标还是很多，有些是一些常规的指标，对于监控来说，没必要拿取。从上面我们可以主要关注以下这些指标: 1os, process, jvm, thread_pool, transport, http, ingest and indices 节点统计 nodes-statistics节点统计值 API 可通过如下命令获取： 1GET /_nodes/stats 得到： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158_nodes: &#123; total: 2, successful: 2, failed: 0&#125;,cluster_name: \"elasticsearch\",nodes: &#123; MSQ_CZ7mTNyOSlYI0yvHag: &#123; timestamp: 1508312932354, name: \"node0\", transport_address: \"192.168.180.110:9300\", host: \"192.168.180.110\", ip: \"192.168.180.110:9300\", roles: [], indices: &#123; docs: &#123; count: 6163666, deleted: 0 &#125;, store: &#123; size_in_bytes: 2301398179, throttle_time_in_millis: 122850 &#125;, indexing: &#123;&#125;, get: &#123;&#125;, search: &#123;&#125;, merges: &#123;&#125;, refresh: &#123;&#125;, flush: &#123;&#125;, warmer: &#123;&#125;, query_cache: &#123;&#125;, fielddata: &#123;&#125;, completion: &#123;&#125;, segments: &#123;&#125;, translog: &#123;&#125;, request_cache: &#123;&#125;, recovery: &#123;&#125; &#125;, os: &#123; timestamp: 1508312932369, cpu: &#123; percent: 0, load_average: &#123; 1m: 0.09, 5m: 0.12, 15m: 0.08 &#125; &#125;, mem: &#123; total_in_bytes: 8358301696, free_in_bytes: 1381613568, used_in_bytes: 6976688128, free_percent: 17, used_percent: 83 &#125;, swap: &#123; total_in_bytes: 8455712768, free_in_bytes: 8455299072, used_in_bytes: 413696 &#125;, cgroup: &#123; cpuacct: &#123;&#125;, cpu: &#123; control_group: \"/user.slice\", cfs_period_micros: 100000, cfs_quota_micros: -1, stat: &#123;&#125; &#125; &#125;&#125;,process: &#123; timestamp: 1508312932369, open_file_descriptors: 228, max_file_descriptors: 65536, cpu: &#123; percent: 0, total_in_millis: 2495040 &#125;, mem: &#123; total_virtual_in_bytes: 5002465280 &#125;&#125;,jvm: &#123; timestamp: 1508312932369, uptime_in_millis: 797735804, mem: &#123; heap_used_in_bytes: 318233768, heap_used_percent: 30, heap_committed_in_bytes: 1038876672, heap_max_in_bytes: 1038876672, non_heap_used_in_bytes: 102379784, non_heap_committed_in_bytes: 108773376, pools: &#123; young: &#123; used_in_bytes: 62375176, max_in_bytes: 279183360, peak_used_in_bytes: 279183360, peak_max_in_bytes: 279183360 &#125;, survivor: &#123; used_in_bytes: 175384, max_in_bytes: 34865152, peak_used_in_bytes: 34865152, peak_max_in_bytes: 34865152 &#125;, old: &#123; used_in_bytes: 255683208, max_in_bytes: 724828160, peak_used_in_bytes: 255683208, peak_max_in_bytes: 724828160 &#125; &#125; &#125;, threads: &#123;&#125;, gc: &#123;&#125;, buffer_pools: &#123;&#125;, classes: &#123;&#125;&#125;, thread_pool: &#123; bulk: &#123;&#125;, fetch_shard_started: &#123;&#125;, fetch_shard_store: &#123;&#125;, flush: &#123;&#125;, force_merge: &#123;&#125;, generic: &#123;&#125;, get: &#123;&#125;, index: &#123; threads: 1, queue: 0, active: 0, rejected: 0, largest: 1, completed: 1 &#125; listener: &#123;&#125;, management: &#123;&#125;, refresh: &#123;&#125;, search: &#123;&#125;, snapshot: &#123;&#125;, warmer: &#123;&#125; &#125;, fs: &#123;&#125;, transport: &#123; server_open: 13, rx_count: 11696, rx_size_in_bytes: 1525774, tx_count: 10282, tx_size_in_bytes: 1440101928 &#125;, http: &#123; current_open: 4, total_opened: 23 &#125;, breakers: &#123;&#125;, script: &#123;&#125;, discovery: &#123;&#125;, ingest: &#123;&#125;&#125; 节点名是一个 UUID，上面列举了很多指标，下面讲解下： 索引部分 indices这部分列出了这个节点上所有索引的聚合过的统计值 ： docs 展示节点内存有多少文档，包括还没有从段里清除的已删除文档数量。 store 部分显示节点耗用了多少物理存储。这个指标包括主分片和副本分片在内。如果限流时间很大，那可能表明你的磁盘限流设置得过低。 indexing 显示已经索引了多少文档。这个值是一个累加计数器。在文档被删除的时候，数值不会下降。还要注意的是，在发生内部 索引操作的时候，这个值也会增加，比如说文档更新。 还列出了索引操作耗费的时间，正在索引的文档数量，以及删除操作的类似统计值。 get 显示通过 ID 获取文档的接口相关的统计值。包括对单个文档的 GET 和 HEAD 请求。 search 描述在活跃中的搜索（ open_contexts ）数量、查询的总数量、以及自节点启动以来在查询上消耗的总时间。用 query_time_in_millis / query_total 计算的比值，可以用来粗略的评价你的查询有多高效。比值越大，每个查询花费的时间越多，你应该要考虑调优了。 fetch 统计值展示了查询处理的后一半流程（query-then-fetch 里的 fetch ）。如果 fetch 耗时比 query 还多，说明磁盘较慢，或者获取了太多文档，或者可能搜索请求设置了太大的分页（比如， size: 10000 ）。 merges 包括了 Lucene 段合并相关的信息。它会告诉你目前在运行几个合并，合并涉及的文档数量，正在合并的段的总大小，以及在合并操作上消耗的总时间。 filter_cache 展示了已缓存的过滤器位集合所用的内存数量，以及过滤器被驱逐出内存的次数。过多的驱逐数 可能 说明你需要加大过滤器缓存的大小，或者你的过滤器不太适合缓存（比如它们因为高基数而在大量产生，就像是缓存一个 now 时间表达式）。 不过，驱逐数是一个很难评定的指标。过滤器是在每个段的基础上缓存的，而从一个小的段里驱逐过滤器，代价比从一个大的段里要廉价的多。有可能你有很大的驱逐数，但是它们都发生在小段上，也就意味着这些对查询性能只有很小的影响。 把驱逐数指标作为一个粗略的参考。如果你看到数字很大，检查一下你的过滤器，确保他们都是正常缓存的。不断驱逐着的过滤器，哪怕都发生在很小的段上，效果也比正确缓存住了的过滤器差很多。 field_data 显示 fielddata 使用的内存， 用以聚合、排序等等。这里也有一个驱逐计数。和 filter_cache 不同的是，这里的驱逐计数是很有用的：这个数应该或者至少是接近于 0。因为 fielddata 不是缓存，任何驱逐都消耗巨大，应该避免掉。如果你在这里看到驱逐数，你需要重新评估你的内存情况，fielddata 限制，请求语句，或者这三者。 segments 会展示这个节点目前正在服务中的 Lucene 段的数量。 这是一个重要的数字。大多数索引会有大概 50–150 个段，哪怕它们存有 TB 级别的数十亿条文档。段数量过大表明合并出现了问题（比如，合并速度跟不上段的创建）。注意这个统计值是节点上所有索引的汇聚总数。记住这点。 memory 统计值展示了 Lucene 段自己用掉的内存大小。 这里包括底层数据结构，比如倒排表，字典，和布隆过滤器等。太大的段数量会增加这些数据结构带来的开销，这个内存使用量就是一个方便用来衡量开销的度量值。 操作系统和进程部分OS 和 Process 部分基本是自描述的，不会在细节中展开讲解。它们列出来基础的资源统计值，比如 CPU 和负载。OS 部分描述了整个操作系统，而 Process 部分只显示 Elasticsearch 的 JVM 进程使用的资源情况。 这些都是非常有用的指标，不过通常在你的监控技术栈里已经都测量好了。统计值包括下面这些： CPU 负载 内存使用率 （mem.used_percent） Swap 使用率 打开的文件描述符 （open_file_descriptors） JVM 部分jvm 部分包括了运行 Elasticsearch 的 JVM 进程一些很关键的信息。 最重要的，它包括了垃圾回收的细节，这对你的 Elasticsearch 集群的稳定性有着重大影响。 123456789101112jvm: &#123; timestamp: 1508312932369, uptime_in_millis: 797735804, mem: &#123; heap_used_in_bytes: 318233768, heap_used_percent: 30, heap_committed_in_bytes: 1038876672, heap_max_in_bytes: 1038876672, non_heap_used_in_bytes: 102379784, non_heap_committed_in_bytes: 108773376, &#125;&#125; jvm 部分首先列出一些和 heap 内存使用有关的常见统计值。你可以看到有多少 heap 被使用了，多少被指派了（当前被分配给进程的），以及 heap 被允许分配的最大值。理想情况下，heap_committed_in_bytes 应该等于 heap_max_in_bytes 。如果指派的大小更小，JVM 最终会被迫调整 heap 大小——这是一个非常昂贵的操作。如果你的数字不相等，阅读 堆内存:大小和交换 学习如何正确的配置它。 heap_used_percent 指标是值得关注的一个数字。Elasticsearch 被配置为当 heap 达到 75% 的时候开始 GC。如果你的节点一直 &gt;= 75%，你的节点正处于 内存压力 状态。这是个危险信号，不远的未来可能就有慢 GC 要出现了。 如果 heap 使用率一直 &gt;=85%，你就麻烦了。Heap 在 90–95% 之间，则面临可怕的性能风险，此时最好的情况是长达 10–30s 的 GC，最差的情况就是内存溢出（OOM）异常。 线程池部分Elasticsearch 在内部维护了线程池。 这些线程池相互协作完成任务，有必要的话相互间还会传递任务。通常来说，你不需要配置或者调优线程池，不过查看它们的统计值有时候还是有用的，可以洞察你的集群表现如何。 每个线程池会列出已配置的线程数量（ threads ），当前在处理任务的线程数量（ active ），以及在队列中等待处理的任务单元数量（ queue ）。 如果队列中任务单元数达到了极限，新的任务单元会开始被拒绝，你会在 rejected 统计值上看到它反映出来。这通常是你的集群在某些资源上碰到瓶颈的信号。因为队列满意味着你的节点或集群在用最高速度运行，但依然跟不上工作的蜂拥而入。 这里的一系列的线程池，大多数你可以忽略，但是有一小部分还是值得关注的： indexing 普通的索引请求的线程池 bulk 批量请求，和单条的索引请求不同的线程池 get Get-by-ID 操作 search 所有的搜索和查询请求 merging 专用于管理 Lucene 合并的线程池 网络部分 transport 显示和 传输地址 相关的一些基础统计值。包括节点间的通信（通常是 9300 端口）以及任意传输客户端或者节点客户端的连接。如果看到这里有很多连接数不要担心；Elasticsearch 在节点之间维护了大量的连接。 http 显示 HTTP 端口（通常是 9200）的统计值。如果你看到 total_opened 数很大而且还在一直上涨，这是一个明确信号，说明你的 HTTP 客户端里有没启用 keep-alive 长连接的。持续的 keep-alive 长连接对性能很重要，因为连接、断开套接字是很昂贵的（而且浪费文件描述符）。请确认你的客户端都配置正确。 参考资料1、nodes-info 2、nodes-stats 3、ES监控指标 最后：转载请注明地址：http://www.54tianzhisheng.cn/2017/10/18/ElasticSearch-nodes-metrics/","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://www.54tianzhisheng.cn/tags/ElasticSearch/"}]},{"title":"Elasticsearch 系列文章（三）：ElasticSearch 集群监控","date":"2017-10-14T16:00:00.000Z","path":"2017/10/15/ElasticSearch-cluster-health-metrics/","text":"最近在做 ElasticSearch 的信息（集群和节点）监控，特此稍微整理下学到的东西。这篇文章主要介绍集群的监控。 要监控哪些 ElasticSearch metrics Elasticsearch 提供了大量的 Metric，可以帮助您检测到问题的迹象，在遇到节点不可用、out-of-memory、long garbage collection times 的时候采取相应措施。但是指标太多了，有时我们并不需要这么多，这就需要我们进行筛选。 集群健康一个 Elasticsearch 集群至少包括一个节点和一个索引。或者它 可能有一百个数据节点、三个单独的主节点，以及一小打客户端节点——这些共同操作一千个索引（以及上万个分片）。 不管集群扩展到多大规模，你都会想要一个快速获取集群状态的途径。Cluster Health API 充当的就是这个角色。你可以把它想象成是在一万英尺的高度鸟瞰集群。它可以告诉你安心吧一切都好，或者警告你集群某个地方有问题。 让我们执行一下 cluster-health API 然后看看响应体是什么样子的： 1GET _cluster/health 和 Elasticsearch 里其他 API 一样，cluster-health 会返回一个 JSON 响应。这对自动化和告警系统来说，非常便于解析。响应中包含了和你集群有关的一些关键信息： 123456789101112&#123; \"cluster_name\": \"elasticsearch_zach\", \"status\": \"green\", \"timed_out\": false, \"number_of_nodes\": 1, \"number_of_data_nodes\": 1, \"active_primary_shards\": 10, \"active_shards\": 10, \"relocating_shards\": 0, \"initializing_shards\": 0, \"unassigned_shards\": 0&#125; 响应信息中最重要的一块就是 status 字段。状态可能是下列三个值之一 : status 含义 green 所有的主分片和副本分片都已分配。你的集群是 100% 可用的。 yellow 所有的主分片已经分片了，但至少还有一个副本是缺失的。不会有数据丢失，所以搜索结果依然是完整的。不过，你的高可用性在某种程度上被弱化。如果 更多的 分片消失，你就会丢数据了。把 yellow 想象成一个需要及时调查的警告。 red 至少一个主分片（以及它的全部副本）都在缺失中。这意味着你在缺少数据：搜索只能返回部分数据，而分配到这个分片上的写入请求会返回一个异常。 number_of_nodes 和 number_of_data_nodes 这个命名完全是自描述的。 active_primary_shards 指出你集群中的主分片数量。这是涵盖了所有索引的汇总值。 active_shards 是涵盖了所有索引的所有分片的汇总值，即包括副本分片。 relocating_shards 显示当前正在从一个节点迁往其他节点的分片的数量。通常来说应该是 0，不过在 Elasticsearch 发现集群不太均衡时，该值会上涨。比如说：添加了一个新节点，或者下线了一个节点。 initializing_shards 是刚刚创建的分片的个数。比如，当你刚创建第一个索引，分片都会短暂的处于 initializing 状态。这通常会是一个临时事件，分片不应该长期停留在 initializing状态。你还可能在节点刚重启的时候看到 initializing 分片：当分片从磁盘上加载后，它们会从initializing 状态开始。 unassigned_shards 是已经在集群状态中存在的分片，但是实际在集群里又找不着。通常未分配分片的来源是未分配的副本。比如，一个有 5 分片和 1 副本的索引，在单节点集群上，就会有 5 个未分配副本分片。如果你的集群是 red 状态，也会长期保有未分配分片（因为缺少主分片）。 集群统计集群统计信息包含 集群的分片数，文档数，存储空间，缓存信息，内存使用率，插件内容，文件系统内容，JVM 作用状况，系统 CPU，OS 信息，段信息。 查看全部统计信息命令： 1curl -XGET &apos;http://localhost:9200/_cluster/stats?human&amp;pretty&apos; 返回 JSON 结果： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177&#123; \"timestamp\": 1459427693515, \"cluster_name\": \"elasticsearch\", \"status\": \"green\", \"indices\": &#123; \"count\": 2, \"shards\": &#123; \"total\": 10, \"primaries\": 10, \"replication\": 0, \"index\": &#123; \"shards\": &#123; \"min\": 5, \"max\": 5, \"avg\": 5 &#125;, \"primaries\": &#123; \"min\": 5, \"max\": 5, \"avg\": 5 &#125;, \"replication\": &#123; \"min\": 0, \"max\": 0, \"avg\": 0 &#125; &#125; &#125;, \"docs\": &#123; \"count\": 10, \"deleted\": 0 &#125;, \"store\": &#123; \"size\": \"16.2kb\", \"size_in_bytes\": 16684, \"throttle_time\": \"0s\", \"throttle_time_in_millis\": 0 &#125;, \"fielddata\": &#123; \"memory_size\": \"0b\", \"memory_size_in_bytes\": 0, \"evictions\": 0 &#125;, \"query_cache\": &#123; \"memory_size\": \"0b\", \"memory_size_in_bytes\": 0, \"total_count\": 0, \"hit_count\": 0, \"miss_count\": 0, \"cache_size\": 0, \"cache_count\": 0, \"evictions\": 0 &#125;, \"completion\": &#123; \"size\": \"0b\", \"size_in_bytes\": 0 &#125;, \"segments\": &#123; \"count\": 4, \"memory\": \"8.6kb\", \"memory_in_bytes\": 8898, \"terms_memory\": \"6.3kb\", \"terms_memory_in_bytes\": 6522, \"stored_fields_memory\": \"1.2kb\", \"stored_fields_memory_in_bytes\": 1248, \"term_vectors_memory\": \"0b\", \"term_vectors_memory_in_bytes\": 0, \"norms_memory\": \"384b\", \"norms_memory_in_bytes\": 384, \"doc_values_memory\": \"744b\", \"doc_values_memory_in_bytes\": 744, \"index_writer_memory\": \"0b\", \"index_writer_memory_in_bytes\": 0, \"version_map_memory\": \"0b\", \"version_map_memory_in_bytes\": 0, \"fixed_bit_set\": \"0b\", \"fixed_bit_set_memory_in_bytes\": 0, \"file_sizes\": &#123;&#125; &#125;, \"percolator\": &#123; \"num_queries\": 0 &#125; &#125;, \"nodes\": &#123; \"count\": &#123; \"total\": 1, \"data\": 1, \"coordinating_only\": 0, \"master\": 1, \"ingest\": 1 &#125;, \"versions\": [ \"5.6.3\" ], \"os\": &#123; \"available_processors\": 8, \"allocated_processors\": 8, \"names\": [ &#123; \"name\": \"Mac OS X\", \"count\": 1 &#125; ], \"mem\" : &#123; \"total\" : \"16gb\", \"total_in_bytes\" : 17179869184, \"free\" : \"78.1mb\", \"free_in_bytes\" : 81960960, \"used\" : \"15.9gb\", \"used_in_bytes\" : 17097908224, \"free_percent\" : 0, \"used_percent\" : 100 &#125; &#125;, \"process\": &#123; \"cpu\": &#123; \"percent\": 9 &#125;, \"open_file_descriptors\": &#123; \"min\": 268, \"max\": 268, \"avg\": 268 &#125; &#125;, \"jvm\": &#123; \"max_uptime\": \"13.7s\", \"max_uptime_in_millis\": 13737, \"versions\": [ &#123; \"version\": \"1.8.0_74\", \"vm_name\": \"Java HotSpot(TM) 64-Bit Server VM\", \"vm_version\": \"25.74-b02\", \"vm_vendor\": \"Oracle Corporation\", \"count\": 1 &#125; ], \"mem\": &#123; \"heap_used\": \"57.5mb\", \"heap_used_in_bytes\": 60312664, \"heap_max\": \"989.8mb\", \"heap_max_in_bytes\": 1037959168 &#125;, \"threads\": 90 &#125;, \"fs\": &#123; \"total\": \"200.6gb\", \"total_in_bytes\": 215429193728, \"free\": \"32.6gb\", \"free_in_bytes\": 35064553472, \"available\": \"32.4gb\", \"available_in_bytes\": 34802409472 &#125;, \"plugins\": [ &#123; \"name\": \"analysis-icu\", \"version\": \"5.6.3\", \"description\": \"The ICU Analysis plugin integrates Lucene ICU module into elasticsearch, adding ICU relates analysis components.\", \"classname\": \"org.elasticsearch.plugin.analysis.icu.AnalysisICUPlugin\", \"has_native_controller\": false &#125;, &#123; \"name\": \"ingest-geoip\", \"version\": \"5.6.3\", \"description\": \"Ingest processor that uses looksup geo data based on ip adresses using the Maxmind geo database\", \"classname\": \"org.elasticsearch.ingest.geoip.IngestGeoIpPlugin\", \"has_native_controller\": false &#125;, &#123; \"name\": \"ingest-user-agent\", \"version\": \"5.6.3\", \"description\": \"Ingest processor that extracts information from a user agent\", \"classname\": \"org.elasticsearch.ingest.useragent.IngestUserAgentPlugin\", \"has_native_controller\": false &#125; ] &#125;&#125; 内存使用和 GC 指标在运行 Elasticsearch 时，内存是您要密切监控的关键资源之一。 Elasticsearch 和 Lucene 以两种方式利用节点上的所有可用 RAM：JVM heap 和文件系统缓存。 Elasticsearch 运行在Java虚拟机（JVM）中，这意味着JVM垃圾回收的持续时间和频率将成为其他重要的监控领域。 上面返回的 JSON，监控的指标有我个人觉得有这些： nodes.successful nodes.failed nodes.total nodes.mem.used_percent nodes.process.cpu.percent nodes.jvm.mem.heap_used 可以看到 JSON 文件是很复杂的，如果从这复杂的 JSON 中获取到对应的指标（key）的值呢，这里请看文章 ：JsonPath —— JSON 解析神器 最后这里主要讲下 ES 集群的一些监控信息，有些监控指标是个人觉得需要监控的，但是具体情况还是得看需求了。下篇文章主要讲节点的监控信息。转载请注明地址：http://www.54tianzhisheng.cn/2017/10/15/ElasticSearch-cluster-health-metrics/ 参考资料1、How to monitor Elasticsearch performance 2、ElasticSearch 性能监控 3、cluster-health 4、cluster-stats 相关阅读1、Elasticsearch 默认分词器和中分分词器之间的比较及使用方法 2、全文搜索引擎 Elasticsearch 集群搭建入门教程","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://www.54tianzhisheng.cn/tags/ElasticSearch/"}]},{"title":"JsonPath —— JSON 解析神器","date":"2017-10-13T16:00:00.000Z","path":"2017/10/14/JsonPath/","text":"真乃神器也，再复杂的 Json 都能给你解析出来，非常方便的获取 JSON 的内容，很强大！语法简介 JsonPath 描述 $ 根节点 @ 当前节点 .or[] 子节点 .. 选择所有符合条件的节点 * 所有节点 [] 迭代器标示，如数组下标 [,] 支持迭代器中做多选 [start:end:step] 数组切片运算符 ?() 支持过滤操作 () 支持表达式计算 JSON 值： 1234567891011121314151617181920&#123; \"store\": &#123; \"book\": [ &#123; \"category\": \"reference\", \"author\": \"Nigel Rees\", \"title\": \"Sayings of the Century\", \"price\": 8.95 &#125;, &#123; \"category\": \"fiction\", \"author\": \"Evelyn Waugh\", \"title\": \"Sword of Honour\", \"price\": 12.99, \"isbn\": \"0-553-21311-3\" &#125; ], \"bicycle\": &#123; \"color\": \"red\", \"price\": 19.95 &#125; &#125;&#125; 导包：import com.jayway.jsonpath.JsonPath 解析代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344//输出book[0]的author值String author = JsonPath.read(json, \"$.store.book[0].author\");System.out.println(\"author\\t\"+author);//输出全部author的值，使用Iterator迭代List&lt;String&gt; authors = JsonPath.read(json, \"$.store.book[*].author\");System.out.println(\"authors\\t\"+authors);//输出book[*]中category == 'reference'的bookList&lt;Object&gt; books = JsonPath.read(json, \"$.store.book[?(@.category == 'reference')]\");System.out.println(\"books\\t\"+books);//输出book[*]中category == 'reference'的book或者List&lt;Object&gt; books2 = JsonPath.read(json, \"$.store.book[?(@.category == 'reference' || @.price&gt;10)]\");System.out.println(\"books2\\t\"+books2);//输出book[*]中category == 'reference'的book的authorList&lt;Object&gt; books1 = JsonPath.read(json, \"$.store.book[?(@.category == 'reference')].author\");System.out.println(\"books1\\t\"+books1);//输出book[*]中price&gt;10的bookList&lt;Object&gt; b1 = JsonPath.read(json, \"$.store.book[?(@.price&gt;10)]\");System.out.println(\"b1\"+b1);//输出book[*]中含有isbn元素的bookList&lt;Object&gt; b2 = JsonPath.read(json, \"$.store.book[?(@.isbn)]\");System.out.println(\"b2\"+b2);//输出该json中所有price的值List&lt;Double&gt; prices = JsonPath.read(json, \"$..price\");System.out.println(\"prices\"+prices);//输出该json中所有title的值List&lt;Double&gt; title = JsonPath.read(json, \"$..title\");System.out.println(\"title\"+title);//输出该json中book 0,1的值List&lt;Double&gt; book01 = JsonPath.read(json, \"$..book[0,1]\");System.out.println(\"book01\"+book01);/* //输出该json中book 0,1的值List&lt;Double&gt; book012 = JsonPath.read(json, \"$..book[-2:]\");System.out.println(\"book012\"+book012);*///可以提前编辑一个路径，并多次使用它JsonPath path = JsonPath.compile(\"$.store.book[*]\");List&lt;Object&gt; b3 = path.read(json);System.out.println(\"path\\t\"+path+\"\\n\"+b3); 用法比较简单，多使用几次就会使用了！文章主要参考网上！原谅很多天不跟博的我现在竟然这样水了这么一篇文章，哈哈！实在是忙！","tags":[{"name":"JSON","slug":"JSON","permalink":"http://www.54tianzhisheng.cn/tags/JSON/"}]},{"title":"Centos7 搭建最新 Nexus3 Maven 私服","date":"2017-10-13T16:00:00.000Z","path":"2017/10/14/Nexus3-Maven/","text":"Maven 介绍Apache Maven 是一个创新的软件项目管理和综合工具。Maven 提供了一个基于项目对象模型（POM）文件的新概念来管理项目的构建，可以从一个中心资料片管理项目构建，报告和文件。Maven 最强大的功能就是能够自动下载项目依赖库。Maven 提供了开发人员构建一个完整的生命周期框架。开发团队可以自动完成项目的基础工具建设，Maven 使用标准的目录结构和默认构建生命周期。在多个开发团队环境时，Maven 可以设置按标准在非常短的时间里完成配置工作。由于大部分项目的设置都很简单，并且可重复使用，Maven 让开发人员的工作更轻松，同时创建报表，检查，构建和测试自动化设置。Maven 项目的结构和内容在一个 XML 文件中声明，pom.xml 项目对象模型（POM），这是整个 Maven 系统的基本单元。 Maven 提供了开发人员的方式来管理：1）Builds2）Documentation3）Reporting4）Dependencies5）SCMs6）Releases7）Distribution8）mailing list概括地说，Maven 简化和标准化项目建设过程。处理编译，分配，文档，团队协作和其他任务的无缝连接。Maven 增加可重用性并负责建立相关的任务。Maven 最初设计，是以简化 Jakarta Turbine 项目的建设。在几个项目，每个项目包含了不同的 Ant 构建文件。 JAR 检查到 CVS。Apache 组织开发 Maven 可以建立多个项目，发布项目信息，项目部署，在几个项目中 JAR 文件提供团队合作和帮助。 Maven 主要目标是提供给开发人员：1）项目是可重复使用，易维护，更容易理解的一个综合模型。2）插件或交互的工具，这种声明性的模式。 私服介绍私服是指私有服务器，是架设在局域网的一种特殊的远程仓库，目的是代理远程仓库及部署第三方构建。有了私服之后，当 Maven 需要下载构件时，直接请求私服，私服上存在则下载到本地仓库；否则，私服请求外部的远程仓库，将构件下载到私服，再提供给本地仓库下载。 Nexus 介绍Nexus 是一个强大的 Maven 仓库管理器，它极大地简化了本地内部仓库的维护和外部仓库的访问。如果使用了公共的 Maven 仓库服务器，可以从 Maven 中央仓库下载所需要的构件（Artifact），但这通常不是一个好的做法。正常做法是在本地架设一个 Maven 仓库服务器，即利用 Nexus 私服可以只在一个地方就能够完全控制访问和部署在你所维护仓库中的每个 Artifact。Nexus 在代理远程仓库的同时维护本地仓库，以降低中央仓库的负荷, 节省外网带宽和时间，Nexus 私服就可以满足这样的需要。Nexus 是一套 “开箱即用” 的系统不需要数据库，它使用文件系统加 Lucene 来组织数据。Nexus 使用 ExtJS 来开发界面，利用 Restlet 来提供完整的 REST APIs，通过 m2eclipse 与 Eclipse 集成使用。Nexus 支持 WebDAV 与 LDAP 安全身份认证。Nexus 还提供了强大的仓库管理功能，构件搜索功能，它基于 REST，友好的 UI 是一个 extjs 的 REST 客户端，它占用较少的内存，基于简单文件系统而非数据库。 为什么要构建 Nexus 私服？如果没有 Nexus 私服，我们所需的所有构件都需要通过 maven 的中央仓库和第三方的 Maven 仓库下载到本地，而一个团队中的所有人都重复的从 maven 仓库下载构件无疑加大了仓库的负载和浪费了外网带宽，如果网速慢的话，还会影响项目的进程。很多情况下项目的开发都是在内网进行的，连接不到 maven 仓库怎么办呢？开发的公共构件怎么让其它项目使用？这个时候我们不得不为自己的团队搭建属于自己的 maven 私服，这样既节省了网络带宽也会加速项目搭建的进程，当然前提条件就是你的私服中拥有项目所需的所有构件。 总之，在本地构建 nexus 私服的好处有：1）加速构建；2）节省带宽；3）节省中央 maven 仓库的带宽；4）稳定（应付一旦中央服务器出问题的情况）；5）控制和审计；6）能够部署第三方构件；7）可以建立本地内部仓库；8）可以建立公共仓库这些优点使得 Nexus 日趋成为最流行的 Maven 仓库管理器。 1. 安装 jdk1.8关于 jdk1.8 的安装, 在这里就不做赘述了 2. 安装 maven关于 maven 的安装, 本文在这里就不详细写了 3. 安装 nexus31. 下载 nexus-3.6.0-02-unix.tar.gz官网链接地址：https://www.sonatype.com/download-oss-sonatype 下载 linux 最新版本，直接下载速度可能很慢，建议用迅雷下载会快很多的。 2. 解压1tar -zxvf nexus-3.6.0-02-unix.tar.gz -C /usr/local/ 3. 启动 nexus312cd /usr/local/nexus-3.6.0-02/bin/./nexus run &amp; 稍等一会 (首次启动会比较慢), 当出现以下日志的时候表示启动成功! 12345-------------------------------------------------Started Sonatype Nexus OSS 3.6.0-02------------------------------------------------- 4. 开启远程访问端口关闭防火墙，并开启远程访问端口 8081 123vim /etc/sysconfig/iptables添加：-A INPUT -p tcp -m state --state NEW -m tcp --dport 8081 -j ACCEPT 5. 测试 123nexus3默认端口是:8081nexus3默认账号是:adminnexus3默认密码是:admin123 6. 设置开机自启动123ln -s /usr/local/nexus-3.6.0-02/bin/nexus /etc/init.d/nexus3chkconfig --add nexus3chkconfig nexus3 on 7. 修改 nexus3 的运行用户为 root1vim nexus.rc 12//设置run_as_user=&quot;root&quot; 8. 修改 nexus3 启动时要使用的 jdk 版本1vim nexus 第 14 行: 1INSTALL4J_JAVA_HOME_OVERRIDE=/usr/local/java/jdk1.8.0_144 9. 修改 nexus3 默认端口 (可选)12cd /usr/local/nexus-3.6.0-02/etc/vim nexus-default.properties 默认端口: 8081 1application-port=8081 10. 修改 nexus3 数据以及相关日志的存储位置 (可选)：12[root@MiWiFi-R3-srv bin]# cd /usr/local/nexus-3.6.0-02/bin/[root@MiWiFi-R3-srv bin]# vim nexus.vmoptions 123-XX:LogFile=./sonatype-work/nexus3/log/jvm.log-Dkaraf.data=./sonatype-work/nexus3-Djava.io.tmpdir=./sonatype-work/nexus3/tmp 出现上面 5 中的测试页面，说明配置 nexus 成功！ 点击右上角 “Log in”， 输入默认用户名 (admin) 和默认密码（admin123）登录 至此, nexus3_maven 的私服就搭建完成了!!! 可以点击上面的 “设置” 图标，在 “设置” 里可以添加用户、角色，对接 LDAP 等的设置，如下： 可以在 “管理” 里查看 nexus 的系统信息 注意：1.component name 的一些说明： 1）maven-central：maven 中央库，默认从 https://repo1.maven.org/maven2 / 拉取 jar 2）maven-releases：私库发行版 jar 3）maven-snapshots：私库快照（调试版本）jar 4）maven-public：仓库分组，把上面三个仓库组合在一起对外提供服务，在本地 maven 基础配置 settings.xml 中使用。 2.Nexus 默认的仓库类型有以下四种： 1）group(仓库组类型)：又叫组仓库，用于方便开发人员自己设定的仓库； 2）hosted(宿主类型)：内部项目的发布仓库（内部开发人员，发布上去存放的仓库）； 3）proxy(代理类型)：从远程中央仓库中寻找数据的仓库（可以点击对应的仓库的 Configuration 页签下 Remote Storage Location 属性的值即被代理的远程仓库的路径）； 4）virtual(虚拟类型)：虚拟仓库（这个基本用不到，重点关注上面三个仓库的使用）； 3.Policy(策略): 表示该仓库为发布 (Release) 版本仓库还是快照 (Snapshot) 版本仓库； 4.Public Repositories 下的仓库 1）3rd party: 无法从公共仓库获得的第三方发布版本的构件仓库，即第三方依赖的仓库，这个数据通常是由内部人员自行下载之后发布上去； 2）Apache Snapshots: 用了代理 ApacheMaven 仓库快照版本的构件仓库 3）Central: 用来代理 maven 中央仓库中发布版本构件的仓库 4）Central M1 shadow: 用于提供中央仓库中 M1 格式的发布版本的构件镜像仓库 5）Codehaus Snapshots: 用来代理 CodehausMaven 仓库的快照版本构件的仓库 6）Releases: 内部的模块中 release 模块的发布仓库，用来部署管理内部的发布版本构件的宿主类型仓库；release 是发布版本； 7）Snapshots: 发布内部的 SNAPSHOT 模块的仓库，用来部署管理内部的快照版本构件的宿主类型仓库；snapshots 是快照版本，也就是不稳定版本所以自定义构建的仓库组代理仓库的顺序为：Releases，Snapshots，3rd party，Central。也可以使用 oschina 放到 Central 前面，下载包会更快。 5.Nexus 默认的端口是 8081，可以在 etc/nexus-default.properties 配置中修改。 6.Nexus 默认的用户名密码是 admin/admin123 当遇到奇怪问题时，重启 nexus，重启后 web 界面要 1 分钟左右后才能访问。 8.Nexus 的工作目录是 sonatype-work（路径一般在 nexus 同级目录下）12345678[root@master-node local]# pwd/usr/local[root@master-node local]# ls nexus/bin deploy etc lib LICENSE.txt NOTICE.txt public system[root@master-node local]# ls sonatype-work/nexus3[root@master-node local]# ls sonatype-work/nexus3/backup blobs cache db elasticsearch etc generated-bundles health-check instances keystores lock log orient port tmp Nexus 仓库分类的概念：1）Maven 可直接从宿主仓库下载构件, 也可以从代理仓库下载构件, 而代理仓库间接的从远程仓库下载并缓存构件2）为了方便, Maven 可以从仓库组下载构件, 而仓库组并没有时间的内容 (下图中用虚线表示, 它会转向包含的宿主仓库或者代理仓库获得实际构件的内容). Nexus 的 web 界面功能介绍1.Browse Server Content 1.1 Search这个就是类似 Maven 仓库上的搜索功能，就是从私服上查找是否有哪些包。注意：1）在 Search 这级是支持模糊搜索的，如图所示： 2）如果进入具体的目录，好像不支持模糊搜索，如图所示： 1.2 Browse 1）Assets这是能看到所有的资源，包含 Jar，已经对 Jar 的一些描述信息。2）Components这里只能看到 Jar 包。 2.Server Adminstration And configuration看到这个选项的前提是要进行登录的，如上面已经介绍登陆方法，右上角点击 “Sign In” 的登录按钮，输入 admin/admin123, 登录成功之后，即可看到此功能，如图所示： 2.1 Blob Stores文件存储的地方，创建一个目录的话，对应文件系统的一个目录，如图所示： 2.2 Repositories 1）Proxy这里就是代理的意思，代理中央 Maven 仓库，当 PC 访问中央库的时候，先通过 Proxy 下载到 Nexus 仓库，然后再从 Nexus 仓库下载到 PC 本地。这样的优势只要其中一个人从中央库下来了，以后大家都是从 Nexus 私服上进行下来，私服一般部署在内网，这样大大节约的宽带。创建 Proxy 的具体步骤1 点击 “Create Repositories” 按钮 2 选择要创建的类型 3 填写详细信息Name：就是为代理起个名字Remote Storage: 代理的地址，Maven 的地址为: https://repo1.maven.org/maven2/Blob Store: 选择代理下载包的存放路径 2）HostedHosted 是宿主机的意思，就是怎么把第三方的 Jar 放到私服上。Hosted 有三种方式，Releases、SNAPSHOT、MixedReleases: 一般是已经发布的 Jar 包Snapshot: 未发布的版本Mixed：混合的Hosted 的创建和 Proxy 是一致的，具体步骤和上面基本一致。如下： 注意事项：Deployment Pollcy: 需要把策略改成 “Allow redeploy”。 3）Group能把两个仓库合成一个仓库来使用，目前没使用过，所以没做详细的研究。 2.3 Security这里主要是用户、角色、权限的配置（上面已经提到了在这里添加用户和角色等） 2.4 Support包含日志及数据分析。 2.5 System主要是邮件服务器，调度的设置地方这部分主要讲怎么和 Maven 做集成, 集成的方式主要分以下种情况：代理中央仓库、Snapshot 包的管理、Release 包的管理、第三方 Jar 上传到 Nexus 上。 代理中央仓库只要在 PMO 文件中配置私服的地址（比如 http://192.168.1.14:8081）即可，配置如下： 12345678910111213&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;maven-central&lt;/id&gt; &lt;name&gt;maven-central&lt;/name&gt; &lt;url&gt;http://192.168.1.14:8081/repository/maven-central/&lt;/url&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/snapshots&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;/repository&gt;&lt;/repositories&gt; Snapshot 包的管理1）修改 Maven 的 settings.xml 文件，加入认证机制 12345&lt;servers&gt; &lt;server&gt;&lt;id&gt;nexus&lt;/id&gt; &lt;username&gt;admin&lt;/username&gt; &lt;password&gt;admin123&lt;/password&gt;&lt;/server&gt; 2）修改工程的 Pom 文件 123456789101112&lt;distributionManagement&gt; &lt;snapshotRepository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;Nexus Snapshot&lt;/name&gt; &lt;url&gt;http://192.168.1.14:8081/repository/maven-snapshots/&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;site&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;Nexus Sites&lt;/name&gt; &lt;url&gt;dav:http://192.168.1.14:8081/repository/maven-snapshots/&lt;/url&gt; &lt;/site&gt;&lt;/distributionManagement&gt; 注意事项: 上面修改的 Pom 文件如截图中的名字要跟 / usr/local/maven/conf/settings.xml 文件中的名字一定要对应上。 3）上传到 Nexus 上 1– 项目编译成的 jar 是 Snapshot(POM 文件的头部) 1234&lt;groupId&gt;com.zhisheng&lt;/groupId&gt;&lt;artifactId&gt;test-nexus&lt;/artifactId&gt;&lt;version&gt;1.0.0-SHAPSHOT&lt;/version&gt;&lt;packaging&gt;jar&lt;/packaging&gt; 2– 使用 mvn deploy 命令运行即可（运行结果在此略过） 3– 因为 Snapshot 是快照版本，默认他每次会把 Jar 加一个时间戳，做为历史备份版本。 Releases 包的管理1）与 Snapshot 大同小异，只是上传到私服上的 Jar 包不会自动带时间戳2）与 Snapshot 配置不同的地方，就是工程的 PMO 文件，加入 repository 配置 1234567&lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;name&gt;Nexus Snapshot&lt;/name&gt; &lt;url&gt;http://192.168.1.14:8081/repository/maven-releases/&lt;/url&gt; &lt;/repository&gt;&lt;/distributionManagement&gt; 3）打包的时候需要把 Snapshot 去掉 1234&lt;groupId&gt;com.zhisheng&lt;/groupId&gt;&lt;artifactId&gt;test-nexus&lt;/artifactId&gt;&lt;version&gt;1.0.0&lt;/version&gt;&lt;packaging&gt;jar&lt;/packaging&gt; | 第三方 Jar 上传到 Nexus[root@master-node src]# mvn deploy:deploy-file -DgroupId=org.jasig.cas.client -DartifactId=cas-client-core -Dversion=3.1.3 -Dpackag注意事项：-DrepositoryId=nexus 对应的就是 Maven 中 settings.xml 的认证配的名字。 最后搭建的时候是参考网上博客，写篇完整的博客再回馈给网上。转载请注明地址：http://www.54tianzhisheng.cn/2017/10/14/Nexus3-Maven/","tags":[{"name":"Maven","slug":"Maven","permalink":"http://www.54tianzhisheng.cn/tags/Maven/"}]},{"title":"Google Guava 缓存实现接口的限流","date":"2017-09-22T16:00:00.000Z","path":"2017/09/23/Guava-limit/","text":"项目背景最近项目中需要进行接口保护，防止高并发的情况把系统搞崩，因此需要对一个查询接口进行限流，主要的目的就是限制单位时间内请求此查询的次数，例如 1000 次，来保护接口。参考了 开涛的博客聊聊高并发系统限流特技 ，学习了其中利用 Google Guava 缓存实现限流的技巧，在网上也查到了很多关于 Google Guava 缓存的博客，学到了好多，推荐一个博客文章：http://ifeve.com/google-guava-cachesexplained/, 关于 Google Guava 缓存的更多细节或者技术，这篇文章讲的很详细；这里我们并不是用缓存来优化查询，而是利用缓存，存储一个计数器，然后用这个计数器来实现限流。 效果实验1234567static LoadingCache&lt;Long, AtomicLong&gt; count = CacheBuilder.newBuilder().expireAfterWrite(1, TimeUnit.SECONDS).build(new CacheLoader&lt;Long, AtomicLong&gt;() &#123; @Override public AtomicLong load(Long o) throws Exception &#123; //System.out.println(\"Load call!\"); return new AtomicLong(0L); &#125; &#125;); 上面，我们通过 CacheBuilder 来新建一个 LoadingCache 缓存对象 count，然后设置其有效时间为 1 秒，即每 1 秒钟刷新一次；缓存中，key 为一个 long 型的时间戳类型，value 是一个计数器，使用原子性的 AtomicLong 保证自增和自减操作的原子性， 每次查询缓存时如果不能命中，即查询的时间戳不在缓存中，则重新加载缓存，执行 load 将当前的时间戳的计数值初始化为 0。这样对于每一秒的时间戳，能计算这一秒内执行的次数，从而达到限流的目的；这是要执行的一个 getCounter 方法： 123456public class Counter &#123; static int counter = 0; public static int getCounter() throws Exception&#123; return counter++; &#125;&#125; 现在我们创建多个线程来执行这个方法： 12345678910111213141516171819202122public class Test &#123; public static void main(String args[]) throws Exception &#123; for(int i = 0;i&lt;100;i++) &#123; new Thread()&#123; @Override public void run() &#123; try &#123; System.out.println(Counter.getCounter()); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;.start(); &#125; &#125;&#125; 这样执行的话，执行结果很简单，就是很快地执行这个 for 循环，迅速打印 0 到 99 折 100 个数，不再贴出。这里的 for 循环执行 100 个进程时间是很快的，那么现在我们要限制每秒只能有 10 个线程来执行 getCounter() 方法，该怎么办呢，上面讲的限流方法就派上用场了： 12345678910111213141516171819202122public class Counter &#123; static LoadingCache&lt;Long, AtomicLong&gt; count = CacheBuilder.newBuilder().expireAfterWrite(1, TimeUnit.SECONDS).build(new CacheLoader&lt;Long, AtomicLong&gt;() &#123; @Override public AtomicLong load(Long o) throws Exception &#123; System.out.println(\"Load call!\"); return new AtomicLong(0L); &#125; &#125;); static long limits = 10; static int counter = 0; public static synchronized int getCounter() throws Exception&#123; while (true) &#123; //获取当前的时间戳作为key Long currentSeconds = System.currentTimeMillis() / 1000; if (count.get(currentSeconds).getAndIncrement() &gt; limits) &#123; continue; &#125; return counter++; &#125; &#125;&#125; 这样一来，就可以限制每秒的执行数了。对于每个线程，获取当前时间戳，如果当前时间 (当前这 1 秒) 内有超过 10 个线程正在执行，那么这个进程一直在这里循环，直到下一秒，或者更靠后的时间，重新加载，执行 load，将新的时间戳的计数值重新为 0。执行结果：每秒执行 11 个（因为从 0 开始），每一秒之后，load 方法会执行一次； 12345678910111213141516171819202122232425为了更加直观，我们可以让每个for循环sleep一段时间：public class Test &#123; public static void main(String args[]) throws Exception &#123; for(int i = 0;i&lt;100;i++) &#123; new Thread()&#123; @Override public void run() &#123; try &#123; System.out.println(Counter.getCounter()); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;.start(); Thread.sleep(100); &#125; &#125;&#125; 在上述这样的情况下，一个线程如果遇到当前时间正在执行的线程超过 limit 值就会一直在 while 循环，这样会浪费大量的资源，我们在做限流的时候，如果出现这种情况，可以不进行 while 循环，而是直接抛出异常或者返回，来拒绝这次执行（查询），这样便可以节省资源。 最后本篇文章地址： http://www.54tianzhisheng.cn/2017/09/23/Guava-limit/","tags":[{"name":"Guava","slug":"Guava","permalink":"http://www.54tianzhisheng.cn/tags/Guava/"}]},{"title":"面试过阿里等互联网大公司，我知道了这些套路","date":"2017-09-16T16:00:00.000Z","path":"2017/09/17/Interview-summary/","text":"前面感谢一波因为看到掘金在做秋招求职征文大赛，赞助商也有牛客网，自己前段时间也稍微写了篇博客总结我的大学生活，那些年我看过的书 —— 致敬我的大学生活 —— Say Good Bye ！ 博客中稍微简单的介绍了下自己的求职，重点是推荐了下我自己看过的那些书籍，对我帮助真的很大。 如今借这么个机会，回馈掘金和牛客网，想想自己这一年在掘金也写过不少文章，从 0 个粉丝到如今被 11047 人（截止写此篇文章时）关注，有点小激动，竟然这么多粉，也不知道真正活跃的用户有多少。不管怎样，这一年在掘金还是收获很多的，不仅可以阅读到很多大神的文章，学习新的知识，而且还遇到了好几个不错的哥们，如今平常也有和他们交流，比如 ：芋道源码 老哥人就很不错，在上海还和老哥见过面，吃过饭，平常对我帮助也很大，会推荐一些很有用的书籍给我看。欢迎大家关注他的博客：芋道源码的博客 ，里面有好几系列的源码分析博客文章呢。至于牛客网，我就更是老用户了，印象中好像是大一的时候注册的，那时有空的话就会去上面刷几道基础题，写写题解，坚持了好久了，如今早已是红名了。（其实是水出来的，哈哈）在牛客网遇到的大神也是超多，好多朋友几乎都是通过牛客网认识的，那时早的时候一起在一群讨论问题，别提那场面了，震惊，我等弱渣瑟瑟发抖。感谢叶神，左神，牛妹！ 说着说着，好像偏题了。 正式进入话题吧！ 正文开始本篇秋招求职征文主要分享如下几方面：招聘职位需求套路 、招聘面试的套路、简历撰写套路、简历投递套路 、找工作经历 、自己面试面经 、实习感悟、书籍推荐 、优秀网站推荐 、优秀博客推荐 、求职资料放送。 招聘职位需求套路摘举下几个公司的招聘需求：（from lagou） 1、Java开发校招生( 有赞 ) 职位诱惑：福利好待遇佳，技术氛围浓，有大牛带成长快职位描述： 有赞2018校招官方网申地址（请在官网投递，勿直接在Lagou上投递）：https://job.youzan.com/campus岗位职责 我们拥有世界级的 SaaS 电商解决方案，每天处理几百万订单、几亿条消息，并且量级不断攀升； 我们开放了有赞云，连接了数十万开发者，大大提升了 SaaS 对商家产生的价值； 我们正在新零售的潮流中激流勇进、开疆拓土，用产品技术撬动巨大的市场； 而你的工作，就是参与这些大流量系统的研发，哪怕提升1%的性能和稳定性都将是激动人心的时刻。 岗位要求 2018届本科及以上学历应届毕业生，计算机或者软件工程相关专业； 具备扎实的计算机基础知识，至少熟练使用一门主流开发语言； 积极参与开发实践，如果拥有引以为豪的项目经历则加分； 热衷数据结构与算法，如果一不小心在 ACM 赛场摘过金，夺过银则加分； 能在 Linux 上写任何脚本，比王者荣耀上手还快则加分； 快速学习新鲜事物，自我驱动追求卓越，积极应对问题和变化。 2、京东居家生活事业部-汽车用品招聘实习生（2018届） 职位诱惑：京东商城 职位描述：京东商城-汽车用品部门招聘实习生 我们需要这样的你： 2018届毕业生（本科或硕士均可） 学习能力强 担当、抗压、接受变化 能长期实习（优秀者有转正机会） 需要一个大的平台来展示和发挥自己的能力 你将收获： 重新认识快速成长的自己 一份世界500强的实习经历 一群优秀的伙伴 3、爱奇艺 Java 实习生 - 游戏事业部 要求：至少 6 个月以上每周三天以上实习。 本科以上学历，计算机、软件工程相关专业； 基础扎实，熟悉 Java 编程，熟悉 Spring、MyBatis 等框架优先； 熟悉 SQL 语句，熟练使用 MySQL 数据库； 良好的沟通、表达、协调能力，富有激情，学习能力强； 有 GitHub 账号或者技术博客优先； 热爱游戏行业优先。 这里随便找了三个，从招聘需求里看，好多公司目前招聘的话在招聘需求中并不怎么会写的很清楚，有的也不会说明要求的技术栈，这其实有时会对我们这种新人来说，有点不好的，这样的话我们就没有明确的目标去复习，还有就是一些加分项，其实也是有点帮助的。就比如有些招聘上面的说有优秀博客和 GitHub 者优先，这两点的话我们其实可以在大学慢慢积累出来的，对面试确实有帮助，我好些面试机会都是靠这两个的。还有套路就是，别光信他这招聘需求，进去面试可能就不问你这些方面的问题了，那些公司几乎都是这么个套路：面试造火箭，入职拧螺丝 ！ 进去公司之前可能需要你懂很多东西，但是进去的话还只是专门做一方面的东西。不管怎样，如果你有机会进去大公司的话（而且适合去），还是去大公司吧，出来大厂光环不少。 认真耐心地拧螺丝钉，说不定有机会去造大火箭——正规大公司的节奏。 短时间把螺丝拧出花，说不定有机会造小火箭——上升中创业公司的节奏。 招聘面试的套路参考：https://mp.weixin.qq.com/s/qRwDowetBkJqpeMeAZsIpA 一个在掘金上认识的老哥，在京东工作，写的不错，干脆分享下。大家可以去看他的博客，http://mindwind.me/ 当时我求职的时候通过作者博客也学到不少东西。 一次集中的扩招需求，有点像每年一度的晋升评审，都需要对大量的候选人进行定级评审，因为每一个新招聘的人员都会对其有一个定级的过程。 维度： 通用能力：考察其沟通表达、学习成长等 专业知识：考察其知识的掌握、深度、广度等 专业能力：考察其技能应用的能力和结果 工作业绩：考察其工作成果、产出、创新点等 价值观：考察其认知、理解、行为等 整个面试过程会包括下面几个部分： 自我介绍一开始的简短自我介绍，考察点在于对自我的总结、归纳和认知能力。观察其表达的逻辑性和清晰性，有个整体印象。 项目经历一般我不会专门问一些比较死的专业技术点之类的知识，都是套在候选人的项目经历和过往经验中穿插。通过其描述，来判断其掌握知识点的范围和深度，以及在实际的案例中如何运用这些知识与技能解决真正的问题的。 所以，不会有所谓的题库。每一个我决定面试的候选人，都是提前细读其简历，提炼场景和发掘需要问的问题，相当于面试前有个二三十分钟的备课过程，组织好面试时的交互过程与场景，以顺利达到我想要了解的点。 团队合作通常还会问候选人其所在团队中的角色，他们的工作模式、协作方式，并给出一些真实的场景化案例观察其应对的反应。评价一下关于他周围的同事、下属或领导，了解他在团队中的自我定位。这里的考察点是沟通协作方面的通用能力。 学习成长这个维度考察的关键点包括：成长潜力、职业生涯规划的清晰度。人与人之间成长速度的关键差距，我自己观察得出的结论在于：自驱力。而路径的清晰性，也是产生自驱的一个源动力，否则可能会感觉迷茫，而陷于困顿。 文化匹配这算是价值观的一部分吧。其实，这是最难考核的，我没有什么好方法，基本靠感觉。曾经有过好几次碰到经历和技能都不错的人，但总是感觉哪里不对，但又着急要人，就放进来了。但最终感觉是对的，合作很快就结束了，人也走了。 综合评价总结点评候选人的优势、劣势并进行技术定级，定级也没有绝对标准，而是相对的。我一般就是和周围觉得差不多级别的人的平均水准比较下，大概就会有一个技术级别的判断。 套路 招聘面试，其实是一个对人的筛选，而筛选的本质是匹配 —— 匹配人与职位。第一，你得非常清楚地理解，这个职位需要什么样属性的人。第二，确定你的候选人是否拥有这个职位要求的必须属性。那么，首先回答第一个问题，一般的职位需要什么样的属性？ 属性，又可以进一步拆解为三个层次。第一层次是「技能（Skills）」，技能是你习得的一种工具，就像程序员会用某种语言和框架来编写某类应用程序。第二层次是「能力（Abilities）」，能力是你运用工具的思考和行为方式，用同样的语言和框架编写同样程序的程序员能力可以差别很大。而第三层次是「价值观（Values）」，价值观是一个人根深蒂固的信念以及驱动行为的原因与动力所在。 简历撰写套路参考：https://mp.weixin.qq.com/s/3f8hGAQ-auLdkxkQ8XG3CQ 简历，是如此重要，它是获得一份满意工作的敲门砖，但不同的简历敲门的声响可不同。 但很多时候简历给人的感觉也似乎微不足道，因为没有人会真正细致的去读一份简历。而仅仅是快速的浏览一遍，就几乎同时对一个候选人形成了一种要么强烈，要么无感的印象。现实中的真实情况是，你的简历只有十几二十秒的时间窗口机会会被浏览到，然后就决定了能否进入下一步。 要让面试官看了你的简历后：知道你做过什么？看看技能、经历与岗位需求的匹配度，然后再问问你是谁？你通过简历散发出来的味道是什么感觉，我愿意和这样的人一起共事么？ 一份简历的最少必要内容包括： 个人信息 姓名 年龄 手机 邮箱 教育经历 博士（硕士、本科） 有多个全部写出来，最高学历写在上面 工作经历（最匹配职位需求的，挑选出来的 TOP3 的项目） 项目1 项目背景上下文（场景、问题） 你在其中的角色（职责、发挥的作用、结果度量） 与此项经历有关的知识与技能（技术栈） 项目2 项目3 附加信息 博客：持续有内容，不碎碎念 开源：GitHub 持续 commit 社区：有一定专业影响力的 书籍：用心写的 演讲：行业大会级别的 专利：凑数的就算了 论文：学术界比较有影响力的 爱好：真正的兴趣点 对于我们学生，缺乏工作经历，那就写写独特的学习或实习经历。同学们大家都共有的经历就不要随便写上去凑数了。对于学生，看重的是通用能力，学习能力，适应能力以及对工作的态度和热情。如果没有区分度高的经历，那么有作品也是很好的。比如将你的做的网站部署出来，把地址写在简历上。 关于技术栈部分的技术术语，很多程序员不太注意。比如，把 Java 写成 java 或 JAVA，Java 已是一个专有品牌名词，大小写要完全符合，这一点和 iOS 类似（i 小写，OS 大写）。另外，像 HTML，CSS 则全部大写，因为这是多个单词的缩写。一些小小的细节就能读出你的专业性和散发出来的味道。最后，技术术语不是罗列得多就好，不是真正熟练的技能，不要轻易写进简历。因为这将给你自己挖坑。你可以将你自己擅长的或者很熟的知识点写进去，有时想着重就加粗或者打个括号，这样可以挖坑给面试官，让他去问你熟悉的（前提要确保你真的能讲清楚，我试过这个方法很有效的）。 然后就是简历格式了，最好是 PDF 了，Word 在不同的电脑上的打开效果可能不一样，格式可能会变，况且有些人的电脑不一定装了 Word，不过我喜欢用 Markdown 写简历，简洁，适合程序员，然后把 Markdown 转换成 PDF 出来。 简历投递套路内推 有内推通道尽量走内推通道，不知道方便多少，而且成功几率也很大！找熟人，找学长学姐吧！牛客网讨论区很多内推帖子，可以去找找。不过今年的好多公司的内推通道都不咋管用了，套路越来越多了。记得去年好多公司内推都是免笔试，直接进入面试阶段，今年直接变成内推免简历筛选，进入笔试。因为现在的内推越来越不靠谱，直接面试的话，会增加公司的面试成本，干脆笔试再筛选一部分人。 拉勾网 拉勾上还是算不错的。 Boss 直聘 虽说前段时间出现了程序员找工作进入传销最后导致死亡的惨事发生，但是里面总比智联招聘和前程无忧靠谱点。因为智联招聘和前程无忧几乎被广告党和培训机构给占领了。 脉脉 里面招应届生和实习生比较少，但是也有，可以试试。 总之，简历投递给公司之前，请确认下这家公司到底咋样，先去百度了解下，别被坑了，每个平台都有一些居心不良的广告党等着你上钩，千万别上当！！！ 找工作经历这段经历，算是自己很难忘记的经历吧。既辛酸既充实的日子！也很感谢自己在这段时间的系统复习，感觉把自己的基础知识再次聚集在一起了，自己的能力在这一段时间提升的也很快。后面有机会的话我也想写一系列的相关文章，为后来准备工作（面试）的同学提供一些自己的帮助。自己在找工作的这段时间面过的公司也有几家大厂，但是结果都不是很好，对我自己有很大的压力，当时心里真的感觉 ：“自己真的有这么差”，为什么一直被拒，当时很怀疑自己的能力，自己也有总结原因。一是面试的时候自己准备的还不够充分，虽说自己脑子里对这些基础有点印象，但是面试的时候自己稍紧张下就描述不怎么清楚了，导致面试官觉得你可能广度够了，深度还不够（这是阿里面试官电话面试说的）；二是自己的表达能力还是有所欠缺，不能够将自己所要表达的东西说出来，这可能我要在后面加强的地方；三是我的学校问题。在面了几家公司失败后，终于面了家公司要我了，我也确定在这家公司了。很幸运，刚出来，就有一个很好（很负责）的架构师带我，这周就给了我一个很牛逼的项目给我看，里面新东西很多，说吃透了这个项目，以后绝对可以拿出去吹逼（一脸正经.jpg）。找工作期间，自己也经常去收集一些博客，并把它保存下来，这样能够让自己下次更好的系统复习，还在牛客网整理了很多面经，每天看几篇面经，知道面试一般问什么问题，都有啥套路，其实你看多了面经就会发现，面试考的题目几乎都差不多，区别不是很大。目前我的找工作经历就简短的介绍到这里了，如果感兴趣的话，可以加群：528776268 期待志同道合的你。 自己面试面经亚信地址：http://www.54tianzhisheng.cn/2017/08/04/yaxin/ 1）自我介绍（说到一个亮点：长期坚持写博客，面试官觉得这个习惯很好，算加分项吧） 2）看到简历项目中用到 Solr，详细的问了下 Solr（自己介绍了下 Solr 的使用场景和建立索引等东西） 3）项目里面写了一个 “ 敏感词和 JS 标签过滤防 XSS 攻击”，面试官让我讲了下这个 XSS 攻击，并且是怎样实现的 4）项目里写了支持 Markdown，问是不是自己写的解析代码，（回答不是，自己引用的是 GitHub上的一个开源项目解析的） 5）想问我前端的知识，我回复到：自己偏后端开发，前端只是了解，然后面试官就不问了 6）问我考不考研？ 7）觉得杭州怎么样？是打算就呆在杭州还是把杭州作为一个跳板？ 8）有啥小目标？以后是打算继续技术方向，还是先技术后管理（还开玩笑的说：是不是赚他几个亿，当时我笑了笑） 9）有啥兴趣爱好？ 总结：面试问的问题不算多，主要是通过简历上项目所涉及的东西提问的，如果自己不太会的切记不要写上去。面试主要考察你回答问题来判断你的逻辑是否很清楚。 爱奇艺地址：http://www.54tianzhisheng.cn/2017/08/04/iqiyi/ 笔试（半个小时）题目：（记得一些） 1、重载重写的区别？ 2、转发和重定向的区别？ 3、画下 HashMap 的结构图？HashMap 、 HashTable 和 ConcurrentHashMap 的区别？ 4、statement 和 preparedstatement 区别？ 5、JSP 中一个 中取值与直接取值的区别？会有什么安全问题？ 6、实现一个线程安全的单例模式 7、一个写 sql 语句的题目 8、自己实现一个 List，（主要实现 add等常用方法） 9、Spring 中 IOC 和 AOP 的理解？ 10、两个对象的 hashcode 相同，是否对象相同？equal() 相同呢？ 11、@RequestBody 和 @ResponseBody 区别？ 12、JVM 一个错误，什么情况下会发生？ 13、常用的 Linux 命令？ 第一轮面试（80 分钟）1、自我介绍 2、介绍你最熟悉的一个项目 3、讲下这个 XSS 攻击 4、HashMap 的结构？HashMap 、 HashTable 和 ConcurrentHashMap 的区别？ 5、HashMap 中怎么解决冲突的？（要我详细讲下） 6、ConcurrentHashMap 和 HashTable 中线程安全的区别？为啥建议用 ConcurrentHashMap ？能把 ConcurrentHashMap 里面的实现详细的讲下吗？ 7、Session 和 Cookie 的区别？ 8、你项目中登录是怎样做的，用的 Cookie 和 Session？ 9、讲讲你对 Spring 中的 IOC 和 AOP 的理解？ 10、问了好几个注解的作用？ 11、statement 和 preparedstatement 区别？ 12、$ 和 # 的区别？以及这两个在哪些地方用？ 13、前面项目介绍了数据是爬虫爬取过来的，那你讲讲你的爬虫是多线程的吧？ 14、讲讲 Python 中的多线程和 Java 中的多线程区别？ 15、自己刚好前几天在看线程池，立马就把面试官带到我熟悉的线程池，和面试官讲了下 JDK 自带的四种线程池、ThreadPoolExecutor 类中的最重要的构造器里面的七个参数，然后再讲了下线程任务进入线程池和核心线程数、缓冲队列、最大线程数量比较。 16、线程同步，你了解哪几种方式？ 17、讲下 Synchronized？ 18、讲下 RecentLock 可重入锁？ 什么是可重入锁？为什么要设计可重入锁？ 19、讲下 Volatile 吧？他是怎样做到同步的？ 20、Volatile 为什么不支持原子性？举个例子 21、Atomic 怎么设计的？（没看过源码，当时回答错了，后来才发现里面全部用 final 修饰的属性和方法） 22、问几个前端的标签吧？（问了一个不会，直接说明我偏后端，前端只是了解，后面就不问了） 23、SpringBoot 的了解？ 24、Linux 常用命令？ 25、JVM 里的几个问题？ 26、事务的特性？ 27、隔离级别？ 28、网络状态码？以 2、3、4、5 开头的代表什么意思。 29、并发和并行的区别？ 30、你有什么问题想问我的？ 一面面完后面试官和说这份试卷是用来考 1~3 年开发工作经验的，让我准备一下，接下来的二面。 第二轮面试（半个小时）1、一上来就问怎么简历名字都没有，我指了简历第一行的我的名字，还特意大写了，然后就问学校是不是在上海，我回答在南昌（感觉被鄙视了一波，后面我在回答问题的时候面试官就一直在玩手机，估计后面对我的印象就不是很好了） 2、自我介绍 3、说一说数据库建表吧（从范式讲） 4、讲讲多态？（这个我答出来了，可是面试官竟然说不是这样吧，可能面试官没听请，后面还说我是不是平时写多态比较少，感觉这个也让面试官对我印象减分） 5、将两个数转换（不借助第三个参数） 6、手写个插入排序吧（写完了和面试官讲了下执行流程） 7、讲讲你对 Spring 中的 IOC 和 AOP 的理解？ 8、问了几个常用的 Linux 命令？ 9、也问到多线程？和一面一样把自己最近看的线程池也讲了一遍 10、学 Java 多久了？ 11、你有什么想问的？ 总结：面试题目大概就是这么多了，有些问题自己也忘记了，面试题目顺序不一定是按照上面所写的。再次感谢爱奇艺的第一面面试官了，要不是他帮忙内推的，我可能还没有机会收到面试机会。自己接到爱奇艺面试邀请电话是星期一晚上快7点中的，之后加了面试官微信约好了星期四面试的（时间准备较短，之前没系统的复习过）。星期四一大早（5点就起床了），然后就收拾了下，去等公交车，转了两次车，然后再做地铁去爱奇艺公司的，总共路上花费时间四个多小时。总的来说，这次面试准备的时间不是很充裕，所以准备的个人觉得不是很好，通过这次的面试，发现面试还是比较注重基础和深度的，我也知道了自己的一些弱处，还需要在哪里加强，面试技巧上也要掌握些。为后面的其他公司继续做好充足的准备。加油！！！ 阿里地址：http://www.54tianzhisheng.cn/2017/08/04/alibaba/ （菜鸟网络部门）（49 分钟） 2017.08.02 晚上9点21打电话过来，预约明天什么时候有空面试，约好第二天下午两点。 2017.08.03 下午两点10分打过来了。 说看了我的博客和 GitHub，觉得我学的还行，知识广度都还不错，但是还是要问问具体情况，为什么没看到你春招的记录，什么原因没投阿里？非得说一个原因，那就是：我自己太菜了，不敢投。 1、先自我介绍 2、什么是多态？哪里体现了多态的概念？ 3、HashMap 源码分析，把里面的东西问了个遍？最后问是不是线程安全？引出 ConcurrentHashMap 4、ConcurrentHashMap 源码分析 5、类加载，双亲委托机制 6、Java内存模型（一开始说的不是他想要的，主要想问我堆和栈的细节） 7、垃圾回收算法 8、线程池，自己之前看过，所以说的比较多，最后面试官说了句：看你对线程池了解还是很深了 9、事务的四种特性 10、什么是死锁？ 11、乐观锁和悲观锁的策略 12、高可用网站的设计（有什么技术实现） 13、低耦合高内聚 14、设计模式了解不？你用过哪几种，为什么用，单例模式帮我们做什么东西？有什么好处？ 15、你参与什么项目中成长比较快？学到了什么东西，以前是没有学过的？ 16、项目中遇到的最大困难是怎样的？是怎么解决的？ 17、智力题（两根不均匀的香，点一头烧完要一个小时，怎么确定15分钟） 18、你有什么问题想要问我的？ 19、问了菜鸟网络他们部门主要做什么？ 20、对我这次面试做个评价：看了你博客和 GitHub，知道你对学习的热情还是很高的，花了不少功夫，后面有通知！ 总结：面试总的来说，第一次电话面试，感觉好紧张，好多问题自己会点，但是其中的细节没弄清楚，自己准备的也不够充分。面试官很友好，看到我紧张，也安慰我说不要紧，不管以后出去面试啥的，不需要紧张，公司问的问题可能很广，你只需要把你知道的说出来就行，不会的直接说不会就行。之前一直不敢投阿里，因为自己准备的完全不够充分，但是在朋友磊哥的帮助下，还是试了下，不管结果怎么样，经历过总比没有的好。 后面说有通知，结果并没有，只看到官网的投递按钮变灰了。在掘金上一个朋友（我隔壁学校的），当时看我挂了说要不要让他租一起的隔壁邻居再内推下淘宝，我想想还是算了，自己目前能力真的是有限，达不到进阿里的要求！不过还是要感谢那个哥们，人真的超级好，虽然我们未曾谋面，但是有机会的话，我一定会请你吃饭的。 哔哩哔哩首先直接根据简历项目开问，自我介绍都没有。 1、登录从前端到后端整个过程描述一遍？越详细越好，说到密码加密，网络传输，后台验证用户名和密码，Cookie 设置等。具体问我密码加密是前台还是后台加密，说了在后台加密？面试官说，那你做这个项目有什么意思？密码传输都是明文的，默认 HTTP 传递是明文传输，当时被面试官带进前台加密还是后台加密的沟里去了，没想到用 HTTPS ，后来后来的路上查了些资料才知道的，面试过程中他很想我说前台加密，但是前台加密算法那代码就摆在那里，很容易就给破解了吧，也没给点提示说 HTTPS，我只好投降 2、写一个查询的 sql 语句 3、线程同步的方法？Synchronized、Volatile、（面试官好像觉得 Volatile 不可以做到同步，我和他说了半天的 Volatile 原理 ，他竟然不认同，我开始怀疑他的实力了）、ThreadLocal、Atomic。 说到这些了，我当时竟然没把他带进我我给他挖的坑里去（线程池，之前好好研究过呢，可惜了） 4、Spring IOC 和 AOP 的理解？叫我写 AOP 的代码，我没写 5、JDK 动态代理和 Cglib 代理区别？ 5、你觉得项目里面你觉得哪些技术比较好？我指了两个，然后他也没有问下去。 6、解释下 XSS 攻击 7、Spring 和 SpringBoot 的区别？ 8、JVM 垃圾回收算法？分代中为什么要分三层？ 9、OOM 是什么？什么情况会发生？ 10、你觉得你有啥优点？ 然后就叫我等一会，一会有人事来通知我，结果过了一会人事叫我可以回去等通知了。 总结：到公司的时候已经一点多钟了，面试直接在一个很多人的地方（吃饭的地方）直接面的，周围还有人再吃饭，场景有点尴尬，面试过程感觉很随意，想到什么问题就问什么，完全没有衔接，问到的有些地方感觉面试官自己都不清楚，还怀疑我所说的，另外就是问题比较刁钻，总体技术也就那样吧！ 目前所在公司当时是我现在的老大（架构师）面的，先是电话面试过一次，问的问题也比较难，不过最后还是觉得我基础还是不错的。最后叫我去公司面试下，来到公司面试问的问题那就更难了，几乎好多都回答不出来，但是简单的说了下思路，最后再叫主任面试了下，问的问题就很简单了，最后就是 HR 面了，主要说了下工资问题和什么时候能报道！这几次面试的问题当时由于时间比较紧，也没去整理，现在也记不清楚了！目前自己已经工作了快一个月了，给的项目也完全是新东西，对我的挑战也很大，有时自己也确实不怎么知道，不过我老大很耐心的教我，对我也很不错，这也是我打算留在这里的原因，碰到个好老大不易！必须好好珍惜！ 实习感悟进公司是架构运维组中的 Java 实习开发，目前实习已经快一个月了，说实话，实习后才发现一天真的很忙，写下这篇征文也是在周末整理大晚上写的。刚进公司就给了一个 Consul 的服务发现与注册和健康检查的项目，里面涉及的东西有 Consul、Docker、Nginx、Lua、ElasticSearch 还有几个很轻量级的框架，对我来说几乎都是新东西，确实需要时间去了解，再优化和改里面的 bug 的过程中，幸好我老大和我理了几次思路，才让我对整个项目有所进展，后续继续是在优化这项目（可能以后这个项目的所有东西都是我来做）。在上海，住的地方离公司有一定的距离，上班几乎要一个小时，每天花在上班路上的时间很多，这也导致我每天感觉很忙。公司上班时间比较弹性，无打卡，虽说公司不加班，但是每天自己都不怎么会按点下班，自己也想在实习阶段多学点东西！这段时间也是最关键的时间，碰到个问题，要花好久时间才能解决，也有可能未必解决得了，有时觉得自己啥都不会，这么点东西都做不好，有点否定自己。这也确实是自己的技术知识栈缺乏，和自己学的 SSM、Spring Boot 这些都不相关，也不怎么写业务逻辑代码。所以感觉很痛苦，不像自己以前写的代码那样顺畅，当然可能是自己以前自己写的项目太 low 了。 看到掘金-凯伦征文中写到： 公司其实并不期望刚刚进来的你，能够创造多少价值。新人是要成长的，在成长期难免会遇到各种各样的小问题，这可能是大多数人的必经之路，因为你所看到的同事，他们都比你在工作领域待的时间更久，有更多的经验，可以把他们作为目标，但不要把他们作为现在自己的标准，那样会压力太大。 感觉这段话对我现在很受用！ 加油，好好挺过这个阶段，别轻易说放弃！ 书籍推荐大学，我不怎么喜欢玩游戏，自己也还算不怎么堕落吧，看了以下的一些书籍，算是对我后面写博客、找工作也有很大的帮助。如果你是大神，请忽略，如果你还是还在大学，和我一样不想把时间浪费在游戏上，可以看看我推荐的一些书籍，有想讨论的请在评论下留下你的评论或者加上面给的群号。 Java1、《Java 核心技术》卷一 、卷二 两本书，算是入门比较好的书籍了 2、《疯狂 Java 讲义》 很厚的一本书，里面的内容也是很注重基础了 3、《Java 并发编程的艺术》—— 方腾飞 、魏鹏、程晓明著 方腾飞 是并发编程网的创始人，里面的文章确实还不错，可以多看看里面的文章，收获绝对很大。 4、《 Java多线程编程核心技术》—— 高洪岩著 这本书也算是入门多线程编程的不错书籍，我之前还写了一篇读书笔记呢，《Java 多线程编程核心技术》学习笔记及总结 , 大家如果不想看书的可以去看我的笔记。 5、《Java 并发编程实战》 这本书讲的有点难懂啊，不过确实也是一本很好的书，以上三本书籍如果都弄懂了，我觉得你并发编程这块可能大概就 OK 了，然后再去看看线程池的源码，了解下线程池，我觉得那就更棒了。不想看的话，请看我的博客：Java 线程池艺术探索 我个人觉得还是写的很不错，那些大厂面试也几乎都会问线程池的东西，然后大概内容也就是我这博客写的 6、《Effective Java》中文版 第二版 算是 Java 的进阶书籍了，面试好多问题也是从这出来的 7、《深入理解 Java 虚拟机——JVM高级特性与最佳实践》第二版 这算是国内讲 JVM 最清楚的书了吧，目前还是只看了一遍，后面继续啃，大厂面试几乎也是都会考 JVM 的，阿里面 JVM 特别多，想进阿里的同学请一定要买这本书去看。 8、《深入分析Java Web技术内幕 修订版》许令波著 里面知识很广，每一章都是一个不同的知识，可见作者的优秀，不愧是阿里大神。 9、《大型网站系统与 Java 中间件实践》—— 曽宪杰 著 作者是前淘宝技术总监，见证了淘宝网的发展，里面的讲的内容也是很好，看完能让自己也站在高处去思考问题。 10、《大型网站技术架构 —— 核心原理与案例分析》 —— 李智慧 著 最好和上面那本书籍一起看，效果更好，两本看完了，提升思想的高度！ 11、《疯狂Java.突破程序员基本功的16课》 李刚 著 书中很注重 Java 的一些细节，讲的很深入，但是书中的错别字特多，可以看看我的读书笔记：《疯狂 Java 突破程序员基本功的 16 课》读书笔记 12、《Spring 实战》 Spring 入门书籍 13、《Spring 揭秘》—— 王福强 著 这本书别提多牛了，出版时期为 2009 年，豆瓣评分为 9.0 分，写的是真棒！把 Spring 的 IOC 和 AOP 特性写的很清楚，把 Spring 的来龙去脉讲的很全。墙裂推荐这本书籍，如果你想看 Spring，作者很牛，资深架构师，很有幸和作者有过一次交流，当时因为自己的一篇博客 Pyspider框架 —— Python爬虫实战之爬取 V2EX 网站帖子，竟然找到我想叫我去实习，可惜了，当时差点就跟着他混了。作者还有一本书 《Spring Boot 揭秘》。 14、《Spring 技术内幕》—— 深入解析 Spring 架构与设计原理 讲解 Spring 源码，深入了内部机制，个人觉得还是不错的。 15、Spring 官方的英文文档 这个别提了，很好，能看英文尽量看英文 16、《跟开涛学 Spring 3》 《跟开涛学 Spring MVC》 京东大神，膜 17、《看透springMvc源代码分析与实践》 算是把 Spring MVC 源码讲的很好的了 见我的笔记： 1、通过源码详解 Servlet 2 、看透 Spring MVC 源代码分析与实践 —— 网站基础知识 3 、看透 Spring MVC 源代码分析与实践 —— 俯视 Spring MVC 4 、看透 Spring MVC 源代码分析与实践 —— Spring MVC 组件分析 18、《Spring Boot 实战》 19、Spring Boot 官方 Reference Guide 网上好多写 SpringBoot 的博客，几乎和这个差不多。 20、《JavaEE开发的颠覆者: Spring Boot实战》 21、MyBatis 当然是官方的文档最好了，而且还是中文的。 自己也写过几篇文章，帮助过很多人入门，传送门： 1、通过项目逐步深入了解Mybatis（一）/) 2、通过项目逐步深入了解Mybatis（二）/) 3、通过项目逐步深入了解Mybatis（三）/) 4、通过项目逐步深入了解Mybatis（四）/) 22、《深入理解 Java 内存模型》—— 程晓明 著 我觉得每个 Java 程序员都应该了解下 Java 的内存模型，该书籍我看的是电子版的，不多，但是讲的却很清楚，把重排序、顺序一致性、Volatile、锁、final等写的很清楚。 Linux《鸟哥的Linux私房菜 基础学习篇(第三版) 》 鸟哥的Linux私房菜：服务器架设篇(第3版) 鸟哥的书 计算机网络《计算机网络第六版——谢希仁 编》 《计算机网络自顶向下方法》 计算机系统《代码揭秘：从C／C.的角度探秘计算机系统 —— 左飞》 《深入理解计算机系统》 《计算机科学导论_佛罗赞》 数据库《高性能MySQL》 《Mysql技术内幕InnoDB存储引擎》 Python这门语言语法很简单，上手快，不过我目前好久没用了，都忘得差不多了。当时是看的廖雪峰的 Python 博客 自己也用 Python 做爬虫写过几篇博客，不过有些是在前人的基础上写的。感谢那些栽树的人！ 工具Git ： 廖雪峰的 Git 教程 IDEA：IntelliJ IDEA 简体中文专题教程 Maven：《Maven实战》 其他《如何高效学习-斯科特杨》 教你怎样高效学习的 《软技能：代码之外的生存指南》 程序员除了写代码，还得懂点其他的软技能。 《提问的智慧“中文版”》 《How-To-Ask-Questions-The-Smart-Way》 作为程序员的你，一定要学会咋提问，不然别人都不想鸟你。 优秀网站推荐1、GitHub 别和我说不知道 2、InfoQ 文章很不错 3、CSDN 经常看博客专家的博客，里面大牛很多，传送门：zhisheng 4、知乎 多关注些大牛，看他们吹逼 5、掘金 自己也在上面写专栏，粉丝已经超过一万了，传送门 ：zhisheng 6、并发编程网 前面已经介绍 7、developerworks 上面的博客也很好 8、博客园 里面应该大牛也很多，不过自己没在上面写过博客 9、微信公众号 关注了很多人，有些人的文章确实很好，平时也经常看。 10、牛客网 刷笔试题不错的地方，里面大牛超多，怀念叶神和左神讲课的时候，还有很有爱的牛妹。 优秀博客推荐廖雪峰 Git 和 Python 入门文章就是从他博客看的 阮一峰的网络日志 酷壳-陈皓 RednaxelaFX R大，牛逼的不得了 江南白衣 老司机 stormzhang 人称帅逼张，微信公众号写的不错 你假笨 阿里搞 JVM 的，很厉害 占小狼 泥瓦匠BYSocket 崔庆才 写了好多 Python 爬虫相关的文章 纯洁的微笑 SpringBoot 系列不错，其他的文章自己看了感觉是自己喜欢的那种文笔 程序猿DD 周立 芋道源码的博客 好多系列的源码分析 zhisheng 这个是我不要脸，竟然把自己博客地址的写上去了 求职资料放送自己在准备找工作那段时间，系统的复习了下大学所学的知识，期间在网上参考了很多不错的博客，并收集下来了，个人觉得还是不错的，因为这是包含了自己的心血，所以一直没怎么送出来，只给过我的几个同学，还有就是一些学习视频和实战项目视频。借着这次征文的机会，我想送给那些有缘人，希望你或许是那种在求职道路上正在艰难走着的人；或许是大一大二的学弟学妹们却想好好学习，有个奋斗的目标，不堪在大学堕落的；或许是工作一两年后感觉基础还比较薄弱的。要资料的时候期望你能简单的介绍下自己，期望你！联系方式请看文章最下面。 最后送一句话，越努力，越幸运，祝早日成为大神！ 这些地方可以找到我： blog: http://www.54tianzhisheng.cn/ GitHub: https://github.com/zhisheng17 QQ 群：528776268","tags":[{"name":"面经","slug":"面经","permalink":"http://www.54tianzhisheng.cn/tags/面经/"}]},{"title":"Linux 下 lua 开发环境安装及安装 luafilesystem","date":"2017-09-14T16:00:00.000Z","path":"2017/09/15/linux-lua-lfs-install/","text":"火云邪神语录：天下武功，无坚不破，唯快不破！Nginx 的看家本领就是速度，Lua 的拿手好戏亦是速度，这两者的结合在速度上无疑有基因上的优势。 最近一直再折腾这个，干脆就稍微整理下。以防后面继续跳坑！ 安装： 1.先安装 lua 的相关依赖安装 C 开发环境由于 gcc 包需要依赖 binutils 和 cpp 包，另外 make 包也是在编译中常用的，所以一共需要 9 个包来完成安装，因此我们只需要执行 9 条指令即可： 12345678910gcc：命令未找到（解决方法）yum install cppyum install binutilsyum install glibcyum install glibc-kernheadersyum install glibc-commonyum install glibc-develyum install gccyum install makeyum install readline-devel 2.安装 lua5.1.5下载地址：http://www.lua.org/ftp/ 1234567891011121314151617181920tar -zxvf lua-5.1.5.tar.gzcd lua-5.1.5vi Makefile设置 INSTALL_TOP= /usr/local/luamake linuxmake testmake installrm -rf /usr/bin/lualn -s /usr/local/lua/bin/lua /usr/bin/lualn -s /usr/local/lua/share/lua /usr/share/lua设置环境变量：vim /etc/profile添加：export LUA_HOME=/usr/local/luaexport PATH=$PATH:$LUA_HOME/bin环境变量生效：source /etc/profile 3、安装 luarocks是一个 Lua 包管理器，基于 Lua 语言开发，提供一个命令行的方式来管理 Lua 包依赖、安装第三方 Lua 包等。 地址： https://github.com/luarocks/luarocks 12345678910111213141516使用 luarocks-2.2.1 版本在我机器上没有问题，但是使用 luarocks-2.4.2 出现问题wget http://luarocks.org/releases/luarocks-2.2.1.tar.gztar -zxvf luarocks-2.2.1.tar.gzcd luarocks-2.2.1./configure --with-lua=/usr/local --with-lua-include=/usr/local/lua/include设置环境变量：export LUA_LUAROCKS_PATH=/usr/local/luarocks-2.2.1export PATH=$PATH:$LUA_LUAROCKS_PATHmake &amp; make install 4、安装 luafilesystem是一个用于 lua 进行文件访问的库，可以支持 lua 5.1 和 lua5.2，且是跨平台的，在为 lua 安装 lfs 之前需要先安装luarocks。因为自己的需求刚好需要这模块。 地址：https://github.com/keplerproject/luafilesystem 文档： http://keplerproject.github.io/luafilesystem/index.html 1luarocks install luafilesystem 5、测试测试 lua 是否安装成功 lua -v 结果： 1Lua 5.1.5 Copyright (C) 1994-2012 Lua.org, PUC-Rio 测试 luafilesystem 是否安装成功 a.lua 123456789local lfs = require&quot;lfs&quot;function Rreturn(filePath) local time = os.date(&quot;%a, %d %b %Y %X GMT&quot;, lfs.attributes(filePath).modification) --打印文件的修改时间 print(time)endRreturn(&quot;/opt/lua/a.txt&quot;) a.txt 123abc 运行： 1lua a.lua 结果： 1Tue, 12 Sep 2017 18:43:13 GMT 出现打印出时间的结果就意味着已经安装好了。 当然以上这是在 Linux 安装的， Windows 上的其实比这还简单了，但是安装 luafilesystem 的话需要自己去下载个 lfs.dll ，然后把这个放到 lua 的安装路径去。很简单的，这里就不细说了。 出现过的错误：123456789101112[root@n1 lua-5.1.5]# make linux testcd src &amp;&amp; make linuxmake[1]: Entering directory `/opt/lua-5.1.5/src&apos;make all MYCFLAGS=-DLUA_USE_LINUX MYLIBS=&quot;-Wl,-E -ldl -lreadline -lhistory -lncurses&quot;make[2]: Entering directory `/opt/lua-5.1.5/src&apos;gcc -O2 -Wall -DLUA_USE_LINUX -c -o lapi.o lapi.cmake[2]: gcc：命令未找到make[2]: *** [lapi.o] 错误 127make[2]: Leaving directory `/opt/lua-5.1.5/src&apos;make[1]: *** [linux] 错误 2make[1]: Leaving directory `/opt/lua-5.1.5/src&apos;make: *** [linux] 错误 2 原因：最开始的那些依赖没安装","tags":[{"name":"lua","slug":"lua","permalink":"http://www.54tianzhisheng.cn/tags/lua/"}]},{"title":"Elasticsearch 系列文章（二）：全文搜索引擎 Elasticsearch 集群搭建入门教程","date":"2017-09-08T16:00:00.000Z","path":"2017/09/09/Elasticsearch-install/","text":"介绍ElasticSearch 是一个基于 Lucene 的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于 RESTful web 接口。Elasticsearch 是用 Java 开发的，并作为 Apache 许可条款下的开放源码发布，是当前流行的企业级搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。基百科、Stack Overflow、Github 都采用它。 本文从零开始，讲解如何使用 Elasticsearch 搭建自己的全文搜索引擎。每一步都有详细的说明，大家跟着做就能学会。 环境1、VMware 2、Centos 6.6 3、Elasticsearch 5.5.2 4、JDK 1.8 VMware 安装以及在 VMware 中安装 Centos 这个就不说了，环境配置直接默认就好，不过分配给机器的内存最好设置大点（建议 2G）， 使用 dhclient 命令来自动获取 IP 地址，查看获取的 IP 地址则使用命令 ip addr 或者 ifconfig ，则会看到网卡信息和 lo 卡信息。 给虚拟机额中的 linux 设置固定的 ip（因为后面发现每次机器重启后又要重新使用 dhclient 命令来自动获取 IP 地址） 1vim /etc/sysconfig/network-scripts/ifcfg-eth0 修改： 12onboot=yesbootproto=static 增加：（下面可设置可不设置） 123IPADDR=192.168.1.113 网卡IP地址GATEWAY=192.168.1.1NETMASK=255.255.255.0 设置好之后，把网络服务重启一下， service network restart 修改 ip 地址参考： http://jingyan.baidu.com/article/e4d08ffdd417660fd3f60d70.html 大环境都准备好了，下面开始安装步骤： 安装 JDK 1.8先卸载自带的 openjdk，查找 openjdk 1rpm -qa | grep java 卸载 openjdk 12yum -y remove java-1.7.0-openjdk-1.7.0.65-2.5.1.2.el65.x8664yum -y remove java-1.6.0-openjdk-1.6.0.0-11.1.13.4.el6.x86_64 解压 JDK 安装包： 附上jdk1.8的下载地址：http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html 解压完成后配置一下环境变量就 ok 1、在/usr/local/下创建Java文件夹 12cd /usr/local/ 进入目录mkdir java 新建java目录 2、文件夹创建完毕，把安装包拷贝到 Java 目录中，然后解压 jdk 到当前目录 12cp /usr/jdk-8u144-linux-x64.tar.gz /usr/local/java/ **注意匹配你自己的文件名** 拷贝到java目录tar -zxvf jdk-8u144-linux-x64.tar.gz 解压到当前目录（Java目录） 3、解压完之后，Java目录中会出现一个jdk1.8.0_144的目录，这就解压完成了。之后配置一下环境变量。编辑/etc/下的profile文件，配置环境变量 12345678vi /etc/profile 进入profile文件的编辑模式在最后边追加一下内容(**配置的时候一定要根据自己的目录情况而定哦！**) JAVA_HOME=/usr/local/java/jdk1.8.0_144 CLASSPATH=$JAVA_HOME/lib/ PATH=$PATH:$JAVA_HOME/bin export PATH JAVA_HOME CLASSPATH 之后保存并退出文件之后。 让文件生效：source /etc/profile 在控制台输入Java 和 Java -version 看有没有信息输出，如下： java -version 123java version &quot;1.8.0_144&quot; Java(TM) SE Runtime Environment (build 1.8.0_60-b27) Java HotSpot(TM) Client VM (build 25.60-b23, mixed mode) 能显示以上信息，就说明 JDK 安装成功啦 安装 Maven因为后面可能会用到 maven ，先装上这个。 1、下载 maven 1wget http://mirrors.hust.edu.cn/apache/maven/maven-3/3.2.5/binaries/apache-maven-3.2.5-bin.tar.gz 2、解压至 /usr/local 目录 1tar -zxvf apache-maven-3.2.5-bin.tar.gz 3、配置公司给的配置 替换成公司给的 setting.xml 文件，修改关于本地仓库的位置, 默认位置: ${user.home}/.m2/repository 4、配置环境变量etc/profile 最后添加以下两行 12export MAVEN_HOME=/usr/local/apache-maven-3.2.5export PATH=$&#123;PATH&#125;:$&#123;MAVEN_HOME&#125;/bin 5、测试 123[root@localhost ~]# mvn -vApache Maven 3.2.5 (12a6b3acb947671f09b81f49094c53f426d8cea1; 2014-12-14T09:29:23-08:00)Maven home: /usr/local/apache-maven-3.2.5 VMware 虚拟机里面的三台机器 IP 分别是： 123192.168.153.133192.168.153.134192.168.153.132 配置 hosts在 /etc/hosts下面编写：ip node 节点的名字（域名解析） 1vim /etc/hosts 新增： 123192.168.153.133 es1192.168.153.134 es2192.168.153.132 es3 设置 SSH 免密码登录安装expect命令 ： yum -y install expect 将 ssh_p2p.jar 随便解压到任何目录下： (这个 jar 包可以去网上下载) 1unzip ssh_p2p.zip 修改 resource 的 ip 值 1vim /ssh_p2p/deploy_data/resource （各个节点和账户名，密码，free代表相互都可以无密码登陆） 123456#设置为你每台虚拟机的ip地址，用户名，密码address=(&quot;192.168.153.133,root,123456,free&quot;&quot;192.168.153,134,root,123456,free&quot;&quot;192.168.153.132,root,123456,free&quot;) 修改 start.sh 的运行权限 1chmod u+x start.sh 运行 1./start.sh 测试： ssh ip地址 （测试是否可以登录） 安装 ElasticSearch下载地址： https://www.elastic.co/downloads/elasticsearch 123wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-5.5.2.tar.gzcd /usr/localtar -zxvf elasticsearch-5.5.2.tar.gz su tzs 切换到 tzs 用户下 ( 默认不支持 root 用户) sh /usr/local/elasticsearch/bin/elasticsearch -d 其中 -d 表示后台启动 在 vmware 上测试是否成功：curl http://localhost:9200/ 出现如上图这样的效果，就代表已经装好了。 elasticsearch 默认 restful-api 的端口是 9200 不支持 IP 地址，也就是说无法从主机访问虚拟机中的服务，只能在本机用 http://localhost:9200 来访问。如果需要改变，需要修改配置文件 /usr/local/elasticsearch/config/elasticsearch.yml 文件，加入以下两行： 12network.bind_host: 0.0.0.0network.publish_host: _nonloopback:ipv4 或去除 network.host 和 http.port 之前的注释，并将 network.host 的 IP 地址修改为本机外网 IP。然后重启，Elasticsearch 关闭方法（输入命令：ps -ef | grep elasticsearch ，找到进程，然后 kill 掉就行了。 如果外网还是不能访问，则有可能是防火墙设置导致的 ( 关闭防火墙：service iptables stop ) 修改配置文件：vim config/elasticsearch.yml cluster.name : my-app (集群的名字，名字相同的就是一个集群) node.name : es1 （节点的名字, 和前面配置的 hosts 中的 name 要一致） path.data: /data/elasticsearch/data （数据的路径。没有要创建（mkdir -p /data/elasticsearch/{data,logs}），并且给执行用户权限 chown tzs /data/elasticsearch/{data,logs} -R ）path.logs: /data/elasticsearch/logs （数据 log 信息的路径，同上）network.host: 0.0.0.0 //允许外网访问，也可以是自己的ip地址http.port: 9200 //访问的端口discovery.zen.ping.unicast.hosts: [“192.168.153.133”, “192.168.153.134”, “192.168.153.132”] //各个节点的ip地址 记得需要添加上：（这个是安装 head 插件要用的， 目前不需要）http.cors.enabled: truehttp.cors.allow-origin: “*” 最后在外部浏览器的效果如下图： 安装 IK 中文分词可以自己下载源码使用 maven 编译，当然如果怕麻烦可以直接下载编译好的 https://github.com/medcl/elasticsearch-analysis-ik/releases/tag/v5.5.2 注意下载对应的版本放在 plugins 目录下 解压 unzip elasticsearch-analysis-ik-5.5.2.zip 在 es 的 plugins 下新建 ik 目录 mkdir ik 将刚才解压的复制到ik目录下 cp -r elasticsearch/* ik 删除刚才解压后的 12rm -rf elasticsearchrm -rf elasticsearch-analysis-ik-5.5.2.zip IK 带有两个分词器ik_max_word ：会将文本做最细粒度的拆分；尽可能多的拆分出词语 ik_smart：会做最粗粒度的拆分；已被分出的词语将不会再次被其它词语占有 安装完 IK 中文分词器后（当然不止这种中文分词器，还有其他的，可以参考我的文章 Elasticsearch 默认分词器和中分分词器之间的比较及使用方法），测试区别如下： ik_max_wordcurl -XGET ‘http://192.168.153.134:9200/_analyze?pretty&amp;analyzer=ik_max_word‘ -d ‘联想是全球最大的笔记本厂商’ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;联想&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 2, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;是&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;全球&quot;, &quot;start_offset&quot; : 3, &quot;end_offset&quot; : 5, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;最大&quot;, &quot;start_offset&quot; : 5, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;的&quot;, &quot;start_offset&quot; : 7, &quot;end_offset&quot; : 8, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 4 &#125;, &#123; &quot;token&quot; : &quot;笔记本&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 11, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 5 &#125;, &#123; &quot;token&quot; : &quot;笔记&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 10, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 6 &#125;, &#123; &quot;token&quot; : &quot;本厂&quot;, &quot;start_offset&quot; : 10, &quot;end_offset&quot; : 12, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 7 &#125;, &#123; &quot;token&quot; : &quot;厂商&quot;, &quot;start_offset&quot; : 11, &quot;end_offset&quot; : 13, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 8 &#125; ]&#125; ik_smartcurl -XGET ‘http://localhost:9200/_analyze?pretty&amp;analyzer=ik_smart‘ -d ‘联想是全球最大的笔记本厂商’ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;联想&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 2, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;是&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;全球&quot;, &quot;start_offset&quot; : 3, &quot;end_offset&quot; : 5, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;最大&quot;, &quot;start_offset&quot; : 5, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;的&quot;, &quot;start_offset&quot; : 7, &quot;end_offset&quot; : 8, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 4 &#125;, &#123; &quot;token&quot; : &quot;笔记本&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 11, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 5 &#125;, &#123; &quot;token&quot; : &quot;厂商&quot;, &quot;start_offset&quot; : 11, &quot;end_offset&quot; : 13, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 6 &#125; ]&#125; 安装 head 插件elasticsearch-head 是一个 elasticsearch 的集群管理工具，它是完全由 html5 编写的独立网页程序，你可以通过插件把它集成到 es。 效果如下图：（图片来自网络） 安装 git123yum remove gityum install gitgit clone git://github.com/mobz/elasticsearch-head.git 拉取 head 插件到本地，或者直接在 GitHub 下载 压缩包下来 安装nodejs先去官网下载 node-v8.4.0-linux-x64.tar.xz 12tar -Jxv -f node-v8.4.0-linux-x64.tar.xzmv node-v8.4.0-linux-x64 node 环境变量设置： 1vim /etc/profile 新增： 123export NODE_HOME=/opt/nodeexport PATH=$PATH:$NODE_HOME/binexport NODE_PATH=$NODE_HOME/lib/node_modules 使配置文件生效（这步很重要，自己要多注意这步） 1source /etc/profile 测试是否全局可用了： 1node -v 然后 12345mv elasticsearch-head headcd head/npm install -g grunt-clinpm installgrunt server 再 es 的配置文件中加： 12http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot; 在浏览器打开 http://192.168.153.133:9100/ 就可以看到效果了， 遇到问题把坑都走了一遍，防止以后再次入坑，特此记录下来 1、ERROR Could not register mbeans java.security.AccessControlException: access denied (“javax.management.MBeanTrustPermission” “register”) 改变 elasticsearch 文件夹所有者到当前用户 sudo chown -R noroot:noroot elasticsearch 这是因为 elasticsearch 需要读写配置文件，我们需要给予 config 文件夹权限，上面新建了 elsearch 用户，elsearch 用户不具备读写权限，因此还是会报错，解决方法是切换到管理员账户，赋予权限即可： sudo -i chmod -R 775 config 2、[WARN ][o.e.b.ElasticsearchUncaughtExceptionHandler] [] uncaught exception in thread [main]org.elasticsearch.bootstrap.StartupException: java.lang.RuntimeException: can not run elasticsearch as root 原因是elasticsearch默认是不支持用root用户来启动的。 解决方案一：Des.insecure.allow.root=true 修改/usr/local/elasticsearch-2.4.0/bin/elasticsearch， 添加 ES_JAVA_OPTS=”-Des.insecure.allow.root=true” 或执行时添加： sh /usr/local/elasticsearch-2.4.0/bin/elasticsearch -d -Des.insecure.allow.root=true 注意：正式环境用root运行可能会有安全风险，不建议用root来跑。 解决方案二：添加专门的用户 1234useradd elasticchown -R elastic:elastic elasticsearch-2.4.0su elasticsh /usr/local/elasticsearch-2.4.0/bin/elasticsearch -d 3、UnsupportedOperationException: seccomp unavailable: requires kernel 3.5+ with CONFIG_SECCOMP and CONFIG_SECCOMP_FILTER compiled in 只是警告，使用新的linux版本，就不会出现此类问题了。 4、ERROR: [4] bootstrap checks failed[1]: max file descriptors [4096] for elasticsearch process is too low, increase to at least [65536] 原因：无法创建本地文件问题,用户最大可创建文件数太小 解决方案：切换到 root 用户，编辑 limits.conf 配置文件， 添加类似如下内容： vim /etc/security/limits.conf 添加如下内容: 1234* soft nofile 65536* hard nofile 131072* soft nproc 2048* hard nproc 4096 [2]: max number of threads [1024] for user [tzs] is too low, increase to at least [2048] 原因：无法创建本地线程问题,用户最大可创建线程数太小 解决方案：切换到root用户，进入limits.d目录下，修改90-nproc.conf 配置文件。 vim /etc/security/limits.d/90-nproc.conf 找到如下内容： soft nproc 1024 修改为 soft nproc 2048 [3]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] 原因：最大虚拟内存太小 root用户执行命令： sysctl -w vm.max_map_count=262144 或者修改 /etc/sysctl.conf 文件，添加 “vm.max_map_count”设置设置后，可以使用$ sysctl -p [4]: system call filters failed to install; check the logs and fix your configuration or disable system call filters at your own risk 原因：Centos6不支持SecComp，而ES5.4.1默认bootstrap.system_call_filter为true进行检测，所以导致检测失败，失败后直接导致ES不能启动。详见 ：https://github.com/elastic/elasticsearch/issues/22899 解决方法：在elasticsearch.yml中新增配置bootstrap.system_call_filter，设为false，注意要在Memory下面:bootstrap.memory_lock: falsebootstrap.system_call_filter: false 5、 java.lang.IllegalArgumentException: property [elasticsearch.version] is missing for plugin [head] 再 es 的配置文件中加： 12http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot; 最后整个搭建的过程全程自己手动安装，不易，如果安装很多台机器，是否可以写个脚本之类的自动搭建呢？可以去想想的。首发于：http://www.54tianzhisheng.cn/2017/09/09/Elasticsearch-install/ ，转载请注明出处，谢谢配合！","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://www.54tianzhisheng.cn/tags/ElasticSearch/"}]},{"title":"Elasticsearch 系列文章（一）：Elasticsearch 默认分词器和中分分词器之间的比较及使用方法","date":"2017-09-07T16:00:00.000Z","path":"2017/09/08/Elasticsearch-analyzers/","text":"介绍：ElasticSearch 是一个基于 Lucene 的搜索服务器。它提供了一个分布式多用户能力的全文搜索引擎，基于 RESTful web 接口。Elasticsearch 是用 Java 开发的，并作为Apache许可条款下的开放源码发布，是当前流行的企业级搜索引擎。设计用于云计算中，能够达到实时搜索，稳定，可靠，快速，安装使用方便。 Elasticsearch中，内置了很多分词器（analyzers）。下面来进行比较下系统默认分词器和常用的中文分词器之间的区别。 系统默认分词器：1、standard 分词器https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-standard-analyzer.html 如何使用：http://www.yiibai.com/lucene/lucene_standardanalyzer.html 英文的处理能力同于StopAnalyzer.支持中文采用的方法为单字切分。他会将词汇单元转换成小写形式，并去除停用词和标点符号。 12345/**StandardAnalyzer分析器*/public void standardAnalyzer(String msg)&#123; StandardAnalyzer analyzer = new StandardAnalyzer(Version.LUCENE_36); this.getTokens(analyzer, msg);&#125; 2、simple 分词器https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-simple-analyzer.html 如何使用: http://www.yiibai.com/lucene/lucene_simpleanalyzer.html 功能强于WhitespaceAnalyzer, 首先会通过非字母字符来分割文本信息，然后将词汇单元统一为小写形式。该分析器会去掉数字类型的字符。 12345/**SimpleAnalyzer分析器*/ public void simpleAnalyzer(String msg)&#123; SimpleAnalyzer analyzer = new SimpleAnalyzer(Version.LUCENE_36); this.getTokens(analyzer, msg); &#125; 3、Whitespace 分词器https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-whitespace-analyzer.html 如何使用：http://www.yiibai.com/lucene/lucene_whitespaceanalyzer.html 仅仅是去除空格，对字符没有lowcase化,不支持中文；并且不对生成的词汇单元进行其他的规范化处理。 12345/**WhitespaceAnalyzer分析器*/ public void whitespaceAnalyzer(String msg)&#123; WhitespaceAnalyzer analyzer = new WhitespaceAnalyzer(Version.LUCENE_36); this.getTokens(analyzer, msg); &#125; 4、Stop 分词器https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-stop-analyzer.html 如何使用：http://www.yiibai.com/lucene/lucene_stopanalyzer.html StopAnalyzer的功能超越了SimpleAnalyzer，在SimpleAnalyzer的基础上增加了去除英文中的常用单词（如the，a等），也可以更加自己的需要设置常用单词；不支持中文 12345/**StopAnalyzer分析器*/ public void stopAnalyzer(String msg)&#123; StopAnalyzer analyzer = new StopAnalyzer(Version.LUCENE_36); this.getTokens(analyzer, msg); &#125; 5、keyword 分词器KeywordAnalyzer把整个输入作为一个单独词汇单元，方便特殊类型的文本进行索引和检索。针对邮政编码，地址等文本信息使用关键词分词器进行索引项建立非常方便。 6、pattern 分词器https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-pattern-analyzer.html 一个pattern类型的analyzer可以通过正则表达式将文本分成”terms”(经过token Filter 后得到的东西 )。接受如下设置: 一个 pattern analyzer 可以做如下的属性设置: lowercase terms是否是小写. 默认为 true 小写. pattern 正则表达式的pattern, 默认是 \\W+. flags 正则表达式的flags stopwords 一个用于初始化stop filter的需要stop 单词的列表.默认单词是空的列表 7、language 分词器https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html 一个用于解析特殊语言文本的analyzer集合。（ arabic,armenian, basque, brazilian, bulgarian, catalan, cjk, czech, danish, dutch, english, finnish, french,galician, german, greek, hindi, hungarian, indonesian, irish, italian, latvian, lithuanian, norwegian,persian, portuguese, romanian, russian, sorani, spanish, swedish, turkish, thai.）可惜没有中文。不予考虑 8、snowball 分词器一个snowball类型的analyzer是由standard tokenizer和standard filter、lowercase filter、stop filter、snowball filter这四个filter构成的。 snowball analyzer 在Lucene中通常是不推荐使用的。 9、Custom 分词器是自定义的analyzer。允许多个零到多个tokenizer，零到多个 Char Filters. custom analyzer 的名字不能以 “_”开头. The following are settings that can be set for a custom analyzer type: Setting Description tokenizer 通用的或者注册的tokenizer. filter 通用的或者注册的token filters char_filter 通用的或者注册的 character filters position_increment_gap 距离查询时，最大允许查询的距离，默认是100 自定义的模板： 1234567891011121314151617181920212223242526index : analysis : analyzer : myAnalyzer2 : type : custom tokenizer : myTokenizer1 filter : [myTokenFilter1, myTokenFilter2] char_filter : [my_html] position_increment_gap: 256 tokenizer : myTokenizer1 : type : standard max_token_length : 900 filter : myTokenFilter1 : type : stop stopwords : [stop1, stop2, stop3, stop4] myTokenFilter2 : type : length min : 0 max : 2000 char_filter : my_html : type : html_strip escaped_tags : [xxx, yyy] read_ahead : 1024 10、fingerprint 分词器https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-fingerprint-analyzer.html 中文分词器：1、ik-analyzerhttps://github.com/wks/ik-analyzer IKAnalyzer是一个开源的，基于java语言开发的轻量级的中文分词工具包。 采用了特有的“正向迭代最细粒度切分算法“，支持细粒度和最大词长两种切分模式；具有83万字/秒（1600KB/S）的高速处理能力。 采用了多子处理器分析模式，支持：英文字母、数字、中文词汇等分词处理，兼容韩文、日文字符 优化的词典存储，更小的内存占用。支持用户词典扩展定义 针对Lucene全文检索优化的查询分析器IKQueryParser(作者吐血推荐)；引入简单搜索表达式，采用歧义分析算法优化查询关键字的搜索排列组合，能极大的提高Lucene检索的命中率。 Maven用法： 12345&lt;dependency&gt; &lt;groupId&gt;org.wltea.ik-analyzer&lt;/groupId&gt; &lt;artifactId&gt;ik-analyzer&lt;/artifactId&gt; &lt;version&gt;3.2.8&lt;/version&gt;&lt;/dependency&gt; 在IK Analyzer加入Maven Central Repository之前，你需要手动安装，安装到本地的repository，或者上传到自己的Maven repository服务器上。 要安装到本地Maven repository，使用如下命令，将自动编译，打包并安装：mvn install -Dmaven.test.skip=true Elasticsearch添加中文分词安装IK分词插件https://github.com/medcl/elasticsearch-analysis-ik 进入elasticsearch-analysis-ik-master 更多安装请参考博客： 1、为elastic添加中文分词 ： http://blog.csdn.net/dingzfang/article/details/42776693 2、如何在Elasticsearch中安装中文分词器(IK+pinyin) ：http://www.cnblogs.com/xing901022/p/5910139.html 3、Elasticsearch 中文分词器 IK 配置和使用 ： http://blog.csdn.net/jam00/article/details/52983056 ik 带有两个分词器ik_max_word ：会将文本做最细粒度的拆分；尽可能多的拆分出词语 ik_smart：会做最粗粒度的拆分；已被分出的词语将不会再次被其它词语占有 区别： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133# ik_max_wordcurl -XGET &apos;http://localhost:9200/_analyze?pretty&amp;analyzer=ik_max_word&apos; -d &apos;联想是全球最大的笔记本厂商&apos;#返回&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;联想&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 2, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;是&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;全球&quot;, &quot;start_offset&quot; : 3, &quot;end_offset&quot; : 5, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;最大&quot;, &quot;start_offset&quot; : 5, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;的&quot;, &quot;start_offset&quot; : 7, &quot;end_offset&quot; : 8, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 4 &#125;, &#123; &quot;token&quot; : &quot;笔记本&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 11, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 5 &#125;, &#123; &quot;token&quot; : &quot;笔记&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 10, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 6 &#125;, &#123; &quot;token&quot; : &quot;本厂&quot;, &quot;start_offset&quot; : 10, &quot;end_offset&quot; : 12, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 7 &#125;, &#123; &quot;token&quot; : &quot;厂商&quot;, &quot;start_offset&quot; : 11, &quot;end_offset&quot; : 13, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 8 &#125; ]&#125;# ik_smartcurl -XGET &apos;http://localhost:9200/_analyze?pretty&amp;analyzer=ik_smart&apos; -d &apos;联想是全球最大的笔记本厂商&apos;# 返回&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;联想&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 2, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;是&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;全球&quot;, &quot;start_offset&quot; : 3, &quot;end_offset&quot; : 5, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;最大&quot;, &quot;start_offset&quot; : 5, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;的&quot;, &quot;start_offset&quot; : 7, &quot;end_offset&quot; : 8, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 4 &#125;, &#123; &quot;token&quot; : &quot;笔记本&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 11, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 5 &#125;, &#123; &quot;token&quot; : &quot;厂商&quot;, &quot;start_offset&quot; : 11, &quot;end_offset&quot; : 13, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 6 &#125; ]&#125; 下面我们来创建一个索引，使用 ik创建一个名叫 iktest 的索引，设置它的分析器用 ik ，分词器用 ik_max_word，并创建一个 article 的类型，里面有一个 subject 的字段，指定其使用 ik_max_word 分词器 12345678910111213141516171819202122curl -XPUT &apos;http://localhost:9200/iktest?pretty&apos; -d &apos;&#123; &quot;settings&quot; : &#123; &quot;analysis&quot; : &#123; &quot;analyzer&quot; : &#123; &quot;ik&quot; : &#123; &quot;tokenizer&quot; : &quot;ik_max_word&quot; &#125; &#125; &#125; &#125;, &quot;mappings&quot; : &#123; &quot;article&quot; : &#123; &quot;dynamic&quot; : true, &quot;properties&quot; : &#123; &quot;subject&quot; : &#123; &quot;type&quot; : &quot;string&quot;, &quot;analyzer&quot; : &quot;ik_max_word&quot; &#125; &#125; &#125; &#125;&#125;&apos; 批量添加几条数据，这里我指定元数据 _id 方便查看，subject 内容为我随便找的几条新闻的标题 123456789101112curl -XPOST http://localhost:9200/iktest/article/_bulk?pretty -d &apos;&#123; &quot;index&quot; : &#123; &quot;_id&quot; : &quot;1&quot; &#125; &#125;&#123;&quot;subject&quot; : &quot;＂闺蜜＂崔顺实被韩检方传唤 韩总统府促彻查真相&quot; &#125;&#123; &quot;index&quot; : &#123; &quot;_id&quot; : &quot;2&quot; &#125; &#125;&#123;&quot;subject&quot; : &quot;韩举行＂护国训练＂ 青瓦台:决不许国家安全出问题&quot; &#125;&#123; &quot;index&quot; : &#123; &quot;_id&quot; : &quot;3&quot; &#125; &#125;&#123;&quot;subject&quot; : &quot;媒体称FBI已经取得搜查令 检视希拉里电邮&quot; &#125;&#123; &quot;index&quot; : &#123; &quot;_id&quot; : &quot;4&quot; &#125; &#125;&#123;&quot;subject&quot; : &quot;村上春树获安徒生奖 演讲中谈及欧洲排外问题&quot; &#125;&#123; &quot;index&quot; : &#123; &quot;_id&quot; : &quot;5&quot; &#125; &#125;&#123;&quot;subject&quot; : &quot;希拉里团队炮轰FBI 参院民主党领袖批其“违法”&quot; &#125;&apos; 查询 “希拉里和韩国” 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071curl -XPOST http://localhost:9200/iktest/article/_search?pretty -d&apos;&#123; &quot;query&quot; : &#123; &quot;match&quot; : &#123; &quot;subject&quot; : &quot;希拉里和韩国&quot; &#125;&#125;, &quot;highlight&quot; : &#123; &quot;pre_tags&quot; : [&quot;&lt;font color=&apos;red&apos;&gt;&quot;], &quot;post_tags&quot; : [&quot;&lt;/font&gt;&quot;], &quot;fields&quot; : &#123; &quot;subject&quot; : &#123;&#125; &#125; &#125;&#125;&apos;#返回&#123; &quot;took&quot; : 113, &quot;timed_out&quot; : false, &quot;_shards&quot; : &#123; &quot;total&quot; : 5, &quot;successful&quot; : 5, &quot;failed&quot; : 0 &#125;, &quot;hits&quot; : &#123; &quot;total&quot; : 4, &quot;max_score&quot; : 0.034062363, &quot;hits&quot; : [ &#123; &quot;_index&quot; : &quot;iktest&quot;, &quot;_type&quot; : &quot;article&quot;, &quot;_id&quot; : &quot;2&quot;, &quot;_score&quot; : 0.034062363, &quot;_source&quot; : &#123; &quot;subject&quot; : &quot;韩举行＂护国训练＂ 青瓦台:决不许国家安全出问题&quot; &#125;, &quot;highlight&quot; : &#123; &quot;subject&quot; : [ &quot;&lt;font color=red&gt;韩&lt;/font&gt;举行＂护&lt;font color=red&gt;国&lt;/font&gt;训练＂ 青瓦台:决不许国家安全出问题&quot; ] &#125; &#125;, &#123; &quot;_index&quot; : &quot;iktest&quot;, &quot;_type&quot; : &quot;article&quot;, &quot;_id&quot; : &quot;3&quot;, &quot;_score&quot; : 0.0076681254, &quot;_source&quot; : &#123; &quot;subject&quot; : &quot;媒体称FBI已经取得搜查令 检视希拉里电邮&quot; &#125;, &quot;highlight&quot; : &#123; &quot;subject&quot; : [ &quot;媒体称FBI已经取得搜查令 检视&lt;font color=red&gt;希拉里&lt;/font&gt;电邮&quot; ] &#125; &#125;, &#123; &quot;_index&quot; : &quot;iktest&quot;, &quot;_type&quot; : &quot;article&quot;, &quot;_id&quot; : &quot;5&quot;, &quot;_score&quot; : 0.006709609, &quot;_source&quot; : &#123; &quot;subject&quot; : &quot;希拉里团队炮轰FBI 参院民主党领袖批其“违法”&quot; &#125;, &quot;highlight&quot; : &#123; &quot;subject&quot; : [ &quot;&lt;font color=red&gt;希拉里&lt;/font&gt;团队炮轰FBI 参院民主党领袖批其“违法”&quot; ] &#125; &#125;, &#123; &quot;_index&quot; : &quot;iktest&quot;, &quot;_type&quot; : &quot;article&quot;, &quot;_id&quot; : &quot;1&quot;, &quot;_score&quot; : 0.0021509775, &quot;_source&quot; : &#123; &quot;subject&quot; : &quot;＂闺蜜＂崔顺实被韩检方传唤 韩总统府促彻查真相&quot; &#125;, &quot;highlight&quot; : &#123; &quot;subject&quot; : [ &quot;＂闺蜜＂崔顺实被&lt;font color=red&gt;韩&lt;/font&gt;检方传唤 &lt;font color=red&gt;韩&lt;/font&gt;总统府促彻查真相&quot; ] &#125; &#125; ] &#125;&#125; 这里用了高亮属性 highlight，直接显示到 html 中，被匹配到的字或词将以红色突出显示。若要用过滤搜索，直接将 match 改为 term 即可 热词更新配置网络词语日新月异，如何让新出的网络热词（或特定的词语）实时的更新到我们的搜索当中呢 先用 ik 测试一下 12345678910111213141516171819202122232425262728293031323334353637curl -XGET &apos;http://localhost:9200/_analyze?pretty&amp;analyzer=ik_max_word&apos; -d &apos;成龙原名陈港生&apos;#返回&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;成龙&quot;, &quot;start_offset&quot; : 1, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;原名&quot;, &quot;start_offset&quot; : 3, &quot;end_offset&quot; : 5, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;陈&quot;, &quot;start_offset&quot; : 5, &quot;end_offset&quot; : 6, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;港&quot;, &quot;start_offset&quot; : 6, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;CN_WORD&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;生&quot;, &quot;start_offset&quot; : 7, &quot;end_offset&quot; : 8, &quot;type&quot; : &quot;CN_CHAR&quot;, &quot;position&quot; : 4 &#125; ]&#125; ik 的主词典中没有”陈港生” 这个词，所以被拆分了。现在我们来配置一下 修改 IK 的配置文件 ：ES 目录/plugins/ik/config/ik/IKAnalyzer.cfg.xml 修改如下： 12345678910111213&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!DOCTYPE properties SYSTEM &quot;http://java.sun.com/dtd/properties.dtd&quot;&gt;&lt;properties&gt; &lt;comment&gt;IK Analyzer 扩展配置&lt;/comment&gt; &lt;!--用户可以在这里配置自己的扩展字典 --&gt; &lt;entry key=&quot;ext_dict&quot;&gt;custom/mydict.dic;custom/single_word_low_freq.dic&lt;/entry&gt; &lt;!--用户可以在这里配置自己的扩展停止词字典--&gt; &lt;entry key=&quot;ext_stopwords&quot;&gt;custom/ext_stopword.dic&lt;/entry&gt; &lt;!--用户可以在这里配置远程扩展字典 --&gt; &lt;entry key=&quot;remote_ext_dict&quot;&gt;http://192.168.1.136/hotWords.php&lt;/entry&gt; &lt;!--用户可以在这里配置远程扩展停止词字典--&gt; &lt;!-- &lt;entry key=&quot;remote_ext_stopwords&quot;&gt;words_location&lt;/entry&gt; --&gt;&lt;/properties&gt; 这里我是用的是远程扩展字典，因为可以使用其他程序调用更新，且不用重启 ES，很方便；当然使用自定义的 mydict.dic 字典也是很方便的，一行一个词，自己加就可以了 既然是远程词典，那么就要是一个可访问的链接，可以是一个页面，也可以是一个txt的文档，但要保证输出的内容是 utf-8 的格式 hotWords.php 的内容 12345678$s = &lt;&lt;&lt;'EOF'陈港生元楼蓝瘦EOF;header('Last-Modified: '.gmdate('D, d M Y H:i:s', time()).' GMT', true, 200);header('ETag: \"5816f349-19\"');echo $s; ik 接收两个返回的头部属性 Last-Modified 和 ETag，只要其中一个有变化，就会触发更新，ik 会每分钟获取一次重启 Elasticsearch ，查看启动记录，看到了三个词已被加载进来 再次执行上面的请求，返回, 就可以看到 ik 分词器已经匹配到了 “陈港生” 这个词，同理一些关于我们公司的专有名字（例如：永辉、永辉超市、永辉云创、云创 …. ）也可以自己手动添加到字典中去。 2、结巴中文分词特点：1、支持三种分词模式： 精确模式，试图将句子最精确地切开，适合文本分析； 全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义； 搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。 2、支持繁体分词 3、支持自定义词典 3、THULACTHULAC（THU Lexical Analyzer for Chinese）由清华大学自然语言处理与社会人文计算实验室研制推出的一套中文词法分析工具包，具有中文分词和词性标注功能。THULAC具有如下几个特点： 能力强。利用我们集成的目前世界上规模最大的人工分词和词性标注中文语料库（约含5800万字）训练而成，模型标注能力强大。 准确率高。该工具包在标准数据集Chinese Treebank（CTB5）上分词的F1值可达97.3％，词性标注的F1值可达到92.9％，与该数据集上最好方法效果相当。 速度较快。同时进行分词和词性标注速度为300KB/s，每秒可处理约15万字。只进行分词速度可达到1.3MB/s。 中文分词工具thulac4j发布 1、规范化分词词典，并去掉一些无用词； 2、重写DAT（双数组Trie树）的构造算法，生成的DAT size减少了8%左右，从而节省了内存； 3、优化分词算法，提高了分词速率。 12345&lt;dependency&gt; &lt;groupId&gt;io.github.yizhiru&lt;/groupId&gt; &lt;artifactId&gt;thulac4j&lt;/artifactId&gt; &lt;version&gt;$&#123;thulac4j.version&#125;&lt;/version&gt;&lt;/dependency&gt; http://www.cnblogs.com/en-heng/p/6526598.html thulac4j支持两种分词模式： SegOnly模式，只分词没有词性标注； SegPos模式，分词兼有词性标注。 12345678910// SegOnly modeString sentence = \"滔滔的流水，向着波士顿湾无声逝去\";SegOnly seg = new SegOnly(\"models/seg_only.bin\");System.out.println(seg.segment(sentence));// [滔滔, 的, 流水, ，, 向着, 波士顿湾, 无声, 逝去]// SegPos modeSegPos pos = new SegPos(\"models/seg_pos.bin\");System.out.println(pos.segment(sentence));//[滔滔/a, 的/u, 流水/n, ，/w, 向着/p, 波士顿湾/ns, 无声/v, 逝去/v] 4、NLPIR中科院计算所 NLPIR：http://ictclas.nlpir.org/nlpir/ (可直接在线分析中文) 下载地址：https://github.com/NLPIR-team/NLPIR 中科院分词系统(NLPIR)JAVA简易教程: http://www.cnblogs.com/wukongjiuwo/p/4092480.html 5、ansj分词器https://github.com/NLPchina/ansj_seg 这是一个基于n-Gram+CRF+HMM的中文分词的java实现. 分词速度达到每秒钟大约200万字左右（mac air下测试），准确率能达到96%以上 目前实现了.中文分词. 中文姓名识别 . 用户自定义词典,关键字提取，自动摘要，关键字标记等功能可以应用到自然语言处理等方面,适用于对分词效果要求高的各种项目. maven 引入： 12345&lt;dependency&gt; &lt;groupId&gt;org.ansj&lt;/groupId&gt; &lt;artifactId&gt;ansj_seg&lt;/artifactId&gt; &lt;version&gt;5.1.1&lt;/version&gt;&lt;/dependency&gt; 调用demo 1234String str = \"欢迎使用ansj_seg,(ansj中文分词)在这里如果你遇到什么问题都可以联系我.我一定尽我所能.帮助大家.ansj_seg更快,更准,更自由!\" ; System.out.println(ToAnalysis.parse(str)); 欢迎/v,使用/v,ansj/en,_,seg/en,,,(,ansj/en,中文/nz,分词/n,),在/p,这里/r,如果/c,你/r,遇到/v,什么/r,问题/n,都/d,可以/v,联系/v,我/r,./m,我/r,一定/d,尽我所能/l,./m,帮助/v,大家/r,./m,ansj/en,_,seg/en,更快/d,,,更/d,准/a,,,更/d,自由/a,! 6、哈工大的LTPhttps://link.zhihu.com/?target=https%3A//github.com/HIT-SCIR/ltp LTP制定了基于XML的语言处理结果表示，并在此基础上提供了一整套自底向上的丰富而且高效的中文语言处理模块（包括词法、句法、语义等6项中文处理核心技术），以及基于动态链接库（Dynamic Link Library, DLL）的应用程序接口、可视化工具，并且能够以网络服务（Web Service）的形式进行使用。 关于LTP的使用，请参考: http://ltp.readthedocs.io/zh_CN/latest/ 7、庖丁解牛下载地址：http://pan.baidu.com/s/1eQ88SZS 使用分为如下几步： 配置dic文件：修改paoding-analysis.jar中的paoding-dic-home.properties文件，将“#paoding.dic.home=dic”的注释去掉，并配置成自己dic文件的本地存放路径。eg：/home/hadoop/work/paoding-analysis-2.0.4-beta/dic 把Jar包导入到项目中：将paoding-analysis.jar、commons-logging.jar、lucene-analyzers-2.2.0.jar和lucene-core-2.2.0.jar四个包导入到项目中，这时就可以在代码片段中使用庖丁解牛工具提供的中文分词技术，例如： 123456789101112Analyzer analyzer = new PaodingAnalyzer(); //定义一个解析器String text = \"庖丁系统是个完全基于lucene的中文分词系统，它就是重新建了一个analyzer，叫做PaodingAnalyzer，这个analyer的核心任务就是生成一个可以切词TokenStream。\"; &lt;span style=\"font-family: Arial, Helvetica, sans-serif;\"&gt;//待分词的内容&lt;/span&gt;TokenStream tokenStream = analyzer.tokenStream(text, new StringReader(text)); //得到token序列的输出流try &#123; Token t; while ((t = tokenStream.next()) != null) &#123; System.out.println(t); //输出每个token &#125;&#125; catch (IOException e) &#123; e.printStackTrace();&#125; 8、sogo在线分词sogo在线分词采用了基于汉字标注的分词方法，主要使用了线性链链CRF（Linear-chain CRF）模型。词性标注模块主要基于结构化线性模型（Structured Linear Model） 在线使用地址为：http://www.sogou.com/labs/webservice/ 9、word分词地址： https://github.com/ysc/word word分词是一个Java实现的分布式的中文分词组件，提供了多种基于词典的分词算法，并利用ngram模型来消除歧义。能准确识别英文、数字，以及日期、时间等数量词，能识别人名、地名、组织机构名等未登录词。能通过自定义配置文件来改变组件行为，能自定义用户词库、自动检测词库变化、支持大规模分布式环境，能灵活指定多种分词算法，能使用refine功能灵活控制分词结果，还能使用词频统计、词性标注、同义标注、反义标注、拼音标注等功能。提供了10种分词算法，还提供了10种文本相似度算法，同时还无缝和Lucene、Solr、ElasticSearch、Luke集成。注意：word1.3需要JDK1.8 maven 中引入依赖： 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apdplat&lt;/groupId&gt; &lt;artifactId&gt;word&lt;/artifactId&gt; &lt;version&gt;1.3&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; ElasticSearch插件： 1234567891011121314151617181920212223242526272829303132333435363738394041424344451、打开命令行并切换到elasticsearch的bin目录cd elasticsearch-2.1.1/bin2、运行plugin脚本安装word分词插件：./plugin install http://apdplat.org/word/archive/v1.4.zip安装的时候注意： 如果提示： ERROR: failed to download 或者 Failed to install word, reason: failed to download 或者 ERROR: incorrect hash (SHA1) 则重新再次运行命令，如果还是不行，多试两次如果是elasticsearch1.x系列版本，则使用如下命令：./plugin -u http://apdplat.org/word/archive/v1.3.1.zip -i word3、修改文件elasticsearch-2.1.1/config/elasticsearch.yml，新增如下配置：index.analysis.analyzer.default.type : &quot;word&quot;index.analysis.tokenizer.default.type : &quot;word&quot;4、启动ElasticSearch测试效果，在Chrome浏览器中访问：http://localhost:9200/_analyze?analyzer=word&amp;text=杨尚川是APDPlat应用级产品开发平台的作者5、自定义配置修改配置文件elasticsearch-2.1.1/plugins/word/word.local.conf6、指定分词算法修改文件elasticsearch-2.1.1/config/elasticsearch.yml，新增如下配置：index.analysis.analyzer.default.segAlgorithm : &quot;ReverseMinimumMatching&quot;index.analysis.tokenizer.default.segAlgorithm : &quot;ReverseMinimumMatching&quot;这里segAlgorithm可指定的值有：正向最大匹配算法：MaximumMatching逆向最大匹配算法：ReverseMaximumMatching正向最小匹配算法：MinimumMatching逆向最小匹配算法：ReverseMinimumMatching双向最大匹配算法：BidirectionalMaximumMatching双向最小匹配算法：BidirectionalMinimumMatching双向最大最小匹配算法：BidirectionalMaximumMinimumMatching全切分算法：FullSegmentation最少词数算法：MinimalWordCount最大Ngram分值算法：MaxNgramScore如不指定，默认使用双向最大匹配算法：BidirectionalMaximumMatching 10、jcseg分词器https://code.google.com/archive/p/jcseg/ 11、stanford分词器Stanford大学的一个开源分词工具，目前已支持汉语。 首先，去【1】下载Download Stanford Word Segmenter version 3.5.2，取得里面的 data 文件夹，放在maven project的 src/main/resources 里。 然后，maven依赖添加： 123456789101112131415161718192021222324&lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;corenlp.version&gt;3.6.0&lt;/corenlp.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt; &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt; &lt;version&gt;$&#123;corenlp.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt; &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt; &lt;version&gt;$&#123;corenlp.version&#125;&lt;/version&gt; &lt;classifier&gt;models&lt;/classifier&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;edu.stanford.nlp&lt;/groupId&gt; &lt;artifactId&gt;stanford-corenlp&lt;/artifactId&gt; &lt;version&gt;$&#123;corenlp.version&#125;&lt;/version&gt; &lt;classifier&gt;models-chinese&lt;/classifier&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 测试： 12345678910111213141516171819202122232425262728293031323334353637383940414243import java.util.Properties;import edu.stanford.nlp.ie.crf.CRFClassifier;public class CoreNLPSegment &#123; private static CoreNLPSegment instance; private CRFClassifier classifier; private CoreNLPSegment()&#123; Properties props = new Properties(); props.setProperty(\"sighanCorporaDict\", \"data\"); props.setProperty(\"serDictionary\", \"data/dict-chris6.ser.gz\"); props.setProperty(\"inputEncoding\", \"UTF-8\"); props.setProperty(\"sighanPostProcessing\", \"true\"); classifier = new CRFClassifier(props); classifier.loadClassifierNoExceptions(\"data/ctb.gz\", props); classifier.flags.setProperties(props); &#125; public static CoreNLPSegment getInstance() &#123; if (instance == null) &#123; instance = new CoreNLPSegment(); &#125; return instance; &#125; public String[] doSegment(String data) &#123; return (String[]) classifier.segmentString(data).toArray(); &#125; public static void main(String[] args) &#123; String sentence = \"他和我在学校里常打桌球。\"; String ret[] = CoreNLPSegment.getInstance().doSegment(sentence); for (String str : ret) &#123; System.out.println(str); &#125; &#125;&#125; 博客： https://blog.sectong.com/blog/corenlp_segment.html http://blog.csdn.net/lightty/article/details/51766602 12、SmartcnSmartcn为Apache2.0协议的开源中文分词系统，Java语言编写，修改的中科院计算所ICTCLAS分词系统。很早以前看到Lucene上多了一个中文分词的contribution，当时只是简单的扫了一下.class文件的文件名，通过文件名可以看得出又是一个改的ICTCLAS的分词系统。 http://lucene.apache.org/core/5_1_0/analyzers-smartcn/org/apache/lucene/analysis/cn/smart/SmartChineseAnalyzer.html 13、pinyin 分词器pinyin分词器可以让用户输入拼音，就能查找到相关的关键词。比如在某个商城搜索中，输入 yonghui，就能匹配到 永辉。这样的体验还是非常好的。 pinyin分词器的安装与IK是一样的。下载地址：https://github.com/medcl/elasticsearch-analysis-pinyin 一些参数请参考 GitHub 的 readme 文档。 这个分词器在1.8版本中，提供了两种分词规则： pinyin,就是普通的把汉字转换成拼音； pinyin_first_letter，提取汉字的拼音首字母 使用： 1.Create a index with custom pinyin analyzer 1234567891011121314151617181920212223curl -XPUT http://localhost:9200/medcl/ -d&apos;&#123; &quot;index&quot; : &#123; &quot;analysis&quot; : &#123; &quot;analyzer&quot; : &#123; &quot;pinyin_analyzer&quot; : &#123; &quot;tokenizer&quot; : &quot;my_pinyin&quot; &#125; &#125;, &quot;tokenizer&quot; : &#123; &quot;my_pinyin&quot; : &#123; &quot;type&quot; : &quot;pinyin&quot;, &quot;keep_separate_first_letter&quot; : false, &quot;keep_full_pinyin&quot; : true, &quot;keep_original&quot; : true, &quot;limit_first_letter_length&quot; : 16, &quot;lowercase&quot; : true, &quot;remove_duplicated_term&quot; : true &#125; &#125; &#125; &#125;&#125;&apos; 2.Test Analyzer, analyzing a chinese name, such as 刘德华 1http://localhost:9200/medcl/_analyze?text=%e5%88%98%e5%be%b7%e5%8d%8e&amp;analyzer=pinyin_analyzer 123456789101112131415161718192021222324252627282930313233343536373839&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;liu&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 1, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;de&quot;, &quot;start_offset&quot; : 1, &quot;end_offset&quot; : 2, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;hua&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;刘德华&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;ldh&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 4 &#125; ]&#125; 3.Create mapping 12345678910111213141516171819curl -XPOST http://localhost:9200/medcl/folks/_mapping -d&apos;&#123; &quot;folks&quot;: &#123; &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;fields&quot;: &#123; &quot;pinyin&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;store&quot;: &quot;no&quot;, &quot;term_vector&quot;: &quot;with_offsets&quot;, &quot;analyzer&quot;: &quot;pinyin_analyzer&quot;, &quot;boost&quot;: 10 &#125; &#125; &#125; &#125; &#125;&#125;&apos; 4.Indexing 1curl -XPOST http://localhost:9200/medcl/folks/andy -d&apos;&#123;&quot;name&quot;:&quot;刘德华&quot;&#125;&apos; 5.Let’s search 12345http://localhost:9200/medcl/folks/_search?q=name:%E5%88%98%E5%BE%B7%E5%8D%8Ecurl http://localhost:9200/medcl/folks/_search?q=name.pinyin:%e5%88%98%e5%be%b7curl http://localhost:9200/medcl/folks/_search?q=name.pinyin:liucurl http://localhost:9200/medcl/folks/_search?q=name.pinyin:ldhcurl http://localhost:9200/medcl/folks/_search?q=name.pinyin:de+hua 6.Using Pinyin-TokenFilter 123456789101112131415161718192021222324252627curl -XPUT http://localhost:9200/medcl1/ -d&apos;&#123; &quot;index&quot; : &#123; &quot;analysis&quot; : &#123; &quot;analyzer&quot; : &#123; &quot;user_name_analyzer&quot; : &#123; &quot;tokenizer&quot; : &quot;whitespace&quot;, &quot;filter&quot; : &quot;pinyin_first_letter_and_full_pinyin_filter&quot; &#125; &#125;, &quot;filter&quot; : &#123; &quot;pinyin_first_letter_and_full_pinyin_filter&quot; : &#123; &quot;type&quot; : &quot;pinyin&quot;, &quot;keep_first_letter&quot; : true, &quot;keep_full_pinyin&quot; : false, &quot;keep_none_chinese&quot; : true, &quot;keep_original&quot; : false, &quot;limit_first_letter_length&quot; : 16, &quot;lowercase&quot; : true, &quot;trim_whitespace&quot; : true, &quot;keep_none_chinese_in_first_letter&quot; : true &#125; &#125; &#125; &#125;&#125;&apos; Token Test:刘德华 张学友 郭富城 黎明 四大天王 1curl -XGET http://localhost:9200/medcl1/_analyze?text=%e5%88%98%e5%be%b7%e5%8d%8e+%e5%bc%a0%e5%ad%a6%e5%8f%8b+%e9%83%ad%e5%af%8c%e5%9f%8e+%e9%bb%8e%e6%98%8e+%e5%9b%9b%e5%a4%a7%e5%a4%a9%e7%8e%8b&amp;analyzer=user_name_analyzer 12345678910111213141516171819202122232425262728293031323334353637383940&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;ldh&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 3, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;zxy&quot;, &quot;start_offset&quot; : 4, &quot;end_offset&quot; : 7, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;gfc&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 11, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;lm&quot;, &quot;start_offset&quot; : 12, &quot;end_offset&quot; : 14, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;sdtw&quot;, &quot;start_offset&quot; : 15, &quot;end_offset&quot; : 19, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 4 &#125; ]&#125; 7.Used in phrase query (1)、 123456789101112131415161718192021222324252627282930PUT /medcl/ &#123; &quot;index&quot; : &#123; &quot;analysis&quot; : &#123; &quot;analyzer&quot; : &#123; &quot;pinyin_analyzer&quot; : &#123; &quot;tokenizer&quot; : &quot;my_pinyin&quot; &#125; &#125;, &quot;tokenizer&quot; : &#123; &quot;my_pinyin&quot; : &#123; &quot;type&quot; : &quot;pinyin&quot;, &quot;keep_first_letter&quot;:false, &quot;keep_separate_first_letter&quot; : false, &quot;keep_full_pinyin&quot; : true, &quot;keep_original&quot; : false, &quot;limit_first_letter_length&quot; : 16, &quot;lowercase&quot; : true &#125; &#125; &#125; &#125; &#125; GET /medcl/folks/_search &#123; &quot;query&quot;: &#123;&quot;match_phrase&quot;: &#123; &quot;name.pinyin&quot;: &quot;刘德华&quot; &#125;&#125; &#125; (2)、 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647PUT /medcl/ &#123; &quot;index&quot; : &#123; &quot;analysis&quot; : &#123; &quot;analyzer&quot; : &#123; &quot;pinyin_analyzer&quot; : &#123; &quot;tokenizer&quot; : &quot;my_pinyin&quot; &#125; &#125;, &quot;tokenizer&quot; : &#123; &quot;my_pinyin&quot; : &#123; &quot;type&quot; : &quot;pinyin&quot;, &quot;keep_first_letter&quot;:false, &quot;keep_separate_first_letter&quot; : true, &quot;keep_full_pinyin&quot; : false, &quot;keep_original&quot; : false, &quot;limit_first_letter_length&quot; : 16, &quot;lowercase&quot; : true &#125; &#125; &#125; &#125; &#125; POST /medcl/folks/andy &#123;&quot;name&quot;:&quot;刘德华&quot;&#125; GET /medcl/folks/_search &#123; &quot;query&quot;: &#123;&quot;match_phrase&quot;: &#123; &quot;name.pinyin&quot;: &quot;刘德h&quot; &#125;&#125; &#125; GET /medcl/folks/_search &#123; &quot;query&quot;: &#123;&quot;match_phrase&quot;: &#123; &quot;name.pinyin&quot;: &quot;刘dh&quot; &#125;&#125; &#125; GET /medcl/folks/_search &#123; &quot;query&quot;: &#123;&quot;match_phrase&quot;: &#123; &quot;name.pinyin&quot;: &quot;dh&quot; &#125;&#125; &#125; 14、Mmseg 分词器也支持 Elasticsearch 下载地址：https://github.com/medcl/elasticsearch-analysis-mmseg/releases 根据对应的版本进行下载 如何使用： 1、创建索引： 1curl -XPUT http://localhost:9200/index 2、创建 mapping 123456789101112curl -XPOST http://localhost:9200/index/fulltext/_mapping -d&apos;&#123; &quot;properties&quot;: &#123; &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;term_vector&quot;: &quot;with_positions_offsets&quot;, &quot;analyzer&quot;: &quot;mmseg_maxword&quot;, &quot;search_analyzer&quot;: &quot;mmseg_maxword&quot; &#125; &#125;&#125;&apos; 3.Indexing some docs 123456789101112131415curl -XPOST http://localhost:9200/index/fulltext/1 -d&apos;&#123;&quot;content&quot;:&quot;美国留给伊拉克的是个烂摊子吗&quot;&#125;&apos;curl -XPOST http://localhost:9200/index/fulltext/2 -d&apos;&#123;&quot;content&quot;:&quot;公安部：各地校车将享最高路权&quot;&#125;&apos;curl -XPOST http://localhost:9200/index/fulltext/3 -d&apos;&#123;&quot;content&quot;:&quot;中韩渔警冲突调查：韩警平均每天扣1艘中国渔船&quot;&#125;&apos;curl -XPOST http://localhost:9200/index/fulltext/4 -d&apos;&#123;&quot;content&quot;:&quot;中国驻洛杉矶领事馆遭亚裔男子枪击 嫌犯已自首&quot;&#125;&apos; 4.Query with highlighting(查询高亮) 123456789101112curl -XPOST http://localhost:9200/index/fulltext/_search -d&apos;&#123; &quot;query&quot; : &#123; &quot;term&quot; : &#123; &quot;content&quot; : &quot;中国&quot; &#125;&#125;, &quot;highlight&quot; : &#123; &quot;pre_tags&quot; : [&quot;&lt;tag1&gt;&quot;, &quot;&lt;tag2&gt;&quot;], &quot;post_tags&quot; : [&quot;&lt;/tag1&gt;&quot;, &quot;&lt;/tag2&gt;&quot;], &quot;fields&quot; : &#123; &quot;content&quot; : &#123;&#125; &#125; &#125;&#125;&apos; 5、结果： 12345678910111213141516171819202122232425262728293031323334353637383940414243&#123; &quot;took&quot;: 14, &quot;timed_out&quot;: false, &quot;_shards&quot;: &#123; &quot;total&quot;: 5, &quot;successful&quot;: 5, &quot;failed&quot;: 0 &#125;, &quot;hits&quot;: &#123; &quot;total&quot;: 2, &quot;max_score&quot;: 2, &quot;hits&quot;: [ &#123; &quot;_index&quot;: &quot;index&quot;, &quot;_type&quot;: &quot;fulltext&quot;, &quot;_id&quot;: &quot;4&quot;, &quot;_score&quot;: 2, &quot;_source&quot;: &#123; &quot;content&quot;: &quot;中国驻洛杉矶领事馆遭亚裔男子枪击 嫌犯已自首&quot; &#125;, &quot;highlight&quot;: &#123; &quot;content&quot;: [ &quot;&lt;tag1&gt;中国&lt;/tag1&gt;驻洛杉矶领事馆遭亚裔男子枪击 嫌犯已自首 &quot; ] &#125; &#125;, &#123; &quot;_index&quot;: &quot;index&quot;, &quot;_type&quot;: &quot;fulltext&quot;, &quot;_id&quot;: &quot;3&quot;, &quot;_score&quot;: 2, &quot;_source&quot;: &#123; &quot;content&quot;: &quot;中韩渔警冲突调查：韩警平均每天扣1艘中国渔船&quot; &#125;, &quot;highlight&quot;: &#123; &quot;content&quot;: [ &quot;均每天扣1艘&lt;tag1&gt;中国&lt;/tag1&gt;渔船 &quot; ] &#125; &#125; ] &#125;&#125; 参考博客： 为elastic添加中文分词: http://blog.csdn.net/dingzfang/article/details/42776693 15、bosonnlp （玻森数据中文分析器）下载地址：https://github.com/bosondata/elasticsearch-analysis-bosonnlp 如何使用： 运行 ElasticSearch 之前需要在 config 文件夹中修改 elasticsearch.yml 来定义使用玻森中文分析器，并填写玻森 API_TOKEN 以及玻森分词 API 的地址，即在该文件结尾处添加： 12345678910111213141516171819202122index: analysis: analyzer: bosonnlp: type: bosonnlp API_URL: http://api.bosonnlp.com/tag/analysis # You MUST give the API_TOKEN value, otherwise it doesn&apos;t work API_TOKEN: *PUT YOUR API TOKEN HERE* # Please uncomment if you want to specify ANY ONE of the following # areguments, otherwise the DEFAULT value will be used, i.e., # space_mode is 0, # oov_level is 3, # t2s is 0, # special_char_conv is 0. # More detials can be found in bosonnlp docs: # http://docs.bosonnlp.com/tag.html # # # space_mode: put your value here(range from 0-3) # oov_level: put your value here(range from 0-4) # t2s: put your value here(range from 0-1) # special_char_conv: put your value here(range from 0-1) 需要注意的是 必须在 API_URL 填写给定的分词地址以及在API_TOKEN：PUT YOUR API TOKEN HERE 中填写给定的玻森数据API_TOKEN，否则无法使用玻森中文分析器。该 API_TOKEN 是注册玻森数据账号所获得。 如果配置文件中已经有配置过其他的 analyzer，请直接在 analyzer 下如上添加 bosonnlp analyzer。 如果有多个 node 并且都需要 BosonNLP 的分词插件，则每个 node 下的 yaml 文件都需要如上安装和设置。 另外，玻森中文分词还提供了4个参数（space_mode，oov_level，t2s，special_char_conv）可满足不同的分词需求。如果取默认值，则无需任何修改；否则，可取消对应参数的注释并赋值。 测试： 建立 index 1curl -XPUT &apos;localhost:9200/test&apos; 测试分析器是否配置成功 1curl -XGET &apos;localhost:9200/test/_analyze?analyzer=bosonnlp&amp;pretty&apos; -d &apos;这是玻森数据分词的测试&apos; 结果 123456789101112131415161718192021222324252627282930313233343536373839404142434445&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;这&quot;, &quot;start_offset&quot; : 0, &quot;end_offset&quot; : 1, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 0 &#125;, &#123; &quot;token&quot; : &quot;是&quot;, &quot;start_offset&quot; : 1, &quot;end_offset&quot; : 2, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 1 &#125;, &#123; &quot;token&quot; : &quot;玻森&quot;, &quot;start_offset&quot; : 2, &quot;end_offset&quot; : 4, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;数据&quot;, &quot;start_offset&quot; : 4, &quot;end_offset&quot; : 6, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;分词&quot;, &quot;start_offset&quot; : 6, &quot;end_offset&quot; : 8, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 4 &#125;, &#123; &quot;token&quot; : &quot;的&quot;, &quot;start_offset&quot; : 8, &quot;end_offset&quot; : 9, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 5 &#125;, &#123; &quot;token&quot; : &quot;测试&quot;, &quot;start_offset&quot; : 9, &quot;end_offset&quot; : 11, &quot;type&quot; : &quot;word&quot;, &quot;position&quot; : 6 &#125; ]&#125; 配置 Token Filter 现有的 BosonNLP 分析器没有内置 token filter，如果有过滤 Token 的需求，可以利用 BosonNLP Tokenizer 和 ES 提供的 token filter 搭建定制分析器。 步骤 配置定制的 analyzer 有以下三个步骤： 添加 BosonNLP tokenizer在 elasticsearch.yml 文件中 analysis 下添加 tokenizer， 并在 tokenizer 中添加 BosonNLP tokenizer 的配置： 123456789101112131415161718192021222324index: analysis: analyzer: ... tokenizer: bosonnlp: type: bosonnlp API_URL: http://api.bosonnlp.com/tag/analysis # You MUST give the API_TOKEN value, otherwise it doesn&apos;t work API_TOKEN: *PUT YOUR API TOKEN HERE* # Please uncomment if you want to specify ANY ONE of the following # areguments, otherwise the DEFAULT value will be used, i.e., # space_mode is 0, # oov_level is 3, # t2s is 0, # special_char_conv is 0. # More detials can be found in bosonnlp docs: # http://docs.bosonnlp.com/tag.html # # # space_mode: put your value here(range from 0-3) # oov_level: put your value here(range from 0-4) # t2s: put your value here(range from 0-1) # special_char_conv: put your value here(range from 0-1) 添加 token filter 在 elasticsearch.yml 文件中 analysis 下添加 filter， 并在 filter 中添加所需 filter 的配置（下面例子中，我们以 lowercase filter 为例）： 123456789index: analysis: analyzer: ... tokenizer: ... filter: lowercase: type: lowercase 添加定制的 analyzer 在 elasticsearch.yml 文件中 analysis 下添加 analyzer， 并在 analyzer 中添加定制的 analyzer 的配置（下面例子中，我们把定制的 analyzer 命名为 filter_bosonnlp）： 12345678index: analysis: analyzer: ... filter_bosonnlp: type: custom tokenizer: bosonnlp filter: [lowercase] 自定义分词器虽然Elasticsearch带有一些现成的分析器，然而在分析器上Elasticsearch真正的强大之处在于，你可以通过在一个适合你的特定数据的设置之中组合字符过滤器、分词器、词汇单元过滤器来创建自定义的分析器。 字符过滤器： 字符过滤器 用来 整理 一个尚未被分词的字符串。例如，如果我们的文本是HTML格式的，它会包含像 &lt;p&gt; 或者 &lt;div&gt; 这样的HTML标签，这些标签是我们不想索引的。我们可以使用 html清除 字符过滤器 来移除掉所有的HTML标签，并且像把 &amp;Aacute; 转换为相对应的Unicode字符 Á 这样，转换HTML实体。 一个分析器可能有0个或者多个字符过滤器。 分词器: 一个分析器 必须 有一个唯一的分词器。 分词器把字符串分解成单个词条或者词汇单元。 标准 分析器里使用的 标准 分词器 把一个字符串根据单词边界分解成单个词条，并且移除掉大部分的标点符号，然而还有其他不同行为的分词器存在。 词单元过滤器: 经过分词，作为结果的 词单元流 会按照指定的顺序通过指定的词单元过滤器 。 词单元过滤器可以修改、添加或者移除词单元。我们已经提到过 lowercase 和 stop 词过滤器 ，但是在 Elasticsearch 里面还有很多可供选择的词单元过滤器。 词干过滤器 把单词 遏制 为 词干。 ascii_folding 过滤器移除变音符，把一个像 “très” 这样的词转换为 “tres” 。 ngram 和 edge_ngram 词单元过滤器 可以产生 适合用于部分匹配或者自动补全的词单元。 创建一个自定义分析器我们可以在 analysis 下的相应位置设置字符过滤器、分词器和词单元过滤器: 1234567891011PUT /my_index&#123; &quot;settings&quot;: &#123; &quot;analysis&quot;: &#123; &quot;char_filter&quot;: &#123; ... custom character filters ... &#125;, &quot;tokenizer&quot;: &#123; ... custom tokenizers ... &#125;, &quot;filter&quot;: &#123; ... custom token filters ... &#125;, &quot;analyzer&quot;: &#123; ... custom analyzers ... &#125; &#125; &#125;&#125; 这个分析器可以做到下面的这些事: 1、使用 html清除 字符过滤器移除HTML部分。 2、使用一个自定义的 映射 字符过滤器把 &amp; 替换为 “和” ： 123456&quot;char_filter&quot;: &#123; &quot;&amp;_to_and&quot;: &#123; &quot;type&quot;: &quot;mapping&quot;, &quot;mappings&quot;: [ &quot;&amp;=&gt; and &quot;] &#125;&#125; 3、使用 标准 分词器分词。 4、小写词条，使用 小写 词过滤器处理。 5、使用自定义 停止 词过滤器移除自定义的停止词列表中包含的词： 123456&quot;filter&quot;: &#123; &quot;my_stopwords&quot;: &#123; &quot;type&quot;: &quot;stop&quot;, &quot;stopwords&quot;: [ &quot;the&quot;, &quot;a&quot; ] &#125;&#125; 我们的分析器定义用我们之前已经设置好的自定义过滤器组合了已经定义好的分词器和过滤器： 12345678&quot;analyzer&quot;: &#123; &quot;my_analyzer&quot;: &#123; &quot;type&quot;: &quot;custom&quot;, &quot;char_filter&quot;: [ &quot;html_strip&quot;, &quot;&amp;_to_and&quot; ], &quot;tokenizer&quot;: &quot;standard&quot;, &quot;filter&quot;: [ &quot;lowercase&quot;, &quot;my_stopwords&quot; ] &#125;&#125; 汇总起来，完整的 创建索引 请求 看起来应该像这样： 1234567891011121314151617181920212223PUT /my_index&#123; &quot;settings&quot;: &#123; &quot;analysis&quot;: &#123; &quot;char_filter&quot;: &#123; &quot;&amp;_to_and&quot;: &#123; &quot;type&quot;: &quot;mapping&quot;, &quot;mappings&quot;: [ &quot;&amp;=&gt; and &quot;] &#125;&#125;, &quot;filter&quot;: &#123; &quot;my_stopwords&quot;: &#123; &quot;type&quot;: &quot;stop&quot;, &quot;stopwords&quot;: [ &quot;the&quot;, &quot;a&quot; ] &#125;&#125;, &quot;analyzer&quot;: &#123; &quot;my_analyzer&quot;: &#123; &quot;type&quot;: &quot;custom&quot;, &quot;char_filter&quot;: [ &quot;html_strip&quot;, &quot;&amp;_to_and&quot; ], &quot;tokenizer&quot;: &quot;standard&quot;, &quot;filter&quot;: [ &quot;lowercase&quot;, &quot;my_stopwords&quot; ] &#125;&#125;&#125;&#125;&#125; 索引被创建以后，使用 analyze API 来 测试这个新的分析器： 12GET /my_index/_analyze?analyzer=my_analyzerThe quick &amp; brown fox 下面的缩略结果展示出我们的分析器正在正确地运行： 12345678&#123; &quot;tokens&quot; : [ &#123; &quot;token&quot; : &quot;quick&quot;, &quot;position&quot; : 2 &#125;, &#123; &quot;token&quot; : &quot;and&quot;, &quot;position&quot; : 3 &#125;, &#123; &quot;token&quot; : &quot;brown&quot;, &quot;position&quot; : 4 &#125;, &#123; &quot;token&quot; : &quot;fox&quot;, &quot;position&quot; : 5 &#125; ]&#125; 这个分析器现在是没有多大用处的，除非我们告诉 Elasticsearch在哪里用上它。我们可以像下面这样把这个分析器应用在一个 string 字段上： 123456789PUT /my_index/_mapping/my_type&#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123; &quot;type&quot;: &quot;string&quot;, &quot;analyzer&quot;: &quot;my_analyzer&quot; &#125; &#125;&#125; 最后整理参考网上资料，如有不正确的地方还请多多指教！","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://www.54tianzhisheng.cn/tags/ElasticSearch/"}]},{"title":"那些年我看过的书 —— 致敬我的大学生活 —— Say Good Bye ！","date":"2017-08-27T16:00:00.000Z","path":"2017/08/28/recommend-books/","text":"开头2017.08.21 正式开启我入职的里程，现在已是工作了一个星期了，这个星期算是我入职的过渡期，算是知道了学校生活和工作的差距了，总之，尽快习惯这种生活吧。下面讲下自己的找工作经历和大学阅读的书籍，算是一种书籍推荐，为还在迷茫的你指引方向，同时为我三年的大学生活致敬！也激励我大四在公司实习能更上一层楼！ 找工作经历这段经历，算是自己很难忘记的经历吧。既辛酸既充实的日子！也很感谢自己在这段时间的系统复习，感觉把自己的基础知识再次聚集在一起了，自己的能力在这一段时间提升的也很快。后面有机会的话我也想写一系列的相关文章，为后来准备工作（面试）的同学提供一些自己的帮助。自己在找工作的这段时间面过的公司也有几家大厂，但是结果都不是很好，对我自己有很大的压力，当时心里真的感觉 ：“自己真的有这么差”，为什么一直被拒，当时很怀疑自己的能力，自己也有总结原因。一是面试的时候自己准备的还不够充分，虽说自己脑子里对这些基础有点印象，但是面试的时候自己稍紧张下就描述不怎么清楚了，导致面试官觉得你可能广度够了，深度还不够（这是阿里面试官电话面试说的）；二是自己的表达能力还是有所欠缺，不能够将自己所要表达的东西说出来，这可能我要在后面加强的地方；三是我的学校问题。在面了几家公司失败后，终于面了家公司要我了，我也确定在这家公司了。很幸运，刚出来，就有一个很好（很负责）的架构师带我，这周就给了我一个很牛逼的项目给我看（虽然自己目前还没有思路改里面的代码），里面新东西很多，说吃透了这个项目，以后绝对可以拿出去吹逼（一脸正经.jpg）。目前我的找工作经历就简短的介绍到这里了，如果感兴趣的话，可以加群：528776268 进来和我讨论交流。 书籍推荐大学，我不怎么喜欢玩游戏，自己也还算不怎么堕落吧，看了以下的一些书籍，算是对我后面写博客、找工作也有很大的帮助。如果你是大神，请忽略，如果你还是还在大学，和我一样不想把时间浪费在游戏上，可以看看我推荐的一些书籍，有想讨论的请在评论下留下你的评论或者加上面给的群号。 Java1、《Java 核心技术》卷一 、卷二 两本书，算是入门比较好的书籍了 2、《疯狂 Java 讲义》 很厚的一本书，里面的内容也是很注重基础了 3、《Java 并发编程的艺术》—— 方腾飞 、魏鹏、程晓明著 方腾飞 是并发编程网的创始人，里面的文章确实还不错，可以多看看里面的文章，收获绝对很大。 4、《 Java多线程编程核心技术》—— 高洪岩著 这本书也算是入门多线程编程的不错书籍，我之前还写了一篇读书笔记呢，《Java 多线程编程核心技术》学习笔记及总结 , 大家如果不想看书的可以去看我的笔记。 5、《Java 并发编程实战》 这本书讲的有点难懂啊，不过确实也是一本很好的书，以上三本书籍如果都弄懂了，我觉得你并发编程这块可能大概就 OK 了，然后再去看看线程池的源码，了解下线程池，我觉得那就更棒了。不想看的话，请看我的博客：Java 线程池艺术探索 我个人觉得还是写的很不错，那些大厂面试也几乎都会问线程池的东西，然后大概内容也就是我这博客写的 6、《Effective Java》中文版 第二版 算是 Java 的进阶书籍了，面试好多问题也是从这出来的 7、《深入理解 Java 虚拟机——JVM高级特性与最佳实践》第二版 这算是国内讲 JVM 最清楚的书了吧，目前还是只看了一遍，后面继续啃，大厂面试几乎也是都会考 JVM 的，阿里面 JVM 特别多，想进阿里的同学请一定要买这本书去看。 8、《深入分析Java Web技术内幕 修订版》许令波著 里面知识很广，每一章都是一个不同的知识，可见作者的优秀，不愧是阿里大神。 9、《大型网站系统与 Java 中间件实践》—— 曽宪杰 著 作者是前淘宝技术总监，见证了淘宝网的发展，里面的讲的内容也是很好，看完能让自己也站在高处去思考问题。 10、《大型网站技术架构 —— 核心原理与案例分析》 —— 李智慧 著 最好和上面那本书籍一起看，效果更好，两本看完了，提升思想的高度！ 11、《疯狂Java.突破程序员基本功的16课》 李刚 著 书中很注重 Java 的一些细节，讲的很深入，但是书中的错别字特多，可以看看我的读书笔记：《疯狂 Java 突破程序员基本功的 16 课》读书笔记 12、《Spring 实战》 Spring 入门书籍 13、《Spring 揭秘》—— 王福强 著 这本书别提多牛了，出版时期为 2009 年，豆瓣评分为 9.0 分，写的是真棒！把 Spring 的 IOC 和 AOP 特性写的很清楚，把 Spring 的来龙去脉讲的很全。墙裂推荐这本书籍，如果你想看 Spring，作者很牛，资深架构师，很有幸和作者有过一次交流，当时因为自己的一篇博客 Pyspider框架 —— Python爬虫实战之爬取 V2EX 网站帖子，竟然找到我想叫我去实习，可惜了，当时差点就跟着他混了。作者还有一本书 《Spring Boot 揭秘》。 14、《Spring 技术内幕》—— 深入解析 Spring 架构与设计原理 讲解 Spring 源码，深入了内部机制，个人觉得还是不错的。 15、Spring 官方的英文文档 这个别提了，很好，能看英文尽量看英文 16、《跟开涛学 Spring 3》 《跟开涛学 Spring MVC》 京东大神，膜 17、《看透springMvc源代码分析与实践》 算是把 Spring MVC 源码讲的很好的了 见我的笔记： 1、通过源码详解 Servlet 2 、看透 Spring MVC 源代码分析与实践 —— 网站基础知识 3 、看透 Spring MVC 源代码分析与实践 —— 俯视 Spring MVC 4 、看透 Spring MVC 源代码分析与实践 —— Spring MVC 组件分析 18、《Spring Boot 实战》 19、Spring Boot 官方 Reference Guide 网上好多写 SpringBoot 的博客，几乎和这个差不多。 20、《JavaEE开发的颠覆者: Spring Boot实战》 21、MyBatis 当然是官方的文档最好了，而且还是中文的。 自己也写过几篇文章，帮助过很多人入门，传送门： 1、通过项目逐步深入了解Mybatis（一）/) 2、通过项目逐步深入了解Mybatis（二）/) 3、通过项目逐步深入了解Mybatis（三）/) 4、通过项目逐步深入了解Mybatis（四）/) 22、《深入理解 Java 内存模型》—— 程晓明 著 我觉得每个 Java 程序员都应该了解下 Java 的内存模型，该书籍我看的是电子版的，不多，但是讲的却很清楚，把重排序、顺序一致性、Volatile、锁、final等写的很清楚。 Linux《鸟哥的Linux私房菜 基础学习篇(第三版) 》 鸟哥的Linux私房菜：服务器架设篇(第3版) 鸟哥的书 计算机网络《计算机网络第六版——谢希仁 编》 《计算机网络自顶向下方法》 计算机系统《代码揭秘：从C／C.的角度探秘计算机系统 —— 左飞》 《深入理解计算机系统》 《计算机科学导论_佛罗赞》 数据库《高性能MySQL》 《Mysql技术内幕InnoDB存储引擎》 Python这门语言语法很简单，上手快，不过我目前好久没用了，都忘得差不多了。当时是看的廖雪峰的 Python 博客 自己也用 Python 做爬虫写过几篇博客，不过有些是在前人的基础上写的。感谢那些栽树的人！ 工具Git ： 廖雪峰的 Git 教程 IDEA：IntelliJ IDEA 简体中文专题教程 Maven：《Maven实战》 其他《如何高效学习-斯科特杨》 教你怎样高效学习的 《软技能：代码之外的生存指南》 程序员除了写代码，还得懂点其他的软技能。 《提问的智慧“中文版”》 《How-To-Ask-Questions-The-Smart-Way》 作为程序员的你，一定要学会咋提问，不然别人都不想鸟你。 优秀网站推荐1、GitHub 别和我说不知道 2、InfoQ 文章很不错 3、CSDN 经常看博客专家的博客，里面大牛很多，传送门：zhisheng 4、知乎 多关注些大牛，看他们吹逼 5、掘金 自己也在上面写专栏，粉丝已经超过一万了，传送门 ：zhisheng 6、并发编程网 前面已经介绍 7、developerworks 上面的博客也很好 8、博客园 里面应该大牛也很多，不过自己没在上面写过博客 9、微信公众号 关注了很多人，有些人的文章确实很好。 10、牛客网 刷笔试题不错的地方，里面大牛超多，怀念叶神和左神讲课的时候，还有很有爱的牛妹。 11、优秀博主的博客地址了 优秀博客推荐廖雪峰 Git 和 Python 入门文章就是从他博客看的 阮一峰的网络日志 酷壳-陈皓 RednaxelaFX R大，牛逼的不得了 江南白衣 老司机 stormzhang 人称帅逼张，微信公众号写的不错 你假笨 阿里搞 JVM 的，很厉害 占小狼 泥瓦匠BYSocket 崔庆才 写了好多 Python 爬虫相关的文章 纯洁的微笑 SpringBoot 系列不错，其他的文章自己看了感觉是自己喜欢的那种文笔 程序猿DD 周立 芋艿V的博客 好多系列的源码分析 zhisheng 这个是我不要脸，竟然把自己博客地址的写上去了 最后送一句话，越努力，越幸运，祝早日成为大神！ 这些地方可以找到我： blog: http://www.54tianzhisheng.cn/ GitHub: https://github.com/zhisheng17 QQ 群：528776268","tags":[{"name":"随笔","slug":"随笔","permalink":"http://www.54tianzhisheng.cn/tags/随笔/"}]},{"title":"Ubuntu16.10 安装 Nginx","date":"2017-08-17T16:00:00.000Z","path":"2017/08/18/Ubuntu-install-Nginx/","text":"安装 Nginx 依赖库安装 gcc g++ 的依赖库Ubuntu 平台使用： 12apt-get install build-essentialapt-get install libtool CentOS 平台使用： 12345centos平台编译环境使用如下指令安装make：yum -y install gcc automake autoconf libtool make安装g++:yum install gcc gcc-c++ 安装 pcre 依赖库12sudo apt-get updatesudo apt-get install libpcre3 libpcre3-dev 安装 zlib 依赖库1apt-get install zlib1g-dev 安装 ssl 依赖库1apt-get install openssl 安装 Nginx在网上下载了 nginx-1.8.1.tar.gz 版本。 1234567891011121314151617#解压：tar -zxvf nginx-1.8.1.tar.gz#进入解压目录：cd nginx-1.8.1#配置：./configure --prefix=/usr/local/nginx#编辑nginx：make注意：这里可能会报错，提示“pcre.h No such file or directory”,具体详见：http://stackoverflow.com/questions/22555561/error-building-fatal-error-pcre-h-no-such-file-or-directory需要安装 libpcre3-dev,命令为：sudo apt-get install libpcre3-dev#安装nginx：sudo make install#启动nginx：sudo /usr/local/nginx/sbin/nginx -c /usr/local/nginx/conf/nginx.conf注意：-c 指定配置文件的路径，不加的话，nginx会自动加载默认路径的配置文件，可以通过 -h查看帮助命令。#查看nginx进程：ps -ef|grep nginx Nginx 常用命令启动 Nginx切换到 /usr/local/nginx/sbin/ 目录下，执行命令 1./nginx 查看效果： 停止 Nginx12./nginx -s stop./nginx -s quit -s 都是采用向 Nginx 发送信号的方式。 Nginx 重新加载配置文件1./nginx -s reload 指定配置文件1./nginx -c /usr/local/nginx/conf/nginx.conf -c 表示 configuration，指定配置文件 查看 Nginx 版本12./nginx -v //查看 Nginx 版本信息的参数./nginx -V //查看 Nginx 详细的版本信息 检查配置文件是否正确1./nginx -t 如果出现测试失败，表示没有访问错误日志文件和进程，可以 sudo 一下。配置正确的话会有相关的提示。 显示帮助信息123./nginx -h或者./nginx -? Nginx 的特点和应用场合见文章：Nginx 基本知识快速入门 最后文章首发地址：zhisheng的博客 ，转载请注明地址 http://www.54tianzhisheng.cn/2017/08/18/Ubuntu-install-Nginx/","tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://www.54tianzhisheng.cn/tags/Nginx/"}]},{"title":"马云热血励志演讲《最伟大的成功》","date":"2017-08-10T16:00:00.000Z","path":"2017/08/11/most-success/","text":"当你乐观的时候，总是会有机会的。那么你可能要问了“机会在哪呢？”你可能没有特别想实现的事，没有迫切要成功的欲望，没有勇攀高峰的决心，没有水滴石穿的毅力，也没有一颗所向披靡的强大心脏。那么，让来自马云的这段演讲来告诉你，你有什么。你有年轻的身体，你有奇妙的想法，你有乐观的心态，你有无限的可能性。 我想，马云的这段励志演讲为我们提供了一面镜子可以用于自照。上面“你有什么”说完了，接下来让我讨论一下“你可能没有什么”。 你总爱抱怨，机会却总是躲在人们抱怨的地方。你没有仔细想想怎么能把事情做得不一样。你没有行动力，你缺少坚持下去的长劲儿。你抗压能力差，你动不动就玻璃心。你不相信自己也不相信别人，你怕犯错。现在是不是觉得这碗鸡汤有点难以下咽了？如果认识到差距，不如从今天开始改变。明天的你只要比今天的你多迈出0.1步，也是进步。 为自己而工作。停止抱怨，用抱怨的时间多做事。把那些夜里冒出来的好点子在白天付诸行动，既然有了设想，那就行动起来。行动是你迈出的第一步，后面可能会更难，历经无数次动摇，面临无数次诱惑，感受无数次失败的苦味和难以为继的辛酸。顶住这一切，比常人更勇敢地去面对，并且坚持下去。排除万难，别被来自世人的非议和质疑影响。相信你自己，相信你的团队。服务好你的客户，之后再想怎么回馈社会。犯足够多的错，年轻时走过的弯路是最棒的收获。 马云还曾经说过“今天很艰难，明天比今天更难，后天可能是美好的，但更多的人死在了明天”。是不是感到膝盖中箭了？不妨干了这碗“毒鸡汤”。成功的法则本就并非千篇一律，你会有你自己向上的学问。那不如从明天开始，去摸索，去践行，哪怕只比昨天的你多迈出0.01步。 送给正在找实习工作的自己！加油！！！","tags":[{"name":"励志","slug":"励志","permalink":"http://www.54tianzhisheng.cn/tags/励志/"}]},{"title":"源码大招：不服来战！撸这些完整项目，你不牛逼都难！","date":"2017-08-07T16:00:00.000Z","path":"2017/08/08/android-projects/","text":"经常有人问我有没有什么项目代码，我回复说去 Github 找，但是还是好多人不知道如何找到那些比较好的项目。 今天花了点时间找了些安卓的项目，觉得还是不错的，几乎就是自己生活常用的一些 app ，如果你是一个 Android 开发者，我觉得撸完这些项目，你想不牛逼都难。 关注我 菜鸟新闻菜鸟新闻 客户端是一个仿照36Kr官方,实 时抓取36Kr官网数据的资讯类新闻客户端。 包括首页新闻,详情,发现,活动,实时数据抓取,侧滑效果,第三方登录以及分享,消息推送等相关功能客户端。 课程地址： http://www.cniao5.com/clazz/view/10076.html视频下载链接： http://pan.baidu.com/s/1eQLyQxc 密码：3ts1 项目源码下载地址：https://github.com/yxs666/cniao5-news 运行截图: .gif) KuaiChuan仿茄子快传的一款文件传输应用， 涉及到Socket通信，包括TCP，UDP通信 项目源码：https://github.com/mayubao/KuaiChuan 运行截图： CoolShopping一个仿拉手团购的购物App，采用Bmob后台实现短信验证码注册、登录、收藏、订单管理、自动更新等功能，数据抓取自拉手团购 项目地址：https://github.com/myxh/CoolShopping 运行截图： RNPolymerPoRNPolymerPo 是一个基于 React Native 的生活类聚合实战项目，目前由于没有 MAC 设备，所以没有适配 iOS，感兴趣的可以自行适配 app 目录下相关 JS 代码即可。 项目地址：https://github.com/yanbober/RNPolymerPo 运行截图： bilibili仿 bilibili 的客户端 项目地址：https://github.com/HotBitmapGG/bilibili-android-client 运行截图： StockChart采用主流rxjava+retrofit+dagger2框架，StockChart看股票的分时图，k线图。 项目地址：https://github.com/AndroidJiang/StockChart Android精准计步器项目地址：https://github.com/linglongxin24/DylanStepCount 运行截图： 菜鸟微博菜鸟微博《通过对新浪微博开发案例的详细解析，讲解了一个完整的 Android 实际项目的开发过程。 有新浪微博的主要功能，有Toolbar,RecyclerView等最新控件的用法；各种快速开发框架的使用，比如 Glide,PhotoView ，EventBus ，OKHttp，pullToRefresh等。 学习视频+源码 视频中还会讲到MVP设计模式以及一些架构师的入门知识。 课程地址： http://www.cniao5.com/clazz/view/10075.html视频下载链接： http://pan.baidu.com/s/1gexq3VP 密码：f0t9 项目地址：https://github.com/yxs666/cniao5-weibo 运行截图： 在线云打印平台一个在线云打印平台（android部分）含订单管理、百度地图、二维码等等 项目地址：https://github.com/LehmanHe/A4print 运行截图： 铜板街项目地址：https://github.com/robotlife/TongBanJie 运行截图： 礼物说项目地址：https://github.com/Orangelittle/Liwusuo IotXmpp本项目是基于XMPP的物联网客户端软件的实现，其实现的主要功能是一款能和物联网节点交互的即时通讯软件。目前支持九类传感器节点交互，主要有：温湿度、风扇、直流电机、LED灯、步进电机、门磁、光电接近、烟雾和光照。本软件不仅能和这些传感器节点交互，还实现了类似微信的订阅和取消订阅功能。当订阅一个节点后节点就会按照设定好的周期向客户端汇报数据，客户端也能设置周期、设置报警上下限等。这些功能的实现极大的方便了我们和物联网节点的交互。 项目地址：https://github.com/tiandawu/IotXmpp 项目截图： Lives生活娱乐结合的APP, 现有主要功能： 图书 翻译 音乐 视频 项目地址：https://github.com/Allyns/Lives 项目截图： CoCoin一款多视图记账APP 项目地址：https://github.com/Nightonke/CoCoin 运行截图： AppLockAppLock应用锁，保护你的隐私 项目地址：https://github.com/lizixian18/AppLock 运行截图： jianshi 简诗一款优雅的中国风Android App，包括Android端和Server端，支持登录注册，数据云端同步，离线数据存储和截屏分享等功能。 项目地址： 运行截图： storage-chooser一款文件管理器app 项目地址：https://github.com/codekidX/storage-chooser 运行截图： LQRWeChat仿最新版微信6.5.7（除图片选择器外）。本项目基于融云SDK，使用目前较火的 Rxjava+Retrofit+MVP+Glide 技术开发。相比上个版本，加入发送位置消息，红包消息等功能。 项目地址：https://github.com/GitLqr/LQRWeChat 运行截图： PonyExpress 小马快递小马快递，您的好帮手。查询并跟踪快递，快递信息及时掌握。支持全国100多家快递公司，支持扫码查询，智能识别快递公司。附带生成二维码小工具，方便实用。体积小巧，无广告，无多余权限。 项目地址：https://github.com/wangchenyan/PonyExpress 运行截图： CloudReader 云阅一款基于网易云音乐UI，使用Gank.Io及豆瓣api开发的符合Google Material Design的Android客户端。项目采取的是MVVM-DataBinding架构开发，现主要包括：干货区、电影区和书籍区三个子模块。DIY网易云音乐原来是如此Cool 项目地址：https://github.com/youlookwhat/CloudReader 运行截图： 硅谷商城是一款按照企业级标准研发的项目。本套代码是目前国内市场第一套详细讲解商城类项目的免费代码。该代码中的内容包括但不仅限于，框架的搭建 、主页模块、分类模块、发现模块、购物车模块和个人中心模块。项目中讲解的主流技术包括且不限于RadioGroup + Fragment、OKHttp、FastJson、RecyclerView、 ScrollViewContainer、Banner、倒计时秒杀、自定义购物车、支付宝等技术。该项目中讲解的技术可应用在电商、新闻、旅游、医疗、在线教育等领域。 项目地址：https://github.com/atguigu01/Shopping 运行截图： 觉得棒的，欢迎点赞和转发分享，谢谢大家！转载的话注明来源地址为 www.54tianzhisheng.cn/2017/05/13/android-projects/ 即可。","tags":[{"name":"Android","slug":"Android","permalink":"http://www.54tianzhisheng.cn/tags/Android/"}]},{"title":"Nginx 基本知识快速入门","date":"2017-08-04T16:00:00.000Z","path":"2017/08/05/Nginx/","text":"什么是 Nginx？Nginx 是一个高性能的 HTTP 和反向代理服务器，以高稳定性、丰富的功能集、示例配置文件和低系统资源的消耗而闻名。 Nginx 特点 处理静态文件，索引文件以及自动索引；打开文件描述符缓冲． 无缓存的反向代理加速，简单的负载均衡和容错． FastCGI，简单的负载均衡和容错． 模块化的结构。包括 gzipping, byte ranges, chunked responses,以及 SSI-filter 等 filter。如果由 FastCGI 或其它代理服务器处理单页中存在的多个 SSI，则这项处理可以并行运行，而不需要相互等待。 支持 SSL 和 TLSSNI． 主要应用场合1、静态 HTTP 服务器首先，Nginx是一个 HTTP 服务器，可以将服务器上的静态文件（如 HTML、图片）通过 HTTP 协议展现给客户端。 配置： 123456server &#123; listen 80; # 端口号 location / &#123; root /usr/share/nginx/html; # 静态文件路径 &#125;&#125; 2、反向代理服务器什么是反向代理？ 客户端本来可以直接通过 HTTP 协议访问某网站应用服务器，如果网站管理员在中间加上一个 Nginx，客户端请求Nginx，Nginx 请求应用服务器，然后将结果返回给客户端，此时 Nginx 就是反向代理服务器。 配置： 123456server &#123; listen 80; location / &#123; proxy_pass http://192.168.20.1:8080; # 应用服务器HTTP地址 &#125;&#125; 既然服务器可以直接 HTTP 访问，为什么要在中间加上一个反向代理，不是多此一举吗？反向代理有什么作用？继续往下看，下面的负载均衡、虚拟主机，都基于反向代理实现，当然反向代理的功能也不仅仅是这些。 3、负载均衡当网站访问量非常大，网站站长开心赚钱的同时，也摊上事儿了。因为网站越来越慢，一台服务器已经不够用了。于是将相同的应用部署在多台服务器上，将大量用户的请求分配给多台机器处理。同时带来的好处是，其中一台服务器万一挂了，只要还有其他服务器正常运行，就不会影响用户使用。 当我们网站进行大的升级更新时，我们不可能直接将所有的服务器都关掉，然后再升级的。通常我们都是批量的关掉一些服务器，去升级网站，当有用户的请求时则分配给其他还在运作的机器处理。当之前关掉的机器更新完成后，再次开启，然后又批量关掉部分机器，如上循环，直到最后全部机器都更新完成。这样就不会影响用户使用。 Nginx 可以通过反向代理来实现负载均衡。 配置： 12345678910upstream myapp &#123; server 192.168.20.1:8080; # 应用服务器1 server 192.168.20.2:8080; # 应用服务器2&#125;server &#123; listen 80; location / &#123; proxy_pass http://myapp; &#125;&#125; 4、虚拟主机网站访问量大，需要负载均衡。然而并不是所有网站都如此出色，有的网站，由于访问量太小，需要节省成本，将多个网站部署在同一台服务器上。 例如将 www.aaa.com 和 www.bbb.com 两个网站部署在同一台服务器上，两个域名解析到同一个 IP 地址，但是用户通过两个域名却可以打开两个完全不同的网站，互相不影响，就像访问两个服务器一样，所以叫两个虚拟主机。 配置： 12345678910111213141516171819server &#123; listen 80 default_server; server_name _; return 444; # 过滤其他域名的请求，返回444状态码&#125;server &#123; listen 80; server_name www.aaa.com; # www.aaa.com域名 location / &#123; proxy_pass http://localhost:8080; # 对应端口号8080 &#125;&#125;server &#123; listen 80; server_name www.bbb.com; # www.bbb.com域名 location / &#123; proxy_pass http://localhost:8081; # 对应端口号8081 &#125;&#125; 在服务器 8080 和 8081 两个端口分别开了一个应用，客户端通过不同的域名访问，根据 server_name 可以反向代理到对应的应用服务器。 虚拟主机的原理是通过 HTTP 请求头中的 Host 是否匹配 server_name 来实现的，有兴趣的同学可以研究一下 HTTP 协议。 另外，server_name 配置还可以过滤有人恶意将某些域名指向你的主机服务器。 5、FastCGINginx 本身不支持 PHP 等语言，但是它可以通过 FastCGI 来将请求扔给某些语言或框架处理（例如 PHP、Python、Perl）。 123456789server &#123; listen 80; location ~ \\.php$ &#123; include fastcgi_params; fastcgi_param SCRIPT_FILENAME /PHP文件路径$fastcgi_script_name; # PHP文件路径 fastcgi_pass 127.0.0.1:9000; # PHP-FPM地址和端口号 # 另一种方式：fastcgi_pass unix:/var/run/php5-fpm.sock; &#125;&#125; 配置中将 .php 结尾的请求通过 FashCGI 交给 PHP-FPM 处理，PHP-FPM 是 PHP 的一个 FastCGI 管理器。有关FashCGI 可以查阅其他资料，本文不再介绍。 fastcgi_pass 和 proxy_pass 有什么区别？下面一张图带你看明白： 参考资料1、Nginx基本功能极速入门 2、Nginx 学习笔记","tags":[{"name":"Nginx","slug":"Nginx","permalink":"http://www.54tianzhisheng.cn/tags/Nginx/"}]},{"title":"秋招第三站 —— 内推阿里（一面）","date":"2017-08-03T16:00:00.000Z","path":"2017/08/04/alibaba/","text":"3、阿里巴巴（菜鸟网络部门）（一面 49 分钟）2017.08.02 晚上9点21打电话过来，预约明天什么时候有空面试，约好第二天下午两点。 2017.08.03 下午两点10分打过来了。 说看了我的博客和 GitHub，觉得我学的还行，知识广度都还不错，但是还是要问问具体情况，为什么没看到你春招的记录，什么原因没投阿里？非得说一个原因，那就是：我自己太菜了，不敢投。1、先自我介绍 2、什么是多态？哪里体现了多态的概念？ 3、HashMap 源码分析，把里面的东西问了个遍？最后问是不是线程安全？引出 ConcurrentHashMap 4、ConcurrentHashMap 源码分析 5、类加载，双亲委托机制 6、Java内存模型（一开始说的不是他想要的，主要想问我堆和栈的细节） 7、垃圾回收算法 8、线程池，自己之前看过，所以说的比较多，最后面试官说了句：看你对线程池了解还是很深了 9、事务的四种特性 10、什么是死锁？ 11、乐观锁和悲观锁的策略 12、高可用网站的设计（有什么技术实现） 13、低耦合高内聚 14、设计模式了解不？你用过哪几种，为什么用，单例模式帮我们做什么东西？有什么好处？ 15、你参与什么项目中成长比较快？学到了什么东西，以前是没有学过的？ 16、项目中遇到的最大困难是怎样的？是怎么解决的？ 17、智力题（两根不均匀的香，点一头烧完要一个小时，怎么确定15分钟） 18、你有什么问题想要问我的？ 19、问了菜鸟网络他们部门主要做什么？ 20、对我这次面试做个评价：看了你博客和 GitHub，知道你对学习的热情还是很高的，花了不少功夫，但是有些东西还是需要加强深度，阿里需要那种对技术有深度，有自己独到见解的人才。意思就是 GG 了。 总结：面试总的来说，第一次电话面试，感觉好紧张，好多问题自己会点，但是其中的细节没弄清楚，自己准备的也不够充分。面试官很友好，看到我紧张，也安慰我说不要紧，不管以后出去面试啥的，不需要紧张，公司问的问题可能很广，你只需要把你知道的说出来就行，不会的直接说不会就行。之前一直不敢投阿里，因为自己准备的完全不够充分，但是在朋友磊哥的帮助下，还是试了下，不管结果怎么样，经历过总比没有的好。","tags":[{"name":"面经","slug":"面经","permalink":"http://www.54tianzhisheng.cn/tags/面经/"}]},{"title":"秋招第二站 —— 内推爱奇艺（一面二面）","date":"2017-08-03T16:00:00.000Z","path":"2017/08/04/iqiyi/","text":"第 2 站 、爱奇艺 后端 Java 开发实习生笔试（半个小时）题目：（记得一些） 1、重载重写的区别？ 2、转发和重定向的区别？3、画下 HashMap 的结构图？HashMap 、 HashTable 和 ConcurrentHashMap 的区别？ 4、statement 和 preparedstatement 区别？ 5、JSP 中一个 中取值与直接取值的区别？会有什么安全问题？ 6、实现一个线程安全的单例模式 7、一个写 sql 语句的题目 8、自己实现一个 List，（主要实现 add等常用方法） 9、Spring 中 IOC 和 AOP 的理解？ 10、两个对象的 hashcode 相同，是否对象相同？equal() 相同呢？ 11、@RequestBody 和 @ResponseBody 区别？ 12、JVM 一个错误，什么情况下会发生？ 13、常用的 Linux 命令？ 第一轮面试（80 分钟）1、自我介绍 2、介绍你最熟悉的一个项目 3、讲下这个 XSS 攻击 4、HashMap 的结构？HashMap 、 HashTable 和 ConcurrentHashMap 的区别？ 5、HashMap 中怎么解决冲突的？（要我详细讲下） 6、ConcurrentHashMap 和 HashTable 中线程安全的区别？为啥建议用 ConcurrentHashMap ？能把 ConcurrentHashMap 里面的实现详细的讲下吗？ 7、Session 和 Cookie 的区别？ 8、你项目中登录是怎样做的，用的 Cookie 和 Session？ 9、讲讲你对 Spring 中的 IOC 和 AOP 的理解？ 10、问了好几个注解的作用？ 11、statement 和 preparedstatement 区别？ 12、$ 和 # 的区别？以及这两个在哪些地方用？ 13、前面项目介绍了数据是爬虫爬取过来的，那你讲讲你的爬虫是多线程的吧？ 14、讲讲 Python 中的多线程和 Java 中的多线程区别？ 15、自己刚好前几天在看线程池，立马就把面试官带到我熟悉的线程池，和面试官讲了下 JDK 自带的四种线程池、ThreadPoolExecutor 类中的最重要的构造器里面的七个参数，然后再讲了下线程任务进入线程池和核心线程数、缓冲队列、最大线程数量比较。 16、线程同步，你了解哪几种方式？ 17、讲下 Synchronized？ 18、讲下 RecentLock 可重入锁？ 什么是可重入锁？为什么要设计可重入锁？ 19、讲下 Volatile 吧？他是怎样做到同步的？ 20、Volatile 为什么不支持原子性？举个例子 21、Atomic 怎么设计的？（没看过源码，当时回答错了，后来才发现里面全部用 final 修饰的属性和方法） 22、问几个前端的标签吧？（问了一个不会，直接说明我偏后端，前端只是了解，后面就不问了） 23、SpringBoot 的了解？ 24、Linux 常用命令？ 25、JVM 里的几个问题？ 26、事务的特性？ 27、隔离级别？ 28、网络状态码？以 2、3、4、5 开头的代表什么意思。 29、并发和并行的区别？ 30、你有什么问题想问我的？ 一面面完后面试官和说这份试卷是用来考 1~3 年开发工作经验的，让我准备一下，接下来的二面。 第二轮面试（半个小时）1、一上来就问怎么简历名字都没有，我指了简历第一行的我的名字，还特意大写了，然后就问学校是不是在上海，我回答在南昌（感觉被鄙视了一波，后面我在回答问题的时候面试官就一直在玩手机，估计后面对我的印象就不是很好了） 2、自我介绍 3、说一说数据库建表吧（从范式讲） 4、讲讲多态？（这个我答出来了，可是面试官竟然说不是这样吧，可能面试官没听请，后面还说我是不是平时写多态比较少，感觉这个也让面试官对我印象减分） 5、将两个数转换（不借助第三个参数） 6、手写个插入排序吧（写完了和面试官讲了下执行流程） 7、讲讲你对 Spring 中的 IOC 和 AOP 的理解？ 8、问了几个常用的 Linux 命令？ 9、也问到多线程？和一面一样把自己最近看的线程池也讲了一遍 10、学 Java 多久了？ 11、你有什么想问的？ 总结：面试题目大概就是这么多了，有些问题自己也忘记了，面试题目顺序不一定是按照上面所写的。再次感谢爱奇艺的第一面面试官了，要不是他帮忙内推的，我可能还没有机会收到面试机会。自己接到爱奇艺面试邀请电话是星期一晚上快7点中的，之后加了面试官微信约好了星期四面试的（时间准备较短，之前没系统的复习过）。星期四一大早（5点就起床了），然后就收拾了下，去等公交车，转了两次车，然后再做地铁去爱奇艺公司的，总共路上花费时间四个多小时。总的来说，这次面试准备的时间不是很充裕，所以准备的个人觉得不是很好，通过这次的面试，发现面试还是比较注重基础和深度的，我也知道了自己的一些弱处，还需要在哪里加强，面试技巧上也要掌握些。为后面的其他公司继续做好充足的准备。加油！！！","tags":[{"name":"面经","slug":"面经","permalink":"http://www.54tianzhisheng.cn/tags/面经/"}]},{"title":"秋招第一站 —— 亚信科技","date":"2017-08-03T16:00:00.000Z","path":"2017/08/04/yaxin/","text":"第 1 站、亚信科技 Java 开发1）自我介绍（说到一个亮点：长期坚持写博客，面试官觉得这个习惯很好，算加分项吧） 2）看到简历项目中用到 Solr，详细的问了下 Solr（自己介绍了下 Solr 的使用场景和建立索引等东西）3）项目里面写了一个 “ 敏感词和 JS 标签过滤防 XSS 攻击”，面试官让我讲了下这个 XSS 攻击，并且是怎样实现的 4）项目里写了支持 Markdown，问是不是自己写的解析代码，（回答不是，自己引用的是 GitHub上的一个开源项目解析的） 5）想问我前端的知识，我回复到：自己偏后端开发，前端只是了解，然后面试官就不问了 6）问我考不考研？ 7）觉得杭州怎么样？是打算就呆在杭州还是把杭州作为一个跳板？ 8）有啥小目标？以后是打算继续技术方向，还是先技术后管理（还开玩笑的说：是不是赚他几个亿，当时我笑了笑） 9）有啥兴趣爱好？ 大概就记得这么多了，目前已经拿到 Offer 了。 总结：面试问的问题不算多，主要是通过简历上项目所涉及的东西提问的，如果自己不太会的切记不要写上去。面试主要考察你回答问题来判断你的逻辑是否很清楚。","tags":[{"name":"面经","slug":"面经","permalink":"http://www.54tianzhisheng.cn/tags/面经/"}]},{"title":"Java 线程池艺术探索","date":"2017-07-28T16:00:00.000Z","path":"2017/07/29/ThreadPool/","text":"线程池Wiki 上是这样解释的：Thread Pool 作用：利用线程池可以大大减少在创建和销毁线程上所花的时间以及系统资源的开销！ 下面主要讲下线程池中最重要的一个类 ThreadPoolExecutor 。 ThreadPoolExecutor ThreadPoolExecutor 构造器： 有四个构造器的，挑了参数最长的一个进行讲解。 七个参数： corePoolSize：核心池的大小，在创建了线程池后，默认情况下，线程池中并没有任何线程，而是等待有任务到来才创建线程去执行任务，当有任务来之后，就会创建一个线程去执行任务，当线程池中的线程数目达到corePoolSize后，就会把到达的任务放到缓存队列当中； maximumPoolSize：线程池最大线程数； keepAliveTime：表示线程没有任务执行时最多保持多久时间会终止； unit：参数keepAliveTime的时间单位（DAYS、HOURS、MINUTES、SECONDS 等）； workQueue：阻塞队列，用来存储等待执行的任务； ArrayBlockingQueue （有界队列） LinkedBlockingQueue （无界队列） SynchronousQueue threadFactory：线程工厂，主要用来创建线程 handler：拒绝处理任务的策略 AbortPolicy：丢弃任务并抛出 RejectedExecutionException 异常。（默认这种） DiscardPolicy：也是丢弃任务，但是不抛出异常 DiscardOldestPolicy：丢弃队列最前面的任务，然后重新尝试执行任务（重复此过程） CallerRunsPolicy：由调用线程处理该任务 重要方法： execute()：通过这个方法可以向线程池提交一个任务，交由线程池去执行； shutdown()：关闭线程池； execute() 方法： 注：JDK 1.7 和 1.8 这个方法有点区别，下面代码是 1.8 中的。 1234567891011121314151617181920212223public void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); int c = ctl.get(); //1、如果当前的线程数小于核心线程池的大小，根据现有的线程作为第一个 Worker 运行的线程，新建一个 Worker，addWorker 自动的检查当前线程池的状态和 Worker 的数量，防止线程池在不能添加线程的状态下添加线程 if (workerCountOf(c) &lt; corePoolSize) &#123; if (addWorker(command, true)) return; c = ctl.get(); &#125; //2、如果线程入队成功，然后还是要进行 double-check 的，因为线程在入队之后状态是可能会发生变化的 if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; int recheck = ctl.get(); // recheck 防止线程池状态的突变，如果突变，那么将 reject 线程，防止 workQueue 中增加新线程 if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0)//上下两个操作都有 addWorker 的操作，但是如果在workQueue.offer 的时候 Worker 变为 0，那么将没有 Worker 执行新的 task，所以增加一个 Worker. addWorker(null, false); &#125; //3、如果 task 不能入队(队列满了)，这时候尝试增加一个新线程，如果增加失败那么当前的线程池状态变化了或者线程池已经满了然后拒绝task else if (!addWorker(command, false)) reject(command); &#125; 其中调用了 addWorker() 方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768private boolean addWorker(Runnable firstTask, boolean core) &#123;// firstTask: 新增一个线程并执行这个任务，可空，增加的线程从队列获取任务；core：是否使用 corePoolSize 作为上限，否则使用 maxmunPoolSize retry: for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. /** * rs!=Shutdown || fistTask！=null || workQueue.isEmpty * 如果当前的线程池的状态 &gt; SHUTDOWN 那么拒绝 Worker 的 add 如果 =SHUTDOWN * 那么此时不能新加入不为 null 的 Task，如果在 workQueue 为 empty 的时候不能加入任何类型的 Worker， * 如果不为 empty 可以加入 task 为 null 的 Worker, 增加消费的 Worker */ if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp;! workQueue.isEmpty())) return false; for (;;) &#123; int wc = workerCountOf(c); //如果当前的数量超过了 CAPACITY，或者超过了 corePoolSize 和 maximumPoolSize（试 core 而定） if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; //CAS 尝试增加线程数，如果失败，证明有竞争，那么重新到 retry。 if (compareAndIncrementWorkerCount(c))// AtomicInteger 的 CAS 操作; break retry; c = ctl.get(); // Re-read ctl //判断当前线程池的运行状态,状态发生改变，重试 retry; if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop &#125; &#125; boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try &#123; w = new Worker(firstTask);// Worker 为内部类，封装了线程和任务，通过 ThreadFactory 创建线程，可能失败抛异常或者返回 null final Thread t = w.thread; if (t != null) &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int rs = runStateOf(ctl.get()); if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) // precheck that t is startable // SHUTDOWN 以后的状态和 SHUTDOWN 状态下 firstTask 为 null，不可新增线程 throw new IllegalThreadStateException(); workers.add(w); int s = workers.size(); if (s &gt; largestPoolSize) largestPoolSize = s;//记录最大线程数 workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; if (workerAdded) &#123; t.start(); workerStarted = true; &#125; &#125; &#125; finally &#123; if (! workerStarted) addWorkerFailed(w);//失败回退,从 wokers 移除 w, 线程数减一，尝试结束线程池(调用tryTerminate 方法) &#125; return workerStarted; &#125; 示意图： 执行流程： 1、当有任务进入时，线程池创建线程去执行任务，直到核心线程数满为止 2、核心线程数量满了之后，任务就会进入一个缓冲的任务队列中 当任务队列为无界队列时，任务就会一直放入缓冲的任务队列中，不会和最大线程数量进行比较 当任务队列为有界队列时，任务先放入缓冲的任务队列中，当任务队列满了之后，才会将任务放入线程池，此时会与线程池中最大的线程数量进行比较，如果超出了，则默认会抛出异常。然后线程池才会执行任务，当任务执行完，又会将缓冲队列中的任务放入线程池中，然后重复此操作。 shutdown() 方法： 1234567891011121314151617public void shutdown() &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; //判断是否可以操作目标线程 checkShutdownAccess(); //设置线程池状态为 SHUTDOWN, 此处之后，线程池中不会增加新 Task advanceRunState(SHUTDOWN); //中断所有的空闲线程 interruptIdleWorkers(); onShutdown(); // hook for ScheduledThreadPoolExecutor &#125; finally &#123; mainLock.unlock(); &#125; //转到 Terminate tryTerminate(); &#125; 参考资料：深入理解java线程池—ThreadPoolExecutor JDK 自带四种线程池分析与比较 1、newFixedThreadPool创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。 2、newSingleThreadExecutor创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 3、newCachedThreadPool创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。 4、newScheduledThreadPool创建一个定长线程池，支持定时及周期性任务执行。 四种线程池其实内部方法都是调用的 ThreadPoolExecutor 类，只不过利用了其不同的构造器方法而已（传入自己需要传入的参数），那么利用这个特性，我们自己也是可以实现自己定义的线程池的。 自定义线程池1、创建任务类 12345678910111213141516171819202122232425262728293031323334353637package com.zhisheng.thread.threadpool.demo;/** * Created by 10412 on 2017/7/24. * 任务 */public class MyTask implements Runnable&#123; private int taskId; //任务 id private String taskName; //任务名字 public int getTaskId() &#123; return taskId; &#125; public void setTaskId(int taskId) &#123; this.taskId = taskId; &#125; public String getTaskName() &#123; return taskName; &#125; public void setTaskName(String taskName) &#123; this.taskName = taskName; &#125; public MyTask(int taskId, String taskName) &#123; this.taskId = taskId; this.taskName = taskName; &#125; @Override public void run() &#123; System.out.println(\"当前正在执行 ****** 线程Id--&gt;\" + taskId + \",任务名称--&gt;\" + taskName); try &#123; Thread.currentThread().sleep(5 * 1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(\"线程Id--&gt;\" + taskId + \",任务名称--&gt;\" + taskName + \" ----------- 执行完毕！\"); &#125;&#125; 2、自定义拒绝策略，实现 RejectedExecutionHandler 接口，重写 rejectedExecution 方法 12345678910111213141516package com.zhisheng.thread.threadpool.demo;import java.util.concurrent.RejectedExecutionHandler;import java.util.concurrent.ThreadPoolExecutor;/** * Created by 10412 on 2017/7/24. * 自定义拒绝策略，实现 RejectedExecutionHandler 接口 */public class RejectedThreadPoolHandler implements RejectedExecutionHandler&#123; public RejectedThreadPoolHandler() &#123; &#125; @Override public void rejectedExecution(Runnable r, ThreadPoolExecutor executor) &#123; System.out.println(\"WARNING 自定义拒绝策略: Task \" + r.toString() + \" rejected from \" + executor.toString()); &#125;&#125; 3、创建线程池 1234567891011121314151617181920212223242526272829303132package com.zhisheng.thread.threadpool.demo;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;/** * Created by 10412 on 2017/7/24. */public class ThreadPool&#123; public static void main(String[] args) &#123; //核心线程数量为 2，最大线程数量 4，空闲线程存活的时间 60s，有界队列长度为 3, //ThreadPoolExecutor pool = new ThreadPoolExecutor(2, 4, 60, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;&gt;(3)); //核心线程数量为 2，最大线程数量 4，空闲线程存活的时间 60s， 无界队列, //ThreadPoolExecutor pool = new ThreadPoolExecutor(2, 4, 60L, TimeUnit.SECONDS, new LinkedBlockingDeque&lt;&gt;()); //核心线程数量为 2，最大线程数量 4，空闲线程存活的时间 60s，有界队列长度为 3, 使用自定义拒绝策略 ThreadPoolExecutor pool = new ThreadPoolExecutor(2, 4, 60, TimeUnit.SECONDS, new ArrayBlockingQueue&lt;Runnable&gt;(3), new RejectedThreadPoolHandler()); for (int i = 1; i &lt;= 10; i++) &#123; //创建 10 个任务 MyTask task = new MyTask(i, \"任务\" + i); //运行 pool.execute(task); System.out.println(\"活跃的线程数：\"+pool.getActiveCount() + \",核心线程数：\" + pool.getCorePoolSize() + \",线程池大小：\" + pool.getPoolSize() + \",队列的大小\" + pool.getQueue().size()); &#125; //关闭线程池 pool.shutdown(); &#125;&#125; 这里运行结果就不截图了，我在本地测试了代码是没问题的，感兴趣的建议还是自己跑一下，然后分析下结果是不是和前面分析的一样，如有问题，请在我博客下面评论！ 总结本文一开始讲了线程池的介绍和好处，然后分析了线程池中最核心的 ThreadPoolExecutor 类中构造器的七个参数的作用、类中两个重要的方法，然后在对比研究了下 JDK 中自带的四种线程池的用法和内部代码细节，最后写了一个自定义的线程池。","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"多线程","slug":"多线程","permalink":"http://www.54tianzhisheng.cn/tags/多线程/"},{"name":"线程池","slug":"线程池","permalink":"http://www.54tianzhisheng.cn/tags/线程池/"}]},{"title":"Java 性能调优需要格外注意的细节","date":"2017-07-24T16:00:00.000Z","path":"2017/07/25/Java-performance-tuning/","text":"昨天写了篇文章 《MySQL 处理海量数据时的一些优化查询速度方法》 ，其实开发中不止数据库要优化，还有我们本身的开发代码也需要优化，这样我们开发的产品才能够得到极致的体验。也许有些人认为这些细小的地方没有啥好修改的，改与不改对运行效率没啥大的影响？首先在我们本地一个人测试下效率是不怎么明显，但是如果到发布上线后，你的用户有几百万，甚至上千万，这些用户同时访问你的网站，那么你的网站是否经得住考验呢，效率那高不高呢，如果效率不高，那么需要多出很多买服务器的经费呢，所以想想还是很有必要注意这些小的细节。今天就讲讲一些 Java 性能调优需要格外注意的一些细节。 代码优化的目标： 减少代码的体积 提高代码的运行效率 代码优化细节1、尽量指定类、方法的final修饰符 带有final修饰符的类是不可派生的。在Java核心API中，有许多应用final的例子，例如java.lang.String，整个类都是final的。为类指定final修饰符可以让类不可以被继承，为方法指定final修饰符可以让方法不可以被重写。如果指定了一个类为final。Java编译器会寻找机会内联所有的final方法，内联对于提升Java运行效率作用重大，具体参见Java运行期优化。 此举能够使性能平均提高50% 。 2、尽量重用对象 特别是String对象的使用，出现字符串连接时应该使用StringBuilder/StringBuffer代替。由于Java虚拟机不仅要花时间生成对象，以后可能还需要花时间对这些对象进行垃圾回收和处理，因此，生成过多的对象将会给程序的性能带来很大的影响。 3、尽可能使用局部变量 调用方法时传递的参数以及在调用中创建的临时变量都保存在栈中速度较快，其他变量，如静态变量、实例变量等，都在堆中创建，速度较慢。另外，栈中创建的变量，随着方法的运行结束，这些内容就没了，不需要额外的垃圾回收。 4、及时关闭流 Java编程过程中，进行数据库连接、I/O流操作时务必小心，在使用完毕后，及时关闭以释放资源。因为对这些大对象的操作会造成系统大的开销，稍有不慎，将会导致严重的后果。 5、尽量减少对变量的重复计算 明确一个概念，对方法的调用，即使方法中只有一句语句，也是有消耗的，包括创建栈帧、调用方法时保护现场、调用方法完毕时恢复现场等。所以例如下面的操作： 12for (int i = 0; i &lt; list.size(); i++)&#123;...&#125; 建议替换为： 12for (int i = 0, int length = list.size(); i &lt; length; i++)&#123;...&#125; 这样，在list.size()很大的时候，就减少了很多的消耗 6、尽量采用懒加载的策略，即在需要的时候才创建 例如： 1234String str = \"aaa\";if (i == 1)&#123; list.add(str);&#125; 建议替换为： 12345if (i == 1)&#123; String str = \"aaa\"; list.add(str);&#125; 7、慎用异常 异常对性能不利。抛出异常首先要创建一个新的对象，Throwable接口的构造函数调用名为 fillInStackTrace() 的本地同步方法，fillInStackTrace() 方法检查堆栈，收集调用跟踪信息。只要有异常被抛出，Java虚拟机就必须调整调用堆栈，因为在处理过程中创建了一个新的对象。异常只能用于错误处理，不应该用来控制程序流程。 8、不要在循环中使用 try…catch…，应该把其放在最外层 除非不得已。如果毫无理由地这么写了，只要你的领导资深一点、有强迫症一点，八成就要骂你为什么写出这种垃圾代码来了。 9、如果能估计到待添加的内容长度，为底层以数组方式实现的集合、工具类指定初始长度 比如 ArrayList、LinkedLlist、StringBuilder、StringBuffer、HashMap、HashSet 等等，以 StringBuilder 为例： （1）StringBuilder() // 默认分配16个字符的空间 （2）StringBuilder(int size) // 默认分配size个字符的空间 （3）StringBuilder(String str) // 默认分配16个字符+str.length()个字符空间 可以通过类（这里指的不仅仅是上面的 StringBuilder ）的来设定它的初始化容量，这样可以明显地提升性能。比如 StringBuilder 吧，length 表示当前的 StringBuilder 能保持的字符数量。因为当 StringBuilder 达到最大容量的时候，它会将自身容量增加到当前的2倍再加2，无论何时只要 StringBuilder 达到它的最大容量，它就不得不创建一个新的字符数组然后将旧的字符数组内容拷贝到新字符数组中—-这是十分耗费性能的一个操作。试想，如果能预估到字符数组中大概要存放5000个字符而不指定长度，最接近5000的2次幂是4096，每次扩容加的2不管，那么： （1）在4096 的基础上，再申请8194个大小的字符数组，加起来相当于一次申请了12290个大小的字符数组，如果一开始能指定5000个大小的字符数组，就节省了一倍以上的空间； （2）把原来的4096个字符拷贝到新的的字符数组中去。 这样，既浪费内存空间又降低代码运行效率。所以，给底层以数组实现的集合、工具类设置一个合理的初始化容量是错不了的，这会带来立竿见影的效果。但是，注意，像HashMap这种是以数组+链表实现的集合，别把初始大小和你估计的大小设置得一样，因为一个table上只连接一个对象的可能性几乎为0。初始大小建议设置为2的N次幂，如果能估计到有2000个元素，设置成 new HashMap(128)、new HashMap(256) 都可以。 10、当复制大量数据时，使用 System.arraycopy() 命令 11、乘法和除法使用移位操作 例如： 12345for (val = 0; val &lt; 100000; val += 5)&#123; a = val * 8; b = val / 2;&#125; 用移位操作可以极大地提高性能，因为在计算机底层，对位的操作是最方便、最快的，因此建议修改为： 12345for (val = 0; val &lt; 100000; val += 5)&#123; a = val &lt;&lt; 3; b = val &gt;&gt; 1;&#125; 移位操作虽然快，但是可能会使代码不太好理解，因此最好加上相应的注释。 12、循环内不要不断创建对象引用 例如： 1234for (int i = 1; i &lt;= count; i++)&#123; Object obj = new Object();&#125; 这种做法会导致内存中有count份Object对象引用存在，count很大的话，就耗费内存了，建议为改为： 12345Object obj = null;for (int i = 0; i &lt;= count; i++) &#123; obj = new Object(); &#125; 这样的话，内存中只有一份 Object 对象引用，每次 new Object() 的时候，Object 对象引用指向不同的Object罢了，但是内存中只有一份，这样就大大节省了内存空间了。 13、基于效率和类型检查的考虑，应该尽可能使用array，无法确定数组大小时才使用 ArrayList 14、尽量使用 HashMap、ArrayList、StringBuilder，除非线程安全需要，否则不推荐使用 Hashtable、Vector、StringBuffer，后三者由于使用同步机制而导致了性能开销 15、不要将数组声明为 public static final 因为这毫无意义，这样只是定义了引用为 static final，数组的内容还是可以随意改变的，将数组声明为 public 更是一个安全漏洞，这意味着这个数组可以被外部类所改变。 16、尽量在合适的场合使用单例 使用单例可以减轻加载的负担、缩短加载的时间、提高加载的效率，但并不是所有地方都适用于单例，简单来说，单例主要适用于以下三个方面： （1）控制资源的使用，通过线程同步来控制资源的并发访问 （2）控制实例的产生，以达到节约资源的目的 （3）控制数据的共享，在不建立直接关联的条件下，让多个不相关的进程或线程之间实现通信 17、尽量避免随意使用静态变量 要知道，当某个对象被定义为static的变量所引用，那么gc通常是不会回收这个对象所占有的堆内存的，如： 1234public class A&#123; private static B b = new B();&#125; 此时静态变量b的生命周期与A类相同，如果A类不被卸载，那么引用B指向的B对象会常驻内存，直到程序终止 18、及时清除不再需要的会话 为了清除不再活动的会话，许多应用服务器都有默认的会话超时时间，一般为30分钟。当应用服务器需要保存更多的会话时，如果内存不足，那么操作系统会把部分数据转移到磁盘，应用服务器也可能根据MRU（最近最频繁使用）算法把部分不活跃的会话转储到磁盘，甚至可能抛出内存不足的异常。如果会话要被转储到磁盘，那么必须要先被序列化，在大规模集群中，对对象进行序列化的代价是很昂贵的。因此，当会话不再需要时，应当及时调用 HttpSession 的 invalidate() 方法清除会话。 19、实现 RandomAccess 接口的集合比如 ArrayList，应当使用最普通的 for 循环而不是 foreach 循环来遍历 这是JDK推荐给用户的。JDK API对于 RandomAccess 接口的解释是：实现 RandomAccess 接口用来表明其支持快速随机访问，此接口的主要目的是允许一般的算法更改其行为，从而将其应用到随机或连续访问列表时能提供良好的性能。实际经验表明，实现 RandomAccess 接口的类实例，假如是随机访问的，使用普通 for 循环效率将高于使用 foreach 循环；反过来，如果是顺序访问的，则使用 Iterator 会效率更高。可以使用类似如下的代码作判断： 12345678910if (list instanceof RandomAccess)&#123; for (int i = 0; i &lt; list.size(); i++)&#123;&#125;&#125;else&#123; Iterator&lt;?&gt; iterator = list.iterable(); while (iterator.hasNext()) &#123; iterator.next(); &#125;&#125; foreach 循环的底层实现原理就是迭代器 Iterator，参见Java语法糖1：可变长度参数以及 foreach 循环原理。所以后半句”反过来，如果是顺序访问的，则使用 Iterator 会效率更高”的意思就是顺序访问的那些类实例，使用 foreach 循环去遍历。 20、使用同步代码块替代同步方法 这点在多线程模块中的 synchronized 锁方法块一文中已经讲得很清楚了，除非能确定一整个方法都是需要进行同步的，否则尽量使用同步代码块，避免对那些不需要进行同步的代码也进行了同步，影响了代码执行效率。 21、将常量声明为 static final，并以大写命名 这样在编译期间就可以把这些内容放入常量池中，避免运行期间计算生成常量的值。另外，将常量的名字以大写命名也可以方便区分出常量与变量 22、不要创建一些不使用的对象，不要导入一些不使用的类 这毫无意义，如果代码中出现”The value of the local variable i is not used”、”The import java.util is never used”，那么请删除这些无用的内容 23、程序运行过程中避免使用反射 关于，请参见反射。反射是Java提供给用户一个很强大的功能，功能强大往往意味着效率不高。不建议在程序运行过程中使用尤其是频繁使用反射机制，特别是Method的invoke方法，如果确实有必要，一种建议性的做法是将那些需要通过反射加载的类在项目启动的时候通过反射实例化出一个对象并放入内存—-用户只关心和对端交互的时候获取最快的响应速度，并不关心对端的项目启动花多久时间。 24、使用数据库连接池和线程池 这两个池都是用于重用对象的，前者可以避免频繁地打开和关闭连接，后者可以避免频繁地创建和销毁线程 25、使用带缓冲的输入输出流进行IO操作 带缓冲的输入输出流，即BufferedReader、BufferedWriter、BufferedInputStream、BufferedOutputStream，这可以极大地提升IO效率 26、顺序插入和随机访问比较多的场景使用ArrayList，元素删除和中间插入比较多的场景使用LinkedList这个，理解ArrayList和LinkedList的原理就知道了 27、不要让public方法中有太多的形参 public方法即对外提供的方法，如果给这些方法太多形参的话主要有两点坏处： 1、违反了面向对象的编程思想，Java讲求一切都是对象，太多的形参，和面向对象的编程思想并不契合 2、参数太多势必导致方法调用的出错概率增加 至于这个”太多”指的是多少个，3、4个吧。比如我们用JDBC写一个insertStudentInfo方法，有10个学生信息字段要插如Student表中，可以把这10个参数封装在一个实体类中，作为insert方法的形参。 28、字符串变量和字符串常量equals的时候将字符串常量写在前面 这是一个比较常见的小技巧了，如果有以下代码： 12String str = \"123\";if (str.equals(\"123\")) &#123;...&#125; 建议修改为： 12345String str = \"123\";if (\"123\".equals(str))&#123;...&#125; 这么做主要是可以避免空指针异常 29、请知道，在java中if (i == 1)和if (1 == i)是没有区别的，但从阅读习惯上讲，建议使用前者 平时有人问，”if (i == 1)”和”if (1== i)”有没有区别，这就要从C/C++讲起。 在C/C++中，”if (i == 1)”判断条件成立，是以0与非0为基准的，0表示false，非0表示true，如果有这么一段代码： 123456int i = 2;if (i == 1)&#123;...&#125;else&#123;...&#125; C/C++判断”i==1″不成立，所以以0表示，即false。但是如果： 1234567int i = 2;if (i = 1)&#123; ... &#125;else&#123;... &#125; 万一程序员一个不小心，把”if (i == 1)”写成”if (i = 1)”，这样就有问题了。在if之内将i赋值为1，if判断里面的内容非0，返回的就是true了，但是明明i为2，比较的值是1，应该返回的false。这种情况在C/C++的开发中是很可能发生的并且会导致一些难以理解的错误产生，所以，为了避免开发者在if语句中不正确的赋值操作，建议将if语句写为： 123456int i = 2;if (1 == i) &#123;... &#125;else&#123;... &#125; 这样，即使开发者不小心写成了”1 = i”，C/C++编译器也可以第一时间检查出来，因为我们可以对一个变量赋值i为1，但是不能对一个常量赋值1为i。 但是，在Java中，C/C++这种”if (i = 1)”的语法是不可能出现的，因为一旦写了这种语法，Java就会编译报错”Type mismatch: cannot convert from int to boolean”。但是，尽管Java的”if (i == 1)”和”if (1 == i)”在语义上没有任何区别，但是从阅读习惯上讲，建议使用前者会更好些。 30、不要对数组使用toString()方法 看一下对数组使用toString()打印出来的是什么： 12345public static void main(String[] args)&#123; int[] is = new int[]&#123;1, 2, 3&#125;; System.out.println(is.toString());&#125; 结果是： 1[I@18a992f 本意是想打印出数组内容，却有可能因为数组引用is为空而导致空指针异常。不过虽然对数组toString()没有意义，但是对集合toString()是可以打印出集合里面的内容的，因为集合的父类AbstractCollections重写了Object的toString()方法。 31、不要对超出范围的基本数据类型做向下强制转型 这绝不会得到想要的结果： 12345public static void main(String[] args)&#123; long l = 12345678901234L;int i = (int)l; System.out.println(i);&#125; 我们可能期望得到其中的某几位，但是结果却是： 1942892530 解释一下。Java中long是8个字节64位的，所以12345678901234在计算机中的表示应该是： 0000 0000 0000 0000 0000 1011 0011 1010 0111 0011 1100 1110 0010 1111 1111 0010 一个int型数据是4个字节32位的，从低位取出上面这串二进制数据的前32位是： 0111 0011 1100 1110 0010 1111 1111 0010 这串二进制表示为十进制1942892530，所以就是我们上面的控制台上输出的内容。从这个例子上还能顺便得到两个结论： 1、整型默认的数据类型是int，long l = 12345678901234L，这个数字已经超出了int的范围了，所以最后有一个L，表示这是一个long型数。顺便，浮点型的默认类型是double，所以定义float的时候要写成””float f = 3.5f” 2、接下来再写一句”int ii = l + i;”会报错，因为long + int是一个long，不能赋值给int 32、公用的集合类中不使用的数据一定要及时remove掉 如果一个集合类是公用的（也就是说不是方法里面的属性），那么这个集合里面的元素是不会自动释放的，因为始终有引用指向它们。所以，如果公用集合里面的某些数据不使用而不去remove掉它们，那么将会造成这个公用集合不断增大，使得系统有内存泄露的隐患。 33、把一个基本数据类型转为字符串，基本数据类型.toString()是最快的方式、String.valueOf(数据)次之、数据+””最慢 把一个基本数据类型转为一般有三种方式，我有一个Integer型数据i，可以使用i.toString()、String.valueOf(i)、i+””三种方式，三种方式的效率如何，看一个测试： 1234567891011121314151617181920212223public static void main(String[] args)&#123; int loopTime = 50000; Integer i = 0; long startTime = System.currentTimeMillis(); for (int j = 0; j &lt; loopTime; j++) &#123; String str = String.valueOf(i); &#125;System.out.println(\"String.valueOf()：\" + (System.currentTimeMillis() - startTime) + \"ms\"); startTime = System.currentTimeMillis(); for (int j = 0; j &lt; loopTime; j++) &#123; String str = i.toString(); &#125;System.out.println(\"Integer.toString()：\" + (System.currentTimeMillis() - startTime) + \"ms\");startTime = System.currentTimeMillis(); for (int j = 0; j &lt; loopTime; j++)&#123; String str = i + \"\";&#125;System.out.println(\"i + \\\"\\\"：\" + (System.currentTimeMillis() - startTime) + \"ms\");&#125; 运行结果为： 1String.valueOf()：11ms Integer.toString()：5ms i + &quot;&quot;：25ms 所以以后遇到把一个基本数据类型转为String的时候，优先考虑使用toString()方法。至于为什么，很简单： 1、String.valueOf()方法底层调用了Integer.toString()方法，但是会在调用前做空判断 2、Integer.toString()方法就不说了，直接调用了 3、i + “”底层使用了StringBuilder实现，先用append方法拼接，再用toString()方法获取字符串 三者对比下来，明显是2最快、1次之、3最慢 34、使用最有效率的方式去遍历Map 遍历Map的方式有很多，通常场景下我们需要的是遍历Map中的Key和Value，那么推荐使用的、效率最高的方式是： 123456789101112public static void main(String[] args)&#123;HashMap&lt;String, String&gt; hm = new HashMap&lt;String, String&gt;();hm.put(\"111\", \"222\");Set&lt;Map.Entry&lt;String, String&gt;&gt; entrySet = hm.entrySet();Iterator&lt;Map.Entry&lt;String, String&gt;&gt; iter = entrySet.iterator(); while (iter.hasNext())&#123;Map.Entry&lt;String, String&gt; entry = iter.next();System.out.println(entry.getKey() + \"\\t\" + entry.getValue());&#125;&#125; 如果你只是想遍历一下这个Map的key值，那用”Set keySet = hm.keySet();”会比较合适一些 35、对资源的close()建议分开操作 意思是，比如我有这么一段代码： 12345try&#123;XXX.close();YYY.close();&#125;catch (Exception e)&#123;...&#125; 建议修改为： 123456789try&#123; XXX.close(); &#125;catch (Exception e) &#123; ... &#125;try&#123; YYY.close(); &#125;catch (Exception e) &#123; ... &#125; 虽然有些麻烦，却能避免资源泄露。我想，如果没有修改过的代码，万一XXX.close()抛异常了，那么就进入了cath块中了，YYY.close()不会执行，YYY这块资源就不会回收了，一直占用着，这样的代码一多，是可能引起资源句柄泄露的。而改为上面的写法之后，就保证了无论如何XXX和YYY都会被close掉。 以上就是 Java 开发编程注意细节的全部内容了，感谢大家的阅读！","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"}]},{"title":"Spring MVC系列文章（五）：看透 Spring MVC 源代码分析与实践 ——  Spring MVC 组件分析","date":"2017-07-20T16:00:00.000Z","path":"2017/07/21/Spring-MVC03/","text":"SpringBoot 系列文章 由于星期一接到面试通知，和面试官约好了星期四面试，所以这几天没更新完这系列的文章，面完试后立马就把这个解决掉。通过这次面试，也让我懂得了很多，知道了自己的一些不足之处，后面还要继续下功夫好好的深入复习下去。这几篇文章写的我觉得还是不够仔细，感兴趣的还是建议自己去看看源码。 第 11 章 —— 组件概览HandlerMapping 根据 request 找到对应的处理器 Handler 和 Interceptors。内部只有一个方法 1HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception; HandlerAdapter Handler 适配器，内部方法如下： 123boolean supports(Object handler);//判断是否可以使用某个 HandlerModelAndView handle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception; //具体使用long getLastModified(HttpServletRequest request, Object handler);//获取资源上一次修改的时间 HandlerExceptionResolver 根据异常设置 ModelAndView ，再交给 render 方法进行渲染。 12ModelAndView resolveException( HttpServletRequest request, HttpServletResponse response, @Nullable Object handler, Exception ex) ViewResolver 用来将 String 类型的视图名和 Locale 解析为 View 类型的视图。 1View resolveViewName(String viewName, Locale locale) throws Exception; 它的一个实现类 BeanNameViewResolver，它重写 resolveViewName 方法如下: 1234567891011121314151617181920212223public View resolveViewName(String viewName, Locale locale) throws BeansException &#123; ApplicationContext context = getApplicationContext(); //如果应用上下文没有找到视图，返回 null if (!context.containsBean(viewName)) &#123; if (logger.isDebugEnabled()) &#123; logger.debug(\"No matching bean found for view name '\" + viewName + \"'\"); &#125; // Allow for ViewResolver chaining... return null; &#125; //如果找到的视图类型不匹配，也返回 null if (!context.isTypeMatch(viewName, View.class)) &#123; if (logger.isDebugEnabled()) &#123; logger.debug(\"Found matching bean for view name '\" + viewName + \"' - to be ignored since it does not implement View\"); &#125; // Since we're looking into the general ApplicationContext here, // let's accept this as a non-match and allow for chaining as well... return null; &#125; //根据视图名称从 Spring 容器中查找 Bean，返回找到的 bean return context.getBean(viewName, View.class); &#125; RequestToViewNameTranslator 获取 request 中的视图名。接口里面也是只有一个方法： 1String getViewName(HttpServletRequest request) throws Exception; //根据 request 查找视图名 LocaleResolver 用于从 request 解析出 Locale。 123456public interface LocaleResolver &#123; //从 request 解析出 Locale Locale resolveLocale(HttpServletRequest request); //根据 request 设置 locale void setLocale(HttpServletRequest request, HttpServletResponse response, @Nullable Locale locale);&#125; ThemeResolver 解析主题 123456public interface ThemeResolver &#123; //通过给定的 request 查找主题名 String resolveThemeName(HttpServletRequest request); //根据给定的 request 设置主题名 void setThemeName(HttpServletRequest request, HttpServletResponse response, String themeName);&#125; 在 RequestContext.java 文件中可以获取主题： 1234567891011121314151617public String getThemeMessage(String code, String defaultMessage) &#123; //获取主题的信息 return getTheme().getMessageSource().getMessage(code, null, defaultMessage, this.locale); &#125;public Theme getTheme() &#123; //判断主题是否为空 if (this.theme == null) &#123; // 通过 RequestContextUtils 获取 request 中的主题名 this.theme = RequestContextUtils.getTheme(this.request); if (this.theme == null) &#123; //如果还是为空的话 //那就是没有有效的主题解析器和主题 this.theme = getFallbackTheme(); &#125; &#125; return this.theme; &#125; RequestContextUtils.getTheme() 方法： 1234567891011public static Theme getTheme(HttpServletRequest request) &#123; ThemeResolver themeResolver = getThemeResolver(request); ThemeSource themeSource = getThemeSource(request); if (themeResolver != null &amp;&amp; themeSource != null) &#123; String themeName = themeResolver.resolveThemeName(request); return themeSource.getTheme(themeName); &#125; else &#123; return null; &#125; &#125; MultipartResolver 用于处理上传请求，处理方法：将普通的 request 包装成 MultipartHttpServletRequest 12345678public interface MultipartResolver &#123; //根据 request 判断是否是上传请求 boolean isMultipart(HttpServletRequest request); //将 request 包装成 MultipartHttpServletRequest MultipartHttpServletRequest resolveMultipart(HttpServletRequest request) throws MultipartException; //清理上传过程中产生的临时资源 void cleanupMultipart(MultipartHttpServletRequest request);&#125; FlashMapManager FlashMap 主要在 redirect 中传递参数，FlashMapManager 用来管理 FlashMap 的。 1234567public interface FlashMapManager &#123; //恢复参数，并将恢复过的和超时的参数从保存介质中删除 @Nullable FlashMap retrieveAndUpdate(HttpServletRequest request, HttpServletResponse response); //将参数保存起来 void saveOutputFlashMap(FlashMap flashMap, HttpServletRequest request, HttpServletResponse response);&#125; 小结介绍 Spring MVC 中九大组件的接口、作用、内部方法实现及作用进行了简单的介绍，详细的还需大家自己去看源码。 总结Spring MVC 原理总结本质是一个 Servlet，这个 Servlet 继承自 HttpServlet。Spring MVC 中提供了三个层次的 Servlet：HttpServletBean、FrameworkServlet 和 DispatcherServlet。他们相互继承， HttpServletBean 直接继承自 Java 的 HttpServlet。HttpServletBean 用于将 Servlet 中的 Servlet 中配置的参数设置到相应的属性中，FrameworkServlet 初始化了 Spring MVC 中所使用的 WebApplicationContext，具体处理请求的 9 大组件是在 DispatcherServlet 中初始化的，整个继承图如下： 最后文章可转发，但请注明原创地址，谢谢支持。","tags":[{"name":"Spring MVC","slug":"Spring-MVC","permalink":"http://www.54tianzhisheng.cn/tags/Spring-MVC/"}]},{"title":"Spring MVC系列文章（三）：看透 Spring MVC 源代码分析与实践 ——  网站基础知识","date":"2017-07-13T16:00:00.000Z","path":"2017/07/14/Spring-MVC01/","text":"网站架构及其演变过程基础结构网络传输分解方式： 标准的 OSI 参考模型 TCP/IP 参考模型 SpringBoot 系列文章 海量数据的解决方案 缓存和页面静态化 缓存 通过程序直接保存在内存中 使用缓存框架 （Encache、Redis、Memcache） 页面静态化 使用模板技术生成（Velocity、FreeMaker等） 数据库优化 表结构优化 SQL 语句优化 分区 分表 索引优化 使用存储过程代替直接操作过程 分离活跃数据 批量读取和延迟修改 读写分离 分布式数据库 NoSQL 和 Hadoop 高并发的解决方案 应用和静态资源的分离：静态文件（图片、视频、JS、CSS等）放在专门的服务器上 页面缓存（Nginx 服务器、Squid 服务器） 集群与分布式 反向代理 CDN 底层优化：网络传输协议 常见协议和标准TCP/IP 协议IP：查找地址，对应着国际互联网 TCP：规范传输规则，对应着传输层 TCP 在传输之前会进行三次沟通，称 “三次握手”，传完数据断开的时候要进行四次沟通，称 “四次挥手”。 TCP 两个序号，三个标志位含义： seq：表示所传数据的序号。TCP 传输时每一个字节都有一个序号，发送数据的时候会将数据的第一个序号发送给对方，接收方会按序号检查是否接收完整了，如果没接收完整就需要重新传送，这样就可以保证数据的完整性。 ack：表示确认号。接收端用它来给发送端反馈已经成功接收到的数据信息，它的值为希望接收的下一个数据包起始序号。 ACK：确认位，只有 ACK = 1 的时候 ack 才起作用。正常通信时 ACK 为 1，第一次发起请求时因为没有需要确认接收的数据所以 ACK 为 0。 SYN：同步位，用于在建立连接时同步序号。刚开始建立连接时并没有历史接收的数据，所以 ack 也就没有办法设置，这是按照正常的机制就无法运行了，SYN 的作用就是解决这个问题的，当接收端接收到 SYN = 1 的报文时就会直接将 ack 设置为接收到的 seq + 1 的值，注意这里的值并不是检验后设置的，而是根据 SYN 直接设置的，这样正常的机制就可以运行了，所以 SYN 叫同步位。SYN 会在前两次握手时都为 1，这是因为通信的双方的 ack 都需要设置一个初始值。 FIN：终止位，用来在数据传输完毕后释放连接。 DNS 的设置DNS 解析参考域名设置，如下是我在腾讯云域名的设置 记录类型： A记录： 将域名指向一个IPv4地址（例如：8.8.8.8）CNAME：将域名指向另一个域名（例如 www.54tianzhisheng.cn）MX： 将域名指向邮件服务器地址TXT： 可任意填写，长度限制255，通常做SPF记录（反垃圾邮件）NS： 域名服务器记录，将子域名指定其他DNS服务器解析AAAA：将域名指向一个iPv6地址（例如：ff06:0:0:0:0:0:0:c3）SRV：记录提供特定服务的服务器（例如_xmpp-server._tcp）显性URL：将域名301重定向到另一个地址隐性URL：类似显性URL，但是会隐藏真实目标地址 主机记录： 要解析 www.54tianzhisheng.cn，请填写 www。主机记录就是域名前缀，常见用法有： www: 解析后的域名为 www.54tianzhisheng.cn。@: 直接解析主域名 54tianzhisheng.cn。*: 泛解析，匹配其他所有域名 .54tianzhisheng.cn。mail: 将域名解析为 mail.54tianzhisheng.cn，通常用于解析邮箱服务器。二级域名: 如：abc.54tianzhisheng.cn，填写abc。*手机网站: 如：m.54tianzhisheng.cn，填写m。 Java 中 Socket 的用法普通 Soket 的用法Socket 分为 ServerSocket 和 Socket 两大类。 ServerSocket 用于服务器端，可以通过 accept 方法监听请求，监听到请求后返回 Socket； Socket 用户具体完成数据传输，客户端直接使用 Socket 发送请求并传输数据。 随便写了个单方面发送消息的 demo： 客户端： 12345678910111213141516171819202122232425import java.io.IOException;import java.io.OutputStream;import java.net.Socket;/** * Created by 10412 on 2017/5/2. * TCP客户端： ①：建立tcp的socket服务，最好明确具体的地址和端口。这个对象在创建时，就已经可以对指定ip和端口进行连接(三次握手)。 ②：如果连接成功，就意味着通道建立了，socket流就已经产生了。只要获取到socket流中的读取流和写入流即可，只要通过getInputStream和getOutputStream就可以获取两个流对象。 ③：关闭资源。 *///单方面的输入！public class TcpClient&#123; public static void main(String[] args) &#123; try &#123; Socket s = new Socket(\"127.0.0.1\", 9999); OutputStream o = s.getOutputStream(); o.write(\"tcp sssss\".getBytes()); s.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 服务器端： 1234567891011121314151617181920212223242526272829303132import java.io.IOException;import java.io.InputStream;import java.net.ServerSocket;import java.net.Socket;/** * Created by 10412 on 2017/5/2. */public class TcpServer&#123; public static void main(String[] args) &#123; try &#123; ServerSocket ss = new ServerSocket(9999);//建立服务端的socket服务 Socket s = ss.accept();//获取客户端对象 String ip = s.getInetAddress().getHostAddress(); int port = s.getPort(); System.out.println(ip + \" : \" + port + \" connected\"); // 可以通过获取到的socket对象中的socket流和具体的客户端进行通讯。 InputStream ins = s.getInputStream();//读取客户端的数据，使用客户端对象的socket读取流 byte[] bytes = new byte[1024]; int len = ins.read(bytes); String text = new String(bytes, 0, len); System.out.println(text); //关闭资源 s.close(); ss.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; NioSocket 的用法见以前的一篇文章：Java NIO 系列教程 书中第五章简单的讲了下实现 HTTP 协议。第六章主要讲 Servlet，写了 Servlet 接口和其实现类。第七章把 Tomcat 分析的很不错，如果有读者感兴趣的话，可以去看看。","tags":[{"name":"Spring MVC","slug":"Spring-MVC","permalink":"http://www.54tianzhisheng.cn/tags/Spring-MVC/"}]},{"title":"Spring MVC系列文章（四）：看透 Spring MVC 源代码分析与实践 ——  俯视 Spring MVC","date":"2017-07-13T16:00:00.000Z","path":"2017/07/14/Spring-MVC02/","text":"Spring MVC SpringBoot 系列文章 Spring MVC 之初体验环境搭建在 IDEA 中新建一个 web 项目，用 Maven 管理项目的话，在 pom.xml 中加入 Spring MVC 和 Servlet 依赖即可。 12345678910111213&lt;!-- https://mvnrepository.com/artifact/org.springframework/spring-webmvc --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;4.3.9.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/javax.servlet/javax.servlet-api --&gt;&lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; Spring MVC 简单配置 在 web.xml 中配置 Servlet 创建 Spring MVC 的 xml 配置文件 创建 Controller 和 View 1、web.xml 12345678910111213141516171819202122232425262728293031323334&lt;!-- Spring MVC配置 --&gt;&lt;!-- ====================================== --&gt;&lt;servlet&gt; &lt;servlet-name&gt;spring&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!-- 可以自定义servlet.xml配置文件的位置和名称，默认为WEB-INF目录下，名称为[&lt;servlet-name&gt;]-servlet.xml，如spring-servlet.xml &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;/WEB-INF/spring-servlet.xml&lt;/param-value&gt;&amp;nbsp; 默认 &lt;/init-param&gt; --&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;spring&lt;/servlet-name&gt; &lt;url-pattern&gt;*.do&lt;/url-pattern&gt;&lt;/servlet-mapping&gt;&lt;!-- Spring配置 --&gt;&lt;!-- ====================================== --&gt;&lt;listener&gt; &lt;listenerclass&gt; org.springframework.web.context.ContextLoaderListener &lt;/listener-class&gt;&lt;/listener&gt;&lt;!-- 指定Spring Bean的配置文件所在目录。默认配置在WEB-INF目录下 --&gt;&lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:config/applicationContext.xml&lt;/param-value&gt;&lt;/context-param&gt; 2、spring-servlet.xml 123456789101112131415161718192021&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:p=\"http://www.springframework.org/schema/p\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop-3.0.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-3.0.xsd http://www.springframework.org/schema/context &lt;a href=\"http://www.springframework.org/schema/context/spring-context-3.0.xsd\"&gt;http://www.springframework.org/schema/context/spring-context-3.0.xsd&lt;/a&gt;\"&gt; &lt;!-- 启用spring mvc 注解 --&gt; &lt;context:annotation-config /&gt; &lt;!-- 设置使用注解的类所在的jar包 --&gt; &lt;context:component-scan base-package=\"controller\"&gt;&lt;/context:component-scan&gt; &lt;!-- 完成请求和注解POJO的映射 --&gt; &lt;bean class=\"org.springframework.web.servlet.mvc.annotation.AnnotationMethodHandlerAdapter\" /&gt; &lt;!-- 对转向页面的路径解析。prefix：前缀， suffix：后缀 --&gt; &lt;bean class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\" p:prefix=\"/jsp/\" p:suffix=\".jsp\" /&gt;&lt;/beans&gt; 3、Controller 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package controller;import javax.servlet.http.HttpServletRequest;import org.springframework.stereotype.Controller;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestParam;import entity.User;@Controller //类似Struts的Actionpublic class TestController &#123; @RequestMapping(\"/test/login.do\") // 请求url地址映射，类似Struts的action-mapping public String testLogin(@RequestParam(value=\"username\")String username, String password, HttpServletRequest request) &#123; // @RequestParam是指请求url地址映射中必须含有的参数(除非属性 required=false, 默认为 true) // @RequestParam可简写为：@RequestParam(\"username\") if (!\"admin\".equals(username) || !\"admin\".equals(password)) &#123; return \"loginError\"; // 跳转页面路径（默认为转发），该路径不需要包含spring-servlet配置文件中配置的前缀和后缀 &#125; return \"loginSuccess\"; &#125; @RequestMapping(\"/test/login2.do\") public ModelAndView testLogin2(String username, String password, int age)&#123; // request和response不必非要出现在方法中，如果用不上的话可以去掉 // 参数的名称是与页面控件的name相匹配，参数类型会自动被转换 if (!\"admin\".equals(username) || !\"admin\".equals(password) || age &lt; 5) &#123; return new ModelAndView(\"loginError\"); // 手动实例化ModelAndView完成跳转页面（转发），效果等同于上面的方法返回字符串 &#125; return new ModelAndView(new RedirectView(\"../index.jsp\")); // 采用重定向方式跳转页面 // 重定向还有一种简单写法 // return new ModelAndView(\"redirect:../index.jsp\"); &#125; @RequestMapping(\"/test/login3.do\") public ModelAndView testLogin3(User user) &#123; // 同样支持参数为表单对象，类似于Struts的ActionForm，User不需要任何配置，直接写即可 String username = user.getUsername(); String password = user.getPassword(); int age = user.getAge(); if (!\"admin\".equals(username) || !\"admin\".equals(password) || age &lt; 5) &#123; return new ModelAndView(\"loginError\"); &#125; return new ModelAndView(\"loginSuccess\"); &#125; @Resource(name = \"loginService\") // 获取applicationContext.xml中bean的id为loginService的，并注入 private LoginService loginService; //等价于spring传统注入方式写get和set方法，这样的好处是简洁工整，省去了不必要得代码 @RequestMapping(\"/test/login4.do\") public String testLogin4(User user) &#123; if (loginService.login(user) == false) &#123; return \"loginError\"; &#125; return \"loginSuccess\"; &#125;&#125; @RequestMapping 可以写在方法上，也可以写在类上，上面代码方法上的 RequestMapping 都含有 /test ， 那么我们就可以将其抽出直接写在类上，那么方法里面就不需要写 /test 了。 如下即可： 12345678910111213141516@Controller@RequestMapping(\"/test\")public class TestController &#123; @RequestMapping(\"/login.do\") // 请求url地址映射，类似Struts的action-mapping public String testLogin(@RequestParam(value=\"username\")String username, String password, HttpServletRequest request) &#123; // @RequestParam是指请求url地址映射中必须含有的参数(除非属性 required=false, 默认为 true) // @RequestParam可简写为：@RequestParam(\"username\") if (!\"admin\".equals(username) || !\"admin\".equals(password)) &#123; return \"loginError\"; // 跳转页面路径（默认为转发），该路径不需要包含spring-servlet配置文件中配置的前缀和后缀 &#125; return \"loginSuccess\"; &#125; //省略其他的&#125; 上面的代码方法的参数中可以看到有一个 @RequestParam 注解，其实还有 @PathVariable 。这两个的区别是啥呢？ @PathVariable 标记在方法的参数上，利用它标记的参数可以利用请求路径传值。 @RequestParam是指请求url地址映射中必须含有的参数(除非属性 required=false, 默认为 true) 看如下例子： 123456789101112@RequestMapping(\"/user/&#123;userId&#125;\") // 请求url地址映射public String userinfo(Model model, @PathVariable(\"userId\") int userId, HttpSession session) &#123; System.out.println(\"进入 userinfo 页面\"); //判断是否有用户登录 User user1 = (User) session.getAttribute(\"user\"); if (user1 == null) &#123; return \"login\"; &#125; User user = userService.selectUserById(userId); model.addAttribute(\"user\", user); return \"userinfo\"; &#125; 上面例子中如果浏览器请求的是 /user/1 的时候，就表示此时的用户 id 为 1，此时就会先从 session 中查找是否有 “user” 属性，如果有的话，就代表用户此时处于登录的状态，如果没有的话，就会让用户返回到登录页面，这种机制在各种网站经常会使用的，然后根据这个 id = 1 ，去查找用户的信息，然后把查找的 “user” 放在 model 中，然后返回用户详情页面，最后在页面中用 $!{user.name} 获取用户的名字，同样的方式可以获取用户的其他信息，把所有的用户详情信息展示出来。 创建 Spring MVC 之器Spring MVC 核心 Servlet 架构图如下： Java 中常用的 Servlet 我在另外一篇文章写的很清楚了，有兴趣的请看：通过源码详解 Servlet ，这里我就不再解释了。 这里主要讲 Spring 中的 HttpServletBean、FrameworkServlet、DispatcherServlet 这三个类的创建过程。 通过上面的图，可以看到这三个类直接实现三个接口：EnvironmentCapable、EnvironmentAware、ApplicationContextAware。下面我们直接看下这三个接口的内部是怎样写的。 EnvironmentCapable.java 12345public interface EnvironmentCapable &#123; //返回组件的环境，可能返回 null 或者默认环境 @Nullable Environment getEnvironment();&#125; EnvironmentAware.java 1234public interface EnvironmentAware extends Aware &#123; //设置组件的运行环境 void setEnvironment(Environment environment);&#125; ApplicationContextAware.java 12345public interface ApplicationContextAware extends Aware &#123; //设置运行对象的应用上下文 //当类实现这个接口后，这个类可以获取ApplicationContext中所有的bean，也就是说这个类可以直接获取Spring配置文件中所有有引用到的bean对象 void setApplicationContext(ApplicationContext applicationContext) throws BeansException;&#125; 怎么使用这个这个接口呢？ 参考文章：org.springframework.context.ApplicationContextAware使用理解 HttpServletBean 这里就直接看其中最重要的 init() 方法的代码了： 12345678910111213141516171819202122232425262728293031323334/** * 将配置参数映射到此servlet的bean属性，并调用子类初始化。 * 如果 bean 配置不合法（或者需要的参数丢失）或者子类初始化发生错误，那么就会抛出 ServletException 异常 */@Overridepublic final void init() throws ServletException &#123; //日志代码删除了 // 从init参数设置bean属性。 //获得web.xml中的contextConfigLocation配置属性，就是spring MVC的配置文件 PropertyValues pvs = new ServletConfigPropertyValues(getServletConfig(), this.requiredProperties); if (!pvs.isEmpty()) &#123; try &#123; BeanWrapper bw = PropertyAccessorFactory.forBeanPropertyAccess(this); //获取服务器的各种信息 ResourceLoader resourceLoader = new ServletContextResourceLoader(getServletContext()); bw.registerCustomEditor(Resource.class, new ResourceEditor(resourceLoader, getEnvironment())); //模板方法，可以在子类中调用，做一些初始化工作，bw代表DispatcherServelt initBeanWrapper(bw); //将配置的初始化值设置到DispatcherServlet中 bw.setPropertyValues(pvs, true); &#125; catch (BeansException ex) &#123; //日志代码 throw ex; &#125; &#125; // Let subclasses do whatever initialization they like. //模板方法，子类初始化的入口方法 initServletBean(); //日志代码删除了&#125; FrameworkServlet 其中重要方法如下：里面也就两句关键代码，日志代码我直接删掉了 12345678910111213141516171819202122protected final void initServletBean() throws ServletException &#123; //日志代码删除了 long startTime = System.currentTimeMillis(); //就是 try 语句里面有两句关键代码 try &#123; //初始化 webApplicationContext this.webApplicationContext = initWebApplicationContext(); //模板方法， initFrameworkServlet(); &#125; catch (ServletException ex) &#123; this.logger.error(\"Context initialization failed\", ex); throw ex; &#125; catch (RuntimeException ex) &#123; this.logger.error(\"Context initialization failed\", ex); throw ex; &#125; //日志代码删除了 &#125; 再来看看上面代码中调用的 initWebApplicationContext() 方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344protected WebApplicationContext initWebApplicationContext() &#123; //获取 rootContext WebApplicationContext rootContext = WebApplicationContextUtils.getWebApplicationContext(getServletContext()); WebApplicationContext wac = null; if (this.webApplicationContext != null) &#123; // 上下文实例在构造时注入 - &gt;使用它 wac = this.webApplicationContext; if (wac instanceof ConfigurableWebApplicationContext) &#123; ConfigurableWebApplicationContext cwac = (ConfigurableWebApplicationContext) wac; if (!cwac.isActive()) &#123; // 如果上下文尚未刷新 -&gt; 提供诸如设置父上下文，设置应用程序上下文ID等服务 if (cwac.getParent() == null) &#123; // 上下文实例被注入没有显式的父类 -&gt; 将根应用程序上下文（如果有的话可能为null）设置为父级 cwac.setParent(rootContext); &#125; configureAndRefreshWebApplicationContext(cwac); &#125; &#125; &#125; if (wac == null) &#123; // 当 WebApplicationContext 已经存在 ServletContext 中时，通过配置在 servlet 中的 ContextAttribute 参数获取 wac = findWebApplicationContext(); &#125; if (wac == null) &#123; // 如果 WebApplicationContext 还没有创建，则创建一个 wac = createWebApplicationContext(rootContext); &#125; if (!this.refreshEventReceived) &#123; // 当 ContextRefreshedEvent 事件没有触发时调用此方法，模板方法，可以在子类重写 onRefresh(wac); &#125; if (this.publishContext) &#123; // 将 ApplicationContext 保存到 ServletContext 中去 String attrName = getServletContextAttributeName(); getServletContext().setAttribute(attrName, wac); if (this.logger.isDebugEnabled()) &#123; this.logger.debug(\"Published WebApplicationContext of servlet '\" + getServletName() + \"' as ServletContext attribute with name [\" + attrName + \"]\"); &#125; &#125; return wac; &#125; initWebApplicationContext 方法做了三件事： 获取 Spring 的根容器 rootContext 设置 webApplicationContext 并根据情况调用 onRefresh 方法 将 webApplicationContext 设置到 ServletContext 中 这里在讲讲上面代码中的 wac == null 的几种情况： 1）、当 WebApplicationContext 已经存在 ServletContext 中时，通过配置在 servlet 中的 ContextAttribute 参数获取，调用的是 findWebApplicationContext() 方法 123456789101112protected WebApplicationContext findWebApplicationContext() &#123; String attrName = getContextAttribute(); if (attrName == null) &#123; return null; &#125; WebApplicationContext wac = WebApplicationContextUtils.getWebApplicationContext(getServletContext(), attrName); if (wac == null) &#123; throw new IllegalStateException(\"No WebApplicationContext found: initializer not registered?\"); &#125; return wac; &#125; 2)、如果 WebApplicationContext 还没有创建，调用的是 createWebApplicationContext 方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758protected WebApplicationContext createWebApplicationContext(@Nullable ApplicationContext parent) &#123; //获取创建类型 Class&lt;?&gt; contextClass = getContextClass(); //删除了打印日志代码 //检查创建类型 if (!ConfigurableWebApplicationContext.class.isAssignableFrom(contextClass)) &#123; throw new ApplicationContextException( \"Fatal initialization error in servlet with name '\" + getServletName() + \"': custom WebApplicationContext class [\" + contextClass.getName() + \"] is not of type ConfigurableWebApplicationContext\"); &#125; //具体创建 ConfigurableWebApplicationContext wac = (ConfigurableWebApplicationContext) BeanUtils.instantiateClass(contextClass); wac.setEnvironment(getEnvironment()); wac.setParent(parent); //并设置的 contextConfigLocation 参数传给 wac，默认是 WEB-INFO/[ServletName]-Servlet.xml wac.setConfigLocation(getContextConfigLocation()); //调用的是下面的方法 configureAndRefreshWebApplicationContext(wac); return wac; &#125;protected void configureAndRefreshWebApplicationContext(ConfigurableWebApplicationContext wac) &#123; if (ObjectUtils.identityToString(wac).equals(wac.getId())) &#123; // The application context id is still set to its original default value // -&gt; assign a more useful id based on available information if (this.contextId != null) &#123; wac.setId(this.contextId); &#125; else &#123; // Generate default id... wac.setId(ConfigurableWebApplicationContext.APPLICATION_CONTEXT_ID_PREFIX + ObjectUtils.getDisplayString(getServletContext().getContextPath()) + '/' + getServletName()); &#125; &#125; wac.setServletContext(getServletContext()); wac.setServletConfig(getServletConfig()); wac.setNamespace(getNamespace()); wac.addApplicationListener(new SourceFilteringListener(wac, new ContextRefreshListener())); // The wac environment's #initPropertySources will be called in any case when the context // is refreshed; do it eagerly here to ensure servlet property sources are in place for // use in any post-processing or initialization that occurs below prior to #refresh ConfigurableEnvironment env = wac.getEnvironment(); if (env instanceof ConfigurableWebEnvironment) &#123; ((ConfigurableWebEnvironment) env).initPropertySources(getServletContext(), getServletConfig()); &#125; postProcessWebApplicationContext(wac); applyInitializers(wac); wac.refresh(); &#125; 里面还有 doXXX() 方法，大家感兴趣的可以去看看。 DispatcherServlet DispatcherServlet 继承自 FrameworkServlet，onRefresh 方法是 DispatcherServlet 的入口方法，在 initStrategies 方法中调用了 9 个初始化的方法。 这里分析其中一个初始化方法：initLocaleResolver() 方法 123456789101112private void initLocaleResolver(ApplicationContext context) &#123; try &#123; //在 context 中获取 this.localeResolver = context.getBean(LOCALE_RESOLVER_BEAN_NAME, LocaleResolver.class); //删除了打印日志的代码 &#125; catch (NoSuchBeanDefinitionException ex) &#123; //使用默认的策略 this.localeResolver = getDefaultStrategy(context, LocaleResolver.class); //删除了打印日志的代码 &#125; &#125; 查看默认策略代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647protected &lt;T&gt; T getDefaultStrategy(ApplicationContext context, Class&lt;T&gt; strategyInterface) &#123; //调用 getDefaultStrategies 方法 List&lt;T&gt; strategies = getDefaultStrategies(context, strategyInterface); if (strategies.size() != 1) &#123; throw new BeanInitializationException( \"DispatcherServlet needs exactly 1 strategy for interface [\" + strategyInterface.getName() + \"]\"); &#125; return strategies.get(0); &#125; /** * Create a List of default strategy objects for the given strategy interface. * &lt;p&gt;The default implementation uses the \"DispatcherServlet.properties\" file (in the same * package as the DispatcherServlet class) to determine the class names. It instantiates * the strategy objects through the context's BeanFactory. */ @SuppressWarnings(\"unchecked\") protected &lt;T&gt; List&lt;T&gt; getDefaultStrategies(ApplicationContext context, Class&lt;T&gt; strategyInterface) &#123; String key = strategyInterface.getName(); //根据策略接口的名字从 defaultStrategies 获取所需策略的类型 String value = defaultStrategies.getProperty(key); if (value != null) &#123; //如果有多个默认值的话，就以逗号分隔为数组 String[] classNames = StringUtils.commaDelimitedListToStringArray(value); List&lt;T&gt; strategies = new ArrayList&lt;&gt;(classNames.length); //按获取到的类型初始化策略 for (String className : classNames) &#123; try &#123; Class&lt;?&gt; clazz = ClassUtils.forName(className, DispatcherServlet.class.getClassLoader()); Object strategy = createDefaultStrategy(context, clazz); strategies.add((T) strategy); &#125; catch (ClassNotFoundException ex) &#123; throw new BeanInitializationException( \"Could not find DispatcherServlet's default strategy class [\" + className + \"] for interface [\" + key + \"]\", ex); &#125; catch (LinkageError err) &#123; throw new BeanInitializationException( \"Error loading DispatcherServlet's default strategy class [\" + className + \"] for interface [\" + key + \"]: problem with class file or dependent class\", err); &#125; &#125; return strategies; &#125; else &#123; return new LinkedList&lt;&gt;(); &#125; &#125; 其他几个方法大概也类似，我就不再写了。 小结主要讲了 Spring MVC 自身创建过程，分析了 Spring MVC 中 Servlet 的三个层次：HttpServletBean、FrameworkServlet 和 DispatcherServlet。HttpServletBean 继承自 Java 的 HttpServlet，其作用是将配置的参数设置到相应的属性上；FrameworkServlet 初始化了 WebApplicationContext；DispatcherServlet 初始化了自身的 9 个组件。 Spring MVC 之用分析 Spring MVC 是怎么处理请求的。首先分析 HttpServletBean、FrameworkServlet 和 DispatcherServlet 这三个 Servlet 的处理过程，最后分析 doDispatcher 的结构。 HttpServletBean 参与了创建工作，并没有涉及请求的处理。 FrameworkServlet 在类中的 service() 、doGet()、doPost()、doPut()、doDelete()、doOptions()、doTrace() 这些方法中可以看到都调用了一个共同的方法 processRequest() ，它是类在处理请求中最核心的方法。 123456789101112131415161718192021222324252627282930313233343536373839404142protected final void processRequest(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException &#123; long startTime = System.currentTimeMillis(); Throwable failureCause = null; //获取 LocaleContextHolder 中原来保存的 LocaleContext LocaleContext previousLocaleContext = LocaleContextHolder.getLocaleContext(); //获取当前请求的 LocaleContext LocaleContext localeContext = buildLocaleContext(request); //获取 RequestContextHolder 中原来保存的 RequestAttributes RequestAttributes previousAttributes = RequestContextHolder.getRequestAttributes(); //获取当前请求的 ServletRequestAttributes ServletRequestAttributes requestAttributes = buildRequestAttributes(request, response, previousAttributes); WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); asyncManager.registerCallableInterceptor(FrameworkServlet.class.getName(), new RequestBindingInterceptor());//将当前请求的 LocaleContext 和 ServletRequestAttributes 设置到 LocaleContextHolder 和 RequestContextHolder initContextHolders(request, localeContext, requestAttributes); try &#123; //实际处理请求的入口，这是一个模板方法，在 Dispatcher 类中才有具体实现 doService(request, response); &#125;catch (ServletException ex) &#123; failureCause = ex; throw ex; &#125;catch (IOException ex) &#123; failureCause = ex; throw ex; &#125;catch (Throwable ex) &#123; failureCause = ex; throw new NestedServletException(\"Request processing failed\", ex); &#125;finally &#123; //将 previousLocaleContext，previousAttributes 恢复到 LocaleContextHolder 和 RequestContextHolder 中 resetContextHolders(request, previousLocaleContext, previousAttributes); if (requestAttributes != null) &#123; requestAttributes.requestCompleted(); &#125; //删除了日志打印代码 //发布了一个 ServletRequestHandledEvent 类型的消息 publishRequestHandledEvent(request, response, startTime, failureCause); &#125; &#125; DispatcherServlet 上一章中其实还没把该类讲清楚，在这个类中，里面的智行处理的入口方法应该是 doService 方法，方法里面调用了 doDispatch 进行具体的处理，在调用 doDispatch 方法之前 doService 做了一些事情：首先判断是不是 include 请求，如果是则对 request 的 Attribute 做个快照备份，等 doDispatcher 处理完之后（如果不是异步调用且未完成）进行还原 ，在做完快照后又对 request 设置了一些属性。 123456789101112131415161718192021222324252627282930313233343536373839protected void doService(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; // Keep a snapshot of the request attributes in case of an include, // to be able to restore the original attributes after the include. Map&lt;String, Object&gt; attributesSnapshot = null; if (WebUtils.isIncludeRequest(request)) &#123; attributesSnapshot = new HashMap&lt;&gt;(); Enumeration&lt;?&gt; attrNames = request.getAttributeNames(); while (attrNames.hasMoreElements()) &#123; String attrName = (String) attrNames.nextElement(); if (this.cleanupAfterInclude || attrName.startsWith(DEFAULT_STRATEGIES_PREFIX))&#123; attributesSnapshot.put(attrName, request.getAttribute(attrName)); &#125; &#125; &#125; // Make framework objects available to handlers and view objects. request.setAttribute(WEB_APPLICATION_CONTEXT_ATTRIBUTE, getWebApplicationContext()); request.setAttribute(LOCALE_RESOLVER_ATTRIBUTE, this.localeResolver); request.setAttribute(THEME_RESOLVER_ATTRIBUTE, this.themeResolver); request.setAttribute(THEME_SOURCE_ATTRIBUTE, getThemeSource()); FlashMap inputFlashMap = this.flashMapManager.retrieveAndUpdate(request, response); if (inputFlashMap != null) &#123; request.setAttribute(INPUT_FLASH_MAP_ATTRIBUTE, Collections.unmodifiableMap(inputFlashMap)); &#125; request.setAttribute(OUTPUT_FLASH_MAP_ATTRIBUTE, new FlashMap()); request.setAttribute(FLASH_MAP_MANAGER_ATTRIBUTE, this.flashMapManager); try &#123; //调用 doDispatch 方法 doDispatch(request, response); &#125;finally &#123; if (!WebAsyncUtils.getAsyncManager(request).isConcurrentHandlingStarted()) &#123; // Restore the original attribute snapshot, in case of an include. if (attributesSnapshot != null) &#123; restoreAttributesAfterInclude(request, attributesSnapshot); &#125; &#125; &#125; &#125; doDispatch() 方法： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; HttpServletRequest processedRequest = request; HandlerExecutionChain mappedHandler = null; boolean multipartRequestParsed = false; WebAsyncManager asyncManager = WebAsyncUtils.getAsyncManager(request); try &#123; ModelAndView mv = null; Exception dispatchException = null; try &#123; //检查是不是上传请求 processedRequest = checkMultipart(request); multipartRequestParsed = (processedRequest != request); // Determine handler for the current request. 根据 request 找到 Handler mappedHandler = getHandler(processedRequest); if (mappedHandler == null || mappedHandler.getHandler() == null) &#123; noHandlerFound(processedRequest, response); return; &#125; // Determine handler adapter for the current request.根据 Handler 找到对应的 HandlerAdapter HandlerAdapter ha = getHandlerAdapter(mappedHandler.getHandler()); // Process last-modified header, if supported by the handler. //处理 GET 、 HEAD 请求的 LastModified String method = request.getMethod(); boolean isGet = \"GET\".equals(method); if (isGet || \"HEAD\".equals(method)) &#123; long lastModified = ha.getLastModified(request, mappedHandler.getHandler()); if (logger.isDebugEnabled()) &#123; logger.debug(\"Last-Modified value for [\" + getRequestUri(request) + \"] is: \" + lastModified); &#125; if (new ServletWebRequest(request, response).checkNotModified(lastModified) &amp;&amp; isGet) &#123; return; &#125; &#125; //执行相应的 Interceptor 的 preHandle if (!mappedHandler.applyPreHandle(processedRequest, response)) &#123; return; &#125; // Actually invoke the handler. HandlerAdapter 使用 Handler 处理请求 mv = ha.handle(processedRequest, response, mappedHandler.getHandler()); //如果需要异步处理，直接返回 if (asyncManager.isConcurrentHandlingStarted()) &#123; return; &#125; //当 view 为空时，根据 request 设置默认 view applyDefaultViewName(processedRequest, mv); //执行相应 Interceptor 的 postHandler mappedHandler.applyPostHandle(processedRequest, response, mv); &#125;catch (Exception ex) &#123; dispatchException = ex; &#125;catch (Throwable err) &#123; // As of 4.3, we're processing Errors thrown from handler methods as well, // making them available for @ExceptionHandler methods and other scenarios. dispatchException = new NestedServletException(\"Handler dispatch failed\", err); &#125; //调用 processDispatchResult 方法处理上面处理之后的结果（包括处理异常，渲染页面，发出完成通知触发 Interceptor 的 afterCompletion） processDispatchResult(processedRequest, response, mappedHandler, mv, dispatchException); &#125;catch (Exception ex) &#123; triggerAfterCompletion(processedRequest, response, mappedHandler, ex); &#125;catch (Throwable err) &#123; triggerAfterCompletion(processedRequest, response, mappedHandler, new NestedServletException(\"Handler processing failed\", err)); &#125;finally &#123; //判断是否执行异步请求 if (asyncManager.isConcurrentHandlingStarted()) &#123; // Instead of postHandle and afterCompletion if (mappedHandler != null) &#123; mappedHandler.applyAfterConcurrentHandlingStarted(processedRequest, response); &#125; &#125;else &#123; // Clean up any resources used by a multipart request. 删除上传请求的资源 if (multipartRequestParsed) &#123; cleanupMultipart(processedRequest); &#125; &#125; &#125; &#125; Handler，HandlerMapping，HandlerAdapter 三个区别： Handler：处理器，对应 MVC 的 C层，也就是 Controller 层，具体表现形式有很多种，可以是类，方法，它的类型是 Object，只要可以处理实际请求就可以是 Handler。 HandlerMapping：用来查找 Handler 的。 HandlerAdapter ：Handler 适配器， 另外 View 和 ViewResolver 的原理与 Handler 和 HandlerMapping 的原理类似。 小结本章分析了 Spring MVC 的请求处理的过程。","tags":[{"name":"Spring MVC","slug":"Spring-MVC","permalink":"http://www.54tianzhisheng.cn/tags/Spring-MVC/"}]},{"title":"通过源码详解 Servlet","date":"2017-07-08T16:00:00.000Z","path":"2017/07/09/servlet/","text":"Servlet 结构 1、ServletServlet 该接口定义了5个方法。 init()，初始化 servlet 对象，完成一些初始化工作。它是由 servlet 容器控制的，该方法只能被调用一次 service()，接受客户端请求对象，执行业务操作，利用响应对象响应客户端请求。 destroy()，当容器监测到一个servlet从服务中被移除时，容器调用该方法，释放资源，该方法只能被调用一次。 getServletConfig()，ServletConfig 是容器向 servlet 传递参数的载体。 getServletInfo()，获取 servlet 相关信息。 Servlet 的生命周期： 1，初始化阶段 调用 init() 方法 2，响应客户请求阶段 调用 service() 方法 3，终止阶段 调用 destroy() 方法 在 Servlet 接口中的五个方法中涉及的接口有三个：ServletConfig 、 ServletRequest、 ServletResponse 这里先讲讲 ServletRequest 和 ServletResponse。 1）ServletRequest 由 Servlet 容器来管理，当客户请求到来时，容器创建一个 ServletRequest 对象，封装请求数据，同时创建一个 ServletResponse 对象，封装响应数据。这两个对象将被容器作为 service（）方法的参数传递给 Servlet，Serlvet 利用 ServletRequest 对象获取客户端发来的请求数据，利用 ServletResponse 对象发送响应数据。 下面是 ServletRequest 中所有的方法，根据方法名大概就可以猜到这些方法到底是干啥用的。 2）ServletResponse 发送响应数据 2、ServletConfigServletConfig 是容器向 servlet 传递参数的载体。 ServletConfig的4个常用方法： 1）public String getInitParameter（String name）：返回指定名称的初始化参数值； 2）public Enumeration getInitParameterNames（）：返回一个包含所有初始化参数名的 Enumeration 对象； 3）public String getServletName()：返回在 DD 文件中&lt;servlet-name&gt;元素指定的 Servlet 名称； 4）public ServletContext getServletContext（）：返回该 Servlet 所在的上下文对象； 这里详细讲下 ServletContext ： Servlet 上下文对象（ServletContext）：每个Web应用程序在被启动时都会创建一个唯一的上下文对象，Servlet 可通过其获得 Web 应用程序的初始化参数或 Servlet 容器的版本等信息，也可被 Servlet 用来与其他 Servlet 共享数据。 1、获得 ServletContext 应用： （1）、直接调用 getServletContext（）方法 ServletContext context = getServletContext（）; （2）、使用 ServletConfig 应用，再调用它的 getServletContext（）方法 ServletContext context = getServletConfig.getServletContext(); 2、获得应用程序的初始化参数： （1）、public String getInitParameter（String name）：返回指定参数名的字符串参数值，没有则返回 null； （2）、public Enumeration getInitParameterNames()：返回一个包含多有初始化参数名的 Enumeration 对象； 3、通过 ServletContext 对象获得资源 （1）、public URl getResource（String path）:返回由给定路径的资源的 URL 对象，以 “/” 开头，为相对路径，相对于Web 应用程序的文档根目录； （2）、public InputStream getResourceAsStream（String path）：从资源上获得一个 InputStream 对象，等价于getResource（path）.oprenStream(); （3）、public String getRealPath(String path)：返回给定的虚拟路径的真实路径； 4、登陆日志：使用 log（）方法可以将指定的消息写到服务器的日志文件中 （1）、public void log（String msg）：参数 msg 为写入日志文件消息 （2）、public void log（String msg，Throwable throwable）：将 msg 指定的消息和异常的栈跟踪信息写入日志文件 5、使用 RequestDispatcher 实现请求转发 （1）、RequestDispatcher getRequestDiapatcher(String path)：必须以 “/“ 开头相对于应用程序根目录，而ServletRequest 可以传递一个相对路径 （2）、RequestDipatcher getNamedDiapatcher（String name）：参数 name 为一个命名的 Servlet 对象 6、使用 ServletContext 对象存储数据 （1）、public void serAttribute（String name，Object object）：将给定名称的属性值对象绑定到上下文对象上； （2）、public Object getAttribute（String name）：返回绑定到上下文对象的给定名称的属性值； （3）、public Enumeration getAttributeNames()：返回绑定到上下文对象上的所有属性名的 Enumeration 对象； （4）、public void removeAttribute（String name）：删除绑定到上下文对象指定名称的属性； ServletRequest 共享的对象仅在请求的生存周期中可以被访问； HttpSession 共享的对象仅在会话的生存周期中可以被访问； ServletContext 共享的对象在整个 Web 应用程序启动的生存周期中可以被访问； 7、检索 Servlet 容器的信息 （1）、public String getServletInfo()：返回 Servlet 所运行容器的名称和版本； （2）、public int getMajorVersion（）：返回容器所支持的 Servlet API 的主版本号； （3）、public int getMinorVersion（）：返回容器所支持的 Servlet API 的次版本号； （4）、public String getServletContext（）：返回 ServletContext 对应的 web 应用程序名称 &lt;display-name&gt;元素定义的名称； 3、GenericServlet 抽象类GenericServlet 定义了一个通用的，不依赖具体协议的 Servlet，它实现了 Servlet 接口和 ServletConfig 接口，它的方法在文章的第一张图就给出了。 4、HttpServlet 抽象类4.1、HTTP 请求方式 GET : 获取由请求 URL 标识的资源 POST : 向 Web 服务器发送无限制长度的数据 PUT : 存储一个资源到请求的 URL DELETE : 删除由 URL 标识的资源 HEAD : 返回 URL 标识的头信息 OPTIONS : 返回服务器支持的 HTTP 方法 TRACE : 返回 TRACE 请求附带的头字段 4.2、对应的服务方法： doGet() : 调用服务器的资源, 并将其作为响应返回给客户端. doGet() 调用在 URL 里显示正在传送给 Servlet 的数据,这在系统的安全方面可能带来一些问题, 比如说, 用户登录时, 表单里的用户名和密码需要发送到服务器端, doGet() 调用会在浏览器的 URL 里显示用户名和密码. doPost() : 它用于把客户端的数据传给服务端, 使用它可以以隐藏方式给服务器端发送数据. Post 适合发送大量数据. doPut() : 调用和 doPost() 相似, 并且它允许客户端把真正的文件存放在服务器上, 而不仅仅是传送数据. doDelete() : 它允许客户端删除服务器端的文件或者 Web 页面．它的使用非常少． doHead() : 它用于处理客户端的 Head 调用,并且返回一个 response. 当客户端只需要响应的 Header 时,它就发出一个Header 请求.这种情况下客户端往往关心响应的长度和响应的 MIME 类型. doOptions(): 它用于处理客户端的 Options 调用,通过这个调用, 客户端可以获得此 Servlet 支持的方法.如果 Servlet 覆盖了 doPost() 方法, 那么将返回: Allow: POST, TRACE, OPTIONS, HEAD doTrace：处理 TRACE 请求 4.3、Servlet Service 方法详解12345678910111213141516public void service(ServletRequest req, ServletResponse res) throws ServletException, IOException &#123; HttpServletRequest request; HttpServletResponse response; // 如果传入的 HTTP 请求和 HTTP 响应不是 HTTP 的领域模型，则抛出 Servlet 异常，这个异常会被 Servlet 容器所处理 if (!(req instanceof HttpServletRequest &amp;&amp; res instanceof HttpServletResponse)) &#123; throw new ServletException(\"non-HTTP request or response\"); &#125; // 既然是 HTTP 协议绑定的 Serlvet, 强制转换到 HTTP 的领域模型 request = (HttpServletRequest) req; response = (HttpServletResponse) res; // 如果传入的请求和响应是预期的 HTTP 请求和 HTTP 响应，则调用 HttpServlet 的 service() 方法。 service(request, response); &#125; 4.4、HttpServlet service 方法详解1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071protected void service(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; // 从 HTTP 请求中取得这次请求所使用的 HTTT 方法 String method = req.getMethod(); // 如果这次请求使用 GET 方法 if (method.equals(METHOD_GET)) &#123; // 取得这个 Servlet 的最后修改的时间 long lastModified = getLastModified(req); if (lastModified == -1) &#123; // servlet doesn't support if-modified-since, no reason // to go through further expensive logic //-1 代表这个 Servlet 不支持最后修改操作，直接调用 doGet() 进行处理 HTTP GET 请求 doGet(req, resp); &#125; else &#123; // 如果这个 Servlet 支持最后修改操作，取得请求头中包含的请求的最后修改时间 long ifModifiedSince = req.getDateHeader(HEADER_IFMODSINCE); if (ifModifiedSince &lt; lastModified) &#123; // If the servlet mod time is later, call doGet() // Round down to the nearest second for a proper compare // A ifModifiedSince of -1 will always be less // 如果请求头中包含的修改时间早于这个 Servlet 的最后修改时间，说明这个 Servlet 自从客户上一次 HTTP 请求已经被修改了 , 设置最新修改时间到响应头中 maybeSetLastModified(resp, lastModified); // 调用 doGet 进行进行处理 HTTP GET 请求 doGet(req, resp); &#125; else &#123; // 如果请求头中包含修改时间晚于这个 Servlet 的最后修改时间，说明这个 Servlet 自从请求的最后修改时间后没有更改过，这种情况下，仅仅返回一个 HTTP 响应状态 SC_NOT_MODIFIED resp.setStatus(HttpServletResponse.SC_NOT_MODIFIED); &#125; &#125; &#125; else if (method.equals(METHOD_HEAD)) &#123; // 如果这次请求使用 HEAD 方法 // 如果这个 Servlet 支持最后修改操作，则设置这个 Servlet 的最后修改时间到响应头中 long lastModified = getLastModified(req); maybeSetLastModified(resp, lastModified); // 和对 HTTP GET 方法处理不同的是，无论请求头中的修改时间是不是早于这个 Sevlet 的最后修改时间，都会发 HEAD 响应给客户，因为 HTTP HEAD 响应是用来查询 Servlet 头信息的操作 doHead(req, resp); &#125; else if (method.equals(METHOD_POST)) &#123; // 如果这次请求使用 POST 方法 doPost(req, resp); &#125; else if (method.equals(METHOD_PUT)) &#123; // 如果这次请求使用 PUT 方法 doPut(req, resp); &#125; else if (method.equals(METHOD_DELETE)) &#123; // 如果这次请求使用 DELETE 方法 doDelete(req, resp); &#125; else if (method.equals(METHOD_OPTIONS)) &#123; // 如果这次请求使用 OPTIONS 方法 doOptions(req,resp); &#125; else if (method.equals(METHOD_TRACE)) &#123; // 如果这次请求使用 TRACE 方法 doTrace(req,resp); &#125; else &#123; // Note that this means NO servlet supports whatever // method was requested, anywhere on this server. // 如果这次请求是其他未知方法，返回错误代码 SC_NOT_IMPLEMENTED 给 HTTP 响应，并且显示一个错误消息，说明这个操作是没有实现的 String errMsg = lStrings.getString(\"http.method_not_implemented\"); Object[] errArgs = new Object[1]; errArgs[0] = method; errMsg = MessageFormat.format(errMsg, errArgs); resp.sendError(HttpServletResponse.SC_NOT_IMPLEMENTED, errMsg); &#125; &#125; 5、Servlet 的多线程问题1、当涉及到 Servlet 需要共享资源是，需保证 Servlet 是线程安全的 2、注意事项： （1）、用方法的局部变量保持请求中的专有数据； （2）、只用 Servlet 的成员变量来存放那些不会改变的数据； （3）、对可能被请求修改的成员变量同步（用 Synchronized 关键字修饰）； （4）、如果 Servlet 访问外部资源，那么需要同步访问这些资源； 3、实现 SingleThreadModel 接口的 Servlet 在被多个客户请求时一个时刻只能有一个线程运行，不推荐使用。 4、如果必须在 servlet 使用同步代码，应尽量在最小的范围上（代码块）进行同步，同步代码越少，Servlet 执行才能越好，避免对 doGet() 或 doPost() 方法同步。 总结全文首先通过一张 Servlet 中的核心 Servlet 类图关系，了解了几种 Servlet 之间的关系及其内部方法。然后在分别介绍这几种 Servlet，通过分析部分重要方法的源码来了解，还介绍了 Servlet 中多线程的问题的解决方法。 注：文章原创，首发于：zhisheng 的博客，文章可转载但请注明地址为：http://www.54tianzhisheng.cn/2017/07/09/servlet/","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"Servlet","slug":"Servlet","permalink":"http://www.54tianzhisheng.cn/tags/Servlet/"}]},{"title":"Velocity 循环指令一种好的解决方法","date":"2017-06-27T16:00:00.000Z","path":"2017/06/28/Velocity-foreach/","text":"前提前台的数据经常是由需要通过 foreach 循环获取。 好的解决方案：（拿我最近做的一个项目做例子）购物商城左边的导航栏，商品大分类和小分类（Category） 1、在 model 包下创建一个 ViewObject 类 1234567891011public class ViewObject&#123; private Map&lt;String, Object&gt; objs = new HashMap&lt;&gt;(); public void set(String key, Object value) &#123; objs.put(key, value); &#125; public Object get(String key) &#123; return objs.get(key); &#125;&#125; 2、在 controller 包下创建个 BaseController 类 12345678910111213141516171819202122232425262728293031323334/** * 在每个页面显示图书大分类，抽离出来 * @return */ public List&lt;ViewObject&gt; selectAllCategory() &#123; List&lt;Category&gt; categories = categoryService.selectAllCategory(); List&lt;ViewObject&gt; vos = new ArrayList&lt;&gt;(); for (Category category : categories) &#123; ViewObject vo = new ViewObject(); vo.set(\"category\", category); vo.set(\"id\", category.getId()); //System.out.println(\"category 中的 id 是 \"+category.getId()); vos.add(vo); &#125; return vos; &#125; /** * 获取图书的小分类，在这里将小分类中的大分类id查找出来，保存在 cds.id 中， * 然后在模板引擎中通过将 vos.id 和 cds.id 相比较。然后如果相同的话，就取出来放在对应的大分类下 * @return */ public List&lt;ViewObject&gt; selectAllCategoryDetail() &#123; List&lt;CategoryDetail&gt; categoryDetails = categoryDetailService.selectAllCategoryDetail(); List&lt;ViewObject&gt; cds = new ArrayList&lt;&gt;(); for (CategoryDetail categoryDetail : categoryDetails) &#123; ViewObject vo = new ViewObject(); vo.set(\"categoryDetail\", categoryDetail); //System.out.println(\"categoryDetail 中的 categoryDetail id =\" + categoryDetail.getId() + \"category id = \" + categoryDetail.getCategory_id() + \" name = \" + categoryDetail.getName()); vo.set(\"id\", categoryDetail.getCategory_id()); cds.add(vo); &#125; return cds; &#125; 3、在 IndexController 类下，需要继承 BaseController.java 类 12345678910111213/** * 返回首页 * @param model * @return */ @RequestMapping(path = &#123;\"/\", \"/index\"&#125;) public String index(Model model) &#123; //模板引擎设置图书分类左边导航栏 model.addAttribute(\"vos\", selectAllCategory()); model.addAttribute(\"cds\", selectAllCategoryDetail()); //返回主页 return \"index\"; &#125; 4、抽离导航部分的代码 left.html 1234567891011121314151617&lt;!--左边图书分类导航栏--&gt;&lt;div class=\"c3_b1_left\"&gt; &lt;dl&gt; #foreach($vo in $vos) &lt;dd&gt; &lt;h1&gt;$!&#123;vo.category.name&#125;&lt;/h1&gt; &lt;p&gt; #foreach($cd in $cds) #if($vo.id == $cd.id) &lt;a href=\"/list\"&gt;$!&#123;cd.categoryDetail.name&#125;&lt;/a&gt; #end #end &lt;/p&gt; &lt;/dd&gt; #end &lt;/dl&gt;&lt;/div&gt; 5、首页中相应的位置引入 left.html 1#parse(\"left.html\") 这样就可以解决问题了，可是有时候我们需要控制循环的个数，因为我们网页端可能只需要特定的数据量 那么就需要中断 foreach，可以使用 #break 指令终止循环 123456#foreach( $vo in $vos ) #if( $foreach.count &gt; 5 ) #break #end $!&#123;vo.customer.Name&#125;#end 参考Velocity入门指南——第七章 循环指令","tags":[{"name":"Velocity","slug":"Velocity","permalink":"http://www.54tianzhisheng.cn/tags/Velocity/"}]},{"title":"AJAX 学习","date":"2017-06-22T16:00:00.000Z","path":"2017/06/23/AJAX/","text":"背景最近的项目中大量地方需要使用 AJAX，无奈，谁叫我既要写前台又要写后台呢，只好学习下这个技术点，主要参考 W3school 文档，下面记录下这些知识点，便于日后自己查阅，下面的一些测试代码建议在 W3school 中测试。 AJAX 基础： AJAX = Asynchronous JavaScript and XML（异步的 JavaScript 和 XML）。 AJAX 是一种在无需重新加载整个网页的情况下，能够更新部分网页的技术。 在很多网站可以见到使用这种技术。 AJAX - XMLHttpRequest 创建 XMLHttpRequest 对象 XMLHttpRequest 是 AJAX 的基础。XMLHttpRequest 用于在后台与服务器交换数据。这意味着可以在不重新加载整个网页的情况下，对网页的某部分进行更新。 创建 XMLHttpRequest 对象的语法： 1variable = new XMLHttpRequest(); 但是对于老版本的 Internet Explorer （IE5 和 IE6）却是使用 ActiveX 对象，所以在开发中为了适应大多数的浏览器，常使用如下： 123456789var xmlhttp;if (window.XMLHttpRequest) &#123;// code for IE7+, Firefox, Chrome, Opera, Safari xmlhttp = new XMLHttpRequest(); &#125;else &#123;// code for IE6, IE5 xmlhttp = new ActiveXObject(\"Microsoft.XMLHTTP\"); &#125; 向服务器发送请求 使用 XMLHttpRequest 对象的 open() 和 send() 方法： 12xmlhttp.open(\"GET\",\"test1.txt\",true);xmlhttp.send(); method description open(method, url, async) 规定请求的类型、URL 以及是否异步处理请求。method：请求的类型；GET 或 POST url：文件在服务器上的位置 async：true（异步）或 false（同步） send(string) 将请求发送到服务器。string：仅用于 POST 请求 GET 还是 POST？ 与 POST 相比，GET 更简单也更快，并且在大部分情况下都能用。 然而，在以下情况中，请使用 POST 请求： 无法使用缓存文件（更新服务器上的文件或数据库） 向服务器发送大量数据（POST 没有数据量限制） 发送包含未知字符的用户输入时，POST 比 GET 更稳定也更可靠 示例：GET 请求 1、简单的 GET 请求 12345678910111213141516171819202122232425262728293031323334&lt;html&gt;&lt;head&gt;&lt;script type=\"text/javascript\"&gt;function loadXMLDoc()&#123;var xmlhttp;if (window.XMLHttpRequest) &#123;// code for IE7+, Firefox, Chrome, Opera, Safari xmlhttp=new XMLHttpRequest(); &#125;else &#123;// code for IE6, IE5 xmlhttp=new ActiveXObject(\"Microsoft.XMLHTTP\"); &#125;xmlhttp.onreadystatechange=function() &#123; if (xmlhttp.readyState==4 &amp;&amp; xmlhttp.status==200) &#123; document.getElementById(\"myDiv\").innerHTML=xmlhttp.responseText; &#125; &#125;xmlhttp.open(\"GET\",\"/ajax/demo_get.asp?t=\" + Math.random(),true);xmlhttp.send();&#125;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;h2&gt;AJAX&lt;/h2&gt;&lt;button type=\"button\" onclick=\"loadXMLDoc()\"&gt;请求数据&lt;/button&gt;&lt;div id=\"myDiv\"&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 2、通过 GET 方法发送信息 12345678910111213141516171819202122232425262728293031323334&lt;html&gt;&lt;head&gt;&lt;script type=\"text/javascript\"&gt;function loadXMLDoc()&#123;var xmlhttp;if (window.XMLHttpRequest) &#123;// code for IE7+, Firefox, Chrome, Opera, Safari xmlhttp=new XMLHttpRequest(); &#125;else &#123;// code for IE6, IE5 xmlhttp=new ActiveXObject(\"Microsoft.XMLHTTP\"); &#125;xmlhttp.onreadystatechange=function() &#123; if (xmlhttp.readyState==4 &amp;&amp; xmlhttp.status==200) &#123; document.getElementById(\"myDiv\").innerHTML=xmlhttp.responseText; &#125; &#125;xmlhttp.open(\"GET\",\"/ajax/demo_get2.asp?fname=Bill&amp;lname=Gates\",true);xmlhttp.send();&#125;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;h2&gt;AJAX&lt;/h2&gt;&lt;button type=\"button\" onclick=\"loadXMLDoc()\"&gt;请求数据&lt;/button&gt;&lt;div id=\"myDiv\"&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 示例：POST 请求 1、简单 POST 请求 12345678910111213141516171819202122232425262728293031323334&lt;html&gt;&lt;head&gt;&lt;script type=\"text/javascript\"&gt;function loadXMLDoc()&#123;var xmlhttp;if (window.XMLHttpRequest) &#123;// code for IE7+, Firefox, Chrome, Opera, Safari xmlhttp=new XMLHttpRequest(); &#125;else &#123;// code for IE6, IE5 xmlhttp=new ActiveXObject(\"Microsoft.XMLHTTP\"); &#125;xmlhttp.onreadystatechange=function() &#123; if (xmlhttp.readyState==4 &amp;&amp; xmlhttp.status==200) &#123; document.getElementById(\"myDiv\").innerHTML=xmlhttp.responseText; &#125; &#125;xmlhttp.open(\"POST\",\"/ajax/demo_post.asp\",true);xmlhttp.send();&#125;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;h2&gt;AJAX&lt;/h2&gt;&lt;button type=\"button\" onclick=\"loadXMLDoc()\"&gt;请求数据&lt;/button&gt;&lt;div id=\"myDiv\"&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 2、像 HTML 表单那样 POST 数据，请使用 setRequestHeader() 来添加 HTTP 头。然后在 send() 方法中规定您希望发送的数据 123456789101112131415161718192021222324252627282930313233343536&lt;html&gt;&lt;head&gt;&lt;script type=\"text/javascript\"&gt;function loadXMLDoc()&#123;var xmlhttp;if (window.XMLHttpRequest) &#123;// code for IE7+, Firefox, Chrome, Opera, Safari xmlhttp=new XMLHttpRequest(); &#125;else &#123;// code for IE6, IE5 xmlhttp=new ActiveXObject(\"Microsoft.XMLHTTP\"); &#125;xmlhttp.onreadystatechange=function() &#123; if (xmlhttp.readyState==4 &amp;&amp; xmlhttp.status==200) &#123; document.getElementById(\"myDiv\").innerHTML=xmlhttp.responseText; &#125; &#125;xmlhttp.open(\"POST\",\"/ajax/demo_post2.asp\",true);xmlhttp.setRequestHeader(\"Content-type\",\"application/x-www-form-urlencoded\");xmlhttp.send(\"fname=Bill&amp;lname=Gates\");&#125;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;h2&gt;AJAX&lt;/h2&gt;&lt;button type=\"button\" onclick=\"loadXMLDoc()\"&gt;请求数据&lt;/button&gt;&lt;div id=\"myDiv\"&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 注意：setRequestHeader(header, value) 向请求添加 HTTP 头，header: 规定头的名称, value: 规定头的值。 url - 服务器上的文件 open() 方法的 url 参数是服务器上文件的地址： 1xmlhttp.open(\"GET\",\"ajax_test.asp\",true); 该文件可以是任何类型的文件，比如 .txt 和 .xml，或者服务器脚本文件，比如 .asp 和 .php （在传回响应之前，能够在服务器上执行任务）。 异步 - True or False ？ XMLHttpRequest 对象如果要用于 AJAX 的话，其 open() 方法的 async 参数必须设置为 true，对于 web 开发人员来说，发送异步请求是一个巨大的进步。很多在服务器执行的任务都相当费时。AJAX 出现之前，这可能会引起应用程序挂起或停止。 通过 AJAX，JavaScript 无需等待服务器的响应，而是： 在等待服务器响应时执行其他脚本 当响应就绪后对响应进行处理 Async = true 当使用 async = true 时，请规定在响应处于 onreadystatechange 事件中的就绪状态时执行的函数 123456789101112131415161718192021222324252627282930313233&lt;html&gt;&lt;head&gt;&lt;script type=\"text/javascript\"&gt;function loadXMLDoc()&#123;var xmlhttp;if (window.XMLHttpRequest) &#123;// code for IE7+, Firefox, Chrome, Opera, Safari xmlhttp=new XMLHttpRequest(); &#125;else &#123;// code for IE6, IE5 xmlhttp=new ActiveXObject(\"Microsoft.XMLHTTP\"); &#125;xmlhttp.onreadystatechange=function() &#123; if (xmlhttp.readyState==4 &amp;&amp; xmlhttp.status==200) &#123; document.getElementById(\"myDiv\").innerHTML=xmlhttp.responseText; &#125; &#125;xmlhttp.open(\"GET\",\"/ajax/test1.txt\",true);xmlhttp.send();&#125;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div id=\"myDiv\"&gt;&lt;h2&gt;Let AJAX change this text&lt;/h2&gt;&lt;/div&gt;&lt;button type=\"button\" onclick=\"loadXMLDoc()\"&gt;通过 AJAX 改变内容&lt;/button&gt;&lt;/body&gt;&lt;/html&gt; Async = false 如需使用 async = false，请将 open() 方法中的第三个参数改为 false 不推荐使用 async = false，但是对于一些小型的请求，也是可以的。 请记住，JavaScript 会等到服务器响应就绪才继续执行。如果服务器繁忙或缓慢，应用程序会挂起或停止。 注释：当您使用 async=false 时，请不要编写 onreadystatechange 函数 - 把代码放到 send() 语句后面即可： 123456789101112131415161718192021222324252627&lt;html&gt;&lt;head&gt;&lt;script type=\"text/javascript\"&gt;function loadXMLDoc()&#123;var xmlhttp;if (window.XMLHttpRequest) &#123;// code for IE7+, Firefox, Chrome, Opera, Safari xmlhttp=new XMLHttpRequest(); &#125;else &#123;// code for IE6, IE5 xmlhttp=new ActiveXObject(\"Microsoft.XMLHTTP\"); &#125;xmlhttp.open(\"GET\",\"/ajax/test1.txt\",false);xmlhttp.send();document.getElementById(\"myDiv\").innerHTML=xmlhttp.responseText;&#125;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;div id=\"myDiv\"&gt;&lt;h2&gt;Let AJAX change this text&lt;/h2&gt;&lt;/div&gt;&lt;button type=\"button\" onclick=\"loadXMLDoc()\"&gt;通过 AJAX 改变内容&lt;/button&gt;&lt;/body&gt;&lt;/html&gt; 服务器响应 使用 XMLHttpRequest 对象的 responseText 或 responseXML 属性。 responseText 获得字符串形式的响应数据。 responseXML 获得 XML 形式的响应数据。 1、responseText 属性 如果来自服务器的响应并非 XML，请使用 responseText 属性。 responseText 属性返回字符串形式的响应，因此您可以这样使用： 1document.getElementById(\"myDiv\").innerHTML=xmlhttp.responseText; 2、responseXML 属性 如果来自服务器的响应是 XML，而且需要作为 XML 对象进行解析，请使用 responseXML 属性： 请求 books.xml 文件，并解析响应： 12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;html&gt;&lt;head&gt;&lt;script type=\"text/javascript\"&gt;function loadXMLDoc()&#123;var xmlhttp;var txt,x,i;if (window.XMLHttpRequest) &#123;// code for IE7+, Firefox, Chrome, Opera, Safari xmlhttp=new XMLHttpRequest(); &#125;else &#123;// code for IE6, IE5 xmlhttp=new ActiveXObject(\"Microsoft.XMLHTTP\"); &#125;xmlhttp.onreadystatechange=function() &#123; if (xmlhttp.readyState==4 &amp;&amp; xmlhttp.status==200) &#123; xmlDoc=xmlhttp.responseXML; txt=\"\"; x=xmlDoc.getElementsByTagName(\"title\"); for (i=0;i&lt;x.length;i++) &#123; txt=txt + x[i].childNodes[0].nodeValue + \"&lt;br /&gt;\"; &#125; document.getElementById(\"myDiv\").innerHTML=txt; &#125; &#125;xmlhttp.open(\"GET\",\"/example/xmle/books.xml\",true);xmlhttp.send();&#125;&lt;/script&gt;&lt;/head&gt;&lt;body&gt;&lt;h2&gt;My Book Collection:&lt;/h2&gt;&lt;div id=\"myDiv\"&gt;&lt;/div&gt;&lt;button type=\"button\" onclick=\"loadXMLDoc()\"&gt;获得我的图书收藏列表&lt;/button&gt;&lt;/body&gt;&lt;/html&gt; onreadystatechange 事件 当请求被发送到服务器时，我们需要执行一些基于响应的任务。每当 readyState 改变时，就会触发 onreadystatechange 事件。readyState 属性存有 XMLHttpRequest 的状态信息。 下面是 XMLHttpRequest 对象的三个重要的属性： onreadystatechange 存储函数（或函数名），每当 readyState 属性改变时，就会调用该函数 readyState 存有 XMLHttpRequest 的状态。从 0 到 4 发生变化。 0: 请求未初始化 1: 服务器连接已建立 2: 请求已接收 3: 请求处理中 4: 请求已完成，且响应已就绪 status 200: “OK” 404: 未找到页面 在 onreadystatechange 事件中，我们规定当服务器响应已做好被处理的准备时所执行的任务。 当 readyState 等于 4 且状态为 200 时，表示响应已就绪： 1234567xmlhttp.onreadystatechange=function() &#123; if (xmlhttp.readyState==4 &amp;&amp; xmlhttp.status==200) &#123; document.getElementById(\"myDiv\").innerHTML=xmlhttp.responseText; &#125; &#125; 使用 Callback 函数 callback 函数是一种以参数形式传递给另一个函数的函数。 如果您的网站上存在多个 AJAX 任务，那么您应该为创建 XMLHttpRequest 对象编写一个标准 的函数，并为每个 AJAX 任务调用该函数。 该函数调用应该包含 URL 以及发生 onreadystatechange 事件时执行的任务（每次调用可能不尽相同）： 12345678910function myFunction()&#123;loadXMLDoc(\"ajax_info.txt\",function() &#123; if (xmlhttp.readyState==4 &amp;&amp; xmlhttp.status==200) &#123; document.getElementById(\"myDiv\").innerHTML=xmlhttp.responseText; &#125; &#125;);&#125; AJAX - 高级ASP/PHP 请求实例 - AJAX 用于创造动态性更强的应用程序。 AJAX 可用来与数据库进行动态通信。 AJAX 可用来与 XML 文件进行交互式通信。 AJAX 实例使用 XMLHttpRequest 对象的实例","tags":[{"name":"AJAX","slug":"AJAX","permalink":"http://www.54tianzhisheng.cn/tags/AJAX/"}]},{"title":"Java IO流学习超详细总结（图文并茂）","date":"2017-06-22T16:00:00.000Z","path":"2017/06/23/java-io/","text":"Java流操作有关的类或接口：Java流类图结构： 流的概念和作用流是一组有顺序的，有起点和终点的字节集合，是对数据传输的总称或抽象。即数据在两设备间的传输称为流，流的本质是数据传输，根据数据传输特性将流抽象为各种类，方便更直观的进行数据操作。 IO流的分类 根据处理数据类型的不同分为：字符流和字节流 根据数据流向不同分为：输入流和输出流 字符流和字节流字符流的由来： 因为数据编码的不同，而有了对字符进行高效操作的流对象。本质其实就是基于字节流读取时，去查了指定的码表。 字节流和字符流的区别： 读写单位不同：字节流以字节（8bit）为单位，字符流以字符为单位，根据码表映射字符，一次可能读多个字节。 处理对象不同：字节流能处理所有类型的数据（如图片、avi等），而字符流只能处理字符类型的数据。 结论：只要是处理纯文本数据，就优先考虑使用字符流。 除此之外都使用字节流。 输入流和输出流对输入流只能进行读操作，对输出流只能进行写操作，程序中需要根据待传输数据的不同特性而使用不同的流。 Java IO流对象1.输入字节流InputStreamIO 中输入字节流的继承图可见上图，可以看出： InputStream 是所有的输入字节流的父类，它是一个抽象类。 ByteArrayInputStream、StringBufferInputStream、FileInputStream 是三种基本的介质流，它们分别从Byte 数组、StringBuffer、和本地文件中读取数据。PipedInputStream 是从与其它线程共用的管道中读取数据，与Piped 相关的知识后续单独介绍。 ObjectInputStream 和所有FilterInputStream 的子类都是装饰流（装饰器模式的主角）。 2.输出字节流OutputStreamIO 中输出字节流的继承图可见上图，可以看出： OutputStream 是所有的输出字节流的父类，它是一个抽象类。 ByteArrayOutputStream、FileOutputStream 是两种基本的介质流，它们分别向Byte 数组、和本地文件中写入数据。PipedOutputStream 是向与其它线程共用的管道中写入数据， ObjectOutputStream 和所有FilterOutputStream 的子类都是装饰流。 3.字节流的输入与输出的对应 图中蓝色的为主要的对应部分，红色的部分就是不对应部分。紫色的虚线部分代表这些流一般要搭配使用。从上面的图中可以看出Java IO 中的字节流是极其对称的。“存在及合理”我们看看这些字节流中不太对称的几个类吧！ LineNumberInputStream 主要完成从流中读取数据时，会得到相应的行号，至于什么时候分行、在哪里分行是由改类主动确定的，并不是在原始中有这样一个行号。在输出部分没有对应的部分，我们完全可以自己建立一个LineNumberOutputStream，在最初写入时会有一个基准的行号，以后每次遇到换行时会在下一行添加一个行号，看起来也是可以的。好像更不入流了。 PushbackInputStream 的功能是查看最后一个字节，不满意就放入缓冲区。主要用在编译器的语法、词法分析部分。输出部分的BufferedOutputStream 几乎实现相近的功能。 StringBufferInputStream 已经被Deprecated，本身就不应该出现在InputStream 部分，主要因为String 应该属于字符流的范围。已经被废弃了，当然输出部分也没有必要需要它了！还允许它存在只是为了保持版本的向下兼容而已。 SequenceInputStream 可以认为是一个工具类，将两个或者多个输入流当成一个输入流依次读取。完全可以从IO 包中去除，还完全不影响IO 包的结构，却让其更“纯洁”――纯洁的Decorator 模式。 PrintStream 也可以认为是一个辅助工具。主要可以向其他输出流，或者FileInputStream 写入数据，本身内部实现还是带缓冲的。本质上是对其它流的综合运用的一个工具而已。一样可以踢出IO 包！System.out 和System.out 就是PrintStream 的实例！ 4.字符输入流Reader在上面的继承关系图中可以看出： Reader 是所有的输入字符流的父类，它是一个抽象类。 CharReader、StringReader 是两种基本的介质流，它们分别将Char 数组、String中读取数据。PipedReader 是从与其它线程共用的管道中读取数据。 BufferedReader 很明显就是一个装饰器，它和其子类负责装饰其它Reader 对象。 FilterReader 是所有自定义具体装饰流的父类，其子类PushbackReader 对Reader 对象进行装饰，会增加一个行号。 InputStreamReader 是一个连接字节流和字符流的桥梁，它将字节流转变为字符流。FileReader 可以说是一个达到此功能、常用的工具类，在其源代码中明显使用了将FileInputStream 转变为Reader 的方法。我们可以从这个类中得到一定的技巧。Reader 中各个类的用途和使用方法基本和InputStream 中的类使用一致。后面会有Reader 与InputStream 的对应关系。 5.字符输出流Writer在上面的关系图中可以看出： Writer 是所有的输出字符流的父类，它是一个抽象类。 CharArrayWriter、StringWriter 是两种基本的介质流，它们分别向Char 数组、String 中写入数据。PipedWriter 是向与其它线程共用的管道中写入数据， BufferedWriter 是一个装饰器为Writer 提供缓冲功能。 PrintWriter 和PrintStream 极其类似，功能和使用也非常相似。 OutputStreamWriter 是OutputStream 到Writer 转换的桥梁，它的子类FileWriter 其实就是一个实现此功能的具体类（具体可以研究一SourceCode）。功能和使用和OutputStream 极其类似，后面会有它们的对应图。 6.字符流的输入与输出的对应 7.字符流与字节流转换转换流的特点： 其是字符流和字节流之间的桥梁 可对读取到的字节数据经过指定编码转换成字符 可对读取到的字符数据经过指定编码转换成字节 何时使用转换流？ 当字节和字符之间有转换动作时； 流操作的数据需要编码或解码时。 具体的对象体现： InputStreamReader:字节到字符的桥梁 OutputStreamWriter:字符到字节的桥梁 这两个流对象是字符体系中的成员，它们有转换作用，本身又是字符流，所以在构造的时候需要传入字节流对象进来。 8.File类File类是对文件系统中文件以及文件夹进行封装的对象，可以通过对象的思想来操作文件和文件夹。 File类保存文件或目录的各种元数据信息，包括文件名、文件长度、最后修改时间、是否可读、获取当前文件的路径名，判断指定文件是否存在、获得当前目录中的文件列表，创建、删除文件和目录等方法。 9.RandomAccessFile类该对象并不是流体系中的一员，其封装了字节流，同时还封装了一个缓冲区（字符数组），通过内部的指针来操作字符数组中的数据。 该对象特点： 该对象只能操作文件，所以构造函数接收两种类型的参数：a.字符串文件路径；b.File对象。 该对象既可以对文件进行读操作，也能进行写操作，在进行对象实例化时可指定操作模式(r,rw) 注意：该对象在实例化时，如果要操作的文件不存在，会自动创建；如果文件存在，写数据未指定位置，会从头开始写，即覆盖原有的内容。 可以用于多线程下载或多个线程同时写数据到文件。 更多精彩文章请见我的个人博客地址：http://www.54tianzhisheng.cn","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"IO","slug":"IO","permalink":"http://www.54tianzhisheng.cn/tags/IO/"}]},{"title":"java.sql.SQLException Field 'id' doesn't have a default value","date":"2017-06-19T16:00:00.000Z","path":"2017/06/20/Java-error1/","text":"1、错误描述 在做一个电商网站项目时，使用 Mybatis + MySQL 时出现问题 Caused by: java.sql.SQLException: Field &#39;id&#39; doesn&#39;t have a default value ，网上很多人说是 MyBatis 插入数据行 ID 没生成自增。但是我尝试好久，没解决该问题。 2、错误原因 后来才发现是因为创建数据库时的建表语句中的 id 是主键的，但是在插入的过程中，没有给予数值，并且没有让 id 自增。 3、解决办法 修改数据库表中的id，让其自增（在插入的过程中，不插入id数据时）。 （我是直接将整个数据库都导出来，然后在每个表的 id 后面加上一个 auto_increment）, 如下 ：12345678910CREATE TABLE `d_user` ( `id` int(11) NOT NULL auto_increment, `name` varchar(45) DEFAULT NULL, `password` varchar(45) DEFAULT NULL, `zip` varchar(45) DEFAULT NULL, `address` varchar(45) DEFAULT NULL, `phone` varchar(45) DEFAULT NULL, `email` varchar(45) DEFAULT NULL, PRIMARY KEY (`id`))","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"}]},{"title":"中缀表达式转换成前缀和后缀表达式这类题目的超实用解题技巧","date":"2017-06-18T16:00:00.000Z","path":"2017/06/19/中缀表达式转换成前缀和后缀表达式这类题目的超实用解题技巧/","text":"看到了标题如果还不了解的这几个概念的请先看看博客：详解前缀、中缀、后缀表达式 先给几个中缀表达式转换成后缀表达式题目做做吧，最后我们在总结超实用的技巧！！ 1. 表达式“X=A+B*（C–D）/E”的后缀表示形式可以为 A. XAB+CDE/-*= B. XA+BC-DE/*= C. XABCD-*E/+= D. XABCDE+*/= 先把答案说出来吧，不过你可以自己先好好的想想可以怎么做才能更快的把答案选出来呢？ 答案：C 先看看下面图片中的这种方法如何？ 2. 已知-算术表达式的中缀表达式为a-(b+c/d)*e,其后缀形式为() A. -a+b*c/d B. -a+b*cd/e C. -+*abc/de D. abcd/+e*- 答案：D 3. 算术表达式a+b*(c+d/e)转为后缀表达式后为() A. ab+cde/* B. abcde/+*+ C. abcde/*++ D. abcde*/++ 答案：B 4. 表达式a*(b+c)-d的后缀表达式是() A. abcd*+- B. abc+*d- C. abc*+d- D. -+*abcd 答案：B 好，题目我们也看了这么多了，那我们该如何解决这一类的题目呢？如果你看了文章首部的那篇 博客的话，那你肯定会觉得那个解法很复杂，如果真的是在笔试中出现这样的题目，那得耗费不 少的时间啊。有人要问了，说了那么一堆，那究竟有没有什么快速的方法呢或者说有没有什么简 单的方法可以直接口算的把答案写出来呢，答案是：有的！而且还真的是特别的简单！！！ 解题重点： 这里我给出一个中缀表达式~ a+b*c-(d+e) 第一步：按照运算符的优先级对所有的运算单位加括号 则式子变成拉：((a+(b*c))-(d+e)) 第二步：转换前缀与后缀表达式 1. 前缀表达式：把运算符号移动到对应的括号前面 则变成拉：-( +(a *(bc)) +(de)) 把括号去掉：-+a*bc+de 前缀表达式出现 2. 后缀表达式：把运算符号移动到对应的括号后面 则变成拉：((a(bc)* )+ (de)+ )- 把括号去掉：abc*+de+- 后缀表达式出现 发现没有，前缀表达式，后缀表达式是不需要用括号来进行优先级的确定的。 如果你习惯拉他的运算方法。计算的时候也就是从两个操作数的前面 或者后面找运算符。而不是中间找，那么也就直接可以口算啦！ 你说这种方法是不是很简单啊！！！ 现在你再去看看刚才的那四道题目，是不是很简单的答案就口算出来了啊！！！ 6不6？","tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://www.54tianzhisheng.cn/tags/数据结构/"},{"name":"表达式","slug":"表达式","permalink":"http://www.54tianzhisheng.cn/tags/表达式/"}]},{"title":"循环队列的相关条件和公式","date":"2017-06-17T16:00:00.000Z","path":"2017/06/18/循环队列的相关条件和公式/","text":"循环队列的相关条件和公式：队尾指针是rear,队头是front，其中QueueSize为循环队列的最大长度 队空条件：rear==front 队满条件：(rear+1) %QueueSIze==front 计算队列长度：（rear-front+QueueSize）%QueueSize 入队：（rear+1）%QueueSize 出队：（front+1）%QueueSize 队列中元素的个数： (rear-front+QueueSize)%QueueSize","tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://www.54tianzhisheng.cn/tags/数据结构/"},{"name":"算法","slug":"算法","permalink":"http://www.54tianzhisheng.cn/tags/算法/"},{"name":"循环队列","slug":"循环队列","permalink":"http://www.54tianzhisheng.cn/tags/循环队列/"}]},{"title":"Bootstrap入门需掌握的知识点（二）","date":"2017-06-17T16:00:00.000Z","path":"2017/06/18/Bootstrap入门需掌握的知识点（二）/","text":"相关阅读：Bootstrap入门需掌握的知识点（一）表格基本实例为任意 &lt;table&gt; 标签添加 .table 类可以为其赋予基本的样式 — 少量的内补（padding）和水平方向的分隔线。 123&lt;table class=&quot;table&quot;&gt; ...&lt;/table&gt; 条纹状表格通过 .table-striped 类可以给 &lt;tbody&gt; 之内的每一行增加斑马条纹样式。 123&lt;table class=\"table table-striped\"&gt; ...&lt;/table&gt; 带边框的表格添加 .table-bordered 类为表格和其中的每个单元格增加边框。 123&lt;table class=\"table table-bordered\"&gt; ...&lt;/table&gt; 鼠标悬停通过添加 .table-hover 类可以让 中的每一行对鼠标悬停状态作出响应。 123&lt;table class=\"table table-hover\"&gt; ...&lt;/table&gt; 状态类通过这些状态类可以为行或单元格设置颜色。 Class 描述 .active 鼠标悬停在行或单元格上时所设置的颜色 .success 标识成功或积极的动作 .info 标识普通的提示信息或动作 .warning 标识警告或需要用户注意 .danger 标识危险或潜在的带来负面影响的动作 123456789101112131415&lt;!-- On rows --&gt;&lt;tr class=\"active\"&gt;...&lt;/tr&gt;&lt;tr class=\"success\"&gt;...&lt;/tr&gt;&lt;tr class=\"warning\"&gt;...&lt;/tr&gt;&lt;tr class=\"danger\"&gt;...&lt;/tr&gt;&lt;tr class=\"info\"&gt;...&lt;/tr&gt;&lt;!-- On cells (`td` or `th`) --&gt;&lt;tr&gt; &lt;td class=\"active\"&gt;...&lt;/td&gt; &lt;td class=\"success\"&gt;...&lt;/td&gt; &lt;td class=\"warning\"&gt;...&lt;/td&gt; &lt;td class=\"danger\"&gt;...&lt;/td&gt; &lt;td class=\"info\"&gt;...&lt;/td&gt;&lt;/tr&gt; 响应式表格将任何 .table 元素包裹在 .table-responsive 元素内，即可创建响应式表格，其会在小屏幕设备上（小于768px）水平滚动。当屏幕大于 768px 宽度时，水平滚动条消失。 12345&lt;div class=\"table-responsive\"&gt; &lt;table class=\"table\"&gt; ... &lt;/table&gt;&lt;/div&gt; 表单基本实例单独的表单控件会被自动赋予一些全局样式。所有设置了 .form-control 类的 &lt;input&gt;、&lt;textarea&gt; 和 &lt;select&gt; 元素都将被默认设置宽度属性为 width: 100%;。 将 label 元素和前面提到的控件包裹在 .form-group 中可以获得最好的排列。 123456789101112131415161718192021&lt;form role=\"form\"&gt; &lt;div class=\"form-group\"&gt; &lt;label for=\"exampleInputEmail1\"&gt;Email address&lt;/label&gt; &lt;input type=\"email\" class=\"form-control\" id=\"exampleInputEmail1\" placeholder=\"Enter email\"&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;label for=\"exampleInputPassword1\"&gt;Password&lt;/label&gt; &lt;input type=\"password\" class=\"form-control\" id=\"exampleInputPassword1\" placeholder=\"Password\"&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;label for=\"exampleInputFile\"&gt;File input&lt;/label&gt; &lt;input type=\"file\" id=\"exampleInputFile\"&gt; &lt;p class=\"help-block\"&gt;Example block-level help text here.&lt;/p&gt; &lt;/div&gt; &lt;div class=\"checkbox\"&gt; &lt;label&gt; &lt;input type=\"checkbox\"&gt; Check me out &lt;/label&gt; &lt;/div&gt; &lt;button type=\"submit\" class=\"btn btn-default\"&gt;Submit&lt;/button&gt;&lt;/form&gt; 注意：不要将表单组直接和输入框组混合使用，建议将输入框组嵌套到表单组中使用。 内联表单为 &lt;form&gt; 元素添加 .form-inline 类可使其内容左对齐并且表现为 inline-block 级别的控件。只适用于视口（viewport）至少在 768px 宽度时（视口宽度再小的话就会使表单折叠）。 注意： 需要手动设置宽度在 Bootstrap 中，输入框和单选/多选框控件默认被设置为 width: 100%; 宽度。在内联表单，我们将这些元素的宽度设置为 width: auto;，因此，多个控件可以排列在同一行。根据你的布局需求，可能需要一些额外的定制化组件。 一定要添加 label 标签如果你没有为每个输入控件设置 label 标签，屏幕阅读器将无法正确识别。对于这些内联表单，你可以通过为label 设置 .sr-only 类将其隐藏。 12345678910111213141516171819202122&lt;form class=\"form-inline\" role=\"form\"&gt; &lt;div class=\"form-group\"&gt; &lt;label class=\"sr-only\" for=\"exampleInputEmail2\"&gt;Email address&lt;/label&gt; &lt;input type=\"email\" class=\"form-control\" id=\"exampleInputEmail2\" placeholder=\"Enter email\"&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;div class=\"input-group\"&gt; &lt;div class=\"input-group-addon\"&gt;@&lt;/div&gt; &lt;input class=\"form-control\" type=\"email\" placeholder=\"Enter email\"&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;label class=\"sr-only\" for=\"exampleInputPassword2\"&gt;Password&lt;/label&gt; &lt;input type=\"password\" class=\"form-control\" id=\"exampleInputPassword2\" placeholder=\"Password\"&gt; &lt;/div&gt; &lt;div class=\"checkbox\"&gt; &lt;label&gt; &lt;input type=\"checkbox\"&gt; Remember me &lt;/label&gt; &lt;/div&gt; &lt;button type=\"submit\" class=\"btn btn-default\"&gt;Sign in&lt;/button&gt;&lt;/form&gt; 水平排列的表单通过为表单添加 .form-horizontal 类，并联合使用 Bootstrap 预置的栅格类，可以将 label 标签和控件组水平并排布局。这样做将改变 .form-group 的行为，使其表现为栅格系统中的行（row），因此就无需再额外添加 .row 了。 12345678910111213141516171819202122232425262728&lt;form class=\"form-horizontal\" role=\"form\"&gt; &lt;div class=\"form-group\"&gt; &lt;label for=\"inputEmail3\" class=\"col-sm-2 control-label\"&gt;Email&lt;/label&gt; &lt;div class=\"col-sm-10\"&gt; &lt;input type=\"email\" class=\"form-control\" id=\"inputEmail3\" placeholder=\"Email\"&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;label for=\"inputPassword3\" class=\"col-sm-2 control-label\"&gt;Password&lt;/label&gt; &lt;div class=\"col-sm-10\"&gt; &lt;input type=\"password\" class=\"form-control\" id=\"inputPassword3\" placeholder=\"Password\"&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;div class=\"col-sm-offset-2 col-sm-10\"&gt; &lt;div class=\"checkbox\"&gt; &lt;label&gt; &lt;input type=\"checkbox\"&gt; Remember me &lt;/label&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;div class=\"col-sm-offset-2 col-sm-10\"&gt; &lt;button type=\"submit\" class=\"btn btn-default\"&gt;Sign in&lt;/button&gt; &lt;/div&gt; &lt;/div&gt;&lt;/form&gt; 被支持的控件表单布局实例中展示了其所支持的标准表单控件。 1、输入框包括大部分表单控件、文本输入域控件，还支持所有 HTML5 类型的输入控件：text、password、datetime、datetime-local、date、month、time、week、number、email、url、search、tel 和 color。 1&lt;input type=\"text\" class=\"form-control\" placeholder=\"Text input\"&gt; 如需在文本输入域 &lt;input&gt; 前面或后面添加文本内容或按钮控件，请参考输入控件组。 2、文本域支持多行文本的表单控件。可根据需要改变 rows 属性。 1&lt;textarea class=\"form-control\" rows=\"3\"&gt;&lt;/textarea&gt; 3、多选和单选框多选框（checkbox）用于选择列表中的一个或多个选项，而单选框（radio）用于从多个选项中只选择一个。 设置了 disabled 属性的单选或多选框都能被赋予合适的样式。对于和多选或单选框联合使用的 &lt;label&gt; 标签，如果也希望将悬停于上方的鼠标设置为“禁止点击”的样式，请将 .disabled 类赋予 .radio、.radio-inline、.checkbox 、.checkbox-inline 或 &lt;fieldset&gt;。 12345678910111213141516171819202122232425262728293031&lt;div class=\"checkbox\"&gt; &lt;label&gt; &lt;input type=\"checkbox\" value=\"\"&gt; Option one is this and that&amp;mdash;be sure to include why it's great &lt;/label&gt;&lt;/div&gt;&lt;div class=\"checkbox disabled\"&gt; &lt;label&gt; &lt;input type=\"checkbox\" value=\"\" disabled&gt; Option two is disabled &lt;/label&gt;&lt;/div&gt;&lt;div class=\"radio\"&gt; &lt;label&gt; &lt;input type=\"radio\" name=\"optionsRadios\" id=\"optionsRadios1\" value=\"option1\" checked&gt; Option one is this and that&amp;mdash;be sure to include why it's great &lt;/label&gt;&lt;/div&gt;&lt;div class=\"radio\"&gt; &lt;label&gt; &lt;input type=\"radio\" name=\"optionsRadios\" id=\"optionsRadios2\" value=\"option2\"&gt; Option two can be something else and selecting it will deselect option one &lt;/label&gt;&lt;/div&gt;&lt;div class=\"radio disabled\"&gt; &lt;label&gt; &lt;input type=\"radio\" name=\"optionsRadios\" id=\"optionsRadios3\" value=\"option3\" disabled&gt; Option three is disabled &lt;/label&gt;&lt;/div&gt; 内联单选和多选框通过将 .checkbox-inline 或 .radio-inline 类应用到一系列的多选框（checkbox）或单选框（radio）控件上，可以使这些控件排列在一行。 12345678910111213141516171819&lt;label class=\"checkbox-inline\"&gt; &lt;input type=\"checkbox\" id=\"inlineCheckbox1\" value=\"option1\"&gt; 1&lt;/label&gt;&lt;label class=\"checkbox-inline\"&gt; &lt;input type=\"checkbox\" id=\"inlineCheckbox2\" value=\"option2\"&gt; 2&lt;/label&gt;&lt;label class=\"checkbox-inline\"&gt; &lt;input type=\"checkbox\" id=\"inlineCheckbox3\" value=\"option3\"&gt; 3&lt;/label&gt;&lt;label class=\"radio-inline\"&gt; &lt;input type=\"radio\" name=\"inlineRadioOptions\" id=\"inlineRadio1\" value=\"option1\"&gt; 1&lt;/label&gt;&lt;label class=\"radio-inline\"&gt; &lt;input type=\"radio\" name=\"inlineRadioOptions\" id=\"inlineRadio2\" value=\"option2\"&gt; 2&lt;/label&gt;&lt;label class=\"radio-inline\"&gt; &lt;input type=\"radio\" name=\"inlineRadioOptions\" id=\"inlineRadio3\" value=\"option3\"&gt; 3&lt;/label&gt; 复选框 12345678910&lt;div class=\"checkbox\"&gt; &lt;label&gt; &lt;input type=\"checkbox\" id=\"blankCheckbox\" value=\"option1\"&gt; &lt;/label&gt;&lt;/div&gt;&lt;div class=\"radio\"&gt; &lt;label&gt; &lt;input type=\"radio\" name=\"blankRadio\" id=\"blankRadio1\" value=\"option1\"&gt; &lt;/label&gt;&lt;/div&gt; 下拉列表（select）使用默认选项或添加 multiple 属性可以同时显示多个选项。 123456789101112131415&lt;select class=\"form-control\"&gt; &lt;option&gt;1&lt;/option&gt; &lt;option&gt;2&lt;/option&gt; &lt;option&gt;3&lt;/option&gt; &lt;option&gt;4&lt;/option&gt; &lt;option&gt;5&lt;/option&gt;&lt;/select&gt;&lt;select multiple class=\"form-control\"&gt; &lt;option&gt;1&lt;/option&gt; &lt;option&gt;2&lt;/option&gt; &lt;option&gt;3&lt;/option&gt; &lt;option&gt;4&lt;/option&gt; &lt;option&gt;5&lt;/option&gt;&lt;/select&gt; 静态控件如果需要在表单中将一行纯文本和 label 元素放置于同一行，为 &lt;p&gt; 元素添加 .form-control-static 类即可。 1234567891011121314&lt;form class=\"form-horizontal\" role=\"form\"&gt; &lt;div class=\"form-group\"&gt; &lt;label class=\"col-sm-2 control-label\"&gt;Email&lt;/label&gt; &lt;div class=\"col-sm-10\"&gt; &lt;p class=\"form-control-static\"&gt;email@example.com&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;label for=\"inputPassword\" class=\"col-sm-2 control-label\"&gt;Password&lt;/label&gt; &lt;div class=\"col-sm-10\"&gt; &lt;input type=\"password\" class=\"form-control\" id=\"inputPassword\" placeholder=\"Password\"&gt; &lt;/div&gt; &lt;/div&gt;&lt;/form&gt; 1234567891011&lt;form class=\"form-inline\" role=\"form\"&gt; &lt;div class=\"form-group\"&gt; &lt;label class=\"sr-only\"&gt;Email&lt;/label&gt; &lt;p class=\"form-control-static\"&gt;email@example.com&lt;/p&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;label for=\"inputPassword2\" class=\"sr-only\"&gt;Password&lt;/label&gt; &lt;input type=\"password\" class=\"form-control\" id=\"inputPassword2\" placeholder=\"Password\"&gt; &lt;/div&gt; &lt;button type=\"submit\" class=\"btn btn-default\"&gt;Confirm identity&lt;/button&gt;&lt;/form&gt; 输入框焦点我们将某些表单控件的默认 outline 样式移除，然后对 :focus 状态赋予 box-shadow 属性。 被禁用的输入框为输入框设置 disabled 属性可以防止用户输入，并能对外观做一些修改，使其更直观。 1&lt;input class=\"form-control\" id=\"disabledInput\" type=\"text\" placeholder=\"Disabled input here...\" disabled&gt; 1234567891011121314151617181920&lt;form role=\"form\"&gt; &lt;fieldset disabled&gt; &lt;div class=\"form-group\"&gt; &lt;label for=\"disabledTextInput\"&gt;Disabled input&lt;/label&gt; &lt;input type=\"text\" id=\"disabledTextInput\" class=\"form-control\" placeholder=\"Disabled input\"&gt; &lt;/div&gt; &lt;div class=\"form-group\"&gt; &lt;label for=\"disabledSelect\"&gt;Disabled select menu&lt;/label&gt; &lt;select id=\"disabledSelect\" class=\"form-control\"&gt; &lt;option&gt;Disabled select&lt;/option&gt; &lt;/select&gt; &lt;/div&gt; &lt;div class=\"checkbox\"&gt; &lt;label&gt; &lt;input type=\"checkbox\"&gt; Can't check this &lt;/label&gt; &lt;/div&gt; &lt;button type=\"submit\" class=\"btn btn-primary\"&gt;Submit&lt;/button&gt; &lt;/fieldset&gt;&lt;/form&gt; 只读输入框为输入框设置 readonly 属性可以禁止用户输入，并且输入框的样式也是禁用状态。 1&lt;input class=\"form-control\" type=\"text\" placeholder=\"Readonly input here…\" readonly&gt; 校验状态Bootstrap 对表单控件的校验状态，如 error、warning 和 success 状态，都定义了样式。使用时，添加 .has-warning、.has-error 或 .has-success 类到这些控件的父元素即可。任何包含在此元素之内的 .control-label、.form-control 和 .help-block 元素都将接受这些校验状态的样式。 123456789101112131415161718192021222324252627282930313233343536&lt;div class=\"form-group has-success\"&gt; &lt;label class=\"control-label\" for=\"inputSuccess1\"&gt;Input with success&lt;/label&gt; &lt;input type=\"text\" class=\"form-control\" id=\"inputSuccess1\"&gt;&lt;/div&gt;&lt;div class=\"form-group has-warning\"&gt; &lt;label class=\"control-label\" for=\"inputWarning1\"&gt;Input with warning&lt;/label&gt; &lt;input type=\"text\" class=\"form-control\" id=\"inputWarning1\"&gt;&lt;/div&gt;&lt;div class=\"form-group has-error\"&gt; &lt;label class=\"control-label\" for=\"inputError1\"&gt;Input with error&lt;/label&gt; &lt;input type=\"text\" class=\"form-control\" id=\"inputError1\"&gt;&lt;/div&gt;&lt;div class=\"has-success\"&gt; &lt;div class=\"checkbox\"&gt; &lt;label&gt; &lt;input type=\"checkbox\" id=\"checkboxSuccess\" value=\"option1\"&gt; Checkbox with success &lt;/label&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class=\"has-warning\"&gt; &lt;div class=\"checkbox\"&gt; &lt;label&gt; &lt;input type=\"checkbox\" id=\"checkboxWarning\" value=\"option1\"&gt; Checkbox with warning &lt;/label&gt; &lt;/div&gt;&lt;/div&gt;&lt;div class=\"has-error\"&gt; &lt;div class=\"checkbox\"&gt; &lt;label&gt; &lt;input type=\"checkbox\" id=\"checkboxError\" value=\"option1\"&gt; Checkbox with error &lt;/label&gt; &lt;/div&gt;&lt;/div&gt; 添加额外的图标你还可以针对校验状态为输入框添加额外的图标。只需设置相应的 .has-feedback 类并添加正确的图标即可。 Feedback icons only work with textual &lt;input class=&quot;form-control&quot;&gt; elements. 图标、label 和输入控件组对于不带有 label 标签的输入框以及右侧带有附加组件的输入框组，需要手动为其图标定位。为了让所有用户都能访问你的网站，我们强烈建议为所有输入框添加 label 标签。如果你不希望将 label 标签展示出来，可以通过添加 sr-only 类来实现。如果的确不能添加 label 标签，请调整图标的 top 值。对于输入框组，请根据你的实际情况调整 right 值。 123456789101112131415&lt;div class=\"form-group has-success has-feedback\"&gt; &lt;label class=\"control-label\" for=\"inputSuccess2\"&gt;Input with success&lt;/label&gt; &lt;input type=\"text\" class=\"form-control\" id=\"inputSuccess2\"&gt; &lt;span class=\"glyphicon glyphicon-ok form-control-feedback\"&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=\"form-group has-warning has-feedback\"&gt; &lt;label class=\"control-label\" for=\"inputWarning2\"&gt;Input with warning&lt;/label&gt; &lt;input type=\"text\" class=\"form-control\" id=\"inputWarning2\"&gt; &lt;span class=\"glyphicon glyphicon-warning-sign form-control-feedback\"&gt;&lt;/span&gt;&lt;/div&gt;&lt;div class=\"form-group has-error has-feedback\"&gt; &lt;label class=\"control-label\" for=\"inputError2\"&gt;Input with error&lt;/label&gt; &lt;input type=\"text\" class=\"form-control\" id=\"inputError2\"&gt; &lt;span class=\"glyphicon glyphicon-remove form-control-feedback\"&gt;&lt;/span&gt;&lt;/div&gt; 控件尺寸通过 .input-lg 类似的类可以为控件设置高度，通过 .col-lg-* 类似的类可以为控件设置宽度。 高度尺寸创建大一些或小一些的表单控件以匹配按钮尺寸。 1234567&lt;input class=\"form-control input-lg\" type=\"text\" placeholder=\".input-lg\"&gt;&lt;input class=\"form-control\" type=\"text\" placeholder=\"Default input\"&gt;&lt;input class=\"form-control input-sm\" type=\"text\" placeholder=\".input-sm\"&gt;&lt;select class=\"form-control input-lg\"&gt;...&lt;/select&gt;&lt;select class=\"form-control\"&gt;...&lt;/select&gt;&lt;select class=\"form-control input-sm\"&gt;...&lt;/select&gt; 按钮预定义样式使用下面列出的类可以快速创建一个带有预定义样式的按钮。 1234567891011121314151617181920&lt;!-- Standard button --&gt;&lt;button type=\"button\" class=\"btn btn-default\"&gt;Default&lt;/button&gt;&lt;!-- Provides extra visual weight and identifies the primary action in a set of buttons --&gt;&lt;button type=\"button\" class=\"btn btn-primary\"&gt;Primary&lt;/button&gt;&lt;!-- Indicates a successful or positive action --&gt;&lt;button type=\"button\" class=\"btn btn-success\"&gt;Success&lt;/button&gt;&lt;!-- Contextual button for informational alert messages --&gt;&lt;button type=\"button\" class=\"btn btn-info\"&gt;Info&lt;/button&gt;&lt;!-- Indicates caution should be taken with this action --&gt;&lt;button type=\"button\" class=\"btn btn-warning\"&gt;Warning&lt;/button&gt;&lt;!-- Indicates a dangerous or potentially negative action --&gt;&lt;button type=\"button\" class=\"btn btn-danger\"&gt;Danger&lt;/button&gt;&lt;!-- Deemphasize a button by making it look like a link while maintaining button behavior --&gt;&lt;button type=\"button\" class=\"btn btn-link\"&gt;Link&lt;/button&gt; 尺寸需要让按钮具有不同尺寸吗？使用 .btn-lg、.btn-sm 或 .btn-xs 可以获得不同尺寸的按钮。 12345678910111213141516&lt;p&gt; &lt;button type=\"button\" class=\"btn btn-primary btn-lg\"&gt;Large button&lt;/button&gt; &lt;button type=\"button\" class=\"btn btn-default btn-lg\"&gt;Large button&lt;/button&gt;&lt;/p&gt;&lt;p&gt; &lt;button type=\"button\" class=\"btn btn-primary\"&gt;Default button&lt;/button&gt; &lt;button type=\"button\" class=\"btn btn-default\"&gt;Default button&lt;/button&gt;&lt;/p&gt;&lt;p&gt; &lt;button type=\"button\" class=\"btn btn-primary btn-sm\"&gt;Small button&lt;/button&gt; &lt;button type=\"button\" class=\"btn btn-default btn-sm\"&gt;Small button&lt;/button&gt;&lt;/p&gt;&lt;p&gt; &lt;button type=\"button\" class=\"btn btn-primary btn-xs\"&gt;Extra small button&lt;/button&gt; &lt;button type=\"button\" class=\"btn btn-default btn-xs\"&gt;Extra small button&lt;/button&gt;&lt;/p&gt; 通过给按钮添加 .btn-block 类可以将其拉伸至父元素100%的宽度，而且按钮也变为了块级（block）元素。 12&lt;button type=\"button\" class=\"btn btn-primary btn-lg btn-block\"&gt;Block level button&lt;/button&gt;&lt;button type=\"button\" class=\"btn btn-default btn-lg btn-block\"&gt;Block level button&lt;/button&gt; 激活状态当按钮处于激活状态时，其表现为被按压下去（底色更深、边框夜色更深、向内投射阴影）。对于 &lt;button&gt; 元素，是通过 :active 状态实现的。对于 &lt;a&gt;元素，是通过 .active 类实现的。然而，你还可以将 .active 应用到 &lt;button&gt;上，并通过编程的方式使其处于激活状态。 button 元素由于 :active 是伪状态，因此无需额外添加，但是在需要让其表现出同样外观的时候可以添加 .active 类。 12&lt;button type=\"button\" class=\"btn btn-primary btn-lg active\"&gt;Primary button&lt;/button&gt;&lt;button type=\"button\" class=\"btn btn-default btn-lg active\"&gt;Button&lt;/button&gt; 链接（&lt;a&gt;）元素可以为基于 &lt;a&gt;元素创建的按钮添加 .active 类。 12&lt;a href=\"#\" class=\"btn btn-primary btn-lg active\" role=\"button\"&gt;Primary link&lt;/a&gt;&lt;a href=\"#\" class=\"btn btn-default btn-lg active\" role=\"button\"&gt;Link&lt;/a&gt; 按钮类为 &lt;a&gt;、&lt;button&gt; 或 &lt;input&gt; 元素应用按钮类 1234&lt;a class=\"btn btn-default\" href=\"#\" role=\"button\"&gt;Link&lt;/a&gt;&lt;button class=\"btn btn-default\" type=\"submit\"&gt;Button&lt;/button&gt;&lt;input class=\"btn btn-default\" type=\"button\" value=\"Input\"&gt;&lt;input class=\"btn btn-default\" type=\"submit\" value=\"Submit\"&gt; 图片响应式图片在 Bootstrap 版本 3 中，通过为图片添加 .img-responsive 类可以让图片支持响应式布局。其实质是为图片设置了 max-width: 100%; 和 height: auto; 属性，从而让图片在其父元素中更好的缩放。 1&lt;img src=\"...\" class=\"img-responsive\" alt=\"Responsive image\"&gt; 图片形状通过为 &lt;img&gt; 元素添加以下相应的类，可以让图片呈现不同的形状。 123&lt;img src=&quot;...&quot; alt=&quot;...&quot; class=&quot;img-rounded&quot;&gt;&lt;img src=&quot;...&quot; alt=&quot;...&quot; class=&quot;img-circle&quot;&gt;&lt;img src=&quot;...&quot; alt=&quot;...&quot; class=&quot;img-thumbnail&quot;&gt;","tags":[{"name":"Bootstrap","slug":"Bootstrap","permalink":"http://www.54tianzhisheng.cn/tags/Bootstrap/"},{"name":"前端","slug":"前端","permalink":"http://www.54tianzhisheng.cn/tags/前端/"}]},{"title":"Bootstrap入门需掌握的知识点（一）","date":"2017-06-17T16:00:00.000Z","path":"2017/06/18/Bootstrap入门需掌握的知识点（一）/","text":"BootstrapBootstrap中文网：http://www.bootcss.com/ 1.什么是 Bootstrap？ 官方介绍：简洁、直观、强悍的前端开发框架，让web开发更迅速、简单。 Bootstrap 下载 Bootstrap3下载地址：http://v3.bootcss.com/getting-started/#download Bootstrap 文件目录结构 1234567891011121314151617dist -css -bootstrap.css -bootstrap.css.map -bootstrap.min.css（常用） -bootstrap-theme.css -bootstrap-theme.css.map -bootstrap-theme.min.css -fonts -glyphicons-halflings-regular.eot -glyphicons-halflings-regular.svg -glyphicons-halflings-regular.ttf -glyphicons-halflings-regular.woff -js -bootstrap.js -bootstrap.min.js（常用） -npm.js Bootstrap 依赖 要想使用 Bootstrap ，那么必须先引入 jQuery（jquery.min.js）文件。 5.使用 Bootstrap 压缩版本适于实际应用，未压缩版本适于开发调试过程 直接引用官网下载下来的文件。 使用 Bootstrap 中文网提供的免费 CDN 服务。 1234567891011&lt;!-- 新 Bootstrap 核心 CSS 文件 --&gt;&lt;link rel=\"stylesheet\" href=\"http://cdn.bootcss.com/bootstrap/3.3.0/css/bootstrap.min.css\"&gt;&lt;!-- 可选的Bootstrap主题文件（一般不用引入） --&gt;&lt;link rel=\"stylesheet\" href=\"http://cdn.bootcss.com/bootstrap/3.3.0/css/bootstrap-theme.min.css\"&gt;&lt;!-- jQuery文件。务必在bootstrap.min.js 之前引入 --&gt;&lt;script src=\"http://cdn.bootcss.com/jquery/1.11.1/jquery.min.js\"&gt;&lt;/script&gt;&lt;!-- 最新的 Bootstrap 核心 JavaScript 文件 --&gt;&lt;script src=\"http://cdn.bootcss.com/bootstrap/3.3.0/js/bootstrap.min.js\"&gt;&lt;/script&gt; Bootstrap 基本模板 123456789101112131415161718192021222324252627&lt;!DOCTYPE html&gt;&lt;html lang=\"zh-cn\"&gt; &lt;head&gt; &lt;meta charset=\"utf-8\"&gt; &lt;meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"&gt; &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"&gt; &lt;title&gt;Bootstrap 基本模板&lt;/title&gt; &lt;!-- Bootstrap --&gt; &lt;link href=\"css/bootstrap.min.css\" rel=\"stylesheet\"&gt; &lt;!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries --&gt; &lt;!-- WARNING: Respond.js doesn't work if you view the page via file:// --&gt; &lt;!--[if lt IE 9]&gt; &lt;script src=\"http://cdn.bootcss.com/html5shiv/3.7.2/html5shiv.min.js\"&gt;&lt;/script&gt; &lt;script src=\"http://cdn.bootcss.com/respond.js/1.4.2/respond.min.js\"&gt;&lt;/script&gt; &lt;![endif]--&gt; &lt;/head&gt; &lt;body&gt; &lt;h1&gt;你好，世界！&lt;/h1&gt; &lt;!-- jQuery (necessary for Bootstrap's JavaScript plugins) --&gt; &lt;script src=\"http://cdn.bootcss.com/jquery/1.11.1/jquery.min.js\"&gt;&lt;/script&gt; &lt;!-- Include all compiled plugins (below), or include individual files as needed --&gt; &lt;script src=\"js/bootstrap.min.js\"&gt;&lt;/script&gt; &lt;/body&gt;&lt;/html&gt; Bootstrap 实例精选：http://v3.bootcss.com/getting-started/#examples 全局 CSS 样式HTML5 文档类型Bootstrap 使用到的某些 HTML 元素和 CSS 属性需要将页面设置为 HTML5 文档类型。 1234&lt;!DOCTYPE html&gt;&lt;html lang=\"zh-CN\"&gt; ...&lt;/html&gt; 移动设备优先在 bootstrap3 中移动设备优先考虑的。为了保证适当的绘制和触屏缩放，需要在&lt;head&gt;之中添加 viewport 元数据标签。 1&lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"&gt; 在移动设备浏览器上，可以通过视口 viewport 设置meta属性为user-scalable=no可以禁用其缩放（zooming）功能，这样后用户只能滚动屏幕。（看情况而定） 1&lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1, maximum-scala=1, user-scalable=no\"&gt; 排版与链接Bootstrap 排版、链接样式设置了基本的全局样式。分别是： 为 body 元素设置 background-color: #fff; 使用 @font-family-base、@font-size-base 和 @line-height-base a变量作为排版的基本参数 为所有链接设置了基本颜色 @link-color ，并且当链接处于 :hover 状态时才添加下划线 这些样式都能在 scaffolding.less 文件中找到对应的源码。 Normalize.css为了增强跨浏览器表现的一致性，bootstrap使用了 Normalize.css，这是由 Nicolas Gallagher 和 Jonathan Neal 维护的一个CSS 重置样式库。 布局容器Bootstrap 需要为页面内容和栅格系统包裹一个 .container 容器。Bootstrap提供了两个作此用处的类。注意，由于 padding等属性的原因，这两种容器类不能互相嵌套。 .container 类用于固定宽度并支持响应式布局的容器。 123&lt;div class=\"container\"&gt; ...&lt;/div&gt; .container-fluid 类用于 100% 宽度，占据全部视口（viewport）的容器。 123&lt;div class=\"container-fluid\"&gt; ...&lt;/div&gt; 栅格系统Bootstrap 提供了一套响应式、移动设备优先的流式栅格系统，随着屏幕或视口（viewport）尺寸的增加，系统会自动分为最多12列。它包含了易于使用的预定义类，还有强大的mixin 用于生成更具语义的布局。 简介栅格系统用于通过一系列的行（row）与列（column）的组合来创建页面布局，你的内容就可以放入这些创建好的布局中。下面就介绍一下 Bootstrap 栅格系统的工作原理： “行（row）”必须包含在 .container （固定宽度）或 .container-fluid （100% 宽度）中，以便为其赋予合适的排列（aligment）和内补（padding）。 通过“行（row）”在水平方向创建一组“列（column）”。 你的内容应当放置于“列（column）”内，并且，只有“列（column）”可以作为行（row）”的直接子元素。 类似 .row 和 .col-xs-4 这种预定义的类，可以用来快速创建栅格布局。Bootstrap 源码中定义的 mixin 也可以用来创建语义化的布局。 通过为“列（column）”设置 padding 属性，从而创建列与列之间的间隔（gutter）。通过为 .row 元素设置负值margin 从而抵消掉为 .container 元素设置的 padding，也就间接为“行（row）”所包含的“列（column）”抵消掉了padding。 The negative margin is why the examples below are outdented. It’s so that content within grid columns is lined up with non-grid content. Grid columns are created by specifying the number of twelve available columns you wish to span. For example, three equal columns would use three .col-xs-4. 如果一“行（row）”中包含了的“列（column）”大于 12，多余的“列（column）”所在的元素将被作为一个整体另起一行排列。 Grid classes apply to devices with screen widths greater than or equal to the breakpoint sizes, and override grid classes targeted at smaller devices. Therefore, applying any .col-md- class to an element will not only affect its styling on medium devices but also on large devices if a .col-lg- class is not present. 通过研究后面的实例，可以将这些原理应用到你的代码中。 媒体查询在栅格系统中，我们在 Less 文件中使用以下媒体查询（media query）来创建关键的分界点阈值。 1234567891011/* 超小屏幕（手机，小于 768px） *//* 没有任何媒体查询相关的代码，因为这在 Bootstrap 中是默认的（还记得 Bootstrap 是移动设备优先的吗？） *//* 小屏幕（平板，大于等于 768px） */@media (min-width: @screen-sm-min) &#123; ... &#125;/* 中等屏幕（桌面显示器，大于等于 992px） */@media (min-width: @screen-md-min) &#123; ... &#125;/* 大屏幕（大桌面显示器，大于等于 1200px） */@media (min-width: @screen-lg-min) &#123; ... &#125; 偶尔也会在媒体查询代码中包含 max-width 从而将 CSS 的影响限制在更小范围的屏幕大小之内 1234@media (max-width: @screen-xs-max) &#123; ... &#125;@media (min-width: @screen-sm-min) and (max-width: @screen-sm-max) &#123; ... &#125;@media (min-width: @screen-md-min) and (max-width: @screen-md-max) &#123; ... &#125;@media (min-width: @screen-lg-min) &#123; ... &#125; 栅格参数通过下表可以详细查看 Bootstrap 的栅格系统是如何在多种屏幕设备上工作的。 超小屏幕 手机 (&lt;768px) 小屏幕 平板 (≥768px) 中等屏幕 桌面显示器 (≥992px) 大屏幕 大桌面显示器 (≥1200px) 栅格系统行为 总是水平排列 开始是堆叠在一起的，当大于这些阈值时将变为水平排列C 同左 同左 .container 最大宽度 None （自动） 750px 970px 1170px 类前缀 .col-xs- .col-sm- .col-md- .col-lg- 列（column）数 12 12 12 12 最大列（column）宽 自动 ~62px ~81px ~97px 槽（gutter）宽 30px （每列左右均有 15px） 同左 同左 同左 可嵌套 是 是 是 是 偏移（Offsets） 是 是 是 是 列排序 是 是 是 是 实例：从堆叠到水平排列使用单一的一组 .col-md-* 栅格类，就可以创建一个基本的栅格系统，在手机和平板设备上一开始是堆叠在一起的（超小屏幕到小屏幕这一范围），在桌面（中等）屏幕设备上变为水平排列。所有“列（column）必须放在 ” .row 内。 123456789101112131415161718192021222324252627&lt;div class=\"row\"&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt; &lt;div class=\"col-md-1\"&gt;.col-md-1&lt;/div&gt;&lt;/div&gt;&lt;div class=\"row\"&gt; &lt;div class=\"col-md-8\"&gt;.col-md-8&lt;/div&gt; &lt;div class=\"col-md-4\"&gt;.col-md-4&lt;/div&gt;&lt;/div&gt;&lt;div class=\"row\"&gt; &lt;div class=\"col-md-4\"&gt;.col-md-4&lt;/div&gt; &lt;div class=\"col-md-4\"&gt;.col-md-4&lt;/div&gt; &lt;div class=\"col-md-4\"&gt;.col-md-4&lt;/div&gt;&lt;/div&gt;&lt;div class=\"row\"&gt; &lt;div class=\"col-md-6\"&gt;.col-md-6&lt;/div&gt; &lt;div class=\"col-md-6\"&gt;.col-md-6&lt;/div&gt;&lt;/div&gt; 实例：移动设备和桌面屏幕是否不希望在小屏幕设备上所有列都堆叠在一起？那就使用针对超小屏幕和中等屏幕设备所定义的类吧，即 .col-xs-*和 .col-md-*。请看下面的实例，研究一下这些是如何工作的。 123456789101112131415161718&lt;!-- Stack the columns on mobile by making one full-width and the other half-width --&gt;&lt;div class=\"row\"&gt; &lt;div class=\"col-xs-12 col-md-8\"&gt;.col-xs-12 .col-md-8&lt;/div&gt; &lt;div class=\"col-xs-6 col-md-4\"&gt;.col-xs-6 .col-md-4&lt;/div&gt;&lt;/div&gt;&lt;!-- Columns start at 50% wide on mobile and bump up to 33.3% wide on desktop --&gt;&lt;div class=\"row\"&gt; &lt;div class=\"col-xs-6 col-md-4\"&gt;.col-xs-6 .col-md-4&lt;/div&gt; &lt;div class=\"col-xs-6 col-md-4\"&gt;.col-xs-6 .col-md-4&lt;/div&gt; &lt;div class=\"col-xs-6 col-md-4\"&gt;.col-xs-6 .col-md-4&lt;/div&gt;&lt;/div&gt;&lt;!-- Columns are always 50% wide, on mobile and desktop --&gt;&lt;div class=\"row\"&gt; &lt;div class=\"col-xs-6\"&gt;.col-xs-6&lt;/div&gt; &lt;div class=\"col-xs-6\"&gt;.col-xs-6&lt;/div&gt;&lt;/div&gt; 排版标题HTML 中的所有标题标签，到 均可使用。另外，还提供了 .h1 到 .h6 类，为的是给内联（inline）属性的文本赋予标题的样式 123456&lt;h1&gt;h1. Bootstrap heading&lt;/h1&gt;&lt;h2&gt;h2. Bootstrap heading&lt;/h2&gt;&lt;h3&gt;h3. Bootstrap heading&lt;/h3&gt;&lt;h4&gt;h4. Bootstrap heading&lt;/h4&gt;&lt;h5&gt;h5. Bootstrap heading&lt;/h5&gt;&lt;h6&gt;h6. Bootstrap heading&lt;/h6&gt; 在标题内还可以包含 &lt;small&gt; 标签或赋予 .small 类的元素，可以用来标记副标题。 123456&lt;h1&gt;h1. Bootstrap heading &lt;small&gt;Secondary text&lt;/small&gt;&lt;/h1&gt;&lt;h2&gt;h2. Bootstrap heading &lt;small&gt;Secondary text&lt;/small&gt;&lt;/h2&gt;&lt;h3&gt;h3. Bootstrap heading &lt;small&gt;Secondary text&lt;/small&gt;&lt;/h3&gt;&lt;h4&gt;h4. Bootstrap heading &lt;small&gt;Secondary text&lt;/small&gt;&lt;/h4&gt;&lt;h5&gt;h5. Bootstrap heading &lt;small&gt;Secondary text&lt;/small&gt;&lt;/h5&gt;&lt;h6&gt;h6. Bootstrap heading &lt;small&gt;Secondary text&lt;/small&gt;&lt;/h6&gt; 页面主体Bootstrap 将全局 font-size 设置为 14px，line-height 设置为 1.428。这些属性直接赋予 元素和所有段落元素。另外， （段落）元素还被设置了等于 1/2 行高（即 10px）的底部外边距（margin）。 中心内容通过添加 .lead 类可以让段落突出显示。 1&lt;p class=\"lead\"&gt;...&lt;/p&gt; 使用 Less 工具构建variables.less 文件中定义的两个 Less 变量决定了排版尺寸：@font-size-base 和 @line-height-base。第一个变量定义了全局 font-size 基准，第二个变量是 line-height 基准。我们使用这些变量和一些简单的公式计算出其它所有页面元素的 margin、 padding 和 line-height。自定义这些变量即可改变 Bootstrap 的默认样式 内联文本元素标记文本为了高亮文本，可以使用 &lt;mark&gt; 标签 1You can use the mark tag to &lt;mark&gt;highlight&lt;/mark&gt; text. 被删除的文本对于被删除的文本，可以使用 &lt;del&gt; 标签。 1&lt;del&gt;This line of text is meant to be treated as deleted text.&lt;/del&gt; 无用文本对于无用文本可以使用 &lt;s&gt; 标签。 1&lt;s&gt;This line of text is meant to be treated as no longer accurate.&lt;/s&gt; 插入文本而外插入文本使用 &lt;ins&gt; 标签 1&lt;ins&gt;This line of text is meant to be treated as an addition to the document.&lt;/ins&gt; 带下划线的文本为文本添加下划线，使用 &lt;u&gt; 标签。 1&lt;u&gt;This line of text will render as underlined&lt;/u&gt; 小号文本使用标签 &lt;small&gt; 着重强调使用标签 &lt;strong&gt; 标签 斜体使用 &lt;em&gt; 标签 文本对齐 12345&lt;p class=\"text-left\"&gt;Left aligned text.&lt;/p&gt;&lt;p class=\"text-center\"&gt;Center aligned text.&lt;/p&gt;&lt;p class=\"text-right\"&gt;Right aligned text.&lt;/p&gt;&lt;p class=\"text-justify\"&gt;Justified text.&lt;/p&gt;&lt;p class=\"text-nowrap\"&gt;No wrap text.&lt;/p&gt; 改变大小写 123&lt;p class=\"text-lowercase\"&gt;Lowercased text.&lt;/p&gt;&lt;p class=\"text-uppercase\"&gt;Uppercased text.&lt;/p&gt;&lt;p class=\"text-capitalize\"&gt;Capitalized text.&lt;/p&gt; 引用在你的文档中引用其他的来源，可以使用 &lt;blockquote&gt; 来表示引用样式。对于直接引用，建议使用 &lt;p&gt; 标签。 123&lt;blockquote&gt; &lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Integer posuere erat a ante.&lt;/p&gt;&lt;/blockquote&gt; 列表无序列表排列顺序无关紧要的一列元素。 123&lt;ul&gt; &lt;li&gt;...&lt;/li&gt;&lt;/ul&gt; 有序列表顺序至关重要的一组元素 123&lt;ol&gt; &lt;li&gt;...&lt;/li&gt;&lt;/ol&gt; 代码内联代码1For example, &lt;code&gt;&amp;lt;section&amp;gt;&lt;/code&gt; should be wrapped as inline. 用户输入通过 kbd 标签标记用户通过键盘输入的内容。 12To switch directories, type &lt;kbd&gt;cd&lt;/kbd&gt; followed by the name of the directory.&lt;br&gt;To edit settings, press &lt;kbd&gt;&lt;kbd&gt;ctrl&lt;/kbd&gt; + &lt;kbd&gt;,&lt;/kbd&gt;&lt;/kbd&gt; 代码块多行代码可以使用 &lt;pre&gt; 标签。为了正确的展示代码，注意将尖括号做转义处理。 变量通过 &lt;var&gt; 标签标记变量 程序输出通过 &lt;samp&gt; 标签来标记程序输出的内容 期待后面的文章！","tags":[{"name":"Bootstrap","slug":"Bootstrap","permalink":"http://www.54tianzhisheng.cn/tags/Bootstrap/"},{"name":"前端","slug":"前端","permalink":"http://www.54tianzhisheng.cn/tags/前端/"}]},{"title":"搭建一个博客项目后的碎碎念","date":"2017-06-16T16:00:00.000Z","path":"2017/06/17/blog-talk/","text":"前言 以前大二的时候就想一个人独立做一个由 Java 开发的个人博客, 可耐当时还很弱鸡，一个人难以独挡一片，因为要会的东西太多，后来自己看到很多都是由 WordPress 搭建的博客，很多模板很漂亮，可是自己要稍微对 “拍黄片” 了解一点，并且里面的各种插件特特别的多。去年的时候就开始用上了 GitHub Page 搭建静态的博客，因为自己一直习惯用 Markdown 写作，写完后，软件可以直接生成 PDF 和 HTML 文件，这样就很方便了，直接将自己的 HTML、PDF 和 MD 文件一起 push 到 GitHub 上，然后自己在通过域名加上文章链接就可以直接访问我的博客了，这样就省了很多事了。还提供了 PDF 和 MD 版本，对有不同需求的人都可满足了。可是后来觉得这样的逼格还是不够高，就又开始折腾 Hexo 了，发现用 Hexo 也是很非常简单的（其实是看到 Hexo 的 yilia 主题非常漂亮）。于是就换上了 Hexo 了，自己在这上面写博客也很方便。每次用软件写完后，在 Git Bash 下敲一行命令 hexo d -g 就行了，很方便！前段时间看到了一款开源的博客（由 Java 搭建而成）—— Tale，主题比较简洁，符合程序员的范。也刚好符合自己最初的想法，但是我是没打算放弃现在的博客，就是有一个想法，自己也跟着在那个基础山修改下。（因为 Tale 使用的是轻量级 mvc 框架 Blade 开发，我好像不太了解这个框架呢），想着就 SpringBoot 开发比较快，上手也简单。当时就有这个想法，可怜没时间，不过前些天发现有人就是基于那个 Tale 博客重新修改了，用的就是 SpringBoot ，哇，果然是英雄所见略同。当时就和作者邮件联系了，于是蹭这些天的时间赶紧去看看，结果不只是看看，完全自己就全部敲了一遍，终于在今天搞定了，为了庆祝，才写下这篇文章，好好记录这些美好的时刻（博客可以完全发挥，不限题材）。通过自己深入这个项目，才能够很了解内部的实现方式，这点收获很大，这十天时间花的值，再此感谢两位原作者 ZHENFENG13 、otale 。 博客介绍Tale 使用了轻量级 mvc 框架 Blade 开发，默认主题使用了漂亮的 pinghsu 。 My-Blog 使用的是 Docker + SpringBoot + Mybatis + thymeleaf 打造的一个个人博客模板。 Blog 是自己花了十天的时间把整个项目的代码都敲了一遍，熟悉了整个项目，做了优化，去除了 Docker， 其中修改了原来的一些 bug，并在原作者的项目中提出了 issue ， 原作者已修复。 : 喜欢该项目的话，可以给项目点个 star，如果你想在这基础上修改，那么建议你 fork 该项目，然后再修改哦。 博客首页： 归档： 友链： 关于： 搜索： 后台管理 管理登录： 管理首页： 发布文章： 文章管理： 页面管理： 分类标签： 文件管理： 友链管理： 系统设置： 最后我什么我这么喜欢折腾博客呢，熟悉我的朋友都知道，我再很多平台都写过博客，有些是他们平台的运营人员邀请过去的。可是在这些平台上写博客终究是没有感觉，如今自己在自己的博客网站写文章，比较轻松，而且也符合我的写作风格。在其他的平台都有些大大小小的不适（对程序员来说应该是 bug），虽然目前还是会在这些平台继续发布我新写的文章，但是我保证最新的文章，首发肯定是我自己的博客网站，有些是不会在其他平台发的，有觉得不错的可以 RSS 订阅我的博客，或者是直接收藏网址下来。自从写博客下来遇到很多志同道合的人，这点正是让我觉得有写下去的必要了。自己将会坚持下去，时刻警醒自己：勿忘初心！最后的最后，还是想说一句：如果你想和我一样折腾博客，那么我建议你先在一家平台坚持写下去，等博客数量上来了，在自己折腾自己的博客网站。还有就是你想提高自己的话，还是需要很在意你的基础，然后就是要多练手几个项目，我自己在练手这个项目的时候就收获很多。","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"}]},{"title":"详解 Filter 过滤器","date":"2017-06-16T16:00:00.000Z","path":"2017/06/17/详解 Filter 过滤器/","text":"1、简介 Filter也称之为过滤器，它是Servlet技术中最实用的技术，WEB开发人员通过Filter技术，对web服务器管理的所有web资源：例如Jsp, Servlet, 静态图片文件或静态 html 文件等进行拦截，从而实现一些特殊的功能。例如实现URL级别的权限访问控制、过滤敏感词汇、压缩响应信息等一些高级功能。 它主要用于对用户请求进行预处理，也可以对HttpServletResponse 进行后处理。使用Filter 的完整流程：Filter 对用户请求进行预处理，接着将请求交给Servlet 进行处理并生成响应，最后Filter 再对服务器响应进行后处理。 Filter功能： 在HttpServletRequest 到达 Servlet 之前，拦截客户的 HttpServletRequest 。 根据需要检查 HttpServletRequest ，也可以修改HttpServletRequest 头和数据。 在HttpServletResponse 到达客户端之前，拦截HttpServletResponse 。 根据需要检查 HttpServletResponse ，也可以修改HttpServletResponse头和数据。 2、如何实现拦截 Filter接口中有一个doFilter方法，当开发人员编写好Filter，并配置对哪个web资源进行拦截后，WEB服务器每次在调用web资源的service方法之前，都会先调用一下filter的doFilter方法，因此，在该方法内编写代码可达到如下目的： 调用目标资源之前，让一段代码执行。 是否调用目标资源（即是否让用户访问web资源）。 web服务器在调用doFilter方法时，会传递一个filterChain对象进来，filterChain对象是filter接口中最重要的一个对象，它也提供了一个doFilter方法，开发人员可以根据需求决定是否调用此方法，调用该方法，则web服务器就会调用web资源的service方法，即web资源就会被访问，否则web资源不会被访问。 3、Filter开发两步走 编写java类实现Filter接口，并实现其doFilter方法。 在 web.xml 文件中使用和元素对编写的filter类进行注册，并设置它所能拦截的资源。 web.xml配置各节点介绍： 12345678910111213141516&lt;filter-name&gt;用于为过滤器指定一个名字，该元素的内容不能为空。&lt;filter-class&gt;元素用于指定过滤器的完整的限定类名。&lt;init-param&gt;元素用于为过滤器指定初始化参数，它的子元素&lt;param-name&gt;指定参数的名字，&lt;param-value&gt;指定参数的值。在过滤器中，可以使用FilterConfig接口对象来访问初始化参数。&lt;filter-mapping&gt;元素用于设置一个 Filter 所负责拦截的资源。一个Filter拦截的资源可通过两种方式来指定：Servlet 名称和资源访问的请求路径&lt;filter-name&gt;子元素用于设置filter的注册名称。该值必须是在&lt;filter&gt;元素中声明过的过滤器的名字&lt;url-pattern&gt;设置 filter 所拦截的请求路径(过滤器关联的URL样式)&lt;servlet-name&gt;指定过滤器所拦截的Servlet名称。&lt;dispatcher&gt;指定过滤器所拦截的资源被 Servlet 容器调用的方式，可以是REQUEST,INCLUDE,FORWARD和ERROR之一，默认REQUEST。用户可以设置多个&lt;dispatcher&gt; 子元素用来指定 Filter 对资源的多种调用方式进行拦截。&lt;dispatcher&gt; 子元素可以设置的值及其意义：REQUEST：当用户直接访问页面时，Web容器将会调用过滤器。如果目标资源是通过RequestDispatcher的include()或forward()方法访问时，那么该过滤器就不会被调用。INCLUDE：如果目标资源是通过RequestDispatcher的include()方法访问时，那么该过滤器将被调用。除此之外，该过滤器不会被调用。FORWARD：如果目标资源是通过RequestDispatcher的forward()方法访问时，那么该过滤器将被调用，除此之外，该过滤器不会被调用。ERROR：如果目标资源是通过声明式异常处理机制调用时，那么该过滤器将被调用。除此之外，过滤器不会被调用。 4、Filter链 在一个web应用中，可以开发编写多个Filter，这些Filter组合起来称之为一个Filter链。 web服务器根据Filter在web.xml文件中的注册顺序，决定先调用哪个Filter，当第一个Filter的doFilter方法被调用时，web服务器会创建一个代表Filter链的FilterChain对象传递给该方法。在doFilter方法中，开发人员如果调用了FilterChain对象的doFilter方法，则web服务器会检查FilterChain对象中是否还有filter，如果有，则调用第2个filter，如果没有，则调用目标资源。 多个过滤器执行顺序 一个目标资源可以指定多个过滤器，过滤器的执行顺序是在web.xml文件中的部署顺序： 12345678910111213141516&lt;filter&gt; &lt;filter-name&gt;myFilter1&lt;/filter-name&gt; &lt;filter-class&gt;cn.cloud.filter.MyFilter1&lt;/filter-class&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;myFilter1&lt;/filter-name&gt; &lt;url-pattern&gt;/index.jsp&lt;/url-pattern&gt; &lt;/filter-mapping&gt; &lt;filter&gt; &lt;filter-name&gt;myFilter2&lt;/filter-name&gt; &lt;filter-class&gt;cn. cloud.filter.MyFilter2&lt;/filter-class&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;myFilter2&lt;/filter-name&gt; &lt;url-pattern&gt;/index.jsp&lt;/url-pattern&gt; &lt;/filter-mapping&gt; MyFilter1 12345678public class MyFilter1 extends HttpFilter &#123; public void doFilter(HttpServletRequest request, HttpServletResponse response, FilterChain chain) throws IOException, ServletException &#123; System.out.println(\"filter1 start...\"); chain.doFilter(request, response);//放行，执行MyFilter2的doFilter()方法 System.out.println(\"filter1 end...\"); &#125;&#125; MyFilter2 12345678public class MyFilter2 extends HttpFilter &#123; public void doFilter(HttpServletRequest request, HttpServletResponse response, FilterChain chain) throws IOException, ServletException &#123; System.out.println(\"filter2 start...\"); chain.doFilter(request, response);//放行，执行目标资源 System.out.println(\"filter2 end...\"); &#125;&#125; 12345&lt;body&gt; This is my JSP page. &lt;br&gt; &lt;h1&gt;index.jsp&lt;/h1&gt; &lt;%System.out.println(\"index.jsp\"); %&gt; &lt;/body&gt; 当有用户访问index.jsp页面时，输出结果如下： 12345filter1 start...filter2 start...index.jspfilter2 end...filter1 end... 5、Filter的生命周期 1public void init(FilterConfig filterConfig) throws ServletException;//初始化 和我们编写的Servlet程序一样，Filter的创建和销毁由WEB服务器负责。 web 应用程序启动时，web 服务器将创建Filter 的实例对象，并调用其init方法，读取web.xml配置，完成对象的初始化功能，从而为后续的用户请求作好拦截的准备工作（filter对象只会创建一次，init方法也只会执行一次）。开发人员通过init方法的参数，可获得代表当前filter配置信息的FilterConfig对象。 1public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException;//拦截请求 这个方法完成实际的过滤操作。当客户请求访问与过滤器关联的URL（目标资源）的时候，Servlet过滤器将先执行doFilter方法。FilterChain参数用于访问后续过滤器。 1public void destroy();//销毁 服务器在创建Filter对象之后，把Filter放到缓存中一直使用（会驻留在内存），通常不会销毁它，当web应用移除或服务器停止时才销毁Filter对象。在Web容器卸载 Filter 对象之前被调用。该方法在Filter的生命周期中仅执行一次。在这个方法中，可以释放过滤器使用的资源。 6、FilterConfig接口 用户在配置filter时，可以使用为filter配置一些初始化参数，当web容器实例化Filter对象，调用其init方法时，会把封装了filter初始化参数的filterConfig对象传递进来。因此开发人员在编写filter时，通过filterConfig对象的方法，就可获得以下内容： 1234String getFilterName();//得到filter的名称；与&lt;filter-name&gt;元素对应。String getInitParameter(String name);//返回在部署描述中指定名称的初始化参数的值。如果不存在返回null，与&lt;init-param&gt;元素对应.Enumeration getInitParameterNames();//返回过滤器的所有初始化参数的名字的枚举集合。public ServletContext getServletContext();//返回Servlet上下文对象的引用。 7、FilterChain doFilter()方法的参数中有一个类型为FilterChain的参数，它只有一个方法：doFilter(ServletRequest,ServletResponse) doFilter() 方法的放行，让请求流访问目标资源！其实调用该方法的意思是，当前 Filter 放行了，但不代表其他过滤器也放行。一个目标资源上，可能部署了多个过滤器，所以调用 FilterChain 类的 doFilter() 方法表示的是执行下一个过滤器的 doFilter() 方法，或者是执行目标资源！ 如果当前过滤器是最后一个过滤器，那么调用 chain.doFilter() 方法表示执行目标资源，而不是最后一个过滤器，那么 chain.doFilter() 表示执行下一个过滤器的 doFilter() 方法。 8、过滤器的应用场景 执行目标资源之前做预处理工作，例如设置编码，这种试通常都会放行，只是在目标资源执行之前做一些准备工作； 通过条件判断是否放行，例如校验当前用户是否已经登录，或者用户IP是否已经被禁用； 在目标资源执行后，做一些后续的特殊处理工作，例如把目标资源输出的数据进行处理 设置目标资源 在web.xml文件中部署Filter时，可以通过“*”来执行目标资源： 1234&lt;filter-mapping&gt; &lt;filter-name&gt;myfilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; 特性与Servlet完全相同！通过这一特性，可以在用户访问敏感资源时，执行过滤器，例如：/admin/*，可以把所有管理员才能访问的资源放到/admin路径下，这时可以通过过滤器来校验用户身份。 还可以为指定目标资源为某个Servlet，例如： 12345678910111213141516&lt;servlet&gt; &lt;servlet-name&gt;myservlet&lt;/servlet-name&gt; &lt;servlet-class&gt;cn.cloud.servlet.MyServlet&lt;/servlet-class&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;myservlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/abc&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; &lt;filter&gt; &lt;filter-name&gt;myfilter&lt;/filter-name&gt; &lt;filter-class&gt;cn.cloud.filter.MyFilter&lt;/filter-class&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;myfilter&lt;/filter-name&gt; &lt;servlet-name&gt;myservlet&lt;/servlet-name&gt; &lt;/filter-mapping&gt; 当用户访问http://localhost:8080/filtertest/abc时，会执行名字为myservlet的Servlet，这时会执行过滤器。 9、四种拦截方式 写一个过滤器，指定过滤的资源为b.jsp，然后在浏览器中直接访问b.jsp，会发现过滤器执行了.但是，当在a.jsp中request.getRequestDispathcer(“/b.jsp”).forward(request,response)时，就不会再执行过滤器了！也就是说，默认情况下，只能直接访问目标资源才会执行过滤器，而forward执行目标资源，不会执行过滤器！ 12345678public class MyFilter extends HttpFilter &#123; public void doFilter(HttpServletRequest request, HttpServletResponse response, FilterChain chain) throws IOException, ServletException &#123; System.out.println(\"myfilter...\"); chain.doFilter(request, response); &#125;&#125; 12345678&lt;filter&gt; &lt;filter-name&gt;myfilter&lt;/filter-name&gt; &lt;filter-class&gt;cn.itcast.filter.MyFilter&lt;/filter-class&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;myfilter&lt;/filter-name&gt; &lt;url-pattern&gt;/b.jsp&lt;/url-pattern&gt; &lt;/filter-mapping&gt; 123&lt;body&gt; &lt;h1&gt;b.jsp&lt;/h1&gt; &lt;/body&gt; 12345&lt;h1&gt;a.jsp&lt;/h1&gt; &lt;% request.getRequestDispatcher(\"/b.jsp\").forward(request, response); %&gt; &lt;/body&gt; 在浏览器输入： http://localhost:8080/filtertest/b.jsp 直接访问b.jsp时，会执行过滤器内容； http://localhost:8080/filtertest/a.jsp 访问a.jsp，但a.jsp会forward到b.jsp，这时就不会执行过滤器！ 过滤器有四种拦截方式！分别是：REQUEST、FORWARD、INCLUDE、ERROR。 REQUEST：直接访问目标资源时执行过滤器。包括：在地址栏中直接访问、表单提交、超链接、重定向，只要在地址栏中可以看到目标资源的路径，就是REQUEST FORWARD：转发访问执行过滤器。包括RequestDispatcher#forward()方法、标签都是转发访问 INCLUDE：包含访问执行过滤器。包括RequestDispatcher#include()方法、标签都是包含访问 ERROR：当目标资源在web.xml中配置为中时，并且真的出现了异常，转发到目标资源时，会执行过滤器。 可以在中添加0~n个子元素，来说明当前访问的拦截方式。 如： 123456&lt;filter-mapping&gt; &lt;filter-name&gt;myfilter&lt;/filter-name&gt; &lt;url-pattern&gt;/b.jsp&lt;/url-pattern&gt; &lt;dispatcher&gt;REQUEST&lt;/dispatcher&gt; &lt;dispatcher&gt;FORWARD&lt;/dispatcher&gt; &lt;/filter-mapping&gt; 最为常用的就是REQUEST和FORWARD两种拦截方式，而INCLUDE和ERROR都比较少用！其中INCLUDE比较好理解，ERROR方式不易理解，下面给出ERROR拦截方式的例子： 123456789&lt;filter-mapping&gt; &lt;filter-name&gt;myfilter&lt;/filter-name&gt; &lt;url-pattern&gt;/b.jsp&lt;/url-pattern&gt; &lt;dispatcher&gt;ERROR&lt;/dispatcher&gt; &lt;/filter-mapping&gt; &lt;error-page&gt; &lt;error-code&gt;500&lt;/error-code&gt; &lt;location&gt;/b.jsp&lt;/location&gt; &lt;/error-page&gt; 1234567&lt;body&gt; &lt;h1&gt;a.jsp&lt;/h1&gt; &lt;% if(true) throw new RuntimeException(\"嘻嘻~\"); %&gt; &lt;/body&gt; 10、Filter使用案例 1、使用Filter验证用户登录安全控制 前段时间参与维护一个项目，用户退出系统后，再去地址栏访问历史，根据url，仍然能够进入系统响应页面。我去检查一下发现对请求未进行过滤验证用户登录。添加一个filter搞定问题！ 先在web.xml配置 123456789101112131415161718192021222324&lt;filter&gt; &lt;filter-name&gt;SessionFilter&lt;/filter-name&gt; &lt;filter-class&gt;com.action.login.SessionFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;logonStrings&lt;/param-name&gt;&lt;!-- 对登录页面不进行过滤 --&gt; &lt;param-value&gt;/project/index.jsp;login.do&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;includeStrings&lt;/param-name&gt;&lt;!-- 只对指定过滤参数后缀进行过滤 --&gt; &lt;param-value&gt;.do;.jsp&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;redirectPath&lt;/param-name&gt;&lt;!-- 未通过跳转到登录界面 --&gt; &lt;param-value&gt;/index.jsp&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;disabletestfilter&lt;/param-name&gt;&lt;!-- Y:过滤无效 --&gt; &lt;param-value&gt;N&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;SessionFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 接着编写FilterServlet： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576package com.action.login;import java.io.IOException;import javax.servlet.Filter;import javax.servlet.FilterChain;import javax.servlet.FilterConfig;import javax.servlet.ServletException;import javax.servlet.ServletRequest;import javax.servlet.ServletResponse;import javax.servlet.http.HttpServletRequest;import javax.servlet.http.HttpServletResponse;import javax.servlet.http.HttpServletResponseWrapper;/** * 判断用户是否登录,未登录则退出系统 */public class SessionFilter implements Filter &#123; public FilterConfig config; public void destroy() &#123; this.config = null; &#125; public static boolean isContains(String container, String[] regx) &#123; boolean result = false; for (int i = 0; i &lt; regx.length; i++) &#123; if (container.indexOf(regx[i]) != -1) &#123; return true; &#125; &#125; return result; &#125; public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; HttpServletRequest hrequest = (HttpServletRequest)request; HttpServletResponseWrapper wrapper = new HttpServletResponseWrapper((HttpServletResponse) response); String logonStrings = config.getInitParameter(\"logonStrings\"); // 登录登陆页面 String includeStrings = config.getInitParameter(\"includeStrings\"); // 过滤资源后缀参数 String redirectPath = hrequest.getContextPath() + config.getInitParameter(\"redirectPath\");// 没有登陆转向页面 String disabletestfilter = config.getInitParameter(\"disabletestfilter\");// 过滤器是否有效 if (disabletestfilter.toUpperCase().equals(\"Y\")) &#123; // 过滤无效 chain.doFilter(request, response); return; &#125; String[] logonList = logonStrings.split(\";\"); String[] includeList = includeStrings.split(\";\"); if (!this.isContains(hrequest.getRequestURI(), includeList)) &#123;// 只对指定过滤参数后缀进行过滤 chain.doFilter(request, response); return; &#125; if (this.isContains(hrequest.getRequestURI(), logonList)) &#123;// 对登录页面不进行过滤 chain.doFilter(request, response); return; &#125; String user = ( String ) hrequest.getSession().getAttribute(\"useronly\");//判断用户是否登录 if (user == null) &#123; wrapper.sendRedirect(redirectPath); return; &#125;else &#123; chain.doFilter(request, response); return; &#125; &#125; public void init(FilterConfig filterConfig) throws ServletException &#123; config = filterConfig; &#125;&#125; 这样既可完成对用户所有请求，均要经过这个Filter进行验证用户登录。 2、防止中文乱码过滤器 项目使用spring框架时。当前台JSP页面和JAVA代码中使用了不同的字符集进行编码的时候就会出现表单提交的数据或者上传/下载中文名称文件出现乱码的问题，那就可以使用这个过滤器。 12345678910111213141516&lt;filter&gt; &lt;filter-name&gt;encoding&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt;&lt;!--用来指定一个具体的字符集--&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;forceEncoding&lt;/param-name&gt;&lt;!--true：无论request是否指定了字符集，都是用encoding；false：如果request已指定一个字符集，则不使用encoding--&gt; &lt;param-value&gt;false&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;encoding&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 3、Spring+Hibernate的OpenSessionInViewFilter控制session的开关 当 hibernate+spring 配合使用的时候，如果设置了lazy=true（延迟加载）,那么在读取数据的时候，当读取了父数据后，hibernate 会自动关闭 session，这样，当要使用与之关联数据、子数据的时候，系统会抛出lazyinit的错误，这时就需要使用 spring 提供的 OpenSessionInViewFilter 过滤器。 OpenSessionInViewFilter主要是保持 Session 状态直到 request 将全部页面发送到客户端，直到请求结束后才关闭 session，这样就可以解决延迟加载带来的问题。 注意：OpenSessionInViewFilter 配置要写在struts2的配置前面。因为 tomcat 容器在加载过滤器的时候是按照顺序加载的，如果配置文件先写的是 struts2 的过滤器配置，然后才是 OpenSessionInViewFilter 过滤器配置，所以加载的顺序导致，action 在获得数据的时候 session 并没有被 spring 管理。 1234567891011121314151617&lt;!-- lazy loading enabled in spring --&gt;&lt;filter&gt; &lt;filter-name&gt;OpenSessionInViewFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.orm.hibernate3.support.OpenSessionInViewFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;sessionFactoryBeanName&lt;/param-name&gt;&lt;!-- 可缺省。默认是从spring容器中找id为sessionFactory的bean，如果id不为sessionFactory，则需要配置如下，此处SessionFactory为spring容器中的bean。 --&gt; &lt;param-value&gt;sessionFactory&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;singleSession&lt;/param-name&gt;&lt;!-- singleSession默认为true,若设为false则等于没用OpenSessionInView --&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;OpenSessionInViewFilter&lt;/filter-name&gt; &lt;url-pattern&gt;*.do&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 4、Struts2的web.xml配置 项目中使用Struts2同样需要在web.xml配置过滤器，用来截取请求，转到Struts2的Action进行处理。 注意：如果在2.1.3以前的Struts2版本，过滤器使用org.apache.struts2.dispatcher.FilterDispatcher。否则使用org.apache.struts2.dispatcher.ng.filter.StrutsPrepareAndExecuteFilter。从Struts2.1.3开始，将废弃ActionContextCleanUp过滤器，而在StrutsPrepareAndExecuteFilter过滤器中包含相应的功能。 三个初始化参数配置： config参数：指定要加载的配置文件。逗号分割。 actionPackages参数：指定Action类所在的包空间。逗号分割。 configProviders参数：自定义配置文件提供者，需要实现ConfigurationProvider接口类。逗号分割。 123456789&lt;!-- struts 2.x filter --&gt;&lt;filter&gt; &lt;filter-name&gt;struts2&lt;/filter-name&gt; &lt;filter-class&gt;org.apache.struts2.dispatcher.ng.filter.StrutsPrepareAndExecuteFilter&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;struts2&lt;/filter-name&gt; &lt;url-pattern&gt;*.do&lt;/url-pattern&gt;&lt;/filter-mapping&gt;","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"Filter过滤器","slug":"Filter过滤器","permalink":"http://www.54tianzhisheng.cn/tags/Filter过滤器/"}]},{"title":"详细深入分析 Java ClassLoader 工作机制","date":"2017-06-16T16:00:00.000Z","path":"2017/06/17/详细深入分析 Java ClassLoader 工作机制/","text":"什么是 ClassLoader ？大家都知道，当我们写好一个 Java 程序之后，不是管是 C/S 还是 B/S 应用，都是由若干个 .class 文件组织而成的一个完整的 Java 应用程序，当程序在运行时，即会调用该程序的一个入口函数来调用系统的相关功能，而这些功能都被封装在不同的 class 文件当中，所以经常要从这个 class 文件中要调用另外一个 class 文件中的方法，如果另外一个文件不存在的，则会引发系统异常。而程序在启动的时候，并不会一次性加载程序所要用的所有class文件，而是根据程序的需要，通过Java的类加载机制（ClassLoader）来动态加载某个 class 文件到内存当中的，从而只有 class 文件被载入到了内存之后，才能被其它 class 所引用。所以 ClassLoader 就是用来动态加载 class 文件到内存当中用的。 ClassLoader 作用： 负责将 Class 加载到 JVM 中 审查每个类由谁加载（父优先的等级加载机制） 将 Class 字节码重新解析成 JVM 统一要求的对象格式 1、ClassLoader 类结构分析为了更好的理解类的加载机制，我们来深入研究一下 ClassLoader 和他的方法。 public abstract class ClassLoader ClassLoader类是一个抽象类，sun公司是这么解释这个类的： 1234567/** * A class loader is an object that is responsible for loading classes. The * class ClassLoader is an abstract class. Given the binary name of a class, a class loader should attempt to * locate or generate data that constitutes a definition for the class. A * typical strategy is to transform the name into a file name and then read a * &quot;class file&quot; of that name from a file system.**/ 大致意思如下： class loader 是一个负责加载 classes 的对象，ClassLoader 类是一个抽象类，需要给出类的二进制名称，class loader 尝试定位或者产生一个 class 的数据，一个典型的策略是把二进制名字转换成文件名然后到文件系统中找到该文件。 以下是 ClassLoader 常用到的几个方法及其重载方法： ClassLoader defineClass(byte[], int, int) 把字节数组 b中的内容转换成 Java 类，返回的结果是java.lang.Class类的实例。这个方法被声明为final的 findClass(String name) 查找名称为 name的类，返回的结果是java.lang.Class类的实例 loadClass(String name) 加载名称为 name的类，返回的结果是java.lang.Class类的实例 resolveClass(Class&lt;?&gt;) 链接指定的 Java 类 其中 defineClass 方法用来将 byte 字节流解析成 JVM 能够识别的 Class 对象，有了这个方法意味着我们不仅仅可以通过 class 文件实例化对象，还可以通过其他方式实例化对象，如果我们通过网络接收到一个类的字节码，拿到这个字节码流直接创建类的 Class 对象形式实例化对象。如果直接调用这个方法生成类的 Class 对象，这个类的 Class 对象还没有 resolve ，这个 resolve 将会在这个对象真正实例化时才进行。 接下来我们看loadClass方法的实现方式： 123456789101112131415161718192021222324252627282930313233343536protected Class&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException &#123; synchronized (getClassLoadingLock(name)) &#123; // First, check if the class has already been loaded Class c = findLoadedClass(name); if (c == null) &#123; long t0 = System.nanoTime(); try &#123; if (parent != null) &#123; c = parent.loadClass(name, false); &#125; else &#123; c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; if (c == null) &#123; // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c; &#125; &#125; 该方法大概意思： 使用指定的二进制名称来加载类，这个方法的默认实现按照以下顺序查找类： 调用findLoadedClass(String) 方法检查这个类是否被加载过 使用父加载器调用 loadClass(String) 方法，如果父加载器为 Null，类加载器装载虚拟机内置的加载器调用 findClass(String) 方法装载类， 如果，按照以上的步骤成功的找到对应的类，并且该方法接收的 resolve 参数的值为 true,那么就调用resolveClass(Class) 方法来处理类。 ClassLoader 的子类最好覆盖 findClass(String) 而不是这个方法。 除非被重写，这个方法默认在整个装载过程中都是同步的（线程安全的）。 2、ClassLoader 的等级加载机制Java默认提供的三个ClassLoader BootStrap ClassLoader：称为启动类加载器，是Java类加载层次中最顶层的类加载器，负责加载JDK中的核心类库，如：rt.jar、resources.jar、charsets.jar等，可通过如下程序获得该类加载器从哪些地方加载了相关的jar或class文件： 12345678910public class BootStrapTest&#123; public static void main(String[] args) &#123; URL[] urls = sun.misc.Launcher.getBootstrapClassPath().getURLs(); for (int i = 0; i &lt; urls.length; i++) &#123; System.out.println(urls[i].toExternalForm()); &#125; &#125;&#125; 以下内容是上述程序从本机JDK环境所获得的结果： 其实上述结果也是通过查找 sun.boot.class.path 这个系统属性所得知的。 1System.out.println(System.getProperty(\"sun.boot.class.path\")); 1打印结果：C:\\Java\\jdk1.8.0_60\\jre\\lib\\resources.jar;C:\\Java\\jdk1.8.0_60\\jre\\lib\\rt.jar;C:\\Java\\jdk1.8.0_60\\jre\\lib\\sunrsasign.jar;C:\\Java\\jdk1.8.0_60\\jre\\lib\\jsse.jar;C:\\Java\\jdk1.8.0_60\\jre\\lib\\jce.jar;C:\\Java\\jdk1.8.0_60\\jre\\lib\\charsets.jar;C:\\Java\\jdk1.8.0_60\\jre\\lib\\jfr.jar;C:\\Java\\jdk1.8.0_60\\jre\\classes Extension ClassLoader：称为扩展类加载器，负责加载Java的扩展类库，Java 虚拟机的实现会提供一个扩展库目录。该类加载器在此目录里面查找并加载 Java 类。默认加载JAVA_HOME/jre/lib/ext/目下的所有jar。 App ClassLoader：称为系统类加载器，负责加载应用程序classpath目录下的所有jar和class文件。一般来说，Java 应用的类都是由它来完成加载的。可以通过 ClassLoader.getSystemClassLoader()来获取它。 ​ 除了系统提供的类加载器以外，开发人员可以通过继承java.lang.ClassLoader类的方式实现自己的类加载器，以满足一些特殊的需求。 除了引导类加载器之外，所有的类加载器都有一个父类加载器。 给出的 getParent()方法可以得到。对于系统提供的类加载器来说，系统类加载器的父类加载器是扩展类加载器，而扩展类加载器的父类加载器是引导类加载器；对于开发人员编写的类加载器来说，其父类加载器是加载此类加载器 Java 类的类加载器。因为类加载器 Java 类如同其它的 Java 类一样，也是要由类加载器来加载的。一般来说，开发人员编写的类加载器的父类加载器是系统类加载器。类加载器通过这种方式组织起来，形成树状结构。树的根节点就是引导类加载器。 ​ ClassLoader加载类的原理1. 原理介绍ClassLoader使用的是双亲委托模型来搜索类的，每个ClassLoader实例都有一个父类加载器的引用（不是继承的关系，是一个包含的关系），虚拟机内置的类加载器（Bootstrap ClassLoader）本身没有父类加载器，但可以用作其它ClassLoader实例的的父类加载器。当一个ClassLoader实例需要加载某个类时，它会试图亲自搜索某个类之前，先把这个任务委托给它的父类加载器，这个过程是由上至下依次检查的，首先由最顶层的类加载器Bootstrap ClassLoader试图加载，如果没加载到，则把任务转交给Extension ClassLoader试图加载，如果也没加载到，则转交给App ClassLoader进行加载，如果它也没有加载得到的话，则返回给委托的发起者，由它到指定的文件系统或网络等URL中加载该类。如果它们都没有加载到这个类时，则抛出ClassNotFoundException异常。否则将这个找到的类生成一个类的定义，并将它加载到内存当中，最后返回这个类在内存中的Class实例对象。 2、为什么要使用双亲委托这种模型呢？因为这样可以避免重复加载，当父亲已经加载了该类的时候，就没有必要 ClassLoader再加载一次。考虑到安全因素，我们试想一下，如果不使用这种委托模式，那我们就可以随时使用自定义的String来动态替代java核心api中定义的类型，这样会存在非常大的安全隐患，而双亲委托的方式，就可以避免这种情况，因为String已经在启动时就被引导类加载器（Bootstrcp ClassLoader）加载，所以用户自定义的ClassLoader永远也无法加载一个自己写的String，除非你改变JDK中ClassLoader搜索类的默认算法。 3、 但是JVM在搜索类的时候，又是如何判定两个class是相同的呢？JVM在判定两个class是否相同时，不仅要判断两个类名是否相同，而且要判断是否由同一个类加载器实例加载的。只有两者同时满足的情况下，JVM才认为这两个class是相同的。就算两个class是同一份class字节码，如果被两个不同的ClassLoader实例所加载，JVM也会认为它们是两个不同class。比如网络上的一个Java类org.classloader.simple.NetClassLoaderSimple，javac编译之后生成字节码文件NetClassLoaderSimple.class，ClassLoaderA和ClassLoaderB这两个类加载器并读取了NetClassLoaderSimple.class文件，并分别定义出了java.lang.Class实例来表示这个类，对于JVM来说，它们是两个不同的实例对象，但它们确实是同一份字节码文件，如果试图将这个Class实例生成具体的对象进行转换时，就会抛运行时异常java.lang.ClassCaseException，提示这是两个不同的类型。现在通过实例来验证上述所描述的是否正确：1）、在web服务器上建一个org.classloader.simple.NetClassLoaderSimple.java类 1234567public class NetClassLoaderSimple&#123; private NetClassLoaderSimple instance; public void setNetClassLoaderSimple(Object object)&#123; this.instance = (NetClassLoaderSimple)object; &#125;&#125; org.classloader.simple.NetClassLoaderSimple类的setNetClassLoaderSimple方法接收一个Object类型参数，并将它强制转换成org.classloader.simple.NetClassLoaderSimple类型。 2）、测试两个class是否相同 NetWorkClassLoader.java 12345678910111213141516171819202122package classloader;public class NewworkClassLoaderTest &#123; public static void main(String[] args) &#123; try &#123; //测试加载网络中的class文件 String rootUrl = &quot;http://localhost:8080/httpweb/classes&quot;; String className = &quot;org.classloader.simple.NetClassLoaderSimple&quot;; NetworkClassLoader ncl1 = new NetworkClassLoader(rootUrl); NetworkClassLoader ncl2 = new NetworkClassLoader(rootUrl); Class&lt;?&gt; clazz1 = ncl1.loadClass(className); Class&lt;?&gt; clazz2 = ncl2.loadClass(className); Object obj1 = clazz1.newInstance(); Object obj2 = clazz2.newInstance(); clazz1.getMethod(&quot;setNetClassLoaderSimple&quot;, Object.class).invoke(obj1, obj2); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 首先获得网络上一个class文件的二进制名称，然后通过自定义的类加载器NetworkClassLoader创建两个实例，并根据网络地址分别加载这份class，并得到这两个ClassLoader实例加载后生成的Class实例clazz1和clazz2，最后将这两个Class实例分别生成具体的实例对象obj1和obj2，再通过反射调用clazz1中的setNetClassLoaderSimple方法。 3）、查看测试结果 结论：从结果中可以看出，运行时抛出了java.lang.ClassCastException异常。虽然两个对象obj1和 obj2的类的名字相同，但是这两个类是由不同的类加载器实例来加载的，所以JVM认为它们就是两个不同的类。 了解了这一点之后，就可以理解代理模式的设计动机了。代理模式是为了保证 Java 核心库的类型安全。所有 Java 应用都至少需要引用 java.lang.Object类，也就是说在运行的时候，java.lang.Object这个类需要被加载到 Java 虚拟机中。如果这个加载过程由 Java 应用自己的类加载器来完成的话，很可能就存在多个版本的 java.lang.Object类，而且这些类之间是不兼容的。通过代理模式，对于 Java 核心库的类的加载工作由引导类加载器来统一完成，保证了Java 应用所使用的都是同一个版本的 Java 核心库的类，是互相兼容的。 不同的类加载器为相同名称的类创建了额外的名称空间。相同名称的类可以并存在 Java 虚拟机中，只需要用不同的类加载器来加载它们即可。不同类加载器加载的类之间是不兼容的，这就相当于在 Java 虚拟机内部创建了一个个相互隔离的 Java 类空间。 ClassLoader的体系架构： 类加载器的树状组织结构测试一： 1234567891011public class ClassLoaderTree&#123; public static void main(String[] args) &#123; ClassLoader loader = ClassLoaderTree.class.getClassLoader(); while (loader!=null)&#123; System.out.println(loader.toString()); loader = loader.getParent(); &#125; System.out.println(loader); &#125;&#125; 每个 Java 类都维护着一个指向定义它的类加载器的引用，通过 getClassLoader()方法就可以获取到此引用。代码中通过递归调用 getParent()方法来输出全部的父类加载器。 结果是： 第一个输出的是ClassLoaderTree类的类加载器，即系统类加载器。它是sun.misc.Launcher$AppClassLoader类的实例；第二个输出的是扩展类加载器，是sun.misc.Launcher$ExtClassLoader类的实例。需要注意的是这里并没有输出引导类加载器，这是由于有些 JDK 的实现对于父类加载器是引导类加载器的情况，getParent()方法返回 null。第三行结果说明：ExtClassLoader的类加器是Bootstrap ClassLoader，因为Bootstrap ClassLoader不是一个普通的Java类，所以ExtClassLoader的parent=null，所以第三行的打印结果为null就是这个原因。 测试二： 将ClassLoaderTree.class打包成ClassLoaderTree.jar，放到Extension ClassLoader的加载目录下（JAVA_HOME/jre/lib/ext），然后重新运行这个程序，得到的结果会是什么样呢？ 此处我在 IDEA 中的运行结果还和上面的一样，与文章 深入分析Java ClassLoader原理 中的有差距，具体原因未弄清楚，还希望读者能够亲自测试。 那文章中的结果是： 打印结果分析：为什么第一行的结果是ExtClassLoader呢？ 因为 ClassLoader 的委托模型机制，当我们要用 ClassLoaderTest.class 这个类的时候，AppClassLoader 在试图加载之前，先委托给 Bootstrcp ClassLoader，Bootstracp ClassLoader 发现自己没找到，它就告诉 ExtClassLoader，兄弟，我这里没有这个类，你去加载看看，然后 Extension ClassLoader 拿着这个类去它指定的类路径（JAVA_HOME/jre/lib/ext）试图加载，唉，它发现在ClassLoaderTest.jar 这样一个文件中包含 ClassLoaderTest.class 这样的一个文件，然后它把找到的这个类加载到内存当中，并生成这个类的 Class 实例对象，最后把这个实例返回。所以 ClassLoaderTest.class 的类加载器是 ExtClassLoader。 第二行的结果为null，是因为ExtClassLoader的父类加载器是Bootstrap ClassLoader。 JVM加载class文件的两种方法； 隐式加载， 程序在运行过程中当碰到通过new 等方式生成对象时，隐式调用类装载器加载对应的类到jvm中。 显式加载， 通过class.forname()、this.getClass.getClassLoader().loadClass()等方法显式加载需要的类，或者我们自己实现的 ClassLoader 的 findlass() 方法。 下面介绍下 class.forName的加载类方法： Class.forName是一个静态方法，同样可以用来加载类。该方法有两种形式：Class.forName(String name,boolean initialize, ClassLoader loader)和Class.forName(String className)。第一种形式的参数 name表示的是类的全名；initialize表示是否初始化类；loader表示加载时使用的类加载器。第二种形式则相当于设置了参数 initialize的值为 true，loader的值为当前类的类加载器。Class.forName的一个很常见的用法是在加载数据库驱动的时候。如Class.forName(&quot;org.apache.derby.jdbc.EmbeddedDriver&quot;)用来加载 Apache Derby 数据库的驱动。 类加载的动态性体现：一个应用程序总是由n多个类组成，Java程序启动时，并不是一次把所有的类全部加载后再运行，它总是先把保证程序运行的基础类一次性加载到jvm中，其它类等到jvm用到的时候再加载，这样的好处是节省了内存的开销，因为java最早就是为嵌入式系统而设计的，内存宝贵，这是一种可以理解的机制，而用到时再加载这也是java动态性的一种体现。 3、如何加载 class 文件 第一阶段找到 .class 文件并把这个文件包含的字节码加载到内存中。 第二阶段中分三步，字节码验证；class 类数据结构分析及相应的内存分配；最后的符号表的链接。 第三阶段是类中静态属性和初始化赋值，以及静态块的执行等。 3.1 、加载字节码到内存。。 3.2 、验证与分析 字节码验证，类装入器对于类的字节码要做很多检测，以确保格式正确，行为正确。 类装备，准备代表每个类中定义的字段、方法和实现接口所必须的数据结构。 解析，装入器装入类所引用的其他所有类。 4、常见加载类错误分析4.1 、 ClassNotFoundExecptionClassNotFoundExecption 异常是平常碰到的最多的。这个异常通常发生在显示加载类的时候。 12345678910public class ClassNotFoundExceptionTest&#123; public static void main(String[] args) &#123; try &#123; Class.forName(&quot;NotFoundClass&quot;); &#125;catch (ClassNotFoundException e)&#123; e.printStackTrace(); &#125; &#125;&#125; 显示加载一个类通常有： 通过类 Class 中的 forName() 方法 通过类 ClassLoader 中的 loadClass() 方法 通过类 ClassLoader 中的 findSystemClass() 方法 出现这种错误其实就是当 JVM 要加载指定文件的字节码到内存时，并没有找到这个文件对应的字节码，也就是这个文件并不存在。解决方法就是检查在当前的 classpath 目录下有没有指定的文件。 4.2 、 NoClassDefFoundError在JavaDoc中对NoClassDefFoundError的产生可能的情况就是使用new关键字、属性引用某个类、继承了某个接口或者类，以及方法的某个参数中引用了某个类，这时就会触发JVM或者类加载器实例尝试加载类型的定义，但是该定义却没有找到，影响了执行路径。换句话说，在编译时这个类是能够被找到的，但是在执行时却没有找到。 解决这个错误的方法就是确保每个类引用的类都在当前的classpath下面。 4.3 、 UnsatisfiedLinkError该错误通常是在 JVM 启动的时候，如果 JVM 中的某个 lib 删除了，就有可能报这个错误。 12345678910public class UnsatisfiedLinkErrorTest&#123; public native void nativeMethod(); static &#123; System.loadLibrary(\"NoLib\"); &#125; public static void main(String[] args) &#123; new UnsatisfiedLinkErrorTest().nativeMethod(); //解析native标识的方法时JVM找不到对应的库文件 &#125;&#125; 4.4 、 ClassCastException该错误通常出现强制类型转换时出现这个错误。 123456789101112public class ClassCastExceptionTest&#123; public static Map m = new HashMap()&#123; &#123; put(\"a\", \"2\"); &#125; &#125;; public static void main(String[] args) &#123; Integer integer = (Integer) m.get(\"a\"); //将m强制转换成Integer类型 System.out.println(integer); &#125;&#125; 注意：JVM 在做类型转换时的规则： 对于普通对象，对象必须是目标类的实例或目标类的子类的实例。如果目标类是接口，那么会把它当作实现了该接口的一个子类。 对于数组类型，目标类必须是数组类型或 java.lang.Object、java.lang.Cloneable、java.io.Serializable。 如果不满足上面的规则，JVM 就会报错，有两种方式可避免错误： 在容器类型中显式的指明这个容器所包含的对象类型。 先通过 instanceof 检查是不是目标类型，然后再进行强制类型的转换。 上面代码中改成如下就可以避免错误了： 4.5 、 ExceptionInInitializerError12345678910public class ExceptionInInitializerErrorTest&#123; public static Map m = new HashMap()&#123;&#123; m.put(\"a\", \"2\"); &#125;&#125;; public static void main(String[] args) &#123; Integer integer = (Integer) m.get(\"a\"); System.out.println(integer); &#125;&#125; 在初始化这个类时，给静态属性 m 赋值时出现了异常导致抛出错误 ExceptionInInitializerError。 4.6 NoSuchMethodErrorNoSuchMethodError代表这个类型确实存在，但是一个不正确的版本被加载了。为了解决这个问题我们可以使用 ‘­verbose:class’ 来判断该JVM加载的到底是哪个版本。 4.7 LinkageError有时候事情会变得更糟，和 ClassCastException 本质一样，加载自不同位置的相同类在同一段逻辑（比如：方法）中交互时，会出现 LinkageError 。 LinkageError 需要观察哪个类被不同的类加载器加载了，在哪个方法或者调用处发生（交汇）的，然后才能想解决方法，解决方法无外乎两种。第一，还是不同的类加载器加载，但是相互不再交汇影响，这里需要针对发生问题的地方做一些改动，比如更换实现方式，避免出现上述问题；第二，冲突的类需要由一个Parent类加载器进行加载。LinkageError 和ClassCastException 本质是一样的，加载自不同类加载器的类型，在同一个类的方法或者调用中出现，如果有转型操作那么就会抛 ClassCastException ，如果是直接的方法调用处的参数或者返回值解析，那么就会产生 LinkageError 。 5、常用的 ClassLoader 分析。。参见书籍《深入分析Java Web技术内幕》 6、如何实现自己的 ClassLoaderClassLoader 能够完成的事情有以下情况： 在自定义路径下查找自定义的class类文件。 对我们自己要加载的类做特殊处理。 可以定义类的实现机制。 虽然在绝大多数情况下，系统默认提供的类加载器实现已经可以满足需求。但是在某些情况下，您还是需要为应用开发出自己的类加载器。比如您的应用通过网络来传输 Java 类的字节代码，为了保证安全性，这些字节代码经过了加密处理。这个时候您就需要自己的类加载器来从某个网络地址上读取加密后的字节代码，接着进行解密和验证，最后定义出要在 Java 虚拟机中运行的类来。 定义自已的类加载器分为两步：1、继承java.lang.ClassLoader2、重写父类的findClass方法 6.1 、文件系统类加载器加载存储在文件系统上的 Java 字节代码。 123456789101112131415161718192021222324252627282930313233343536373839404142public class FileSystemClassLoader extends ClassLoader&#123; private String rootDir; public FileSystemClassLoader(String rootDir)&#123; this.rootDir = rootDir; &#125; protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; byte[] classData = getClassData(name); if (classData == null)&#123; throw new ClassNotFoundException(); &#125; else &#123; return defineClass(name, classData, 0, classData.length); &#125; &#125; private byte[] getClassData(String className) &#123; String path = classNameToPath(className); try &#123; InputStream ins = new FileInputStream(path); ByteArrayOutputStream baos = new ByteArrayOutputStream(); int bufferSize = 4096; byte[] buffer = new byte[bufferSize]; int bytesNumRead = 0; while ((bytesNumRead = ins.read(buffer)) != -1)&#123; baos.write(buffer, 0, bytesNumRead); &#125; return baos.toByteArray(); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; private String classNameToPath(String className) &#123; return rootDir + File.separatorChar + className.replace('.', File.separatorChar) + \".class\"; &#125;&#125; 类 FileSystemClassLoader继承自类java.lang.ClassLoader。java.lang.ClassLoader类的方法loadClass()封装了前面提到的代理模式的实现。该方法会首先调用 findLoadedClass()方法来检查该类是否已经被加载过；如果没有加载过的话，会调用父类加载器的loadClass()方法来尝试加载该类；如果父类加载器无法加载该类的话，就调用 findClass()方法来查找该类。因此，为了保证类加载器都正确实现代理模式，在开发自己的类加载器时，最好不要覆写 loadClass()方法，而是覆写findClass()方法。 类 FileSystemClassLoader的 findClass()方法首先根据类的全名在硬盘上查找类的字节代码文件（.class 文件），然后读取该文件内容，最后通过 defineClass()方法来把这些字节代码转换成 java.lang.Class类的实例。 6.2 、 网络类加载器一个网络类加载器来说明如何通过类加载器来实现组件的动态更新。即基本的场景是：Java 字节代码（.class）文件存放在服务器上，客户端通过网络的方式获取字节代码并执行。当有版本更新的时候，只需要替换掉服务器上保存的文件即可。通过类加载器可以比较简单的实现这种需求。 类 NetworkClassLoader 负责通过网络下载 Java 类字节代码并定义出 Java 类。它的实现与FileSystemClassLoader 类似。在通过 NetworkClassLoader 加载了某个版本的类之后，一般有两种做法来使用它。第一种做法是使用 Java 反射 API。另外一种做法是使用接口。需要注意的是，并不能直接在客户端代码中引用从服务器上下载的类，因为客户端代码的类加载器找不到这些类。使用 Java 反射 API 可以直接调用 Java 类的方法。而使用接口的做法则是把接口的类放在客户端中，从服务器上加载实现此接口的不同版本的类。在客户端通过相同的接口来使用这些实现类。 网络类加载器的代码：ClassLoader 7、类加载器与Web容器对于运行在 Java EE™容器中的 Web 应用来说，类加载器的实现方式与一般的 Java 应用有所不同。不同的 Web 容器的实现方式也会有所不同。以 Apache Tomcat 来说，每个Web 应用都有一个对应的类加载器实例。该类加载器也使用代理模式，所不同的是它是首先尝试去加载某个类，如果找不到再代理给父类加载器。这与一般类加载器的顺序是相反的。这是 Java Servlet 规范中的推荐做法，其目的是使得Web 应用自己的类的优先级高于 Web 容器提供的类。这种代理模式的一个例外是：Java 核心库的类是不在查找范围之内的。这也是为了保证 Java 核心库的类型安全。 绝大多数情况下，Web 应用的开发人员不需要考虑与类加载器相关的细节。下面给出几条简单的原则： 每个 Web 应用自己的 Java 类文件和使用的库的 jar 包，分别放在 WEB-INF/classes和 WEB-INF/lib目录下面。 多个应用共享的 Java 类文件和 jar 包，分别放在 Web 容器指定的由所有 Web 应用共享的目录下面。 当出现找不到类的错误时，检查当前类的类加载器和当前线程的上下文类加载器是否正确 8、总结本篇文章详细深入的介绍了 ClassLoader 的工作机制，还写了如何自己实现所需的 ClassLoader 。 参考资料1、深度分析 Java 的 ClassLoader 机制（源码级别） 2、深入浅出ClassLoader 3、深入探讨 Java 类加载器 4、深入分析Java ClassLoader原理 5、《深入分析 Java Web 技术内幕》修订版 —— 深入分析 ClassLoader 工作机制","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"JVM","slug":"JVM","permalink":"http://www.54tianzhisheng.cn/tags/JVM/"},{"name":"类加载机制","slug":"类加载机制","permalink":"http://www.54tianzhisheng.cn/tags/类加载机制/"}]},{"title":"通过项目逐步深入了解Spring MVC（一）","date":"2017-06-15T16:00:00.000Z","path":"2017/06/16/通过项目逐步深入了解Spring MVC（一）/","text":"相关阅读：本文档和项目代码地址：https://github.com/zhisheng17/springmvc 了解 Spring： Spring 官网：http://spring.io/ 一个好的东西一般都会有一个好的文档解释说明，如果你英语还行，建议还是看官方文档。 Spring MVC基础知识什么是Spring MVC？ Spring MVC框架原理（掌握） ​ 前端控制器、处理器映射器、处理器适配器、试图解析器Spring MVC 入门程序 ​ 目的：对前端控制器、处理器映射器、处理器适配器、试图解析器学习 ​ 非注解的处理器映射器、处理器适配器 ​ 注解的处理器映射器、处理器适配器（掌握） Spring MVC 和 Mybatis 整合（掌握） Spring MVC 注解开发：（掌握） ​ 常用的注解学习 ​ 参数绑定（简单类型，pojo类型、集合类型） ​ 自定义的参数绑定（掌握） Spring MVC 和 Struts2区别 Spring MVC高级应用参数绑定（集合类型） 数据回显 上传图片 json 数据交互 RESTful 支持 拦截器 Spring MVC 框架什么是Spring MVC？springmvc是spring框架的一个模块，springmvc和spring无需通过中间整合层进行整合。springmvc是一个基于mvc的web框架。 Web MVCMVC 设计模式在 B/S 系统下应用： 1、 用户发起request请求至控制器(Controller) 控制接收用户请求的数据，委托给模型进行处理 2、 控制器通过模型(Model)处理数据并得到处理结果 模型通常是指业务逻辑 3、 模型处理结果返回给控制器 4、 控制器将模型数据在视图(View)中展示 web中模型无法将数据直接在视图上显示，需要通过控制器完成。如果在C/S应用中模型是可以将数据在视图中展示的。 5、 控制器将视图response响应给用户 通过视图展示给用户要的数据或处理结果。 Spring MVC 框架 第一步：发起请求到前端控制器(DispatcherServlet) 第二步：前端控制器请求HandlerMapping查找 Handler 可以根据xml配置、注解进行查找 第三步：处理器映射器HandlerMapping向前端控制器返回Handler 第四步：前端控制器调用处理器适配器去执行Handler 第五步：处理器适配器去执行Handler 第六步：Handler执行完成给适配器返回ModelAndView 第七步：处理器适配器向前端控制器返回ModelAndView ModelAndView是springmvc框架的一个底层对象，包括Model和view 第八步：前端控制器请求视图解析器去进行视图解析 根据逻辑视图名解析成真正的视图(jsp) 第九步：视图解析器向前端控制器返回View 第十步：前端控制器进行视图渲染 视图渲染将模型数据(在ModelAndView对象中)填充到request域 第十一步：前端控制器向用户响应结果 组件： 1、前端控制器DispatcherServlet（不需要程序员开发） 作用接收请求，响应结果，相当于转发器，中央处理器。 有了DispatcherServlet减少了其它组件之间的耦合度。 2、处理器映射器HandlerMapping(不需要程序员开发) 作用：根据请求的url查找Handler 3、处理器适配器HandlerAdapter 作用：按照特定规则（HandlerAdapter要求的规则）去执行Handler 4、处理器Handler(需要程序员开发) 注意：编写Handler时按照HandlerAdapter的要求去做，这样适配器才可以去正确执行Handler 5、视图解析器View resolver(不需要程序员开发) 作用：进行视图解析，根据逻辑视图名解析成真正的视图（view） 6、视图View(需要程序员开发jsp) View是一个接口，实现类支持不同的View类型（jsp、freemarker、pdf…）","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"Spring MVC","slug":"Spring-MVC","permalink":"http://www.54tianzhisheng.cn/tags/Spring-MVC/"}]},{"title":"通过项目逐步深入了解Mybatis（四）","date":"2017-06-14T16:00:00.000Z","path":"2017/06/15/通过项目逐步深入了解Mybatis(四)/","text":"相关阅读： 1、通过项目逐步深入了解Mybatis&lt;一&gt; 2、通过项目逐步深入了解Mybatis&lt;二&gt; 3、通过项目逐步深入了解Mybatis&lt;三&gt; 本项目所有代码及文档都托管在 Github地址：https://github.com/zhisheng17/mybatis 延迟加载什么是延迟加载？resultMap可以实现高级映射（使用association、collection实现一对一及一对多映射），association、collection具备延迟加载功能。需求：如果查询订单并且关联查询用户信息。如果先查询订单信息即可满足要求，当我们需要查询用户信息时再查询用户信息。把对用户信息的按需去查询就是延迟加载。 延迟加载：先从单表查询、需要时再从关联表去关联查询，大大提高 数据库性能，因为查询单表要比关联查询多张表速度要快。 打开延迟加载开关在mybatis核心配置文件中配置： lazyLoadingEnabled、aggressiveLazyLoading 设置项 描述 允许值 默认值 lazyLoadingEnabled 全局性设置懒加载。如果设为‘false’，则所有相关联的都会被初始化加载。 true \\ false false aggressiveLazyLoading 当设置为‘true’的时候，懒加载的对象可能被任何懒属性全部加载。否则，每个属性都按需加载。 true \\ false true 1234&lt;settings&gt; &lt;setting name=\"lazyLoadingEnabled\" value=\"true\"/&gt; &lt;setting name=\"aggressiveLazyLoading\" value=\"false\"/&gt;&lt;/settings&gt; 使用 association 实现延迟加载需求：查询订单并且关联查询用户信息 Mapper.xml需要定义两个 mapper 的方法对应的 statement。 1、只查询订单信息 SQL 语句： select * from orders 在查询订单的 statement 中使用 association 去延迟加载（执行）下边的 statement (关联查询用户信息) 1234&lt;!--查询订单并且关联查询用户信息，关联用户信息需要通过 association 延迟加载--&gt; &lt;select id=\"findOrdersUserLazyLoading\" resultMap=\"OrdersUserLazyLoadingResultMap\"&gt; select * from orders &lt;/select&gt; 2、关联查询用户信息 通过上面查询订单信息中的 user_id 来关联查询用户信息。使用 UserMapper.xml 中的 findUserById SQL语句：select * from user where id = user_id 123&lt;select id=\"findUserById\" parameterType=\"int\" resultType=\"user\"&gt; select * from user where id = #&#123;value&#125; &lt;/select&gt; 上边先去执行 findOrdersUserLazyLoading，当需要去查询用户的时候再去执行 findUserById ，通过 resultMap的定义将延迟加载执行配置起来。也就是通过 resultMap 去加载 UserMapper.xml 文件中的 select = findUserById 延迟加载的 resultMap1234567891011121314151617181920&lt;!--定义 关联用户信息（通过 association 延迟加载）的resultMap--&gt; &lt;resultMap id=\"OrdersUserLazyLoadingResultMap\" type=\"cn.zhisheng.mybatis.po.Orders\"&gt; &lt;!--对订单信息映射--&gt; &lt;id column=\"id\" property=\"id\"/&gt; &lt;result column=\"user_id\" property=\"userId\"/&gt; &lt;result column=\"number\" property=\"number\"/&gt; &lt;result column=\"createtime\" property=\"createtime\"/&gt; &lt;result column=\"note\" property=\"note\"/&gt; &lt;!-- 实现对用户信息进行延迟加载 select：指定延迟加载需要执行的statement的id（是根据user_id查询用户信息的statement） 要使用userMapper.xml中findUserById完成根据用户id(user_id)用户信息的查询，如果findUserById不在本mapper中需要前边加namespace column：订单信息中关联用户信息查询的列，是user_id 关联查询的sql理解为： SELECT orders.*, (SELECT username FROM USER WHERE orders.user_id = user.id)username, (SELECT sex FROM USER WHERE orders.user_id = user.id)sex FROM orders--&gt; &lt;association property=\"user\" javaType=\"cn.zhisheng.mybatis.po.User\" select=\"cn.zhisheng.mybatis.mapper.UserMapper.findUserById\" column=\"user_id\"&gt; &lt;/association&gt; &lt;/resultMap&gt; OrderMapperCustom.java1public List&lt;Orders&gt; findOrdersUserLazyLoading() throws Exception; 测试代码：1234567891011121314151617@Test public void testFindOrdersUserLazyLoading() throws Exception &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); //创建OrdersMapperCustom对象,mybatis自动生成代理对象 OrdersMapperCustom ordersMapperCustom = sqlSession.getMapper(OrdersMapperCustom.class); //查询订单信息 List&lt;Orders&gt; list = ordersMapperCustom.findOrdersUserLazyLoading(); //遍历所查询的的订单信息 for (Orders orders : list) &#123; //查询用户信息 User user = orders.getUser(); System.out.println(user); &#125; sqlSession.close(); &#125; 测试结果： 整个延迟加载的思路： 1、执行上边mapper方法（findOrdersUserLazyLoading），内部去调用cn.zhisheng.mybatis.mapper.OrdersMapperCustom 中的 findOrdersUserLazyLoading 只查询 orders 信息（单表）。 2、在程序中去遍历上一步骤查询出的 List，当我们调用 Orders 中的 getUser 方法时，开始进行延迟加载。 3、延迟加载，去调用 UserMapper.xml 中 findUserbyId 这个方法获取用户信息。 思考：不使用 mybatis 提供的 association 及 collection 中的延迟加载功能，如何实现延迟加载？？ 实现方法如下： 定义两个mapper方法： 1、查询订单列表 2、根据用户id查询用户信息 实现思路： 先去查询第一个mapper方法，获取订单信息列表 在程序中（service），按需去调用第二个mapper方法去查询用户信息。 总之： 使用延迟加载方法，先去查询 简单的 sql（最好单表，也可以关联查询），再去按需要加载关联查询的其它信息。 一对多延迟加载上面的那个案例是一对一延迟加载，那么如果我们想一对多进行延迟加载呢，其实也是很简单的。 一对多延迟加载的方法同一对一延迟加载，在collection标签中配置select内容。 延迟加载总结：作用： 当需要查询关联信息时再去数据库查询，默认不去关联查询，提高数据库性能。只有使用resultMap支持延迟加载设置。 场合： 当只有部分记录需要关联查询其它信息时，此时可按需延迟加载，需要关联查询时再向数据库发出sql，以提高数据库性能。 当全部需要关联查询信息时，此时不用延迟加载，直接将关联查询信息全部返回即可，可使用resultType或resultMap完成映射。 查询缓存什么是查询缓存？mybatis提供查询缓存，用于减轻数据压力，提高数据库性能。 mybaits提供一级缓存，和二级缓存。 一级缓存是SqlSession级别的缓存。在操作数据库时需要构造 sqlSession对象，在对象中有一个数据结构（HashMap）用于存储缓存数据。不同的sqlSession之间的缓存数据区域（HashMap）是互相不影响的。 二级缓存是mapper级别的缓存，多个SqlSession去操作同一个Mapper的sql语句，多个SqlSession可以共用二级缓存，二级缓存是跨SqlSession的。 为什么要用缓存？ 如果缓存中有数据就不用从数据库中获取，大大提高系统性能。 一级缓存工作原理： 第一次发起查询用户id为1的用户信息，先去找缓存中是否有id为1的用户信息，如果没有，从数据库查询用户信息。 得到用户信息，将用户信息存储到一级缓存中。 如果sqlSession去执行commit操作（执行插入、更新、删除），清空SqlSession中的一级缓存，这样做的目的为了让缓存中存储的是最新的信息，避免脏读。 第二次发起查询用户id为1的用户信息，先去找缓存中是否有id为1的用户信息，缓存中有，直接从缓存中获取用户信息。 一级缓存测试 Mybatis 默认支持一级缓存，不需要在配置文件中配置。 所以我们直接按照上面的步骤进行测试： 123456789101112131415//一级缓存测试 @Test public void testCache1() throws Exception &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); //创建UserMapper对象,mybatis自动生成代理对象 UserMapper userMapper = sqlSession.getMapper(UserMapper.class); //查询使用的是同一个session //第一次发起请求，查询Id 为1的用户信息 User user1 = userMapper.findUserById(1); System.out.println(user1); //第二次发起请求，查询Id 为1的用户信息 User user2 = userMapper.findUserById(1); System.out.println(user2); sqlSession.close(); &#125; 通过结果可以看出第二次没有发出sql查询请求， 所以我们需要在中间执行 commit 操作 123456789//如果sqlSession去执行commit操作（执行插入、更新、删除），// 清空SqlSession中的一级缓存，这样做的目的为了让缓存中存储的是最新的信息，避免脏读。//更新user1的信息，user1.setUsername(\"李飞\");//user1.setSex(\"男\");//user1.setAddress(\"北京\");userMapper.updateUserById(user1);//提交事务,才会去清空缓存sqlSession.commit(); 测试 一级缓存应用 正式开发，是将 mybatis 和 spring 进行整合开发，事务控制在 service 中。 一个 service 方法中包括很多 mapper 方法调用。 service{ //开始执行时，开启事务，创建SqlSession对象 //第一次调用mapper的方法findUserById(1) //第二次调用mapper的方法findUserById(1)，从一级缓存中取数据 //方法结束，sqlSession关闭 } 如果是执行两次service调用查询相同的用户信息，不走一级缓存，因为session方法结束，sqlSession就关闭，一级缓存就清空。 二级缓存原理 首先开启mybatis的二级缓存。 sqlSession1去查询用户id为1的用户信息，查询到用户信息会将查询数据存储到二级缓存中。 如果SqlSession3去执行相同 mapper下sql，执行commit提交，清空该 mapper下的二级缓存区域的数据。 sqlSession2去查询用户id为1的用户信息，去缓存中找是否存在数据，如果存在直接从缓存中取出数据。 二级缓存与一级缓存区别，二级缓存的范围更大，多个sqlSession可以共享一个UserMapper的二级缓存区域。 UserMapper有一个二级缓存区域（按namespace分） ，其它mapper也有自己的二级缓存区域（按namespace分）。 每一个namespace的mapper都有一个二缓存区域，两个mapper的namespace如果相同，这两个mapper执行sql查询到数据将存在相同的二级缓存区域中。 开启二级缓存： mybaits的二级缓存是mapper范围级别，除了在SqlMapConfig.xml设置二级缓存的总开关，还要在具体的mapper.xml中开启二级缓存 在 SqlMapConfig.xml 开启二级开关 12&lt;!-- 开启二级缓存 --&gt;&lt;setting name=\"cacheEnabled\" value=\"true\"/&gt; 然后在你的 Mapper 映射文件中添加一行： ，表示此 mapper 开启二级缓存。 调用 pojo 类实现序列化接口： 二级缓存需要查询结果映射的pojo对象实现java.io.Serializable接口实现序列化和反序列化操作（因为二级缓存数据存储介质多种多样，在内存不一样），注意如果存在父类、成员pojo都需要实现序列化接口。 12public class Orders implements Serializablepublic class User implements Serializable 测试 12345678910111213141516171819202122232425262728293031323334//二级缓存测试 @Test public void testCache2() throws Exception &#123; SqlSession sqlSession1 = sqlSessionFactory.openSession(); SqlSession sqlSession2 = sqlSessionFactory.openSession(); SqlSession sqlSession3 = sqlSessionFactory.openSession(); //创建UserMapper对象,mybatis自动生成代理对象 UserMapper userMapper1 = sqlSession1.getMapper(UserMapper.class); //sqlSession1 执行查询 写入缓存(第一次查询请求) User user1 = userMapper1.findUserById(1); System.out.println(user1); sqlSession1.close(); //sqlSession3 执行提交 清空缓存 UserMapper userMapper3 = sqlSession3.getMapper(UserMapper.class); User user3 = userMapper3.findUserById(1); user3.setSex(\"女\"); user3.setAddress(\"山东济南\"); user3.setUsername(\"崔建\"); userMapper3.updateUserById(user3); //提交事务，清空缓存 sqlSession3.commit(); sqlSession3.close(); //sqlSession2 执行查询(第二次查询请求) UserMapper userMapper2 = sqlSession2.getMapper(UserMapper.class); User user2 = userMapper2.findUserById(1); System.out.println(user2); sqlSession2.close(); &#125; 结果： useCache 配置 在 statement 中设置 useCache=false 可以禁用当前 select 语句的二级缓存，即每次查询都会发出sql去查询，默认情况是true，即该sql使用二级缓存。 1&lt;select id=\"findUserById\" parameterType=\"int\" resultType=\"user\" useCache=\"false\"&gt; 总结：针对每次查询都需要最新的数据sql，要设置成useCache=false，禁用二级缓存。 刷新缓存（清空缓存） 在mapper的同一个namespace中，如果有其它insert、update、delete操作数据后需要刷新缓存，如果不执行刷新缓存会出现脏读。 设置statement配置中的flushCache=”true” 属性，默认情况下为true即刷新缓存，如果改成false则不会刷新。使用缓存时如果手动修改数据库表中的查询数据会出现脏读。 如下： 1&lt;insert id=\"insetrUser\" parameterType=\"cn.zhisheng.mybatis.po.User\" flushCache=\"true\"&gt; 一般下执行完commit操作都需要刷新缓存，flushCache=true表示刷新缓存，这样可以避免数据库脏读。 Mybatis Cache参数flushInterval（刷新间隔）可以被设置为任意的正整数，而且它们代表一个合理的毫秒形式的时间段。默认情况是不设置，也就是没有刷新间隔，缓存仅仅调用语句时刷新。 size（引用数目）可以被设置为任意正整数，要记住你缓存的对象数目和你运行环境的可用内存资源数目。默认值是1024。 readOnly（只读）属性可以被设置为true或false。只读的缓存会给所有调用者返回缓存对象的相同实例。因此这些对象不能被修改。这提供了很重要的性能优势。可读写的缓存会返回缓存对象的拷贝（通过序列化）。这会慢一些，但是安全，因此默认是false。 如下例子： 1&lt;cache eviction=\"FIFO\" flushInterval=\"60000\" size=\"512\" readOnly=\"true\"/&gt; 这个更高级的配置创建了一个 FIFO 缓存,并每隔 60 秒刷新,存数结果对象或列表的 512 个引用,而且返回的对象被认为是只读的,因此在不同线程中的调用者之间修改它们会导致冲突。可用的收回策略有, 默认的是 LRU: LRU – 最近最少使用的:移除最长时间不被使用的对象。 FIFO – 先进先出:按对象进入缓存的顺序来移除它们。 SOFT – 软引用:移除基于垃圾回收器状态和软引用规则的对象。 WEAK – 弱引用:更积极地移除基于垃圾收集器状态和弱引用规则的对象。 Mybatis 整合 ehcacheehcache 是一个分布式缓存框架。 分布缓存 我们系统为了提高系统并发，性能、一般对系统进行分布式部署（集群部署方式） 不使用分布缓存，缓存的数据在各各服务单独存储，不方便系统 开发。所以要使用分布式缓存对缓存数据进行集中管理。 mybatis无法实现分布式缓存，需要和其它分布式缓存框架进行整合。 整合方法 mybatis 提供了一个二级缓存 cache 接口（org.apache.ibatis.cache 下的 Cache），如果要实现自己的缓存逻辑，实现cache接口开发即可。 12345678910import java.util.concurrent.locks.ReadWriteLock;public interface Cache &#123; String getId(); void putObject(Object var1, Object var2); Object getObject(Object var1); Object removeObject(Object var1); void clear(); int getSize(); ReadWriteLock getReadWriteLock();&#125; mybatis和ehcache整合，mybatis 和 ehcache 整合包中提供了一个 cache 接口的实现类(org.apache.ibatis.cache.impl 下的 PerpetualCache)。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package org.apache.ibatis.cache.impl;import java.util.HashMap;import java.util.Map;import java.util.concurrent.locks.ReadWriteLock;import org.apache.ibatis.cache.Cache;import org.apache.ibatis.cache.CacheException;public class PerpetualCache implements Cache &#123; private String id; private Map&lt;Object, Object&gt; cache = new HashMap(); public PerpetualCache(String id) &#123; this.id = id; &#125; public String getId() &#123; return this.id; &#125; public int getSize() &#123; return this.cache.size(); &#125; public void putObject(Object key, Object value) &#123; this.cache.put(key, value); &#125; public Object getObject(Object key) &#123; return this.cache.get(key); &#125; public Object removeObject(Object key) &#123; return this.cache.remove(key); &#125; public void clear() &#123; this.cache.clear(); &#125; public ReadWriteLock getReadWriteLock() &#123; return null; &#125; public boolean equals(Object o) &#123; if(this.getId() == null) &#123; throw new CacheException(\"Cache instances require an ID.\"); &#125; else if(this == o) &#123; return true; &#125; else if(!(o instanceof Cache)) &#123; return false; &#125; else &#123; Cache otherCache = (Cache)o; return this.getId().equals(otherCache.getId()); &#125; &#125; public int hashCode() &#123; if(this.getId() == null) &#123; throw new CacheException(\"Cache instances require an ID.\"); &#125; else &#123; return this.getId().hashCode(); &#125; &#125;&#125; 通过实现 Cache 接口可以实现 mybatis 缓存数据通过其它缓存数据库整合，mybatis 的特长是sql操作，缓存数据的管理不是 mybatis 的特长，为了提高缓存的性能将 mybatis 和第三方的缓存数据库整合，比如 ehcache、memcache、redis等。 引入依赖包 ehcache-core-2.6.5.jar 和 mybatis-ehcache-1.0.2.jar 引入缓存配置文件 classpath下添加：ehcache.xml 内容如下： 1234567891011121314&lt;ehcache xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:noNamespaceSchemaLocation=\"http://ehcache.org/ehcache.xsd\"&gt; &lt;diskStore path=\"C:\\JetBrains\\IDEAProject\\ehcache\" /&gt; &lt;defaultCache maxElementsInMemory=\"1000\" maxElementsOnDisk=\"10000000\" eternal=\"false\" overflowToDisk=\"false\" timeToIdleSeconds=\"120\" timeToLiveSeconds=\"120\" diskExpiryThreadIntervalSeconds=\"120\" memoryStoreEvictionPolicy=\"LRU\"&gt; &lt;/defaultCache&gt;&lt;/ehcache&gt; 属性说明： diskStore：指定数据在磁盘中的存储位置。 defaultCache：当借助 CacheManager.add(“demoCache”) 创建Cache时，EhCache 便会采用指定的的管理策略 以下属性是必须的： maxElementsInMemory - 在内存中缓存的element的最大数目 maxElementsOnDisk - 在磁盘上缓存的element的最大数目，若是0表示无穷大 eternal - 设定缓存的elements是否永远不过期。如果为true，则缓存的数据始终有效，如果为false那么还要根据timeToIdleSeconds，timeToLiveSeconds判断 overflowToDisk- 设定当内存缓存溢出的时候是否将过期的element缓存到磁盘上 以下属性是可选的： timeToIdleSeconds - 当缓存在EhCache中的数据前后两次访问的时间超过timeToIdleSeconds的属性取值时，这些数据便会删除，默认值是0,也就是可闲置时间无穷大 timeToLiveSeconds - 缓存element的有效生命期，默认是0.,也就是element存活时间无穷大 diskSpoolBufferSizeMB 这个参数设置DiskStore(磁盘缓存)的缓存区大小.默认是30MB.每个Cache都应该有自己的一个缓冲区. diskPersistent- 在VM重启的时候是否启用磁盘保存EhCache中的数据，默认是false。 diskExpiryThreadIntervalSeconds - 磁盘缓存的清理线程运行间隔，默认是120秒。每个120s，相应的线程会进行一次EhCache中数据的清理工作 memoryStoreEvictionPolicy - 当内存缓存达到最大，有新的element加入的时候， 移除缓存中element的策略。默认是LRU（最近最少使用），可选的有LFU（最不常使用）和FIFO（先进先出） 开启ehcache缓存 EhcacheCache 是ehcache对Cache接口的实现；修改mapper.xml文件，在cache中指定EhcacheCache。 根据需求调整缓存参数： 123456789&lt;cache type=\"org.mybatis.caches.ehcache.EhcacheCache\" &gt; &lt;property name=\"timeToIdleSeconds\" value=\"3600\"/&gt; &lt;property name=\"timeToLiveSeconds\" value=\"3600\"/&gt; &lt;!-- 同ehcache参数maxElementsInMemory --&gt; &lt;property name=\"maxEntriesLocalHeap\" value=\"1000\"/&gt; &lt;!-- 同ehcache参数maxElementsOnDisk --&gt; &lt;property name=\"maxEntriesLocalDisk\" value=\"10000000\"/&gt; &lt;property name=\"memoryStoreEvictionPolicy\" value=\"LRU\"/&gt; &lt;/cache&gt; 测试 ：(这命中率就代表成功将ehcache 与 mybatis 整合了) 应用场景对于访问多的查询请求且用户对查询结果实时性要求不高，此时可采用 mybatis 二级缓存技术降低数据库访问量，提高访问速度，业务场景比如：耗时较高的统计分析sql、电话账单查询sql等。 实现方法如下：通过设置刷新间隔时间，由 mybatis 每隔一段时间自动清空缓存，根据数据变化频率设置缓存刷新间隔 flushInterval，比如设置为30分钟、60分钟、24小时等，根据需求而定。 局限性mybatis 二级缓存对细粒度的数据级别的缓存实现不好，比如如下需求：对商品信息进行缓存，由于商品信息查询访问量大，但是要求用户每次都能查询最新的商品信息，此时如果使用 mybatis 的二级缓存就无法实现当一个商品变化时只刷新该商品的缓存信息而不刷新其它商品的信息，因为 mybaits 的二级缓存区域以 mapper 为单位划分，当一个商品信息变化会将所有商品信息的缓存数据全部清空。解决此类问题需要在业务层根据需求对数据有针对性缓存。","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"http://www.54tianzhisheng.cn/tags/Mybatis/"},{"name":"SpringMVC","slug":"SpringMVC","permalink":"http://www.54tianzhisheng.cn/tags/SpringMVC/"}]},{"title":"通过项目逐步深入了解Mybatis（三）","date":"2017-06-13T16:00:00.000Z","path":"2017/06/14/通过项目逐步深入了解Mybatis(三)/","text":"相关阅读：1、通过项目逐步深入了解Mybatis&lt;一&gt; 2、 通过项目逐步深入了解Mybatis&lt;二&gt; 本项目所有代码及文档都托管在 Github地址：https://github.com/zhisheng17/mybatis Mybatis 高级知识安排：对订单商品数据模型进行分析 订单商品数据模型 数据模型分析思路：1、每张表记录的数据内容（分模块对每张表记录的内容进行熟悉，相当于学习系统需求的过程） 2、每张表重要的的字段设置（非空字段、外键字段） 3、数据库级别表与表之间的关系（外键关系） 4、表与表业务之间的关系（要建立在每个业务意义的基础上去分析） 数据模型分析模型 用户表 user：记录购买商品的用户信息 订单表 order：记录用户所创建的订单(购买商品的订单) 订单明细表 orderdetail：（记录了订单的详细信息即购买商品的信息） 商品表 items：记录了商品信息 表与表业务之间的关系： 在分析表与表之间的业务关系时需要建立在某个业务意义基础上去分析。 先分析数据级别之间有关系的表之间的业务关系： 1、usre和orders： user —&gt; orders：一个用户可以创建多个订单，一对多 orders —&gt; user：一个订单只由一个用户创建，一对一 2、 orders和orderdetail： orders —&gt; orderdetail：一个订单可以包括 多个订单明细，因为一个订单可以购买多个商品，每个商品的购买信息在orderdetail记录，一对多关系 orderdetail —&gt; orders：一个订单明细只能包括在一个订单中，一对一 3、 orderdetail 和 itesm： orderdetail —&gt; itesms：一个订单明细只对应一个商品信息，一对一 items —&gt; orderdetail:一个商品可以包括在多个订单明细 ，一对多 再分析数据库级别没有关系的表之间是否有业务关系： 4、 orders 和 items： orders 和 items 之间可以通过 orderdetail 表建立 关系。 一对一查询需求：查询订单信息，关联查询创建订单的用户信息 使用 resultType sql 语句 确定查询的主表：订单表 确定查询的关联表：用户表 关联查询使用内链接？还是外链接？ 由于orders表中有一个外键（user_id），通过外键关联查询用户表只能查询出一条记录，可以使用内链接。 1SELECT orders.*, USER.username, USER.sex, USER.address FROM orders, USER WHERE orders.user_id = user.id 创建 pojo Orders.java 123456789101112public class Orders &#123; private Integer id; private Integer userId; private String number; private Date createtime; private String note; //用户信息 private User user; //订单明细 private List&lt;Orderdetail&gt; orderdetails; //getter and setter&#125; OrderCustom.java 1234567891011//通过此类映射订单和用户查询的结果，让此类继承包括 字段较多的pojo类public class OrdersCustom extends Orders&#123; //添加用户属性 /*USER.username, USER.sex, USER.address */ private String username; private String sex; private String address; //getter and setter&#125; 映射文件 OrdersMapperCustom.xml 1234&lt;!--查询订单关联查询用户信息--&gt; &lt;select id=\"findOrdersUser\" resultType=\"cn.zhisheng.mybatis.po.OrdersCustom\"&gt; SELECT orders.*, USER.username, USER.sex, USER.address FROM orders, USER WHERE orders.user_id = user.id &lt;/select&gt; Mapper 文件 OrdersMapperCustom.java 1234public interface OrdersMapperCustom&#123; public OrdersCustom findOrdersUser() throws Exception;&#125; 测试代码（记得在 SqlConfig.xml中添加载 OrdersMapperCustom.xml 文件） 1234567891011@Test public void testFindOrdersUser() throws Exception &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); //创建OrdersMapperCustom对象,mybatis自动生成代理对象 OrdersMapperCustom ordersMapperCustom = sqlSession.getMapper(OrdersMapperCustom.class); //调用OrdersMapperCustom的方法 List&lt;OrdersCustom&gt; list = ordersMapperCustom.findOrdersUser(); System.out.println(list); sqlSession.close(); &#125; 测试结果 ​ ​ 使用 resultMap sql 语句（和上面的一致） 使用 resultMap 映射思路 使用 resultMap 将查询结果中的订单信息映射到 Orders 对象中，在 orders 类中添加 User 属性，将关联查询出来的用户信息映射到 orders 对象中的 user 属性中。 12//用户信息private User user; 映射文件 OrdersMapperCustom.xml 先定义 resultMap 1234567891011121314151617181920212223242526272829303132&lt;!--定义查询订单关联查询用户信息的resultMap 将整个查询结果映射到cn.zhisheng.mybatis.po.Orders --&gt; &lt;resultMap id=\"OrdersUserResultMap\" type=\"cn.zhisheng.mybatis.po.Orders\"&gt; &lt;!--配置映射的订单信息--&gt; &lt;!--id表示查询结果中的唯一标识 在这里是订单的唯一标识 如果是由多列组成的唯一标识，那么就需要配置多个id column：id 是订单信息中的唯一标识列 property：id 是订单信息唯一标识列所映射到orders中的id属性 最终resultMap对column和property做一个映射关系（对应关系） --&gt; &lt;id column=\"id\" property=\"id\"/&gt; &lt;result column=\"user_id\" property=\"userId\"/&gt; &lt;result column=\"number\" property=\"number\"/&gt; &lt;result column=\"createtime\" property=\"createtime\"/&gt; &lt;result column=\"note\" property=\"note\"/&gt; &lt;!--配置映射的关联用户信息 association 用于映射关联查询单个对象的信息 property 将要关联查询的用户信息映射到 orders中的属性中去 --&gt; &lt;association property=\"user\" javaType=\"cn.zhisheng.mybatis.po.User\"&gt; &lt;!--id 关联用户信息的唯一标识 column: 指定唯一标识用户的信息 property：映射到user的那个属性 --&gt; &lt;id column=\"user_id\" property=\"id\"/&gt; &lt;result column=\"username\" property=\"username\"/&gt; &lt;result column=\"sex\" property=\"sex\"/&gt; &lt;result column=\"address\" property=\"address\"/&gt; &lt;result column=\"birthday\" property=\"birthday\"/&gt; &lt;/association&gt; &lt;/resultMap&gt; 1234&lt;!--查询订单关联查询用户信息, 使用 resultMap--&gt; &lt;select id=\"findOrdersUserResultMap\" resultMap=\"OrdersUserResultMap\"&gt; SELECT orders.*, USER.username, USER.sex, USER.address FROM orders, USER WHERE orders.user_id = user.id &lt;/select&gt; Mapper 文件 1public List&lt;Orders&gt; findOrdersUserResultMap() throws Exception; 测试代码 1234567891011@Test public void testFindOrdersUserResultMap() throws Exception &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); //创建OrdersMapperCustom对象,mybatis自动生成代理对象 OrdersMapperCustom ordersMapperCustom = sqlSession.getMapper(OrdersMapperCustom.class); //调用OrdersMapperCustom的方法 List&lt;Orders&gt; list = ordersMapperCustom.findOrdersUserResultMap(); System.out.println(list); sqlSession.close(); &#125; 测试结果 使用 resultType 和 resultMap 一对一查询小结 resultType：使用resultType实现较为简单，如果pojo中没有包括查询出来的列名，需要增加列名对应的属性，即可完成映射。如果没有查询结果的特殊要求建议使用resultType。 resultMap：需要单独定义resultMap，实现有点麻烦，如果对查询结果有特殊的要求，使用resultMap可以完成将关联查询映射pojo的属性中。resultMap可以实现延迟加载，resultType无法实现延迟加载。 一对多查询需求：查询订单及订单明细信息 SQL语句： 确定主查询表：订单表 确定关联查询表：订单明细表 在一对一查询基础上添加订单明细表关联即可。 12SELECT orders.*, USER.username, USER.sex, USER.address, orderdetail.id orderdetail_id, orderdetail.items_id, orderdetail.items_num, orderdetail.orders_id FROM orders, USER,orderdetail WHERE orders.user_id = user.id AND orderdetail.orders_id=orders.id 分析： 使用 resultType 将上边的查询结果映射到 pojo 中，订单信息的就是重复。 要求： 对 orders 映射不能出现重复记录。 在 orders.java 类中添加 List orderDetails 属性。 最终会将订单信息映射到 orders 中，订单所对应的订单明细映射到 orders 中的 orderDetails 属性中。 映射成的 orders 记录数为两条（orders信息不重复） 每个 orders 中的 orderDetails 属性存储了该订单所对应的订单明细。 映射文件： 首先定义 resultMap 1234567891011121314151617181920&lt;!--定义查询订单及订单明细信息的resultMap使用extends继承，不用在中配置订单信息和用户信息的映射--&gt; &lt;resultMap id=\"OrdersAndOrderDetailResultMap\" type=\"cn.zhisheng.mybatis.po.Orders\" extends=\"OrdersUserResultMap\"&gt; &lt;!-- 订单信息 --&gt; &lt;!-- 用户信息 --&gt; &lt;!-- 使用extends继承，不用在中配置订单信息和用户信息的映射 --&gt; &lt;!-- 订单明细信息 一个订单关联查询出了多条明细，要使用collection进行映射 collection：对关联查询到多条记录映射到集合对象中 property：将关联查询到多条记录映射到cn.zhisheng.mybatis.po.Orders哪个属性 ofType：指定映射到list集合属性中pojo的类型 --&gt; &lt;collection property=\"orderdetails\" ofType=\"cn.zhisheng.mybatis.po.Orderdetail\"&gt; &lt;!-- id：订单明细唯 一标识 property:要将订单明细的唯 一标识 映射到cn.zhisheng.mybatis.po.Orderdetail的哪个属性--&gt; &lt;id column=\"orderdetail_id\" property=\"id\"/&gt; &lt;result column=\"items_id\" property=\"itemsId\"/&gt; &lt;result column=\"items_num\" property=\"itemsNum\"/&gt; &lt;result column=\"orders_id\" property=\"ordersId\"/&gt; &lt;/collection&gt; &lt;/resultMap&gt; 12345&lt;!--查询订单及订单明细信息, 使用 resultMap--&gt; &lt;select id=\"findOrdersAndOrderDetailResultMap\" resultMap=\"OrdersAndOrderDetailResultMap\"&gt; SELECT orders.*, USER.username, USER.sex, USER.address, orderdetail.id orderdetail_id, orderdetail.items_id, orderdetail.items_num, orderdetail.orders_id FROM orders, USER,orderdetail WHERE orders.user_id = user.id AND orderdetail.orders_id=orders.id &lt;/select&gt; Mapper 文件 1public List&lt;Orders&gt; findOrdersAndOrderDetailResultMap() throws Exception; 测试文件 1234567891011@Test public void testFindOrdersAndOrderDetailResultMap() throws Exception &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); //创建OrdersMapperCustom对象,mybatis自动生成代理对象 OrdersMapperCustom ordersMapperCustom = sqlSession.getMapper(OrdersMapperCustom.class); //调用OrdersMapperCustom的方法 List&lt;Orders&gt; list = ordersMapperCustom.findOrdersAndOrderDetailResultMap(); System.out.println(list); sqlSession.close(); &#125; 测试结果 总结：mybatis使用resultMap的collection对关联查询的多条记录映射到一个list集合属性中。 使用resultType实现：将订单明细映射到orders中的orderdetails中，需要自己处理，使用双重循环遍历，去掉重复记录，将订单明细放在orderdetails中。 多对多查询需求：查询用户及用户购买商品信息。 SQL语句： 查询主表是：用户表 关联表：由于用户和商品没有直接关联，通过订单和订单明细进行关联，所以关联表： orders、orderdetail、items 123SELECT orders.*, USER.username, USER.sex, USER.address, orderdetail.id orderdetail_id,orderdetail.items_id, orderdetail.items_num, orderdetail.orders_id, items.name items_name,items.detail items_detail, items.price items_price FROM orders, USER, orderdetail, items WHERE orders.user_id = user.id AND orderdetail.orders_id=orders.id AND orderdetail.items_id = items.id 映射思路： 将用户信息映射到 user 中。在 user 类中添加订单列表属性List orderslist，将用户创建的订单映射到orderslist在Orders中添加订单明细列表属性Listorderdetials，将订单的明细映射到orderdetials在OrderDetail中添加Items属性，将订单明细所对应的商品映射到Items 定义 resultMap：12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;!--定义查询用户及用户购买商品信息的 resultMap--&gt; &lt;resultMap id=\"UserAndItemsResultMap\" type=\"cn.zhisheng.mybatis.po.User\"&gt; &lt;!--用户信息--&gt; &lt;id column=\"user_id\" property=\"id\"/&gt; &lt;result column=\"username\" property=\"username\"/&gt; &lt;result column=\"sex\" property=\"sex\"/&gt; &lt;result column=\"birthday\" property=\"birthday\"/&gt; &lt;result column=\"address\" property=\"address\"/&gt; &lt;!--订单信息 一个用户对应多个订单，使用collection映射 --&gt; &lt;collection property=\"ordersList\" ofType=\"cn.zhisheng.mybatis.po.Orders\"&gt; &lt;id column=\"id\" property=\"id\"/&gt; &lt;result column=\"user_id\" property=\"userId\"/&gt; &lt;result column=\"number\" property=\"number\"/&gt; &lt;result column=\"createtime\" property=\"createtime\"/&gt; &lt;result column=\"note\" property=\"note\"/&gt; &lt;!-- 订单明细 一个订单包括 多个明细 --&gt; &lt;collection property=\"orderdetails\" ofType=\"cn.zhisheng.mybatis.po.Orderdetail\"&gt; &lt;id column=\"orderdetail_id\" property=\"id\"/&gt; &lt;result column=\"orders_id\" property=\"ordersId\"/&gt; &lt;result column=\"items_id\" property=\"itemsId\"/&gt; &lt;result column=\"items_num\" property=\"itemsNum\"/&gt; &lt;!-- 商品信息 一个订单明细对应一个商品 --&gt; &lt;association property=\"items\" javaType=\"cn.zhisheng.mybatis.po.Items\"&gt; &lt;id column=\"items_id\" property=\"id\"/&gt; &lt;result column=\"items_name\" property=\"name\"/&gt; &lt;result column=\"items_price\" property=\"price\"/&gt; &lt;result column=\"items_pic\" property=\"pic\"/&gt; &lt;result column=\"items_createtime\" property=\"createtime\"/&gt; &lt;result column=\"items_detail\" property=\"detail\"/&gt; &lt;/association&gt; &lt;/collection&gt; &lt;/collection&gt; &lt;/resultMap&gt; 映射文件12345&lt;!--查询用户及用户购买商品信息, 使用 resultMap--&gt; &lt;select id=\"findUserAndItemsResultMap\" resultMap=\"UserAndItemsResultMap\"&gt; SELECT orders.*, USER.username, USER.sex, USER.address, orderdetail.id orderdetail_id, orderdetail.items_id, orderdetail.items_num, orderdetail.orders_id FROM orders, USER,orderdetail WHERE orders.user_id = user.id AND orderdetail.orders_id=orders.id &lt;/select&gt; Mapper 文件1public List&lt;User&gt; findUserAndItemsResultMap() throws Exception; 测试文件1234567891011@Test public void testFindUserAndItemsResultMap() throws Exception &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); //创建OrdersMapperCustom对象,mybatis自动生成代理对象 OrdersMapperCustom ordersMapperCustom = sqlSession.getMapper(OrdersMapperCustom.class); //调用OrdersMapperCustom的方法 List&lt;User&gt; list = ordersMapperCustom.findUserAndItemsResultMap(); System.out.println(list); sqlSession.close(); &#125; 测试： 我去，竟然报错了，但是不要怕，通过查看报错信息可以知道我忘记在 User.java 中加入 orderlist 属性了，接下来我加上去，并加上 getter 和 setter 方法。 12345678//用户创建的订单列表 private List&lt;Orders&gt; ordersList; public List&lt;Orders&gt; getOrdersList() &#123; return ordersList; &#125; public void setOrdersList(List&lt;Orders&gt; ordersList) &#123; this.ordersList = ordersList; &#125; 再次测试就能成功了。 多对多查询总结将查询用户购买的商品信息明细清单，（用户名、用户地址、购买商品名称、购买商品时间、购买商品数量） 针对上边的需求就使用resultType将查询到的记录映射到一个扩展的pojo中，很简单实现明细清单的功能。 一对多是多对多的特例，如下需求： 查询用户购买的商品信息，用户和商品的关系是多对多关系。 需求1： 查询字段：用户账号、用户名称、用户性别、商品名称、商品价格(最常见) 企业开发中常见明细列表，用户购买商品明细列表， 使用resultType将上边查询列映射到pojo输出。 需求2： 查询字段：用户账号、用户名称、购买商品数量、商品明细（鼠标移上显示明细） 使用resultMap将用户购买的商品明细列表映射到user对象中。 总结： 使用resultMap是针对那些对查询结果映射有特殊要求的功能，，比如特殊要求映射成list中包括多个list。 ResultMap 总结resultType：作用： 将查询结果按照sql列名pojo属性名一致性映射到pojo中。 场合： 常见一些明细记录的展示，比如用户购买商品明细，将关联查询信息全部展示在页面时，此时可直接使用resultType将每一条记录映射到pojo中，在前端页面遍历list（list中是pojo）即可。 resultMap： 使用association和collection完成一对一和一对多高级映射（对结果有特殊的映射要求）。 association：作用： 将关联查询信息映射到一个pojo对象中。 场合： 为了方便查询关联信息可以使用association将关联订单信息映射为用户对象的pojo属性中，比如：查询订单及关联用户信息。使用resultType无法将查询结果映射到pojo对象的pojo属性中，根据对结果集查询遍历的需要选择使用resultType还是resultMap。 collection：作用： 将关联查询信息映射到一个list集合中。 场合： 为了方便查询遍历关联信息可以使用collection将关联信息映射到list集合中，比如：查询用户权限范围模块及模块下的菜单，可使用collection将模块映射到模块list中，将菜单列表映射到模块对象的菜单list属性中，这样的作的目的也是方便对查询结果集进行遍历查询。如果使用resultType无法将查询结果映射到list集合中。","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"http://www.54tianzhisheng.cn/tags/Mybatis/"},{"name":"SpringMVC","slug":"SpringMVC","permalink":"http://www.54tianzhisheng.cn/tags/SpringMVC/"}]},{"title":"Hexo + yilia 主题实现文章目录","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/Hexo-yilia-toc/","text":"前提为了方便查看每篇文章的目录结构，可以定位到想看的地方，特地找了下如何实现这个功能。 添加 CSS 样式打开 themes\\yilia\\source 下的 main.234bc0.css 文件，直接在后面添加如下代码：123456789/* 新添加的 */#container .show-toc-btn,#container .toc-article&#123;display:block&#125;.toc-article&#123;z-index:100;background:#fff;border:1px solid #ccc;max-width:250px;min-width:150px;max-height:500px;overflow-y:auto;-webkit-box-shadow:5px 5px 2px #ccc;box-shadow:5px 5px 2px #ccc;font-size:12px;padding:10px;position:fixed;right:35px;top:129px&#125;.toc-article .toc-close&#123;font-weight:700;font-size:20px;cursor:pointer;float:right;color:#ccc&#125;.toc-article .toc-close:hover&#123;color:#000&#125;.toc-article .toc&#123;font-size:12px;padding:0;line-height:20px&#125;.toc-article .toc .toc-number&#123;color:#333&#125;.toc-article .toc .toc-text:hover&#123;text-decoration:underline;color:#2a6496&#125;.toc-article li&#123;list-style-type:none&#125;.toc-article .toc-level-1&#123;margin:4px 0&#125;.toc-article .toc-child&#123;&#125;@-moz-keyframes cd-bounce-1&#123;0%&#123;opacity:0;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;60%&#123;opacity:1;-o-transform:scale(1.01);-webkit-transform:scale(1.01);-moz-transform:scale(1.01);-ms-transform:scale(1.01);transform:scale(1.01)&#125;100%&#123;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;&#125;@-webkit-keyframes cd-bounce-1&#123;0%&#123;opacity:0;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;60%&#123;opacity:1;-o-transform:scale(1.01);-webkit-transform:scale(1.01);-moz-transform:scale(1.01);-ms-transform:scale(1.01);transform:scale(1.01)&#125;100%&#123;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;&#125;@-o-keyframes cd-bounce-1&#123;0%&#123;opacity:0;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;60%&#123;opacity:1;-o-transform:scale(1.01);-webkit-transform:scale(1.01);-moz-transform:scale(1.01);-ms-transform:scale(1.01);transform:scale(1.01)&#125;100%&#123;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;&#125;@keyframes cd-bounce-1&#123;0%&#123;opacity:0;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;60%&#123;opacity:1;-o-transform:scale(1.01);-webkit-transform:scale(1.01);-moz-transform:scale(1.01);-ms-transform:scale(1.01);transform:scale(1.01)&#125;100%&#123;-o-transform:scale(1);-webkit-transform:scale(1);-moz-transform:scale(1);-ms-transform:scale(1);transform:scale(1)&#125;&#125;.show-toc-btn&#123;display:none;z-index:10;width:30px;min-height:14px;overflow:hidden;padding:4px 6px 8px 5px;border:1px solid #ddd;border-right:none;position:fixed;right:40px;text-align:center;background-color:#f9f9f9&#125;.show-toc-btn .btn-bg&#123;margin-top:2px;display:block;width:16px;height:14px;background:url(http://7xtawy.com1.z0.glb.clouddn.com/show.png) no-repeat;-webkit-background-size:100%;-moz-background-size:100%;background-size:100%&#125;.show-toc-btn .btn-text&#123;color:#999;font-size:12px&#125;.show-toc-btn:hover&#123;cursor:pointer&#125;.show-toc-btn:hover .btn-bg&#123;background-position:0 -16px&#125;.show-toc-btn:hover .btn-text&#123;font-size:12px;color:#ea8010&#125;.toc-article li ol, .toc-article li ul &#123; margin-left: 30px;&#125;.toc-article ol, .toc-article ul &#123; margin: 10px 0;&#125; 修改 article.ejs 文件打开 themes\\yilia\\layout\\_partial 文件夹下的 article.ejs 文件, 在 &lt;/header&gt; &lt;% } %&gt; 下面加入如下内容（注意位置） 123456789101112131415161718192021222324252627&lt;!-- 目录内容 --&gt;&lt;% if (!index &amp;&amp; post.toc)&#123; %&gt; &lt;p class=&quot;show-toc-btn&quot; id=&quot;show-toc-btn&quot; onclick=&quot;showToc();&quot; style=&quot;display:none&quot;&gt; &lt;span class=&quot;btn-bg&quot;&gt;&lt;/span&gt; &lt;span class=&quot;btn-text&quot;&gt;文章导航&lt;/span&gt; &lt;/p&gt; &lt;div id=&quot;toc-article&quot; class=&quot;toc-article&quot;&gt; &lt;span id=&quot;toc-close&quot; class=&quot;toc-close&quot; title=&quot;隐藏导航&quot; onclick=&quot;showBtn();&quot;&gt;×&lt;/span&gt; &lt;strong class=&quot;toc-title&quot;&gt;文章目录&lt;/strong&gt; &lt;%- toc(post.content) %&gt; &lt;/div&gt; &lt;script type=&quot;text/javascript&quot;&gt; function showToc()&#123; var toc_article = document.getElementById(&quot;toc-article&quot;); var show_toc_btn = document.getElementById(&quot;show-toc-btn&quot;); toc_article.setAttribute(&quot;style&quot;,&quot;display:block&quot;); show_toc_btn.setAttribute(&quot;style&quot;,&quot;display:none&quot;); &#125;; function showBtn()&#123; var toc_article = document.getElementById(&quot;toc-article&quot;); var show_toc_btn = document.getElementById(&quot;show-toc-btn&quot;); toc_article.setAttribute(&quot;style&quot;,&quot;display:none&quot;); show_toc_btn.setAttribute(&quot;style&quot;,&quot;display:block&quot;); &#125;; &lt;/script&gt; &lt;% &#125; %&gt;&lt;!-- 目录内容结束 --&gt; 然后若想要文章显示目录，在每篇文章开头加入：toc: true 即可。 参考文章：Hexo+yilia主题实现文章目录和添加视频 新增由于问题太多了，所以新写了篇文章：Github page + Hexo + yilia 搭建博客可能会遇到的所有疑问","tags":[{"name":"hexo","slug":"hexo","permalink":"http://www.54tianzhisheng.cn/tags/hexo/"}]},{"title":"Java连接Oracle数据库的三种连接方式","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/Java连接Oracle数据库的三种连接方式/","text":"背景：这两天在学习Oracle数据库，这里就总结下自己上课所学的知识，同时记录下来，方便整理当天所学下的知识，也同时方便日后自己查询。 SQL语句的话，这里我就不多讲了，感觉和其他的数据库（MySQL、SQL Server）都是类似，区别不大。 今天在这里就写下 Java 连接 Oracle 数据库的三种连接方式。 工具： Oracle Database 10g Express Edition cmd命令窗口 IDEA 2016.1.3 ojdbc6_g.jar（数据库驱动包） jdk 1.8 创建数据库表：首先在本地写好创建的数据库表的创建代码后，然后粘贴在cmd命令窗口下，即可创建成功。（前提是进入安装好了oracle，进入了用户，然后在当前用户下创建这个表） 部门表：tb1_dept （含有id name city三个属性） 12345create table tb1_dept( id number(5) primary key, name varchar2(10) not null, city varchar2(10) not null); 插入数据：然后同样写好插入数据的sql语句，这里我就写三条数据。 123insert into tb1_dept(id, name, city) values(1,&apos;java&apos;, &apos;南昌&apos;);insert into tb1_dept(id, name, city) values(2,&apos;c&apos;, &apos;上海&apos;);insert into tb1_dept(id, name, city) values(3,&apos;java&apos;, &apos;南昌&apos;); 好，数据库表已经创建好了，接下来我们需要准备的是数据库驱动包。 这里我用的是 ojdbc6_g.jar 驱动包。 接下来先了解一些基础知识： JDBC的六大步骤：这里我们就按照jdbc的这六大步骤执行下去： 注册驱动 获取连接 获取执行sql语句对象 执行sql语句 处理结果集 关闭资源 URL：统一资源定位器 oracle URL： jdbc:oracle:thin:@localhost:1521:XE jdbc:oracle:thin:@127.0.0.1:1521:XE MySQL URL：jdbc:mysql://localhost:3306/数据库名称 thin：小型驱动，驱动方式 @localhost 本机ip地址 127.0.0.1 XE：数据库的名字 ipconfig：ip地址查询 URI：统一资源标识符 URN：用特定命名空间的名字标识资源 如果你不知道 URL、 URI、URN三者的区别的话，那么你可以参考下面我推荐的一篇文章。 你知道URL、URI和URN三者之间的区别吗？ 三种连接方式代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586package cn.zhisheng.test.jdbc;import oracle.jdbc.driver.OracleDriver;import java.sql.*;import java.util.Properties;/** * Created by 10412 on 2016/12/27. * JDBC的六大步骤 * JAVA连接Oracle的三种方式 */public class JdbcTest&#123; public static void main(String[] args) &#123; Connection connect = null; Statement statement = null; ResultSet resultSet = null; try &#123; //第一步：注册驱动 //第一种方式：类加载(常用) //Class.forName(\"oracle.jdbc.OracleDriver\"); //第二种方式：利用Driver对象 Driver driver = new OracleDriver(); DriverManager.deregisterDriver(driver); //第三种方式:利用系统参数 需在idea中配置program arguments为下面的参数 //-Djdbc.drivers = oracle.jdbc.OracleDriver //第二步：获取连接 //第一种方式：利用DriverManager（常用） //connect = DriverManager.getConnection(\"jdbc:oracle:thin:@localhost:1521:XE\", \"你的oracle数据库用户名\", \"用户名密码\"); //第二种方式：直接使用Driver Properties pro = new Properties(); pro.put(\"user\", \"你的oracle数据库用户名\"); pro.put(\"password\", \"用户名密码\"); connect = driver.connect(\"jdbc:oracle:thin:@localhost:1521:XE\", pro); //测试connect正确与否 System.out.println(connect); //第三步：获取执行sql语句对象 //第一种方式:statement //statement = connect.createStatement(); //第二种方式：PreStatement PreparedStatement preState = connect.prepareStatement(\"select * from tb1_dept where id = ?\"); //第四步：执行sql语句 //第一种方式： //resultSet = statement.executeQuery(\"select * from tb1_dept\"); //第二种方式： preState.setInt(1, 2);//1是指sql语句中第一个？, 2是指第一个？的values值 //resultSet = preState.executeQuery(); //执行查询语句 //查询任何语句，如果有结果集，返回true，没有的话返回false,注意如果是插入一条数据的话，虽然是没有结果集，返回false，但是却能成功的插入一条数据 boolean execute = preState.execute(); System.out.println(execute); //第五步：处理结果集 while (resultSet.next()) &#123; int id = resultSet.getInt(\"id\"); String name = resultSet.getString(\"name\"); String city = resultSet.getString(\"city\"); System.out.println(id+\" \"+name+\" \"+city); //打印输出结果集 &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally &#123; //第六步：关闭资源 try &#123; if (resultSet!=null) resultSet.close(); if (statement!=null) statement.close(); if (connect!=null) connect.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 注解：1、 第一步：注册驱动 中的第三种方法 利用系统参数 需在idea中配置program arguments为下面的参数 这里我说一下怎么在IDEA中的配置方式吧 运行截图： OK ! 下篇文章将写 JDBC 的封装。","tags":[{"name":"数据库","slug":"数据库","permalink":"http://www.54tianzhisheng.cn/tags/数据库/"},{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"Oracle","slug":"Oracle","permalink":"http://www.54tianzhisheng.cn/tags/Oracle/"}]},{"title":"MyBatis的foreach语句详解","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/MyBatis-foreach/","text":"foreach 的主要用在构建in条件中，它可以在SQL语句中进行迭代一个集合。 foreach 元素的属性主要有 item，index，collection，open，separator，close。 item 表示集合中每一个元素进行迭代时的别名， index 指 定一个名字，用于表示在迭代过程中，每次迭代到的位置， open 表示该语句以什么开始， separator 表示在每次进行迭代之间以什么符号作为分隔 符， close 表示以什么结束。 在使用 foreach 的时候最关键的也是最容易出错的就是 collection 属性，该属性是必须指定的，但是在不同情况 下，该属性的值是不一样的，主要有一下3种情况： 如果传入的是单参数且参数类型是一个List的时候，collection 属性值为 list 如果传入的是单参数且参数类型是一个 array 数组的时候，collection 的属性值为 array 如果传入的参数是多个的时候，我们就需要把它们封装成一个 Map 了，当然单参数也可以封装成map，实际上如果你在传入参数的时候，在 breast 里面也是会把它封装成一个 Map 的，map 的 key 就是参数名，所以这个时候 collection 属性值就是传入的 List 或 array 对象在自己封装的 map 里面的 key 。 下面分别来看看上述三种情况的示例代码： 1.单参数 List 的类型： 123456&lt;select id=\"dynamicForeachTest\" resultType=\"Blog\"&gt; select * from t_blog where id in &lt;foreach collection=\"list\" index=\"index\" item=\"item\" open=\"(\" separator=\",\" close=\")\"&gt; #&#123;item&#125; &lt;/foreach&gt;&lt;/select&gt; 上述 collection 的值为list，对应的 Mapper 是这样的 1public List&lt;Blog&gt; dynamicForeachTest(List&lt;Integer&gt; ids); 测试代码： 12345678910111213@Test public void dynamicForeachTest() &#123; SqlSession session = Util.getSqlSessionFactory().openSession(); BlogMapper blogMapper = session.getMapper(BlogMapper.class); List&lt;Integer&gt; ids = new ArrayList&lt;Integer&gt;(); ids.add(1); ids.add(3); ids.add(6); List&lt;Blog&gt; blogs = blogMapper.dynamicForeachTest(ids); for (Blog blog : blogs) System.out.println(blog); session.close(); &#125; 2.单参数array数组的类型： 123456&lt;select id=\"dynamicForeach2Test\" resultType=\"Blog\"&gt; select * from t_blog where id in &lt;foreach collection=\"array\" index=\"index\" item=\"item\" open=\"(\" separator=\",\" close=\")\"&gt; #&#123;item&#125; &lt;/foreach&gt;&lt;/select&gt; 上述collection为array，对应的Mapper代码： 1public List&lt;Blog&gt; dynamicForeach2Test(int[] ids); 对应的测试代码： 12345678910@Test public void dynamicForeach2Test() &#123; SqlSession session = Util.getSqlSessionFactory().openSession(); BlogMapper blogMapper = session.getMapper(BlogMapper.class); int[] ids = new int[] &#123;1,3,6,9&#125;; List&lt;Blog&gt; blogs = blogMapper.dynamicForeach2Test(ids); for (Blog blog : blogs) System.out.println(blog); session.close(); &#125; 3.自己把参数封装成Map的类型 123456&lt;select id=\"dynamicForeach3Test\" resultType=\"Blog\"&gt; select * from t_blog where title like \"%\"#&#123;title&#125;\"%\" and id in &lt;foreach collection=\"ids\" index=\"index\" item=\"item\" open=\"(\" separator=\",\" close=\")\"&gt; #&#123;item&#125; &lt;/foreach&gt;&lt;/select&gt; 上述collection的值为ids，是传入的参数Map的key，对应的Mapper代码： 1public List&lt;Blog&gt; dynamicForeach3Test(Map&lt;String, Object&gt; params); 对应测试代码： 12345678910111213141516171819@Test public void dynamicForeach3Test() &#123; SqlSession session = Util.getSqlSessionFactory().openSession(); BlogMapper blogMapper = session.getMapper(BlogMapper.class); final List&lt;Integer&gt; ids = new ArrayList&lt;Integer&gt;(); ids.add(1); ids.add(2); ids.add(3); ids.add(6); ids.add(7); ids.add(9); Map&lt;String, Object&gt; params = new HashMap&lt;String, Object&gt;(); params.put(\"ids\", ids); params.put(\"title\", \"中国\"); List&lt;Blog&gt; blogs = blogMapper.dynamicForeach3Test(params); for (Blog blog : blogs) System.out.println(blog); session.close(); &#125;","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"http://www.54tianzhisheng.cn/tags/Mybatis/"},{"name":"foreach","slug":"foreach","permalink":"http://www.54tianzhisheng.cn/tags/foreach/"}]},{"title":"Spring MVC系列文章（一）：Spring MVC + Hibernate JPA + Bootstrap 搭建的博客系统 Demo","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/Spring MVC + Hibernate JPA + Bootstrap 搭建的博客系统/","text":"SpringBoot 系列文章 相关阅读：1、Spring MVC+Hibernate JPA+ Bootstrap 搭建的博客系统项目中所遇到的坑 由于整个系统不是很难，这里就不详细介绍了，我相信看源码的话，应该能够看得懂。 源码地址：https://github.com/zhisheng17/springmvc数据库：springdemo.sql 下面给出下整个系统的截图吧，觉得不错，可以给个 star ，哈哈！后续继续在这个项目中加入新的项目。 截图：首页 用户管理模块 用户列表 添加用户 用户信息详情 更新用户信息 删除用户 博客管理模块 博客列表 博客详情 添加博客 更新博客 删除博客","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"Spring","slug":"Spring","permalink":"http://www.54tianzhisheng.cn/tags/Spring/"},{"name":"Spring MVC","slug":"Spring-MVC","permalink":"http://www.54tianzhisheng.cn/tags/Spring-MVC/"},{"name":"Hibernate JPA","slug":"Hibernate-JPA","permalink":"http://www.54tianzhisheng.cn/tags/Hibernate-JPA/"},{"name":"Bootstrap","slug":"Bootstrap","permalink":"http://www.54tianzhisheng.cn/tags/Bootstrap/"},{"name":"MySQL","slug":"MySQL","permalink":"http://www.54tianzhisheng.cn/tags/MySQL/"}]},{"title":"关于String s = new String(\"xyz\"); 创建几个对象的问题","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/String-new/","text":"你知道在 java 中除了 8 种基本类型外，其他的都是类对象以及其引用。所以 &quot;xyz &quot;在 java 中它是一个String 对象.对于 string 类对象来说他的对象值是不能修改的，也就是具有不变性。 看：1234String s= &quot;hello &quot;; s= &quot;Java &quot;; String s1= &quot;hello &quot;; String s2=new String( &quot;hello &quot;); 结果如下图： 啊，s 所引用的 string 对象不是被修改了吗？之前所说的不变性，去那里了啊？你别着急，让我告诉你说发生了什么事情：在 jvm 的工作过程中，会创建一片的内存空间专门存入 string 对象。我们把这片内存空间叫做 string 池。 String s = “hello “; 当 jvm 看到 “hello”，在 string 池创建 string 对象存储它，并将他的引用返回给s。s = “java “，当 jvm 看到 “java “，在 string 池创建新的 string 对象存储它，再把新建的 string 对象的引用返回给s。而原先的 “hello”仍然在 string 池内。没有消失，他是不能被修改的。 所以我们仅仅是改变了s的引用，而没有改变他所引用的对象，因为 string 对象的值是不能被修改的。 String s1 = “hello” ; jvm 首先在 string 池内里面看找不找得到字符串 “hello”, 如果找得到,返回他的引用给 s1，否则的话就会创建新的 string 对象，放到 string 池里。这里由于 s = “hello”了,对象已经被引用，所以依据规则 s 和 s1 都是引用同一个对象。所以 s == s1 将返回 true。( == 对于非基本类型，是比较两引用是否引用内存中的同一个对象，这里先不区分 == 和 equals 的区别 ) String s2 = new String( “hello”); jvm 首先在 string 池内里面看找不找得到字符串 “hello”, 如果找得到, 不做任何事情，否则的话就会创建新的 string 对象，放到 string 池里面。由于遇到了 new，还会在内存上（不是 string 池里面）创建 string 对象存储 “hello”，并将内存上的（不是 string 池内的）string 对象返回给 s2。所以 s == s2 将返回 false，不是引用同一个对象。 好现在我们看题目：String s = new String( “xyz “);首先在 string 池内找，找到？不创建 string 对象，否则创建一个对象，遇到 new 运算符号了，在内存上创建 string 对象，并将其返回给 s，又一个对象 所以总共是 1个 或者 2个对象 。 而为什么在网上都说 String s=new String(“hello”); 创建了2个对象？那可能因为它就写这么一句代码，误让人默认的认为执行代码之前并不实例任何一个 String 对象过（也许 很多人不会这么想，），跟着别人或者不经思考的就说2个，斟是说存放在栈内存中专门存放 String 对象引用的 s 变量是一个对象！","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"String","slug":"String","permalink":"http://www.54tianzhisheng.cn/tags/String/"}]},{"title":"【字符串】判断两字符串是否互为旋转词？","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/【字符串】判断两字符串是否互为旋转词？/","text":"相关阅读：字符串逆序问题的解决方法题目： 如果对于一个字符串A，将A的前面任意一部分挪到后边去形成的字符串称为A的旋转词。 比如A=”12345”,A的旋转词有”12345”,”23451”,”34512”,”45123”和”51234”。 对于两个字符串A和B，请判断A和B是否互为旋转词。 给定两个字符串A和B及他们的长度lena，lenb，请返回一个bool值，代表他们是否互为旋转词。 测试样例： “cdab”,4,”abcd”,4 返回：true 通过代码： 1234567891011121314import java.util.*;public class Rotation&#123; public static boolean chkRotation(String A, int lena, String B, int lenb) &#123; // write code here if (lena != lenb)&#123; return false; &#125;else &#123; String str = A + A; return str.contains(B); &#125; &#125;&#125; 也可以使用 indexOf。 其区别是： contains 是找指定字符串是否包含一个字符串，返回值的 boolean 类型，即只有 true 和 false indexOf 有多个重载，但无论哪个，都是做一定的匹配，然后把匹配的第一个字符的位置返回，返回的是 int 类型，如果没找到，那么返回 -1 稍微再深究一下的我看了下 contains 的源码，结果发现他调用的是 indexOf 方法。 源码如下： 1234567891011/** * Returns true if and only if this string contains the specified * sequence of char values. * * @param s the sequence to search for * @return true if this string contains &#123;@code s&#125;, false otherwise * @since 1.5 */ public boolean contains(CharSequence s) &#123; return indexOf(s.toString()) &gt; -1; &#125; 意思就是如上面的区别所说的，他只有两个返回值 true 和 false。 于是我们继续看一下 indexOf 方法的源码： 1234567891011121314151617/** * Returns the index within this string of the first occurrence of the * specified substring. * * &lt;p&gt;The returned index is the smallest value &lt;i&gt;k&lt;/i&gt; for which: * &lt;blockquote&gt;&lt;pre&gt; * this.startsWith(str, &lt;i&gt;k&lt;/i&gt;) * &lt;/pre&gt;&lt;/blockquote&gt; * If no such value of &lt;i&gt;k&lt;/i&gt; exists, then &#123;@code -1&#125; is returned. * * @param str the substring to search for. * @return the index of the first occurrence of the specified substring, * or &#123;@code -1&#125; if there is no such occurrence. public int indexOf(String str) &#123; return indexOf(str, 0); &#125; 继续可以发现他又调用了 indexOf 的两个参数方法，只不过索引是 0 。 然后我继续看带有两个参数的 indexOf 方法源码如下： 123456789101112131415161718192021/** * Returns the index within this string of the first occurrence of the * specified substring, starting at the specified index. * * &lt;p&gt;The returned index is the smallest value &lt;i&gt;k&lt;/i&gt; for which: * &lt;blockquote&gt;&lt;pre&gt; * &lt;i&gt;k&lt;/i&gt; &amp;gt;= fromIndex &#123;@code &amp;&amp;&#125; this.startsWith(str, &lt;i&gt;k&lt;/i&gt;) * &lt;/pre&gt;&lt;/blockquote&gt; * If no such value of &lt;i&gt;k&lt;/i&gt; exists, then &#123;@code -1&#125; is returned. * * @param str the substring to search for. * @param fromIndex the index from which to start the search. * @return the index of the first occurrence of the specified substring, * starting at the specified index, * or &#123;@code -1&#125; if there is no such occurrence. */ public int indexOf(String str, int fromIndex) &#123; return indexOf(value, 0, value.length, str.value, 0, str.value.length, fromIndex); &#125; 哈哈，发现他又调用了 indexOf 的方法，这次终于我们可以看到最后的 查找算法 如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * Code shared by String and StringBuffer to do searches. The * source is the character array being searched, and the target * is the string being searched for. * * @param source the characters being searched. * @param sourceOffset offset of the source string. * @param sourceCount count of the source string. * @param target the characters being searched for. * @param targetOffset offset of the target string. * @param targetCount count of the target string. * @param fromIndex the index to begin searching from. */static int indexOf(char[] source, int sourceOffset, int sourceCount, char[] target, int targetOffset, int targetCount, int fromIndex) &#123; if (fromIndex &gt;= sourceCount) &#123; return (targetCount == 0 ? sourceCount : -1); &#125; if (fromIndex &lt; 0) &#123; fromIndex = 0; &#125; if (targetCount == 0) &#123; return fromIndex; &#125; char first = target[targetOffset]; int max = sourceOffset + (sourceCount - targetCount); for (int i = sourceOffset + fromIndex; i &lt;= max; i++) &#123; /* Look for first character. */ if (source[i] != first) &#123; while (++i &lt;= max &amp;&amp; source[i] != first); &#125; /* Found first character, now look at the rest of v2 */ if (i &lt;= max) &#123; int j = i + 1; int end = j + targetCount - 1; for (int k = targetOffset + 1; j &lt; end &amp;&amp; source[j] == target[k]; j++, k++); if (j == end) &#123; /* Found whole string. */ return i - sourceOffset; &#125; &#125; &#125; return -1;&#125; 总结：遇到这种问题多查看源码，想深入就得从底层做起！","tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://www.54tianzhisheng.cn/tags/数据结构/"},{"name":"算法","slug":"算法","permalink":"http://www.54tianzhisheng.cn/tags/算法/"},{"name":"字符串","slug":"字符串","permalink":"http://www.54tianzhisheng.cn/tags/字符串/"},{"name":"旋转词","slug":"旋转词","permalink":"http://www.54tianzhisheng.cn/tags/旋转词/"}]},{"title":"字符串","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/【字符串】字符串逆序/","text":"题目一：如果一个字符串 str ，把字符串 str 前面的任意部分挪到后面去形成的字符串叫做 str 的旋转词。比如 str = “ 1234 ” ， 那么 str 的旋转词有 “ 1234 ” ， “ 2341 ” ， “ 3412 ” ， “ 4123 ” 。给定两个字符串 a 和 b ，请判断 a 和 b 是否互为旋转词？举例： a = “ cdab “ , b = “ abcd “ 。返回 true。 a = “ 1ab2 “ , b = “ ab12 “ 。返回 false。 a = “ 2ab1 “ , b= “ ab12 “ 。 返回 true。 思路：最优解时间复杂度为 O(N) 先判断字符串 a 和 b 是否长度相等。 如果长度相等，生成 a + a 的大字符串。 然后判断大字符串中是否包含 b 字符串。（使用 kmp 算法判断）如果大字符串中包含字符串 b ，那么字符串 a 和 b 就互为旋转词。 举例： a = “ 1234 “ a + a = “ 12341234 “ 很明显发现，如果字符串 a 的长度为 N，在 a + a 的大字符串中，任意一个长度为 N 的子串都是 a 的旋转词。 题目二：给定一个字符串 a， 请在单词间做逆序调整。 举例： “ pig loves dog “ 逆序成 “ dog loves pig “ 。 “ I’m a student. “ 逆序成 “ student. a I’m “ 思路： 实现将字符串局部所有字符逆序的函数 f 利用 f 将字符串所有字符逆序 找到逆序后的字符串中每一个单词的区域，利用 f 将每一个单词的区域逆序 题目三：给定一个字符串 a 和一个整数 i。N为字符串的长度，i 为 a 中的位置，将 a [ 0 … i ] 移到右侧，a [ i + 1 … N - 1 ]移到左侧。 举例： a = “ ABCDE “ ，i = 2 。将 str 调整为 “ DEABC “ 。 要求：时间复杂度为 O(N)，额外空间复杂度为 O(1)。 思路： 先将 a[ 0 … i ] 部分的字符逆序 再将 a[ i + 1 … N - 1 ] 部分的字符逆序 最后将整体的字符 a 逆序 题目四：给定一个字符串类型的数组 strs，请找到一种拼接顺序，使得将所有的字符串拼接起来组成的大字符串是所有可能性中字典顺序最小的，并返回这个字符串。 举例： strs = [ “ abc “ , “ de “ ]，可以拼接成 “ abcde “，也可以拼接成 “ deabc “，但是前者的字典顺序更小，所以返回 “ abcde “ 。 strs = [ “ b “, “ ba “ ], 可以拼接成 “ bba “, 也可以拼接成 “ bab “,但是后者的字典顺序更小，所以返回 “ bab “。 思路：最优解的时间复杂度O(N*logN)，其实质是一种排序的实现。 方案二中是比较两个字符串彼此拼接后的字典顺序，所以能成功。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://www.54tianzhisheng.cn/tags/数据结构/"},{"name":"算法","slug":"算法","permalink":"http://www.54tianzhisheng.cn/tags/算法/"},{"name":"字符串","slug":"字符串","permalink":"http://www.54tianzhisheng.cn/tags/字符串/"}]},{"title":"奇怪的Java题：为什么128 == 128返回为False，而127 == 127会返回为True?","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/奇怪的Java题：为什么128 == 128返回为False，而127 == 127会返回为True-/","text":"这是我们今天要讨论的话题，因为我觉得它非常的有趣。 如果你运行如下代码： 12345678910class A&#123; public static void main(String[] args) &#123; Integer a = 128, b = 128; System.out.println(a == b); Integer c = 127, d = 127; System.out.println(c == d); &#125;&#125; 你会得到如下结果： 12falsetrue 我们知道，如果两个引用指向同一个对象，那么==就成立；反之，如果两个引用指向的不是同一个对象，那么==就不成立，即便两个引用的内容是一样的。因此，结果就会出现false。 这是非常有趣的地方。如果你查看Integer.java类，你会找到IntegerCache.java这个内部私有类，它为-128到127之间的所有整数对象提供缓存。 这个东西为那些数值比较小的整数提供内部缓存，当进行如此声明时： 1Integer c = 127 它的内部就是这样的： 1Integer var3 = Integer.valueOf(127); 其实我通过将A.class文件反编译后，代码如下图： 如果我们观察valueOf()类函数，我们可以看到： 12345public static Integer valueOf(int i) &#123; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i); &#125; 通过看源码能够知道，整数类型在-128～127之间时，会使用缓存，造成的效果就是，如果已经创建了一个相同的整数，使用valueOf创建第二次时，不会使用new关键字，而用已经缓存的对象。所以使用valueOf方法创建两次对象，若对应的数值相同，且数值在-128～127之间时，两个对象都指向同一个地址。 因此。。。 1Integer c = 127, d = 127; 两者指向同样的对象。 这就是为什么下面这段代码的结果为true了： 1System.out.println(c == d); 现在你可能会问，为什么会为-128到127之间的所有整数设置缓存？ 这是因为在这个范围内的小数值整数在日常生活中的使用频率要比其它的大得多，多次使用相同的底层对象这一特性可以通过该设置进行有效的内存优化。你可以使用reflection API任意使用这个功能。 运行下面的这段代码，你就会明白它的神奇所在了。 12345678910111213public static void main(String[] args) throws NoSuchFieldException, IllegalAccessException &#123; Class cache = Integer.class.getDeclaredClasses()[0]; Field myCache = cache.getDeclaredField(&quot;cache&quot;); myCache.setAccessible(true); Integer[] newCache = (Integer[]) myCache.get(cache); newCache[132] = newCache[133]; int a = 2; int b = a + a; System.out.printf(&quot;%d + %d = %d&quot;, a, a, b); // &#125; 打印结果竟然是： 12 + 2 = 5 我们再次看一下反汇编代码： 是不是又和上面的是同一个问题呢？ 但是结果为什么是 2 + 2 = 5 呢？ 我们继续去看一下 Integer 源码，去深入了解 Integer 缓存机制，下面截个图： 根据源码可以发现最后修改 Integer 缓存上限时候的方法有点小瑕疵。我们看看Api给我们怎么建议的一段话：1the size of the cache may be controlled by the &#123;@code -XX:AutoBoxCacheMax=&lt;size&gt;&#125; option. 原来我们只需要：运行时设置 -XX:AutoBoxCacheMax=133 就OK。 参考文章： 奇怪的Java题：为什么1000 == 1000返回为False，而100 == 100会返回为True?","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"}]},{"title":"程序访问文件的几种方式","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/程序访问文件的几种方式/","text":"IO程序访问文件的几种方式 读取和写入文件 I/O 操作都调用操作系统提供的接口。因为磁盘设备是由操作系统管理的，应用程序要访问物理设备只能通过系统调用的方式来工作。读和写分别对应 read() 和 write() 两个系统调用。而只要是系统调用就可能存在内核空间地址和用户空间地址切换的问题，这也是为什么操作系统为了保护系统本身的运行安全而将内核程序运行使用的内存空间和用户程序运行的内存空间进行隔离造成的。虽然这样可以保证内核程序运行的安全性，但是也存在数据可能需要从内核空间向用户空间复制的问题。 如果遇到非常耗时的操作，如磁盘 I/O， 数据从磁盘复制到内核空间，然后又从内核空间复制到用户空间，将会非常缓慢。这时操作系统为了加速 I/O 访问，在内核空间使用缓存机制，也就是将从磁盘读取的文件按照一定的组织方式进行缓存，如果用户程序访问的是同一段磁盘地址的空间数据，那么操作系统将从内核缓存中直接取出返回给用户程序，这样就可以减小 I/O 的响应时间。 1. 标准访问文件的方式标准访问文件的方式就是当应用程序调用 read() 接口时，操作系统检查在内核的高速缓存中有没有需要的数据，如果已经缓存了，那么就直接从缓存中返回，如果没有，则从磁盘中读取，然后缓存在操作系统的缓存中。 写入的方式是，用户的应用程序调用 write() 接口将数据从用户地址空间复制到内核地址空间的缓存中。这时对用户程序来说写操作就已经完成了，至于什么时候再写到磁盘中由操作系统决定，除非显示地调用 sync 同步命令。 标准访问文件的方式如下图所示： 2. 直接 I/O 的方式直接 I/O 方式就是应用程序直接访问磁盘数据，而不经过操作系统内核数据缓冲区，这样做的目的就是减少一次从内核缓冲区到用户程序缓存的数据复制。此种方式通常是在对数据的缓存管理由应用程序实现的数据库管理系统中。如在数据库管理系统中，系统明确的知道应该缓存哪些数据，应该失效哪些数据，还可以对一些热点的数据进行预加载，提前将热点数据加载到内存，可以加速数据的访问效率。在这些情况下，如果是由操作系统进行缓存，则很难做到，因为操作系统并不知道哪些是热点数据，哪些数据是访问一次后再也不会访问了，操作系统就是简单的缓存最近一次从磁盘读取的数据。 但是直接 I/O 也有负面的影响，如果访问的数据不再应用程序缓存中，则每次数据的加载都需要从磁盘读取，样加载的话速度非常的慢，通常是直接 I/O 与 异步 I/O 结合使用，会得到较好的性能。 直接 I/O 的方式如下图所示： 3. 同步访问文件的方式同步访问文件的方式就是数据的读取和写入都是同步操作的，它与标准访问文件的方式不同的是，只有当数据被成功写到磁盘时才返回给应用程序成功的标志。 这种访问文件的方式性能比较差，只有在一些数据安全性要求比较高的场景中才会使用，而且通常这种方式的硬件都是定制的。 同步访问文件的方式如下图所示： 4. 异步访问文件的方式异步访问文件的方式就是当访问数据的线程发出请求之后，线程会接着去处理其他事情，而不是阻塞等待，当请求的数据返回后继续处理下面的操作。这种方式可以明显的提高应用程序的效率，但是不会改变访问文件的效率。 异步访问文件的方式如下图所示： 5. 内存映射的方式内存映射的方式是指操作系统将内存中的某一块区域与磁盘中的文件关联起来，当要访问内存中的一段数据时，转换为访问文件的某一段数据。这种方式的目的同样是减少数据从内核空间缓存到用户空间缓存的数据复制操作，因为这两个空间的数据是共享的。 内存映射的方式如下图所示： 注：以上参考书籍《深入分析Java Web 技术内幕修订版》许令波，更多精彩知识还请看原书。","tags":[{"name":"IO","slug":"IO","permalink":"http://www.54tianzhisheng.cn/tags/IO/"}]},{"title":"MySQL 处理海量数据时的一些优化查询速度方法","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/MySQL-select-good/","text":"在参与实际项目中，当 MySQL 表的数据量达到百万级时，普通的 SQL 查询效率呈直线下降，而且如果 where 中的查询条件较多时，其查询速度无法容忍。想想可知，假如我们查询淘宝的一个订单详情，如果查询时间高达几十秒，这么高的查询延时，任何用户都会抓狂。因此如何提高 SQL 语句查询效率，显得十分重要。 查询速度慢的原因1、没有索引或者没有用到索引（这是查询慢最常见的问题，是程序设计的缺陷） 2、I/O 吞吐量小，形成了瓶颈效应。 3、没有创建计算列导致查询不优化。 4、内存不足 5、网络速度慢 6、查询出的数据量过大（可采用多次查询，其他的方法降低数据量） 7、锁或者死锁（这是查询慢最常见的问题，是程序设计的缺陷） 8、sp_lock,sp_who,活动的用户查看,原因是读写竞争资源。 9、返回了不必要的行和列 10、查询语句不好，没有优化 30 种 SQL 查询语句的优化方法：1、应尽量避免在 where 子句中使用 != 或者 &lt;&gt; 操作符，否则将引擎放弃使用索引而进行全表扫描。 2、应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，如： 1select id from t where num is null; 可以在 num 上设置默认值 0 ，确保表中 num 列没有 null 值，然后这样查询： 1select id from t where num = 0; 3、对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。 4、尽量避免在 where 子句中使用 or 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描，如： 1select id from t where num = 10 or num = 20; 可以这样查询： 123select id from t where num = 10union allselect id from t where num = 20; 5、下面的查询也将导致全表扫描：（不能前置百分号） 1select id from t where name like '%abc%'; 若要提高效率，可以考虑全文检索。 6、in 和 not in 也要慎用，否则会导致全表扫描，如： 1select id from t where num in(1, 2, 3); 对于连续的数值，能用 between 就不要用 in 了： 1select id from t where num between 1 and 3; 12345select xx,phone FROM send a JOIN ( select '13891030091' phone union select '13992085916' ………… UNION SELECT '13619100234' ) b on a.Phone=b.phone--替代下面 很多数据隔开的时候in('13891030091','13992085916','13619100234'…………) 7、如果在 where 子句中使用参数，也会导致全表扫描。因为 SQL 只有在运行时才会解析局部变量，但优化程序不能将访问计划的选择到运行时；它必须在编译时进行选择。然而，如果在编译时简历访问计划，变量的值还是未知的，因而无法作为索引选择的输入项。如下面语句将进行全表扫描： 1select id from t where num = @num; 可以改为强制查询使用索引： 1select id from t with(index(索引名)) where num = @num; 8、应尽量避免在 where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。如： 1select id from t where num/2 = 100; 应改为： 1select id from t where num = 100 * 2; 9、应尽量避免在 where 子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。如： 12select id from t where substring(name, 1, 3) = ’abc’–name; //以abc开头的idselect id from t where datediff(day,createdate,’2005-11-30′) = 0–’2005-11-30′; //生成的id 应改为: 12select id from t where name like ‘abc%’select id from t where createdate &gt;= ’2005-11-30′ and createdate &lt; ’2005-12-1′; 10、不要在 where 子句中的 “=” 左边进行函数，算术运算或者其他表达式运算，否则系统将可能无法正确使用索引。 11、在使用索引字段作为条件时，如果该索引是复合索引，那么必须使用到该索引的第一个字段作为条件时才能保证系统使用该索引，否则该索引将不会被使用，并且应尽可能的让字段顺序与索引顺序相一致。 12、不要些一些没有意义的查询，如需要生成一个空表结构： 1select col1,col2 into #t from t where 1=0; 这类代码不会返回任何结果集，但是会消耗系统资源的，应改成这样： 1create table #t(…) 13、很多时候用 exists 代替 in 是一个好的选择： 1select num from a where num in(select num from b); 用下面的语句替换： 1select num from a where exists(select 1 from b where num=a.num); 14、并不是所有索引对查询都有效，SQL是根据表中数据来进行查询优化的，当索引列有大量数据重复时，SQL查询可能不会去利用索引，如一表中有字段 sex，male、female几乎各一半，那么即使在sex上建了索引也对查询效率起不了作用。 15、索引并不是越多越好，索引固然可以提高相应的 select 的效率，但同时也降低了 insert 及 update 的效率，因为 insert 或 update 时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。一个表的索引数最好不要超过6个，若太多则应考虑一些不常使用到的列上建的索引是否有必要。 16、应尽可能的避免更新 clustered 索引数据列，因为 clustered 索引数据列的顺序就是表记录的物理存储顺序，一旦该列值改变将导致整个表记录的顺序的调整，会耗费相当大的资源。若应用系统需要频繁更新 clustered 索引数据列，那么需要考虑是否应将该索引建为 clustered 索引。 17、尽量使用数字型字段，若只含数值信息的字段尽量不要设计为字符型，这会降低查询和连接的性能，并会增加存储开销。这是因为引擎在处理查询和连接时会 逐个比较字符串中每一个字符，而对于数字型而言只需要比较一次就够了。 18、尽可能的使用 varchar/nvarchar 代替 char/nchar ，因为首先变长字段存储空间小，可以节省存储空间，其次对于查询来说，在一个相对较小的字段内搜索效率显然要高些。 19、任何地方都不要使用 select * from t ，用具体的字段列表代替 *，不要返回用不到的任何字段。 20、尽量使用表变量来代替临时表。如果表变量包含大量数据，请注意索引非常有限（只有主键索引）。 21、避免频繁创建和删除临时表，以减少系统表资源的消耗。 22、临时表并不是不可使用，适当地使用它们可以使某些例程更有效，例如，当需要重复引用大型表或常用表中的某个数据集时。但是，对于一次性事件，最好使用导出表。 23、在新建临时表时，如果一次性插入数据量很大，那么可以使用 select into 代替 create table，避免造成大量 log ，以提高速度；如果数据量不大，为了缓和系统表的资源，应先 create table，然后 insert。 24、如果使用到了临时表，在存储过程的最后务必将所有的临时表显式删除，先 truncate table ，然后 drop table ，这样可以避免系统表的较长时间锁定。 25、尽量避免使用游标，因为游标的效率较差，如果游标操作的数据超过1万行，那么就应该考虑改写。 26、使用基于游标的方法或临时表方法之前，应先寻找基于集的解决方案来解决问题，基于集的方法通常更有效。 27、与临时表一样，游标并不是不可使用。对小型数据集使用 FAST_FORWARD 游标通常要优于其他逐行处理方法，尤其是在必须引用几个表才能获得所需的数据时。在结果集中包括“合计”的例程通常要比使用游标执行的速度快。如果开发时 间允许，基于游标的方法和基于集的方法都可以尝试一下，看哪一种方法的效果更好。 28、在所有的存储过程和触发器的开始处设置 SET NOCOUNT ON ，在结束时设置 SET NOCOUNT OFF 。无需在执行存储过程和触发器的每个语句后向客户端发送 DONE_IN_PROC 消息。 29、尽量避免向客户端返回大数据量，若数据量过大，应该考虑相应需求是否合理。 30、尽量避免大事务操作，提高系统并发能力。","tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://www.54tianzhisheng.cn/tags/MySQL/"}]},{"title":"Pyspider框架 —— Python爬虫实战之爬取 V2EX 网站帖子","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/Pyspider框架 —— Python爬虫实战之爬取 V2EX 网站帖子/","text":"背景： PySpider：一个国人编写的强大的网络爬虫系统并带有强大的WebUI。采用Python语言编写，分布式架构，支持多种数据库后端，强大的WebUI支持脚本编辑器，任务监视器，项目管理器以及结果查看器。在线示例： http://demo.pyspider.org/ 官方文档： http://docs.pyspider.org/en/latest/ Github : https://github.com/binux/pyspider 本文爬虫代码 Github 地址：https://github.com/zhisheng17/Python-Projects/blob/master/v2ex/V2EX.py 说了这么多，我们还是来看正文吧！ 前提: 你已经安装好了Pyspider 和 MySQL-python（保存数据） 如果你还没安装的话，请看看我的前一篇文章，防止你也走弯路。 Pyspider 框架学习时走过的一些坑 HTTP 599: SSL certificate problem: unable to get local issuer certificate错误 我所遇到的一些错误： 首先，本爬虫目标：使用 Pyspider 框架爬取 V2EX 网站的帖子中的问题和内容，然后将爬取的数据保存在本地。 V2EX 中大部分的帖子查看是不需要登录的，当然也有些帖子是需要登陆后才能够查看的。（因为后来爬取的时候发现一直 error ，查看具体原因后才知道是需要登录的才可以查看那些帖子的）所以我觉得没必要用到 Cookie，当然如果你非得要登录，那也很简单，简单地方法就是添加你登录后的 cookie 了。 我们在 https://www.v2ex.com/ 扫了一遍，发现并没有一个列表能包含所有的帖子，只能退而求其次，通过抓取分类下的所有的标签列表页，来遍历所有的帖子： https://www.v2ex.com/?tab=tech 然后是 https://www.v2ex.com/go/programmer 最后每个帖子的详情地址是 （举例）： https://www.v2ex.com/t/314683#reply1 创建一个项目 在 pyspider 的 dashboard 的右下角，点击 “Create” 按钮 替换 on_start 函数的 self.crawl 的 URL： 123@every(minutes=24 * 60) def on_start(self): self.crawl(&apos;https://www.v2ex.com/&apos;, callback=self.index_page, validate_cert=False) self.crawl 告诉 pyspider 抓取指定页面，然后使用 callback 函数对结果进行解析。 @every) 修饰器，表示 on_start 每天会执行一次，这样就能抓到最新的帖子了。 validate_cert=False 一定要这样，否则会报 HTTP 599: SSL certificate problem: unable to get local issuer certificate错误 首页： 点击绿色的 run 执行，你会看到 follows 上面有一个红色的 1，切换到 follows 面板，点击绿色的播放按钮： 第二张截图一开始是出现这个问题了，解决办法看前面写的文章，后来问题就不再会出现了。 Tab 列表页 : 在 tab 列表页 中，我们需要提取出所有的主题列表页 的 URL。你可能已经发现了，sample handler 已经提取了非常多大的 URL 代码：1234@config(age=10 * 24 * 60 * 60) def index_page(self, response): for each in response.doc(&apos;a[href^=&quot;https://www.v2ex.com/?tab=&quot;]&apos;).items(): self.crawl(each.attr.href, callback=self.tab_page, validate_cert=False) 由于帖子列表页和 tab列表页长的并不一样，在这里新建了一个 callback 为 self.tab_page @config(age=10 24 60 * 60) 在这表示我们认为 10 天内页面有效，不会再次进行更新抓取 Go列表页 : 代码： 1234@config(age=10 * 24 * 60 * 60) def tab_page(self, response): for each in response.doc(&apos;a[href^=&quot;https://www.v2ex.com/go/&quot;]&apos;).items(): self.crawl(each.attr.href, callback=self.board_page, validate_cert=False) 帖子详情页（T）: 你可以看到结果里面出现了一些reply的东西，对于这些我们是可以不需要的，我们可以去掉。 同时我们还需要让他自己实现自动翻页功能。 代码：123456789@config(age=10 * 24 * 60 * 60) def board_page(self, response): for each in response.doc(&apos;a[href^=&quot;https://www.v2ex.com/t/&quot;]&apos;).items(): url = each.attr.href if url.find(&apos;#reply&apos;)&gt;0: url = url[0:url.find(&apos;#&apos;)] self.crawl(url, callback=self.detail_page, validate_cert=False) for each in response.doc(&apos;a.page_normal&apos;).items(): self.crawl(each.attr.href, callback=self.board_page, validate_cert=False) #实现自动翻页功能 去掉后的运行截图： 实现自动翻页后的截图： 此时我们已经可以匹配了所有的帖子的 url 了。 点击每个帖子后面的按钮就可以查看帖子具体详情了。 代码： 12345678910@config(priority=2) def detail_page(self, response): title = response.doc(&apos;h1&apos;).text() content = response.doc(&apos;div.topic_content&apos;).html().replace(&apos;&quot;&apos;, &apos;\\\\&quot;&apos;) self.add_question(title, content) #插入数据库 return &#123; &quot;url&quot;: response.url, &quot;title&quot;: title, &quot;content&quot;: content, &#125; 插入数据库的话，需要我们在之前定义一个add_question函数。 123456789101112131415#连接数据库def __init__(self): self.db = MySQLdb.connect(&apos;localhost&apos;, &apos;root&apos;, &apos;root&apos;, &apos;wenda&apos;, charset=&apos;utf8&apos;) def add_question(self, title, content): try: cursor = self.db.cursor() sql = &apos;insert into question(title, content, user_id, created_date, comment_count) values (&quot;%s&quot;,&quot;%s&quot;,%d, %s, 0)&apos; % (title, content, random.randint(1, 10) , &apos;now()&apos;); #插入数据库的SQL语句 print sql cursor.execute(sql) print cursor.lastrowid self.db.commit() except Exception, e: print e self.db.rollback() 查看爬虫运行结果： 先debug下，再调成running。pyspider框架在windows下的bug 设置跑的速度，建议不要跑的太快，否则很容易被发现是爬虫的，人家就会把你的IP给封掉的 查看运行工作 查看爬取下来的内容 然后再本地数据库GUI软件上查询下就可以看到数据已经保存到本地了。 自己需要用的话就可以导入出来了。 在开头我就告诉大家爬虫的代码了，如果详细的看看那个project，你就会找到我上传的爬取数据了。（仅供学习使用，切勿商用！） 当然你还会看到其他的爬虫代码的了，如果你觉得不错可以给个 Star，或者你也感兴趣的话，你可以fork我的项目，和我一起学习，这个项目长期更新下去。 最后： 代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# created by 10412# !/usr/bin/env python# -*- encoding: utf-8 -*-# Created on 2016-10-20 20:43:00# Project: V2EXfrom pyspider.libs.base_handler import *import reimport randomimport MySQLdbclass Handler(BaseHandler): crawl_config = &#123; &#125; def __init__(self): self.db = MySQLdb.connect(&apos;localhost&apos;, &apos;root&apos;, &apos;root&apos;, &apos;wenda&apos;, charset=&apos;utf8&apos;) def add_question(self, title, content): try: cursor = self.db.cursor() sql = &apos;insert into question(title, content, user_id, created_date, comment_count) values (&quot;%s&quot;,&quot;%s&quot;,%d, %s, 0)&apos; % (title, content, random.randint(1, 10) , &apos;now()&apos;); print sql cursor.execute(sql) print cursor.lastrowid self.db.commit() except Exception, e: print e self.db.rollback() @every(minutes=24 * 60) def on_start(self): self.crawl(&apos;https://www.v2ex.com/&apos;, callback=self.index_page, validate_cert=False) @config(age=10 * 24 * 60 * 60) def index_page(self, response): for each in response.doc(&apos;a[href^=&quot;https://www.v2ex.com/?tab=&quot;]&apos;).items(): self.crawl(each.attr.href, callback=self.tab_page, validate_cert=False) @config(age=10 * 24 * 60 * 60) def tab_page(self, response): for each in response.doc(&apos;a[href^=&quot;https://www.v2ex.com/go/&quot;]&apos;).items(): self.crawl(each.attr.href, callback=self.board_page, validate_cert=False) @config(age=10 * 24 * 60 * 60) def board_page(self, response): for each in response.doc(&apos;a[href^=&quot;https://www.v2ex.com/t/&quot;]&apos;).items(): url = each.attr.href if url.find(&apos;#reply&apos;)&gt;0: url = url[0:url.find(&apos;#&apos;)] self.crawl(url, callback=self.detail_page, validate_cert=False) for each in response.doc(&apos;a.page_normal&apos;).items(): self.crawl(each.attr.href, callback=self.board_page, validate_cert=False) @config(priority=2) def detail_page(self, response): title = response.doc(&apos;h1&apos;).text() content = response.doc(&apos;div.topic_content&apos;).html().replace(&apos;&quot;&apos;, &apos;\\\\&quot;&apos;) self.add_question(title, content) #插入数据库 return &#123; &quot;url&quot;: response.url, &quot;title&quot;: title, &quot;content&quot;: content, &#125;","tags":[{"name":"Python","slug":"Python","permalink":"http://www.54tianzhisheng.cn/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"http://www.54tianzhisheng.cn/tags/爬虫/"},{"name":"Pyspider","slug":"Pyspider","permalink":"http://www.54tianzhisheng.cn/tags/Pyspider/"}]},{"title":"Spring MVC系列文章（二）：Spring MVC+Hibernate JPA搭建的博客系统项目中所遇到的坑","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/Spring MVC+Hibernate JPA搭建的博客系统项目中所遇到的坑/","text":"SpringBoot 系列文章 项目代码地址：https://github.com/zhisheng17/springmvc最近在学习 Spring MVC ，其中在做一个简单的博客系统demo，是使用 SpringMVC 集成 Spring Data JPA（由 Hibernate JPA 提供），来进行强大的数据库访问。结果其中遇到的坑不 是一点点啊，我差点崩溃了，其中最大的原因就是由于 Hibernate JPA 中的bug了，反正一开始 还不知道是这个问题，导致折腾了快一天的时间。想想都可怕啊。 mvc-dispatch-servlet.xml代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:mvc=\"http://www.springframework.org/schema/mvc\" xmlns:jpa=\"http://www.springframework.org/schema/data/jpa\" xmlns:tx=\"http://www.springframework.org/schema/tx\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd http://www.springframework.org/schema/data/jpa http://www.springframework.org/schema/data/jpa/spring-jpa.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd\"&gt; &lt;!--指明 controller 所在包，并扫描其中的注解--&gt; &lt;context:component-scan base-package=\"cn.zhisheng.controller\"/&gt; &lt;!-- 静态资源(js、image等)的访问 --&gt; &lt;mvc:default-servlet-handler/&gt; &lt;!-- 开启注解 --&gt; &lt;mvc:annotation-driven/&gt; &lt;!--ViewResolver 视图解析器--&gt; &lt;!--用于支持Servlet、JSP视图解析--&gt; &lt;bean id=\"jspViewResolver\" class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\"&gt; &lt;property name=\"viewClass\" value=\"org.springframework.web.servlet.view.JstlView\"/&gt; &lt;property name=\"prefix\" value=\"/WEB-INF/pages/\"/&gt; &lt;property name=\"suffix\" value=\".jsp\"/&gt; &lt;/bean&gt; &lt;!-- 表示JPA Repository所在的包 --&gt; &lt;jpa:repositories base-package=\"cn.zhisheng.repository\"/&gt; &lt;bean id=\"entityManagerFactory\" class=\"org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean\"&gt; &lt;property name=\"persistenceUnitName\" value=\"defaultPersistenceUnit\"/&gt; &lt;property name=\"packagesToScan\" value=\"cn.zhisheng.model\" /&gt; &lt;property name=\"jpaVendorAdapter\"&gt; &lt;bean class=\"org.springframework.orm.jpa.vendor.HibernateJpaVendorAdapter\"/&gt; &lt;/property&gt; &lt;property name=\"jpaProperties\"&gt; &lt;props&gt; &lt;prop key=\"hibernate.connection.driver_class\"&gt;com.mysql.jdbc.Driver&lt;/prop&gt; &lt;prop key=\"hibernate.connection.url\"&gt;jdbc:mysql://localhost:3306/springdemo?useSSL=false&lt;/prop&gt; &lt;prop key=\"hibernate.connection.username\"&gt;root&lt;/prop&gt; &lt;prop key=\"hibernate.connection.password\"&gt;root&lt;/prop&gt; &lt;prop key=\"hibernate.show_sql\"&gt;false&lt;/prop&gt; &lt;prop key=\"hibernate.connection.useUnicode\"&gt;true&lt;/prop&gt; &lt;prop key=\"hibernate.connection.characterEncoding\"&gt;UTF-8&lt;/prop&gt; &lt;prop key=\"hibernate.format_sql\"&gt;true&lt;/prop&gt; &lt;prop key=\"hibernate.use_sql_comments\"&gt;true&lt;/prop&gt; &lt;prop key=\"hibernate.hbm2ddl.auto\"&gt;update&lt;/prop&gt; &lt;prop key=\"hibernate.connection.autoReconnect\"&gt;true&lt;/prop&gt; &lt;prop key=\"hibernate.dialect\"&gt;org.hibernate.dialect.MySQL5Dialect&lt;/prop&gt; &lt;prop key=\"connection.autoReconnectForPools\"&gt;true&lt;/prop&gt; &lt;prop key=\"connection.is-connection-validation-required\"&gt;true&lt;/prop&gt; &lt;prop key=\"hibernate.c3p0.validate\"&gt;true&lt;/prop&gt; &lt;prop key=\"hibernate.connection.provider_class\"&gt;org.hibernate.service.jdbc.connections.internal.C3P0ConnectionProvider&lt;/prop&gt; &lt;prop key=\"hibernate.c3p0.min_size\"&gt;5&lt;/prop&gt; &lt;prop key=\"hibernate.c3p0.max_size\"&gt;600&lt;/prop&gt; &lt;prop key=\"hibernate.c3p0.timeout\"&gt;1800&lt;/prop&gt; &lt;prop key=\"hibernate.c3p0.max_statements\"&gt;50&lt;/prop&gt; &lt;prop key=\"hibernate.c3p0.preferredTestQuery\"&gt;SELECT 1;&lt;/prop&gt; &lt;prop key=\"hibernate.c3p0.testConnectionOnCheckout\"&gt;true&lt;/prop&gt; &lt;prop key=\"hibernate.c3p0.idle_test_period\"&gt;3000&lt;/prop&gt; &lt;prop key=\"javax.persistence.validation.mode\"&gt;none&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;/bean&gt; &lt;!-- 事务管理 --&gt; &lt;bean id=\"transactionManager\" class=\"org.springframework.orm.jpa.JpaTransactionManager\"&gt; &lt;property name=\"entityManagerFactory\" ref=\"entityManagerFactory\"/&gt; &lt;/bean&gt; &lt;!-- 开启事务管理注解 --&gt; &lt;tx:annotation-driven transaction-manager=\"transactionManager\"/&gt;&lt;/beans&gt; pom.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.zhisheng&lt;/groupId&gt; &lt;artifactId&gt;springmvc&lt;/artifactId&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;name&gt;springmvc Maven Webapp&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;spring.version&gt;4.2.6.RELEASE&lt;/spring.version&gt; &lt;hibernate.version&gt;5.1.0.Final&lt;/hibernate.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-jpa&lt;/artifactId&gt; &lt;version&gt;1.10.1.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-entitymanager&lt;/artifactId&gt; &lt;version&gt;$&#123;hibernate.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.hibernate&lt;/groupId&gt; &lt;artifactId&gt;hibernate-c3p0&lt;/artifactId&gt; &lt;version&gt;$&#123;hibernate.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;com.mchange&lt;/groupId&gt; &lt;artifactId&gt;c3p0&lt;/artifactId&gt; &lt;version&gt;0.9.5.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.39&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;finalName&gt;springmvc&lt;/finalName&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 一开始我是用默认的在resources文件里面生成了persistence.xml配置文件进行数据库配置的，后来由于用那种方法，碰到的问题有很多，自己搞了好几个个小时都没弄好，只好换种方法，没想到竟然还是这种效果（泪崩），看来是不治标也不治本。 无奈，只好硬刚了，碰到错误，百度+google，看了大量的的解决方法，都是没用，慢慢的我所加的jar包越来越多，用maven管理的依赖的也变得多起来了，但终究是不能够解决问题的。 其实这时我看了这么多的博客和解决方法，我已经知道了是 Hibernate JPA 的bug问题，途中自己也换了一些版本，还是没能解决办法。 最后在吃完完晚饭后，又折腾了快三小时，终于找到可靠有用的解决方案了。 运行成功后，我当时就激动起来了。马丹，老子终于将你解决了。 所以在这里立马就将自己这次的血崩历史纪录下来。 下面写下遇到的问题：（其中有些可能还不记得写了） java.lang.ClassNotFoundException: javax.persistence.EntityManager java.lang.NoSuchMethodError: javax.persistence.JoinColumn.foreignKey()Ljavax/persistence/ForeignKey; javax.persistence.PersistenceException: No Persistence provider for EntityManager named defaultPersistenceUnit javax.persistence.PersistenceException: No Persistence provider for EntityManager named defaultPersi java.lang.NoClassDefFoundError: org/hibernate/ejb/HibernatePersistence java.lang.NoClassDefFoundError: org/slf4j/LoggerFactory java.lang.ClassNotFoundException: org.hibernate.MappingException NoSuchMethodError: javax.persistence.xxx 等，还有几个，忘记了。。 首先通过报错信息可以知道有些是因为jar包的问题，但是并不是光是缺少jar包的问题，很大的原 因就是因为jar包的版本不同，刚好那个jar包又是有问题的（自身有bug）。 就比如错误： java.lang.NoSuchMethodError: javax.persistence.JoinColumn.foreignKey()Ljavax/persistence/ForeignKey; 就是因为JAVAEE6.0中的 javax.persistence.jar与 hibernate4.3.8中的hibernate-jpa-2.1-api-1.0.0.Final.jar冲突 JoinColumn.foreignKey() was introduced with JPA 2.1, which was not implemented by Hibernate 4 until version 4.3. If you’re using an older version of Hibernate 4 then try upgrading to 4.3.x. If you’re already using Hibernate 4.3 then make sure you’re also using JPA 2.1 to make sure the API and implementation match up. 图片来自 : http://stackoverflow.com/questions/24588860/error-javax-persistence-joincolumn-foreignkeyljavax-persistence-foreignkey-wi I finally solved this similar problem, there was an old version(hibernate-jpa-2.0-api-1.0.0-Final.jar) in my lib folder which I guess has been preventing maven dependency from loading. So after I manually deleted it and added (hibernate-jpa-2.1-api-1.0.0-Final.jar) everything started to work. 意思大概就是： 因为JAVAEE6.0中的 javax.persistence.jar与 hibernate4.3.8中的hibernate-jpa-2.1-api-1.0.0.Final.jar冲突 ，我们在pom文件下添加依赖后，竟然没发现在 springmvc（项目名称）\\target\\springmvc（项目名称）\\WEB-INF\\lib 下看到 javax.persistence.jar 文件，结果竟然在 springmvc\\lib下找到他了。 解决办法就是在 pom文件和 mvc-dispatcher-servlet.xml 都配置好的情况下，将 springmvc\\lib下的 javax.persistence.jar 删除。 最后再说一句：Though the error drove almost crazy, hold on, you wil get smile ！ Fighting","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"Spring","slug":"Spring","permalink":"http://www.54tianzhisheng.cn/tags/Spring/"},{"name":"Spring MVC","slug":"Spring-MVC","permalink":"http://www.54tianzhisheng.cn/tags/Spring-MVC/"},{"name":"Hibernate JPA","slug":"Hibernate-JPA","permalink":"http://www.54tianzhisheng.cn/tags/Hibernate-JPA/"},{"name":"Bootstrap","slug":"Bootstrap","permalink":"http://www.54tianzhisheng.cn/tags/Bootstrap/"},{"name":"MySQL","slug":"MySQL","permalink":"http://www.54tianzhisheng.cn/tags/MySQL/"}]},{"title":"记录下自己第一次坐飞机的感受","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/feiji/","text":"前提因为学校的原因，需要在大三的时候出去实训几个月，然后选择的是在昆山，公司规定要在 2017 年 4 月 19 号之前赶到，所以我提前订的机票是 17 号，之所以会订机票，其实很大一部分原因是由于自己之前没有坐过飞机，所以想体验一下坐飞机的感脚（嘿嘿），再加上这是机票的淡季，所以坐飞机的价钱比做高铁还便宜。整个行程：学校 ——&gt; 南昌机场 ——&gt; 上海虹桥机场 ——&gt; 公司 学校出发10 点左右去食堂吃了个早餐，然后打包好自己的行李，由于第一次坐飞机，所以打算提前时间去机场，然后在机场熟悉熟悉环境，不然的话错过飞机，那就 gg 了。大概 11：30 了，就拖着行李出门，在学校门口叫了辆 滴滴车 去机场（因为公交车要坐好久并且去机场的路好烂），叫的滴滴车他走的是高速，很快。 南昌机场不到半小时就到了机场，首先干的事就是取票。 然后将行李箱免费托运（小于20公斤可免费托运，超过的话那就准备好 Money 吧），一个同行的同学就超重了。干完了这些事就没啥事了，然后就坐在凳子上看起了最近很火的电视剧《人民的名义》，的确很好看的，没看过的话，可以去看看。 大概 13：40 时，准备进站了，这里要说的就比较有趣了。我就具体的说下： 一开始排队（人不多），轮到我，把机票和身份证给她，然后对着摄像头拍了个照就进去检查随身携带的物品了。 这里之前我就听说了：说是飞机上不能携带液体，不知道我书包带的洗发液能不能过关？同学和我之前还在一起开玩笑说：1、如果不能带，我就当场一口闷了它。（这也行，下图随意找了个表情包） 2、我免费请大家洗个头 电脑和雨伞而外检查 书包和随身携带物品都拿出来过安检 人扫描（检查的挺细致的，比火车严多了） 结果检查人员告知我：把 洗发液 和 两个螺丝刀 拿出来。（我当场就震惊了，我那么小的螺丝刀都被检查到了，这几乎把我的书包给透视了一个遍） 然后呢，自己只好将违禁物品都拿出来，再次检查就可以通过了。 走进去的时候拍的一张照片, （电梯好长，不过后来在上海机场下车才发现那个也还好了） 进去后，又有一个小的进站室，又在那里等待了会。 又开始检票，检完票后坐公交车去飞机下面（因为飞机场太大，我坐的那架飞机不是靠近站台的） 上飞机后从窗口拍的一张照片 飞机上看见飞机的空姐和空少了。。（空姐很漂亮，嘻嘻，没拍照片） 飞机起飞的时候，机舱里有语音播放叫我们系上安全带，并且关闭手机，所以后面就没照片了。 起飞感觉走了好远，然后突然一加速，飞机就慢慢的向上飞起，整个人重心都是往后倒的。 不一会，飞机就飞的好高了，从窗口望下去，下面的感觉好美，一路上的风景感觉都挺漂亮的，有时还可以看到白云。 大概一个小时后，空姐推着个车，提供给我们点零食（面包、饼干、榨菜）和饮料（橙汁、苹果醋、矿泉水、咖啡，可自选一样，可再加一杯），服务还是挺贴心的，我在这里给中国东方航空点个赞。 飞机中途有时会颠簸，机舱里语音说是遇到空气气流的影响。（大部分时间还是很好的） 说说飞机下降的时候吧，同样，感觉声音很大，耳朵有点受不了。不知道空姐、空少们怎么长期受得了这这声音，长期下去，那估计耳朵都要出毛病的吧？ 上海虹桥机场机场给人的感觉很大，走了那个电梯都走了很远。 就只有一张照片。 然后再去取我的行李箱，有个专门来自南昌的取行李处，行李箱一个的放在传送带上，自己拿自己的行李出去。这里有个疑问，如果行李被别人提走的话，那该咋办，我看出口都没设置检查行李是否和本人的匹配？ 公司公司提前有大巴来接我们，坐上大巴，然后在车上躺了下，就睡着了，直到到达公司。 住宿地址和公司有点远，所以买了辆 死飞自行车，方便出行，自己周末也可以出去玩玩。 总结第一次坐飞机，感觉还是挺好的。流程也不复杂，所以不麻烦。 不过自己要注意一些东西： 不要携带飞机上违禁物品（液体、超过多大的充电宝、刀枪等） 一定要提前动身，以防发生突发事件 注意保管好自己的身份证 最后这是我在我博客上写的第一篇随笔文章，不知道咋写，就随便写。 因为星期六（昨天）出去玩了，所以今天早上起来写下来这篇，有些事还是得赶紧做掉去，不然一直拖，拖着拖着就没有想做的欲望了。 公司周围环境挺好的，水乡之地，有个阳澄湖，盛产大闸蟹，周围的大闸蟹庄，那叫一个字：多。有空的时候我尽量多去逛逛，拍点照片回来。 今后也会多写点随笔，记录生活中一些有趣且值得纪念的点滴。 感谢阅读！！！","tags":[{"name":"随笔","slug":"随笔","permalink":"http://www.54tianzhisheng.cn/tags/随笔/"}]},{"title":"从对象深入分析 Java 中实例变量和类变量的区别","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/java-var/","text":"实例变量 和 类变量局部变量特点：作用时间短，存储在方法的栈内存中 种类： 形参：方法签名中定义的局部变量，由方法调用者负责为其赋值，随方法结束而消亡 方法内的局部变量：方法内定义的局部变量，必须在方法内对其进行显示初始化，从初始化后开始生效，随方法结束而消亡 代码块内的局部变量：在代码块中定义的局部变量，必须在代码块中进行显示初始化，从初始化后开始生效，随代码块结束而消亡 成员变量类体内定义的变量，如果该成员变量没有使用 static 修饰，那该成员变量又被称为非静态变量或实例变量，如果使用 static 修饰，则该成员变量又可被称为静态变量或类变量。 实例变量和类变量的属性使用 static 修饰的成员变量是类变量，属于该类本身，没有使用 static 修饰的成员变量是实例变量，属于该类的实例，在同一个类中，每一个类只对应一个 Class 对象，但每个类可以创建多个对象。 由于同一个 JVM 内的每个类只对应一个 CLass 对象，因此同一个 JVM 内的一个类的类变量只需要一块内存空间；但对于实例变量而言，该类每创建一次实例，就需要为该实例变量分配一块内存空间。也就是说，程序中创建了几个实例，实例变量就需要几块内存空间。 这里我想到一道面试题目： 123456789101112public class A&#123; &#123; System.out.println(\"我是代码块\"); &#125; static&#123; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); &#125;&#125; 结果： 123我是静态代码块我是代码块我是代码块 静态代码块只执行一次，而代码块每创建一个实例，就会打印一次。 实例变量的初始化时机程序可在3个地方对实例变量执行初始化： 定义实例变量时指定初始值 非静态初始化块中对实例变量指定初始值 构造器中对实例变量指定初始值 上面第一种和第二种方式比第三种方式更早执行，但第一、二种方式的执行顺序与他们在源程序中的排列顺序相同。 同样在上面那个代码上加上一个变量 weight 的成员变量，我们来验证下上面的初始化顺序： 1、定义实例变量指定初始值 在 非静态初始化块对实例变量指定初始值 之后: 123456789101112131415public class A&#123; &#123; weight = 2.1; System.out.println(\"我是代码块\"); &#125; double weight = 2.0; static&#123; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); &#125;&#125; 结果是： 1234我是静态代码块我是代码块我是代码块2.0 2、定义实例变量指定初始值 在 非静态初始化块对实例变量指定初始值 之前: 123456789101112131415public class A&#123; double weight = 2.0; &#123; weight = 2.1; System.out.println(\"我是代码块\"); &#125; static&#123; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); &#125;&#125; 结果为： 1234我是静态代码块我是代码块我是代码块2.1 大家有没有觉得很奇怪？ 我来好好说清楚下： 定义实例变量时指定的初始值、初始代码块中为实例变量指定初始值的语句的地位是平等的，当经过编译器处理后，他们都将会被提取到构造器中。也就是说，这条语句 double weight = 2.0; 实际上会被分成如下 2 次执行： double weight; : 创建 Java 对象时系统根据该语句为该对象分配内存。 weight = 2.1; : 这条语句将会被提取到 Java 类的构造器中执行。 只说原理，大家肯定不怎么信，那么还有拿出源码来，这样才有信服能力的吗？是不？ 这里我直接使用软件将代码的字节码文件反编译过来，看看里面是怎样的组成？ 第一个代码的反编译源码如下： 1234567891011121314151617181920public class A&#123; double weight; public A() &#123; this.weight = 2.1D; System.out.println(\"我是代码块\"); this.weight = 2.0D; &#125; static &#123; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); &#125;&#125; 第二个代码反编译源码如下： 1234567891011121314151617181920public class A&#123; double weight; public A() &#123; this.weight = 2.0D; this.weight = 2.1D; System.out.println(\"我是代码块\"); &#125; static &#123; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); &#125;&#125; 这下子满意了吧！ 通过反编译的源码可以看到该类定义的 weight 实例变量时不再有初始值，为 weight 指定初始值的代码也被提到了构造器中去了，但是我们也可以发现之前规则也是满足的。 他们的赋值语句都被合并到构造器中，在合并过程中，定义的变量语句转换得到的赋值语句，初始代码块中的语句都转换得到的赋值语句，总是位于构造器的所有语句之前，合并后，两种赋值语句的顺序也保持了它们在 Java 源代码中的顺序。 大致过程应该了解了吧？如果还不怎么清楚的，建议还是自己将怎个过程在自己的电脑上操作一遍，毕竟光看不练假把式。 类变量的初始化时机JVM 对每一个 Java 类只初始化一次，因此 Java 程序每运行一次，系统只为类变量分配一次内存空间，执行一次初始化。程序可在两个地方对类变量执行初始化： 定义类变量时指定初始值 静态初始化代码块中对类变量指定初始值 这两种方式的执行顺序与它们在源代码中的排列顺序相同。 还是用上面那个示例，我们在其基础上加个被 static 修饰的变量 height： 1、定义类变量时指定初始值 在 静态初始化代码块中对类变量指定初始值 之后： 123456789101112131415161718public class A&#123; double weight = 2.0; &#123; weight = 2.1; System.out.println(\"我是代码块\"); &#125; static&#123; height = 10.1; System.out.println(\"我是静态代码块\"); &#125; static double height = 10.0; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); System.out.println(height); &#125;&#125; 运行结果： 12345我是静态代码块我是代码块我是代码块2.110.0 2、定义类变量时指定初始值 在 静态初始化代码块中对类变量指定初始值 之前： 123456789101112131415161718public class A&#123; static double height = 10.0; double weight = 2.0; &#123; weight = 2.1; System.out.println(\"我是代码块\"); &#125; static&#123; height = 10.1; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); System.out.println(height); &#125;&#125; 运行结果： 12345我是静态代码块我是代码块我是代码块2.110.1 其运行结果正如我们预料，但是我们还是看看反编译后的代码吧！ 第一种情况下反编译的代码： 1234567891011121314151617181920212223public class A&#123; double weight; public A() &#123; this.weight = 2.0D; this.weight = 2.1D; System.out.println(\"我是代码块\"); &#125; static &#123; System.out.println(\"我是静态代码块\"); &#125; static double height = 10.0D; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); System.out.println(height); &#125;&#125; 第二种情况下反编译的代码： 123456789101112131415161718192021222324public class A&#123; static double height = 10.0D; double weight; public A() &#123; this.weight = 2.0D; this.weight = 2.1D; System.out.println(\"我是代码块\"); &#125; static &#123; height = 10.1D; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); System.out.println(height); &#125;&#125; 通过反编译源码，可以看到第一种情况下(定义类变量时指定初始值 在 静态初始化代码块中对类变量指定初始值 之后): 我们在 静态初始化代码块中对类变量指定初始值 已经不存在了，只有一个类变量指定的初始值 static double height = 10.0D; , 而在第二种情况下（定义类变量时指定初始值 在 静态初始化代码块中对类变量指定初始值 之前）和之前的源代码顺序是一样的，没啥区别。 上面的代码中充分的展示了类变量的两种初始化方式 ：每次运行该程序时，系统会为 A 类执行初始化，先为所有类变量分配内存空间，再按照源代码中的排列顺序执行静态初始代码块中所指定的初始值和定义类变量时所指定的初始值。","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"}]},{"title":"Java读取文件","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/java读取文件/","text":"以字节为单位读取文件 以字符为单位读取文件 以行为单位读取文件 随机读取文件内容 ReadFromFile.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263package cn.zhisheng.io;import java.io.*;/** * java读取文件 * Created by 10412 on 2016/12/29. */public class ReadFromFile&#123; /** * 以字节为单位读取文件，常用于读二进制文件，如图片、声音、影像等文件 * @param fileName 文件名 */ public static void readFileByBytes(String fileName) &#123; File file = new File(fileName); InputStream in = null; try &#123; System.out.println(\"以字节为单位读取文件内容，一次读取一个字节\"); //一次读一个字节 in = new FileInputStream(file); int tempbyte; while ((tempbyte = in.read()) != -1) &#123; System.out.println(tempbyte); &#125; in.close(); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); return; &#125; try &#123; System.out.println(\"以字节为单位读取文件内容，一次读取多个字节\"); //一次读取多个字节 byte[] tempbytes = new byte[100]; int byteread = 0; in = new FileInputStream(fileName); ReadFromFile.showAvailableBytes(in); // 读入多个字节到字节数组中，byteread为一次读入的字节数 while ((byteread = in.read(tempbytes)) != -1) &#123; System.out.write(tempbytes, 0, byteread); &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally &#123; if (in != null) &#123; try &#123; in.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; /** * 以字符为单位读取文件，常用于读文本，数字等类型的文件 * @param fileName 文件名 */ public static void readFileByChars(String fileName) &#123; File file = new File(fileName); Reader reader = null; try &#123; System.out.println(\"以字符为单位读取文件内容，一次读一个字符：\"); // 一次读一个字符 reader = new InputStreamReader(new FileInputStream(file)); int tempchar; while ((tempchar = reader.read()) != -1) &#123; // 对于windows下，\\r\\n这两个字符在一起时，表示一个换行。 // 但如果这两个字符分开显示时，会换两次行。 // 因此，屏蔽掉\\r，或者屏蔽\\n。否则，将会多出很多空行。 if (((char)tempchar) != '\\r') &#123; System.out.print((char) tempchar); &#125; &#125; reader.close(); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; try &#123; System.out.println(\"以字符为单位读取文件内容，一次读多个字符：\"); //一次读多个字符 char[] tempchars = new char[30]; int charread = 0; reader = new InputStreamReader(new FileInputStream(fileName)); // 读入多个字符到字符数组中，charread为一次读取字符数 while ((charread = reader.read(tempchars)) != -1) &#123; // 同样屏蔽掉\\r不显示 if ((charread == tempchars.length) &amp;&amp; (tempchars[tempchars.length - 1]) != '\\r') &#123; System.out.print(tempchars); &#125; else &#123; for (int i = 0; i &lt; charread; i++ ) &#123; if (tempchars[i] == '\\r') &#123; continue; &#125; else &#123; System.out.print(tempchars[i]); &#125; &#125; &#125; &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally &#123; if (reader != null) &#123; try &#123; reader.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; /** * 以行为单位读取文件，常用于读面向行的格式化文件 * @param fileName 文件名 */ public static void readFileByLines(String fileName) &#123; File file = new File(fileName); BufferedReader reader =null; try &#123; System.out.println(\"以行为单位读取文件内容，一次读一整行：\"); reader = new BufferedReader(new FileReader(file)); String tempString = null; int line = 1; // 一次读入一行，直到读入null为文件结束 while ((tempString = reader.readLine()) != null) &#123; // 显示行号 System.out.println(\"line \"+line+\": \"+tempString); line++; &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally &#123; if (reader != null) &#123; try &#123; reader.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; /** * 随机读取文件内容 * @param fileName 文件名 */ public static void readFileBRandomAccess(String fileName) &#123; RandomAccessFile randomFile = null; try &#123; System.out.println(\"随机读取一段文件内容：\"); // 打开一个随机访问文件流，按只读方式 randomFile = new RandomAccessFile(fileName, \"r\"); // 文件长度，字节数 long fileLength = randomFile.length(); // 读文件的起始位置 int beginIndex = (fileLength &gt; 4) ? 4 : 0; // 将读文件的开始位置移到beginIndex位置 randomFile.seek(beginIndex); byte[] bytes = new byte[10]; int byteread = 0; // 一次读10个字节，如果文件内容不足10个字节，则读剩下的字节。 // 将一次读取的字节数赋给byteread while ((byteread = randomFile.read(bytes)) != -1) &#123; System.out.write(bytes, 0, byteread); &#125; &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125;finally &#123; if (randomFile != null) try &#123; randomFile.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; /** * 显示输入流中剩余的字节数 * @param in */ public static void showAvailableBytes(InputStream in) &#123; try &#123; System.out.println(\"当前字节流输入流中剩余的字节数为:\"+in.available()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; String fileName = \"C:\\\\Users\\\\10412\\\\Desktop\\\\1.txt\"; //文本文件 //String fileName = \"C:\\\\Users\\\\10412\\\\Desktop\\\\sp20161227_204413.png\"; //图片文件 //readFileByBytes(fileName); //readFileByChars(fileName); //readFileByLines(fileName); readFileBRandomAccess(fileName); &#125;&#125; 文件追加内容AppendToFile.java12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576package cn.zhisheng.io;import java.io.FileNotFoundException;import java.io.FileWriter;import java.io.IOException;import java.io.RandomAccessFile;/** * 追加内容到文件尾部 * Created by 10412 on 2016/12/29. */public class AppendToFile&#123; /** * 第一种方法追加文件：使用RandomAccessFile * @param fileName 文件名 * @param content 追加内容 */ public static void appendMethod1(String fileName, String content) &#123; try &#123; // 打开一个随机访问文件流，按读写方式 RandomAccessFile randomFile = new RandomAccessFile(fileName, \"rw\"); // 文件长度，字节数 long fileLength = randomFile.length(); //将写文件指针移到文件尾 randomFile.seek(fileLength); randomFile.writeBytes(content); randomFile.close(); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; /** * 第二种方法追加文件：使用FileWriter * @param fileName 文件名 * @param content 追加内容 */ public static void appendMethod2(String fileName, String content) &#123; try &#123; //打开一个写文件器，构造函数中的第二个参数true表示以追加形式写文件 FileWriter writer = new FileWriter(fileName, true); writer.write(content); writer.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; String fileName = \"C:\\\\Users\\\\10412\\\\Desktop\\\\1.txt\"; //文本文件 String content = \"new append!\"; //按方法1追加文件// AppendToFile.appendMethod1(fileName, content);// AppendToFile.appendMethod1(fileName, \"\\new append. 第一种方法\\n\"); //按照方法2追加文件 AppendToFile.appendMethod2(fileName, content); AppendToFile.appendMethod2(fileName, \"\\nnew append. 第二种方法\\n\"); //显示文件内容 ReadFromFile.readFileByLines(fileName); &#125;&#125;","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"文件","slug":"文件","permalink":"http://www.54tianzhisheng.cn/tags/文件/"}]},{"title":"利用Github Page 搭建个人博客网站","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/利用Github Page 搭建个人博客网站/","text":"前言最近这几天，没事干，想找点事折腾下，于是自己便想到了自己一直想干的一件事：搭建一个属于自己的博客网站。目前搭建个人 blog 网站最好的是用 wordpress ，但是那个折腾起来好像还挺麻烦的，再加上还需要自己修改些前端代码和用 PHP 做（虽然我学了几天拍黄片，但是早已忘了），然后就是用 Github Page 吧，自己也一直在这个最大的交友网站装 X 。想想就用这个吧（后来好像觉得这个还挺省事的） 再说说拥有个人博客网站的好处吧： 装 X（如果网站够炫） 很好的用来总结自己所学的知识 面试加分（在简历上放上自己的个人网站链接，面试官就可以更好的了解你，知道你所学知识的深度和广度） 不再受其他博客平台的规则所束缚 如果你现在还没有自己个人博客网站的话，那么我觉得你看完本篇博客后，强烈的建议你去折腾折腾下，搞个自己的，让自己也能够体验装 X 的感觉。 要想用搭建一个个人博客网站，首先你得有一个域名，这样别人才可以通过域名访问，其次你还要一个空间来存放你的页面。 域名 域名的话，你可以在万网、阿里云、腾讯云等注册，我的域名 www.54tianzhisheng.cn 就是在腾讯云注册的，记得是腾讯云一元钱（一个域名+主机）搞的，这是腾讯云对学生才有这优惠。 .cn 的域名需要备案，备案的审核速度我觉得还是挺快的，还需要上传证件。当然你也可以买其他的那些不需要备案的域名，省得麻烦事。 空间 空间有免费的空间，也有收费的空间。免费的当然就不够稳定了，收费的就很贵了，终究是很不爽，有没有什么地方是既免费又稳定的空间呢？有，Github 。它允许上传个人网站项目并自定义你的域名，而且又有稳定的服务，实在是不能够在好了。 下面就一起跟着我来一步一步的利用 Github 搭建个人博客网站吧！ 1. 拥有一个域名这个步骤我就不详述了。 举例： 打开腾讯云官网 搜索你想要的域名，下单买一个 2. 拥有一个 Github账号互联网崇尚自由与分享。Github 是一个全世界程序员聚集的地方，大家相互分享自己写的代码，提升别人，也提升自己。大家都在为着开源社区努力着。因为我从开源项目中学到很多知识，所以我也非常愿意分享我的所见所学所得，我的 Github 主页：https://github.com/zhisheng17 （欢迎 follow 和对我的项目给个 star 或者 fork 我的项目一起来和我完善项目） 如果还没有 Github 账号的话你就先去注册一个吧，有的话，直接登录就行，后面的操作都要用到 Github 的。 3. Github 上新建个人网站项目登录 GitHub 之后，在页面右上角点击 + 加号按钮，点击 New repository。 由于我们是新建一个个人网站项目，所有仓库的名称需要安装 GitHub 个人网站项目的规定来写。 规则就是： YOUR-GITHUB-USERNAME.github.io 比如我的 GitHub 用户名是 zhisheng17，那我就要填写 zhisheng17.github.io。然后选择公开模式，接着点击创建仓库按钮。 创建成功之后，进入了项目主页面。点击设置按钮。 进入之后，滚动页面到下方。点击页面自动生成器按钮。 点击右下方继续去布局按钮。 选择一个模板，点击发布页面按钮。 这个时候，你就可以通过YOUR-GITHUB-USERNAME.github.io来访问此页面了。 4. 上传个人网页到 Github自动生成页面，肯定不符合我们的要求，我们希望能够自己设计自己的个人网站。我们可以自己编写一个网页文件，命名为 index.html。然后上传到 GitHub个人网站项目上。这里为了节约时间，可以先下载我的个人网站项目代码，然后修改为你的网页上传到 GitHub。 下面介绍详细步骤。 进入此项目https://github.com/zhisheng17/zhisheng17.github.io，然后下载源码。解压之后，拿到里面的index.html文件。 然后进入自己的个人网站项目主页 YOUR-GITHUB-USERNAME/YOUR-GITHUB-USERNAME.github.io。点击上传文件按钮，进入上传文件页面，将 index.html 文件拖入蓝色大圈圈区域，点击提交按钮即可提交成功。此时打开网址 YOUR-GITHUB-USERNAME.github.io 就可以看到主页已经改变为我们自己的网页了。 通过 zhisheng17.github.io 查看效果： 5. 域名CNAME到个人网站项目网页上传成功了，我们不想一直通过YOUR-GITHUB-USERNAME.github.io来访问我们的个人网站，而是希望通过自己的域名来访问。 下面讲述详细步骤。 点击我们的个人网站项目设置选项卡，滚动到下面，就会发现一个自定义域名卡片。输入我们买的域名，然后点击保存。 接着我们还要将我们的域名解析到这个个人网站项目上。因为我的域名是在腾讯云上面买的，所以我打开腾讯云域名管理页面，进行相关的设置。 接着，点击添加一条域名解析记录，主机填写www，代表你是一级域名来访问，指向填写YOUR-GITHUB-USERNAME.github.io，然后点击保存按钮。应该要等会，域名的解析时间可能不一样，我的腾讯云就是很慢的 6. 访问你的域名所有这些步骤做完之后，在浏览器里输入自己的域名，回车键一按，就会返回我们刚刚上传到 GitHub 的index.html 页面了。 这里只是入门了 GitHub 搭建个人网站的功能，GitHub 官方推荐 Jekyll 博客系统来发布自己的页面。以后有数据更新，都可以通过 Jekyll 来重新编译整个网站。（期待后续我的使用 Jekyll 博客系统发布自己博客的文章吧） 7. 注意事项尽管GitHub个人网站项目是免费的，但是却有一些限制。总体来说，完全够用，甚至太多了。 单个仓库大小不超过1GB，上传单个文件大小不能超过100MB，如果通过浏览器上传不能超过25MB 个人网站项目也不例外，最大空间1GB 个人网站项目每个月访问请求数不能超过10万次，总流量不能超过100GB 个人网站项目一小时创建数量不能超过10个 当然了，这些政策可能随时改变，可以通过此网页查看最新政策。 https://help.github.com/articles/what-is-github-pages/#recommended-limits 新增由于问题太多了，所以新写了篇文章：Github page + Hexo + yilia 搭建博客可能会遇到的所有疑问","tags":[{"name":"Github Page","slug":"Github-Page","permalink":"http://www.54tianzhisheng.cn/tags/Github-Page/"},{"name":"博客网站","slug":"博客网站","permalink":"http://www.54tianzhisheng.cn/tags/博客网站/"}]},{"title":"解决jdk1.8中发送邮件失败（handshake_failure）问题","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/解决jdk1.8中发送邮件失败（handshake_failure）问题/","text":"暑假在家做一个类似知乎的问答型网站（代码可见：Github/wenda 喜欢的可以给个star或者自己fork然后修改，目前功能还未很完善），其中有一个站内邮件通知系统（这里简单的讲一个例子：如果用户登录的时候出现异常，那么就会通过邮件发送通知用户）。然而却碰到一个问题。问题错误信息如下： 发送邮件失败Mail server connection failed; nested exception is javax.mail.MessagingException: Could not connect to SMTP host: smtp.qq.com, port: 465;nested exception is: javax.net.ssl.SSLHandshakeException: Received fatal alert: handshake_failure. Failed messages: javax.mail. MessagingException: Could not connect to SMTP host: smtp.qq.com, port: 465;nested exception is: javax.net.ssl.SSLHandshakeException: Received fatal alert: handshake_failure 自己在将错误信息代码google了一下，找了很久发现很多解决方案，包括stackoverflow上的一些解决方案，但还是没用。然后呢用百度试了下，结果在第一条是开源中国的一篇博客:javax.net.ssl.SSLHandshakeException: Received fatal alert: handshake_failure。 点进去是这样的：（如下图） 结果就是：这个问题是jdk导致的，jdk1.8里面有一个jce的包，安全性机制导致的访问https会报错，官网上有替代的jar包，如果替换掉就可以了。问题的解决方法还可以就是在整个项目中把你的jdk换成是1.7去，同样也可以解决这个我问题。 这两个jar包的下载地址：http://www.oracle.com/technetwork/java/javase/downloads/jce-7-download-432124.html 然后下载之后，把这个压缩文件解压，得到两个jar包去覆盖jdk安装目录下的jre\\lib\\security\\下相同的jar包就能解决java8的邮件发送问题。 接着用QQ邮箱我亲测有用，但是要注意一点就是：开启SMTP服务后要记得将你的16位授权码作为你的qq邮箱登录密码。 MailSender.java中mailSender.setPassword(“16位授权码”); mailSender.setHost(“smtp.qq.com”);mailSender.setPort(465); 下面把完整代码发布出来： 1. LoginExceptionHandler.java 12345678910111213141516171819202122232425262728293031323334package com.nowcoder.async.handler;import com.nowcoder.async.EventHandler;import com.nowcoder.async.EventModel;import com.nowcoder.async.EventType;import com.nowcoder.util.MailSender;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;import java.util.Arrays;import java.util.HashMap;import java.util.List;import java.util.Map;/** * Created by 10412 on 2016/8/10. */@Componentpublic class LoginExceptionHandler implements EventHandler&#123; @Autowired MailSender mailSender; @Override public void doHandle(EventModel model) &#123; // xxxx判断发现这个用户登陆异常 Map&lt;String, Object&gt; map = new HashMap&lt;String, Object&gt;(); map.put(\"username\", model.getExt(\"username\")); mailSender.sendWithHTMLTemplate(model.getExt(\"email\"), \"登陆IP异常\", \"mails/login_exception.html\", map); &#125; @Override public List&lt;EventType&gt; getSupportEventTypes() &#123; return Arrays.asList(EventType.LOGIN); &#125;&#125; 2. LoginController.java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114package com.nowcoder.controller;import com.nowcoder.async.EventModel;import com.nowcoder.async.EventProducer;import com.nowcoder.async.EventType;import com.nowcoder.service.UserService;import org.apache.commons.lang.StringUtils;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Controller;import org.springframework.ui.Model;import org.springframework.web.bind.annotation.CookieValue;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestMethod;import org.springframework.web.bind.annotation.RequestParam;import javax.servlet.http.Cookie;import javax.servlet.http.HttpServletResponse;import java.util.Map;/** * Created by 10412 on 2016/7/2. */@Controllerpublic class LoginController &#123; private static final Logger logger = LoggerFactory.getLogger(LoginController.class); @Autowired UserService userService; @Autowired EventProducer eventProducer; @RequestMapping(path = &#123;&quot;/reg/&quot;&#125;, method = &#123;RequestMethod.POST&#125;) public String reg(Model model, @RequestParam(&quot;username&quot;) String username, @RequestParam(&quot;password&quot;) String password, @RequestParam(&quot;next&quot;) String next, @RequestParam(value=&quot;rememberme&quot;, defaultValue = &quot;false&quot;) boolean rememberme, HttpServletResponse response) &#123; try &#123; Map&lt;String, Object&gt; map = userService.register(username, password); if (map.containsKey(&quot;ticket&quot;)) &#123; Cookie cookie = new Cookie(&quot;ticket&quot;, map.get(&quot;ticket&quot;).toString()); cookie.setPath(&quot;/&quot;); if (rememberme) &#123; cookie.setMaxAge(3600*24*5); &#125; response.addCookie(cookie); if (StringUtils.isNotBlank(next)) &#123; return &quot;redirect:&quot; + next; &#125; return &quot;redirect:/&quot;; &#125; else &#123; model.addAttribute(&quot;msg&quot;, map.get(&quot;msg&quot;)); return &quot;login&quot;; &#125; &#125; catch (Exception e) &#123; logger.error(&quot;注册异常&quot; + e.getMessage()); model.addAttribute(&quot;msg&quot;, &quot;服务器错误&quot;); return &quot;login&quot;; &#125; &#125; @RequestMapping(path = &#123;&quot;/reglogin&quot;&#125;, method = &#123;RequestMethod.GET&#125;) public String regloginPage(Model model, @RequestParam(value = &quot;next&quot;, required = false) String next) &#123; model.addAttribute(&quot;next&quot;, next); return &quot;login&quot;; &#125; @RequestMapping(path = &#123;&quot;/login/&quot;&#125;, method = &#123;RequestMethod.POST&#125;) public String login(Model model, @RequestParam(&quot;username&quot;) String username, @RequestParam(&quot;password&quot;) String password, @RequestParam(value=&quot;next&quot;, required = false) String next, @RequestParam(value=&quot;rememberme&quot;, defaultValue = &quot;false&quot;) boolean rememberme, HttpServletResponse response) &#123; try &#123; Map&lt;String, Object&gt; map = userService.login(username, password); if (map.containsKey(&quot;ticket&quot;)) &#123; Cookie cookie = new Cookie(&quot;ticket&quot;, map.get(&quot;ticket&quot;).toString()); cookie.setPath(&quot;/&quot;); if (rememberme) &#123; cookie.setMaxAge(3600*24*5); &#125; response.addCookie(cookie); eventProducer.fireEvent(new EventModel(EventType.LOGIN) .setExt(&quot;username&quot;, username).setExt(&quot;email&quot;, &quot;****@qq.com&quot;) .setActorId((int)map.get(&quot;userId&quot;))); if (StringUtils.isNotBlank(next)) &#123; return &quot;redirect:&quot; + next; &#125; return &quot;redirect:/&quot;; &#125; else &#123; model.addAttribute(&quot;msg&quot;, map.get(&quot;msg&quot;)); return &quot;login&quot;; &#125; &#125; catch (Exception e) &#123; logger.error(&quot;登陆异常&quot; + e.getMessage()); return &quot;login&quot;; &#125; &#125; @RequestMapping(path = &#123;&quot;/logout&quot;&#125;, method = &#123;RequestMethod.GET, RequestMethod.POST&#125;) public String logout(@CookieValue(&quot;ticket&quot;) String ticket) &#123; userService.logout(ticket); return &quot;redirect:/&quot;; &#125;&#125; 3. EventHandler.java1234567891011121314package com.nowcoder.async;import java.util.List;/** * Created by 10412 on 2016/8/10. */public interface EventHandler&#123; void doHandle(EventModel model); List&lt;EventType&gt; getSupportEventTypes();&#125; 4. MailSender.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970package com.nowcoder.util;import org.apache.velocity.app.VelocityEngine;import org.slf4j.Logger;import org.slf4j.LoggerFactory;import org.springframework.beans.factory.InitializingBean;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.mail.javamail.JavaMailSenderImpl;import org.springframework.mail.javamail.MimeMessageHelper;import org.springframework.stereotype.Service;import org.springframework.ui.velocity.VelocityEngineUtils;import javax.mail.internet.InternetAddress;import javax.mail.internet.MimeMessage;import javax.mail.internet.MimeUtility;import java.util.Map;import java.util.Properties;/** * Created by 10412 on 2016/8/10. // ***@qq.com wnppafhsbrcgbfbh（16位授权码） */@Servicepublic class MailSender implements InitializingBean &#123; private static final Logger logger = LoggerFactory.getLogger(MailSender.class); private JavaMailSenderImpl mailSender; @Autowired private VelocityEngine velocityEngine; public boolean sendWithHTMLTemplate(String to, String subject, String template, Map&lt;String, Object&gt; model) &#123; try &#123; String nick = MimeUtility.encodeText(\"***\"); InternetAddress from = new InternetAddress(nick + \"&lt;****@qq.com&gt;\"); MimeMessage mimeMessage = mailSender.createMimeMessage(); MimeMessageHelper mimeMessageHelper = new MimeMessageHelper(mimeMessage); String result = VelocityEngineUtils .mergeTemplateIntoString(velocityEngine, template, \"UTF-8\", model); mimeMessageHelper.setTo(to); mimeMessageHelper.setFrom(from); mimeMessageHelper.setSubject(subject); mimeMessageHelper.setText(result, true); mailSender.send(mimeMessage); return true; &#125; catch (Exception e) &#123; logger.error(\"发送邮件失败\" + e.getMessage()); return false; &#125; &#125; @Override public void afterPropertiesSet() throws Exception &#123; mailSender = new JavaMailSenderImpl(); mailSender.setUsername(\"***@qq.com\"); mailSender.setPassword(\"wnppafhsbrcgbfbh\"); //qq邮箱开启smtp服务后使用16位授权码在第三方登录// mailSender.setHost(\"smtp.exmail.qq.com\"); mailSender.setHost(\"smtp.qq.com\"); mailSender.setPort(465);// mailSender.setHost(\"smtp.163.com\"); //163邮箱// mailSender.setPort(25); mailSender.setProtocol(\"smtps\"); mailSender.setDefaultEncoding(\"utf8\"); Properties javaMailProperties = new Properties(); javaMailProperties.put(\"mail.smtp.ssl.enable\", true); //javaMailProperties.put(\"mail.smtp.auth\", true); //javaMailProperties.put(\"mail.smtp.starttls.enable\", true); mailSender.setJavaMailProperties(javaMailProperties); &#125;&#125; 5. login_exception.html 发送消息模板（可自定义） 1你好$username，你的登陆有问题! 一切都好了，运行。 登录。 发送邮件过来了。 总结来说：这个错误就是jdk1.8中的一个jce的包，安全性机制导致访问https会报错。","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"邮件发送","slug":"邮件发送","permalink":"http://www.54tianzhisheng.cn/tags/邮件发送/"}]},{"title":"HashMap、Hashtable、HashSet 和 ConcurrentHashMap 的比较","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/HashMap-Hashtable/","text":"HashMap 和 Hashtable 的比较是 Java 面试中的常见问题，用来考验程序员是否能够正确使用集合类以及是否可以随机应变使用多种思路解决问题。HashMap 的工作原理、ArrayList 与 Vector 的比较以及这个问题是有关 Java 集合框架的最经典的问题。Hashtable 是个过时的集合类，存在于 Java API 中很久了。在 Java 4 中被重写了，实现了 Map 接口，所以自此以后也成了 Java 集合框架中的一部分。Hashtable 和 HashMap 在 Java 面试中相当容易被问到，甚至成为了集合框架面试题中最常被考的问题，所以在参加任何 Java 面试之前，都不要忘了准备这一题。这篇文章中，我们不仅将会看到 HashMap 和 Hashtable 的区别，还将看到它们之间的相似之处。 HashMap 和 Hashtable 的区别HashMap 和 Hashtable 都实现了 Map 接口，但决定用哪一个之前先要弄清楚它们之间的分别。主要的区别有：线程安全性，同步 (synchronization)，以及速度。 HashMap 几乎可以等价于 Hashtable，除了 HashMap 是非 synchronized 的，并可以接受 null(HashMap 可以接受为 null 的键值 (key) 和值 (value)，而 Hashtable 则不行)。 HashMap 是非 synchronized，而 Hashtable 是 synchronized，这意味着 Hashtable 是线程安全的，多个线程可以共享一个 Hashtable；而如果没有正确的同步的话，多个线程是不能共享 HashMap 的。Java 5 提供了 ConcurrentHashMap，它是 HashTable 的替代，比 HashTable 的扩展性更好。 另一个区别是 HashMap 的迭代器 (Iterator) 是 fail-fast 迭代器，而 Hashtable 的 enumerator 迭代器不是 fail-fast 的。所以当有其它线程改变了 HashMap 的结构（增加或者移除元素），将会抛出ConcurrentModificationException，但迭代器本身的 remove() 方法移除元素则不会抛出ConcurrentModificationException 异常。但这并不是一个一定发生的行为，要看 JVM。这条同样也是Enumeration 和 Iterato r的区别。 由于 Hashtable 是线程安全的也是 synchronized，所以在单线程环境下它比 HashMap 要慢。如果你不需要同步，只需要单一线程，那么使用 HashMap 性能要好过 Hashtable。 HashMap 不能保证随着时间的推移 Map 中的元素次序是不变的。 要注意的一些重要术语：1) sychronized 意味着在一次仅有一个线程能够更改 Hashtable。就是说任何线程要更新 Hashtable 时要首先获得同步锁，其它线程要等到同步锁被释放之后才能再次获得同步锁更新 Hashtable。 2) Fail-safe 和 iterator 迭代器相关。如果某个集合对象创建了 Iterator 或者 ListIterator，然后其它的线程试图“结构上”更改集合对象，将会抛出 ConcurrentModificationException 异常。但其它线程可以通过 set() 方法更改集合对象是允许的，因为这并没有从“结构上”更改集合。但是假如已经从结构上进行了更改，再调用 set() 方法，将会抛出 IllegalArgumentException 异常。 3) 结构上的更改指的是删除或者插入一个元素，这样会影响到 map 的结构。 我们能否让 HashMap 同步？HashMap 可以通过下面的语句进行同步：Map m = Collections.synchronizeMap(hashMap); 结论Hashtable 和 HashMap 有几个主要的不同：线程安全以及速度。仅在你需要完全的线程安全的时候使用Hashtable，而如果你使用 Java 5 或以上的话，请使用 ConcurrentHashMap 吧。 转载自：HashMap和Hashtable的区别 关于 HashMap 线程不安全这一点，《Java并发编程的艺术》一书中是这样说的： HashMap 在并发执行 put 操作时会引起死循环，导致 CPU 利用率接近 100%。因为多线程会导致 HashMap 的 Node 链表形成环形数据结构，一旦形成环形数据结构，Node 的 next 节点永远不为空，就会在获取 Node 时产生死循环。 原因： 疫苗：JAVA HASHMAP的死循环 —— 酷壳 HashMap在java并发中如何发生死循环 How does a HashMap work in JAVA 下面的是自己有道云笔记中记录的： HashMap ， HashTable 和 HashSet 区别 关于 HashMap 的一些说法： a) HashMap 实际上是一个“链表散列”的数据结构，即数组和链表的结合体。HashMap 的底层结构是一个数组，数组中的每一项是一条链表。 b) HashMap 的实例有俩个参数影响其性能： “初始容量” 和 装填因子。 c) HashMap 实现不同步，线程不安全。 HashTable 线程安全 d) HashMap 中的 key-value 都是存储在 Entry 中的。 e) HashMap 可以存 null 键和 null 值，不保证元素的顺序恒久不变，它的底层使用的是数组和链表，通过hashCode() 方法和 equals 方法保证键的唯一性 f) 解决冲突主要有三种方法：定址法，拉链法，再散列法。HashMap 是采用拉链法解决哈希冲突的。 注： 链表法是将相同 hash 值的对象组成一个链表放在 hash 值对应的槽位； 用开放定址法解决冲突的做法是：当冲突发生时，使用某种探查(亦称探测)技术在散列表中形成一个探查(测)序列。 沿此序列逐个单元地查找，直到找到给定 的关键字，或者碰到一个开放的地址(即该地址单元为空)为止（若要插入，在探查到开放的地址，则可将待插入的新结点存人该地址单元）。 拉链法解决冲突的做法是： 将所有关键字为同义词的结点链接在同一个单链表中 。若选定的散列表长度为m，则可将散列表定义为一个由m个头指针组成的指针数 组T[0..m-1]。凡是散列地址为i的结点，均插入到以T[i]为头指针的单链表中。T中各分量的初值均应为空指针。在拉链法中，装填因子α可以大于1，但一般均取α≤1。拉链法适合未规定元素的大小。 Hashtable 和 HashMap 的区别： a) 继承不同。 public class Hashtable extends Dictionary implements Map public class HashMap extends AbstractMap implements Map b) Hashtable 中的方法是同步的，而 HashMap 中的方法在缺省情况下是非同步的。在多线程并发的环境下，可以直接使用 Hashtable，但是要使用 HashMap 的话就要自己增加同步处理了。 c) Hashtable 中， key 和 value 都不允许出现 null 值。 在 HashMap 中， null 可以作为键，这样的键只有一个；可以有一个或多个键所对应的值为 null 。当 get() 方法返回 null 值时，即可以表示 HashMap 中没有该键，也可以表示该键所对应的值为 null 。因此，在 HashMap 中不能由 get() 方法来判断 HashMap 中是否存在某个键， 而应该用 containsKey() 方法来判断。 d) 两个遍历方式的内部实现上不同。Hashtable、HashMap 都使用了Iterator。而由于历史原因，Hashtable还使用了 Enumeration 的方式 。 e) 哈希值的使用不同，HashTable 直接使用对象的 hashCode。而 HashMap 重新计算 hash 值。 f) Hashtable 和 HashMap 它们两个内部实现方式的数组的初始大小和扩容的方式。HashTable 中 hash 数组默认大小是11，增加的方式是 old*2+1。HashMap 中 hash 数组的默认大小是 16，而且一定是2的指数。 注： HashSet 子类依靠 hashCode() 和 equal() 方法来区分重复元素。 HashSet 内部使用 Map 保存数据，即将 HashSet 的数据作为 Map 的 key 值保存，这也是 HashSet 中元素不能重复的原因。而 Map 中保存 key 值的,会去判断当前 Map 中是否含有该 Key 对象，内部是先通过 key 的hashCode, 确定有相同的 hashCode 之后，再通过 equals 方法判断是否相同。 《HashMap 的工作原理》 HashMap的工作原理是近年来常见的Java面试题。几乎每个Java程序员都知道HashMap，都知道哪里要用HashMap，知道 Hashtable和HashMap之间的区别，那么为何这道面试题如此特殊呢？是因为这道题考察的深度很深。这题经常出现在高级或中高级面试中。投资银行更喜欢问这个问题，甚至会要求你实现HashMap来考察你的编程能力。ConcurrentHashMap和其它同步集合的引入让这道题变得更加复杂。让我们开始探索的旅程吧！ 先来些简单的问题“你用过HashMap吗？” “什么是HashMap？你为什么用到它？” 几乎每个人都会回答“是的”，然后回答HashMap的一些特性，譬如HashMap可以接受null键值和值，而Hashtable则不能；HashMap是非synchronized;HashMap很快；以及HashMap储存的是键值对等等。这显示出你已经用过HashMap，而且对它相当的熟悉。但是面试官来个急转直下，从此刻开始问出一些刁钻的问题，关于HashMap的更多基础的细节。面试官可能会问出下面的问题： “你知道HashMap的工作原理吗？” “你知道HashMap的get()方法的工作原理吗？” 你也许会回答“我没有详查标准的Java API，你可以看看Java源代码或者Open JDK。”“我可以用Google找到答案。” 但一些面试者可能可以给出答案，“HashMap是基于hashing的原理，我们使用put(key, value)存储对象到HashMap中，使用get(key)从HashMap中获取对象。当我们给put()方法传递键和值时，我们先对键调用hashCode()方法，返回的hashCode用于找到bucket位置来储存Entry对象。”这里关键点在于指出，HashMap是在bucket中储存键对象和值对象，作为Map.Entry。这一点有助于理解获取对象的逻辑。如果你没有意识到这一点，或者错误的认为仅仅只在bucket中存储值的话，你将不会回答如何从HashMap中获取对象的逻辑。这个答案相当的正确，也显示出面试者确实知道hashing以及HashMap的工作原理。但是这仅仅是故事的开始，当面试官加入一些Java程序员每天要碰到的实际场景的时候，错误的答案频现。下个问题可能是关于HashMap中的碰撞探测(collision detection)以及碰撞的解决方法： “当两个对象的hashcode相同会发生什么？” 从这里开始，真正的困惑开始了，一些面试者会回答因为hashcode相同，所以两个对象是相等的，HashMap将会抛出异常，或者不会存储它们。然后面试官可能会提醒他们有equals()和hashCode()两个方法，并告诉他们两个对象就算hashcode相同，但是它们可能并不相等。一些面试者可能就此放弃，而另外一些还能继续挺进，他们回答“因为hashcode相同，所以它们的bucket位置相同，‘碰撞’会发生。因为HashMap使用链表存储对象，这个Entry(包含有键值对的Map.Entry对象)会存储在链表中。”这个答案非常的合理，虽然有很多种处理碰撞的方法，这种方法是最简单的，也正是HashMap的处理方法。但故事还没有完结，面试官会继续问： “如果两个键的hashcode相同，你如何获取值对象？” 面试者会回答：当我们调用get()方法，HashMap会使用键对象的hashcode找到bucket位置，然后获取值对象。面试官提醒他如果有两个值对象储存在同一个bucket，他给出答案:将会遍历链表直到找到值对象。面试官会问因为你并没有值对象去比较，你是如何确定确定找到值对象的？除非面试者直到HashMap在链表中存储的是键值对，否则他们不可能回答出这一题。 其中一些记得这个重要知识点的面试者会说，找到bucket位置之后，会调用keys.equals()方法去找到链表中正确的节点，最终找到要找的值对象。完美的答案！ 许多情况下，面试者会在这个环节中出错，因为他们混淆了hashCode()和equals()方法。因为在此之前hashCode()屡屡出现，而equals()方法仅仅在获取值对象的时候才出现。一些优秀的开发者会指出使用不可变的、声明作final的对象，并且采用合适的equals()和hashCode()方法的话，将会减少碰撞的发生，提高效率。不可变性使得能够缓存不同键的hashcode，这将提高整个获取对象的速度，使用String，Interger这样的wrapper类作为键是非常好的选择。 如果你认为到这里已经完结了，那么听到下面这个问题的时候，你会大吃一惊。 “如果HashMap的大小超过了负载因子(load factor)定义的容量，怎么办？” 除非你真正知道HashMap的工作原理，否则你将回答不出这道题。默认的负载因子大小为0.75，也就是说，当一个map填满了75%的bucket时候，和其它集合类(如ArrayList等)一样，将会创建原来HashMap大小的两倍的bucket数组，来重新调整map的大小，并将原来的对象放入新的bucket数组中。这个过程叫作rehashing，因为它调用hash方法找到新的bucket位置。 如果你能够回答这道问题，下面的问题来了： “你了解重新调整HashMap大小存在什么问题吗？” 你可能回答不上来，这时面试官会提醒你当多线程的情况下，可能产生条件竞争(race condition)。 当重新调整HashMap大小的时候，确实存在条件竞争，因为如果两个线程都发现HashMap需要重新调整大小了，它们会同时试着调整大小。在调整大小的过程中，存储在链表中的元素的次序会反过来，因为移动到新的bucket位置的时候，HashMap并不会将元素放在链表的尾部，而是放在头部，这是为了避免尾部遍历(tail traversing)。如果条件竞争发生了，那么就死循环了。这个时候，你可以质问面试官，为什么这么奇怪，要在多线程的环境下使用HashMap呢？：） 热心的读者贡献了更多的关于HashMap的问题： 为什么String, Interger这样的wrapper类适合作为键？ String, Interger这样的wrapper类作为HashMap的键是再适合不过了，而且String最为常用。因为String是不可变的，也是final的，而且已经重写了equals()和hashCode()方法了。其他的wrapper类也有这个特点。不可变性是必要的，因为为了要计算hashCode()，就要防止键值改变，如果键值在放入时和获取时返回不同的hashcode的话，那么就不能从HashMap中找到你想要的对象。不可变性还有其他的优点如线程安全。如果你可以仅仅通过将某个field声明成final就能保证hashCode是不变的，那么请这么做吧。因为获取对象的时候要用到equals()和hashCode()方法，那么键对象正确的重写这两个方法是非常重要的。如果两个不相等的对象返回不同的hashcode的话，那么碰撞的几率就会小些，这样就能提高HashMap的性能。 我们可以使用自定义的对象作为键吗？ 这是前一个问题的延伸。当然你可能使用任何对象作为键，只要它遵守了equals()和hashCode()方法的定义规则，并且当对象插入到Map中之后将不会再改变了。如果这个自定义对象时不可变的，那么它已经满足了作为键的条件，因为当它创建之后就已经不能改变了。 我们可以使用CocurrentHashMap来代替Hashtable吗？ 这是另外一个很热门的面试题，因为ConcurrentHashMap越来越多人用了。我们知道Hashtable是synchronized的，但是ConcurrentHashMap同步性能更好，因为它仅仅根据同步级别对map的一部分进行上锁。ConcurrentHashMap当然可以代替HashTable，但是HashTable提供更强的线程安全性。看看 这篇博客 查看Hashtable和ConcurrentHashMap的区别。 我个人很喜欢这个问题，因为这个问题的深度和广度，也不直接的涉及到不同的概念。让我们再来看看这些问题设计哪些知识点： hashing的概念 HashMap中解决碰撞的方法 equals()和hashCode()的应用，以及它们在HashMap中的重要性 不可变对象的好处 HashMap多线程的条件竞争 重新调整HashMap的大小 总结HashMap的工作原理HashMap基于hashing原理，我们通过put()和get()方法储存和获取对象。当我们将键值对传递给put()方法时，它调用键对象的hashCode()方法来计算hashcode，让后找到bucket位置来储存值对象。当获取对象时，通过键对象的equals()方法找到正确的键值对，然后返回值对象。HashMap使用链表来解决碰撞问题，当发生碰撞了，对象将会储存在链表的下一个节点中。 HashMap在每个链表节点中储存键值对对象。 当两个不同的键对象的hashcode相同时会发生什么？ 它们会储存在同一个bucket位置的链表中。键对象的equals()方法用来找到键值对。 因为HashMap的好处非常多，我曾经在电子商务的应用中使用HashMap作为缓存。因为金融领域非常多的运用Java，也出于性能的考虑，我们会经常用到HashMap和ConcurrentHashMap。你可以查看更多的关于HashMap的文章: HashMap和Hashtable的区别 HashMap和HashSet的区别 转载自：HashMap的工作原理 其他的 HashMap 学习资料： jdk7中HashMap知识点整理 HashMap源码分析（四）put-jdk8-红黑树的引入 JDK7与JDK8中HashMap的实现 JDK1.8HashMap原理和源码分析(java面试收藏) 谈谈ConcurrentHashMap1.7和1.8的不同实现 jdk1.8的HashMap和ConcurrentHashMap ConcurrentHashMap源码分析（JDK8版本） 最后谢谢阅读，如果可以的话欢迎大家转发和点赞。如需转载注明原地址就行。 群 528776268 欢迎各位大牛进群一起讨论。","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"}]},{"title":"JVM性能调优监控工具jps、jstack、jmap、jhat、jstat等使用详解","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/JVM性能调优监控工具jps、jstack、jmap、jhat、jstat等使用详解/","text":"javap 和 javac javac -verbose 类名.java java -verbose 类名 javap -c 类名 javap -verbose 类名 javap -help用法: javap 其中, 可能的选项包括: -help –help -? 输出此用法消息 -version 版本信息 -v -verbose 输出附加信息 -l 输出行号和本地变量表 -public 仅显示公共类和成员 -protected 显示受保护的/公共类和成员 -package 显示程序包/受保护的/公共类 和成员 (默认) -p -private 显示所有类和成员 -c 对代码进行反汇编 -s 输出内部类型签名 -sysinfo 显示正在处理的类的 系统信息 (路径, 大小, 日期, MD5 散列) -constants 显示最终常量 -classpath 指定查找用户类文件的位置 -cp 指定查找用户类文件的位置 -bootclasspath 覆盖引导类文件的位置 javac -help用法: javac 其中, 可能的选项包括: -g 生成所有调试信息 -g:none 不生成任何调试信息 -g:{lines,vars,source} 只生成某些调试信息 -nowarn 不生成任何警告 -verbose 输出有关编译器正在执行的操作的消息 -deprecation 输出使用已过时的 API 的源位置 -classpath &lt;路径&gt; 指定查找用户类文件和注释处理程序的位置 -cp &lt;路径&gt; 指定查找用户类文件和注释处理程序的位置 -sourcepath &lt;路径&gt; 指定查找输入源文件的位置 -bootclasspath &lt;路径&gt; 覆盖引导类文件的位置 -extdirs &lt;目录&gt; 覆盖所安装扩展的位置 -endorseddirs &lt;目录&gt; 覆盖签名的标准路径的位置 -proc:{none,only} 控制是否执行注释处理和/或编译。 -processor [,,…] 要运行的注释处理程序的名称; 绕过默认的搜索进程 -processorpath &lt;路径&gt; 指定查找注释处理程序的位置 -parameters 生成元数据以用于方法参数的反射 -d &lt;目录&gt; 指定放置生成的类文件的位置 -s &lt;目录&gt; 指定放置生成的源文件的位置 -h &lt;目录&gt; 指定放置生成的本机标头文件的位置 -implicit:{none,class} 指定是否为隐式引用文件生成类文件 -encoding &lt;编码&gt; 指定源文件使用的字符编码 -source &lt;发行版&gt; 提供与指定发行版的源兼容性 -target &lt;发行版&gt; 生成特定 VM 版本的类文件 -profile &lt;配置文件&gt; 请确保使用的 API 在指定的配置文件中可用 -version 版本信息 -help 输出标准选项的提要 -A关键字[=值] 传递给注释处理程序的选项 -X 输出非标准选项的提要 -J&lt;标记&gt; 直接将 &lt;标记&gt; 传递给运行时系统 -Werror 出现警告时终止编译 @&lt;文件名&gt; 从文件读取选项和文件名 jps用来查看基于HotSpot的JVM里面中，所有具有访问权限的Java进程的具体状态, 包括进程ID，进程启动的路径及启动参数等等，与unix上的ps类似，只不过jps是用来显示java进程，可以把jps理解为ps的一个子集。 使用jps时，如果没有指定hostid，它只会显示本地环境中所有的Java进程；如果指定了hostid，它就会显示指定hostid上面的java进程，不过这需要远程服务上开启了jstatd服务。 jps -helpusage: jps [-help] jps [-q] [-mlvV] [&lt;hostid&gt;] Definitions: &lt;hostid&gt;: &lt;hostname&gt;[:&lt;port&gt;] -q：忽略输出的类名、Jar名以及传递给main方法的参数，只输出pid。 -m：输出传递给main方法的参数，如果是内嵌的JVM则输出为null。 -l：输出完全的包名，应用主类名，jar的完全路径名 -v：输出传给jvm的参数 -V：输出通过标记的文件传递给JVM的参数（.hotspotrc文件，或者是通过参数-XX:Flags=指定的文件）。 -J 用于传递jvm选项到由javac调用的java加载器中，例如，“-J-Xms48m”将把启动内存设置为48M，使用-J选项可以非常方便的向基于Java的开发的底层虚拟机应用程序传递参数。 jstackjstack用于打印出给定的java进程ID或core file或远程调试服务的Java堆栈信息，如果是在64位机器上，需要指定选项”-J-d64”，Windows的jstack使用方式只支持以下的这种方式： jstack [-l] pid 如果java程序崩溃生成core文件，jstack工具可以用来获得core文件的java stack和native stack的信息，从而可以轻松地知道java程序是如何崩溃和在程序何处发生问题。另外，jstack工具还可以附属到正在运行的java程序中，看到当时运行的java程序的java stack和native stack的信息, 如果现在运行的java程序呈现hung的状态，jstack是非常有用的。 jstack -helpUsage: jstack [-l] &lt;pid&gt; (to connect to running process) jstack -F [-m] [-l] &lt;pid&gt; (to connect to a hung process) jstack [-m] [-l] &lt;executable&gt; &lt;core&gt; (to connect to a core file) jstack [-m] [-l] [server_id@]&lt;remote server IP or hostname&gt; (to connect to a remote debug server) Options: -F to force a thread dump. Use when jstack &lt;pid&gt; does not respond (process is hung)(当’jstack [-l] pid’没有相应的时候强制打印栈信息) -m to print both java and native frames (mixed mode)(打印java和native c/c++框架的所有栈信息.) -l long listing. Prints additional information about locks (长列表. 打印关于锁的附加信息,例如属于java.util.concurrent的ownable synchronizers列表.) -h or -help to print this help message (打印帮助信息) jstatJstat 用于监控基于HotSpot的JVM，对其堆的使用情况进行实时的命令行的统计，使用jstat我们可以对指定的JVM做如下监控： 类的加载及卸载情况 查看新生代、老生代及持久代的容量及使用情况 查看新生代、老生代及持久代的垃圾收集情况，包括垃圾回收的次数及垃圾回收所占用的时间 查看新生代中Eden区及Survior区中容量及分配情况等 jstat -help Usage: jstat -help|-options jstat -&lt;option&gt; [-t] [-h&lt;lines&gt;] &lt;vmid&gt; [&lt;interval&gt; [&lt;count&gt;]] Definitions:&gt; An option reported by the -options option Virtual Machine Identifier. A vmid takes the following form: [@[:]] Where is the local vm identifier for the target Java virtual machine, typically a process id; is the name of the host running the target Java virtual machine; and is the port number for the rmiregistry on the target host. See the jvmstat documentation for a more complete description of the Virtual Machine Identifier. Number of samples between header lines. Sampling interval. The following forms are allowed: [“ms”|”s”] Where is an integer and the suffix specifies the units as milliseconds(“ms”) or seconds(“s”). The default units are “ms”. Number of samples to take before terminating. -J Pass directly to the runtime system. 参考文章1、jstat命令详解 2、jstat命令(Java Virtual Machine Statistics Monitoring Tool) 3、http://docs.oracle.com/javase/1.5.0/docs/tooldocs/share/jstat.html#class_option jmap打印出某个java进程（使用pid）内存内的，所有‘对象’的情况（如：产生那些对象，及其数量）。 可以输出所有内存中对象的工具，甚至可以将VM 中的heap，以二进制输出成文本。使用方法 jmap -histo pid 如果连用SHELL jmap -histo pid&gt;a.log 可以将其保存到文本中去，在一段时间后，使用文本对比工具，可以对比出GC回收了哪些对象。 jmap -dump:format=b,file=outfile 3024 可以将3024进程的内存heap输出出来到outfile文件里，再配合MAT（内存分析工具(Memory Analysis Tool），使用参见：http://blog.csdn.net/fenglibing/archive/2011/04/02/6298326.aspx）或与jhat (Java Heap Analysis Tool)一起使用，能够以图像的形式直观的展示当前内存是否有问题。 64位机上使用需要使用如下方式： jmap -J-d64 -heap pid jmap -helpUsage: jmap [option] &lt;pid&gt; (to connect to running process) jmap [option] &lt;executable &lt;core&gt; (to connect to a core file) jmap [option] [server_id@]&lt;remote server IP or hostname&gt; (to connect to remote debug server) where is one of: &lt;none&gt; to print same info as Solaris pmap -heap to print java heap summary -histo[:live] to print histogram of java object heap; if the &quot;live&quot; suboption is specified, only count live objects -clstats to print class loader statistics -finalizerinfo to print information on objects awaiting finalization -dump:&lt;dump-options&gt; to dump java heap in hprof binary format dump-options: live dump only live objects; if not specified, all objects in the heap are dumped. format=b binary format file=&lt;file&gt; dump heap to &lt;file&gt; Example: jmap -dump:live,format=b,file=heap.bin &lt;pid&gt; -F force. Use with -dump:&lt;dump-options&gt; &lt;pid&gt; or -histo to force a heap dump or histogram when &lt;pid&gt; does not respond. The &quot;live&quot; suboption is not supported in this mode. -h | -help to print this help message -J&lt;flag&gt; to pass &lt;flag&gt; directly to the runtime system 参数说明 1)、options： executable Java executable from which the core dump was produced.(可能是产生core dump的java可执行程序) core 将被打印信息的core dump文件 remote-hostname-or-IP 远程debug服务的主机名或ip server-id 唯一id,假如一台主机上多个远程debug服务 2）、基本参数： -dump:[live,]format=b,file= 使用hprof二进制形式,输出jvm的heap内容到文件=. live子选项是可选的，假如指定live选项,那么只输出活的对象到文件. -finalizerinfo 打印正等候回收的对象的信息. -heap 打印heap的概要信息，GC使用的算法，heap的配置及wise heap的使用情况. -histo[:live] 打印每个class的实例数目,内存占用,类全名信息. VM的内部类名字开头会加上前缀”*”. 如果live子参数加上后,只统计活的对象数量. -permstat 打印classload和jvm heap长久层的信息. 包含每个classloader的名字,活泼性,地址,父classloader和加载的class数量. 另外,内部String的数量和占用内存数也会打印出来. -F 强迫.在pid没有相应的时候使用-dump或者-histo参数. 在这个模式下,live子参数无效. -h | -help 打印辅助信息 -J 传递参数给jmap启动的jvm. pid 需要被打印配相信息的java进程id,创业与打工的区别 - 博文预览,可以用jps查问. jinfojinfo 可以输出并修改运行时的java 进程的opts。 用处比较简单，用于输出JAVA系统参数及命令行参数。 用法是 jinfo -opt pid 如：查看2788的MaxPerm大小可以用 jinfo -flag MaxPermSize 2788。 jinfo -help Usage: jinfo [option] &lt;pid&gt; (to connect to running process) jinfo [option] &lt;executable &lt;core&gt; (to connect to a core file) jinfo [option] [server_id@]&lt;remote server IP or hostname&gt; (to connect to remote debug server) where is one of: -flag &lt;name&gt; to print the value of the named VM flag -flag [+|-]&lt;name&gt; to enable or disable the named VM flag -flag &lt;name&gt;=&lt;value&gt; to set the named VM flag to the given value -flags to print VM flags -sysprops to print Java system properties &lt;no option&gt; to print both of the above -h | -help to print this help message jconsole一个java GUI监视工具，可以以图表化的形式显示各种数据。并可通过远程连接监视远程的服务器VM。用java写的GUI程序，用来监控VM，并可监控远程的VM，非常易用，而且功能非常强。命令行里打 jconsole，选则进程就可以了。 需要注意的就是在运行jconsole之前，必须要先设置环境变量DISPLAY，否则会报错误，Linux下设置环境变量如下： export DISPLAY=:0.0 可以这里选择查看本地进程的状况，还是远程进程的状况 通过这张图可以看到内存、线程、类及CPU使用的一些情况。 jvisualvm参考文章： 程序员必备利器—Java程序性能分析工具Java VisualVM（Visual GC） jhat用于对JAVA heap进行离线分析的工具，他可以对不同虚拟机中导出的heap信息文件进行分析，如Linux上导出的文件可以拿到WINDOWS上进行分析，可以查找诸如内存方面的问题，使用方式可以查看这篇文章： jhat命令 不过jhat和MAT比较起来，就没有MAT那么直观了，MAT是以图形界面的方式展现结果，MAT的使用方式可以参看文章： MAT(Memory Analyzer Tool)工具入门介绍 Usage:jhat [-stack ] [-refs ] [-port ] [-baseline ] [-debug ] [-version] [-h|-help] -J&lt;flag&gt; Pass &lt;flag&gt; directly to the runtime system. For example, -J-mx512m to use a maximum heap size of 512MB -stack false: Turn off tracking object allocation call stack. -refs false: Turn off tracking of references to objects -port &lt;port&gt;: Set the port for the HTTP server. Defaults to 7000 -exclude &lt;file&gt;: Specify a file that lists data members that should be excluded from the reachableFrom query. -baseline &lt;file&gt;: Specify a baseline object dump. Objects in both heap dumps with the same ID and same class will be marked as not being &quot;new&quot;. -debug &lt;int&gt;: Set debug level. 0: No debug output 1: Debug hprof file parsing 2: Debug hprof file parsing, no server -version Report version number -h|-help Print this help and exit &lt;file&gt; The file to read jdb用来对core文件和正在运行的Java进程进行实时地调试，里面包含了丰富的命令帮助您进行调试，它的功能和Sun studio里面所带的dbx非常相似，但 jdb是专门用来针对Java应用程序的。 jstatdjstatd是一个基于RMI（Remove Method Invocation）的服务程序，它用于监控基于HotSpot的JVM中资源的创建及销毁，并且提供了一个远程接口允许远程的监控工具连接到本地的JVM执行命令。 jstatd是基于RMI的，所以在运行jstatd的服务器上必须存在RMI注册中心，如果没有通过选项”-p port”指定要连接的端口，jstatd会尝试连接RMI注册中心的默认端口。 用法： jstatd [-nr] [-p port] [-n rminame] -nr 如果RMI注册中心没有找到，不会创建一个内部的RMI注册中心。 -p port RMI注册中心的端口号，默认为1099。 -n rminame 默认为JStatRemoteHost；如果同一台主机上同时运行了多个jstatd服务，rminame可以用于唯一确定一个jstatd服务；这里需要注意一下，如果开启了这个选项，那么监控客户端远程连接时，必须同时指定hostid及vmid，才可以唯一确定要连接的服务，这个可以参看jps章节中列出远程服务器上Java进程的示例。 -J 用于传递jvm选项到由javac调用的java加载器中，例如，“-J-Xms48m”将把启动内存设置为48M，使用-J选项可以非常方便的向基于Java的开发的底层虚拟机应用程序传递参数。 参考文章 JDK内置工具使用","tags":[{"name":"JVM","slug":"JVM","permalink":"http://www.54tianzhisheng.cn/tags/JVM/"},{"name":"性能调优工具","slug":"性能调优工具","permalink":"http://www.54tianzhisheng.cn/tags/性能调优工具/"}]},{"title":"Python爬虫实战之爬取百度贴吧帖子","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/Python爬虫实战之爬取百度贴吧帖子/","text":"大家好，上次我们实验了爬取了糗事百科的段子，那么这次我们来尝试一下爬取百度贴吧的帖子。与上一篇不同的是，这次我们需要用到文件的相关操作。 本篇目标 对百度贴吧的任意帖子进行抓取 指定是否只抓取楼主发帖内容 将抓取到的内容分析并保存到文件 1. URL格式的确定首先，我们先观察一下百度贴吧的任意一个帖子。 比如：http://tieba.baidu.com/p/3138733512?see_lz=1&amp;pn=1，这是一个关于NBA50大的盘点，分析一下这个地址。 http:// 代表资源传输使用http协议 tieba.baidu.com 是百度的二级域名，指向百度贴吧的服务器。 /p/3138733512 是服务器某个资源，即这个帖子的地址定位符 see_lz 和 pn 是该 URL 的两个参数，分别代表了只看楼主和帖子页码，等于1表示该条件为真 所以我们可以把URL分为两部分，一部分为基础部分，一部分为参数部分。 例如，上面的URL我们划分基础部分是 http://tieba.baidu.com/p/3138733512，参数部分是 ?see_lz=1&amp;pn=1 2. 页面的抓取熟悉了URL的格式，那就让我们用urllib2库来试着抓取页面内容吧。上一篇糗事百科我们最后改成了面向对象的编码方式，这次我们直接尝试一下，定义一个类名叫BDTB(百度贴吧)，一个初始化方法，一个获取页面的方法。 其中，有些帖子我们想指定给程序是否要只看楼主，所以我们把只看楼主的参数初始化放在类的初始化上，即init方法。另外，获取页面的方法我们需要知道一个参数就是帖子页码，所以这个参数的指定我们放在该方法中。 综上，我们初步构建出基础代码如下： 1234567891011121314151617181920212223242526#-*-coding:utf8-*-#created by 10412import urllibimport urllib2import re#百度贴吧爬虫类class BDTB: #初始化，传入基地址，是否只看楼主的参数 def __init__(self, baseUrl, seeLZ): self.baseURL = baseUrl self.seeLZ = &apos;?see_lz=&apos; + str(seeLZ) #传入页码，获取该页帖子的代码 def getPage(self, pageNum): try: url = self.baseURL + self.seeLZ + &apos;&amp;pn=&apos; + str(pageNum) request = urllib2.Request(url) response = urllib2.urlopen(request) print response.read() return response except urllib2.URLError, e: if hasattr(e, &quot;reason&quot;): print u&quot;连接百度贴吧失败,错误原因&quot;,e.reason return NonebaseURL = &apos;http://tieba.baidu.com/p/3138733512&apos;bdtb = BDTB(baseURL, 1)bdtb.getPage(1) 运行代码，我们可以看到屏幕上打印出了这个帖子第一页楼主发言的所有内容，形式为HTML代码。 3. 提取相关信息1)提取帖子标题在浏览器中审查元素，或者按F12，查看页面源代码，我们找到标题所在的代码段如下: 1&lt;h3 class=\"core_title_txt pull-left text-overflow \" title=\"纯原创我心中的NBA2014-2015赛季现役50大\" style=\"width: 416px\"&gt;纯原创我心中的NBA2014-2015赛季现役50大&lt;/h3&gt; 所以我们要提取 &lt;h3&gt; 中的内容，因为一开始可以查看整个界面的原代码，查看里面含有 &lt;h3&gt;标签的不止一个。所以需要写正则表达式来匹配，如下： 1&lt;h3 class=&quot;core_title_txt.*?&gt;(.*?)&lt;/h3&gt; 然后，我们可以写个获取标题的方法 12345678910# 获取帖子标题 def getTitle(self): page = self.getPage(1) pattern = re.compile(&apos;&lt;h3 class=&quot;core_title_txt.*?&gt;(.*?)&lt;/h3&gt;&apos;, re.S) result = re.search(pattern, page) if result: # print result.group(1) #测试输出 return result.group(1).strip() else: return None 2）提取帖子页数同样地，帖子总页数我们也可以通过分析页面中的共?页来获取。 1&lt;li class=\"l_reply_num\" style=\"margin-left:8px\"&gt;&lt;span class=\"red\" style=\"margin-right:3px\"&gt;4784&lt;/span&gt;回复贴，共&lt;span class=\"red\"&gt;36&lt;/span&gt;页&lt;/li&gt; 所以我们的获取总页数的方法如下 12345678910#获取帖子一共有多少页def getPageNum(self): page = self.getPage(1) pattern = re.compile('&lt;li class=\"l_reply_num.*?&lt;/span&gt;.*?&lt;span.*?&gt;(.*?)&lt;/span&gt;',re.S) result = re.search(pattern,page) if result: #print result.group(1) #测试输出 return result.group(1).strip() else: return None 3）提取正文内容审查元素，可以看到百度贴吧每一层楼的主要内容都在标签里面，所以我们可以写如下的正则表达式 1&lt;div id=\"post_content_.*?&gt;(.*?)&lt;/div&gt; 所以提取正文内容的方法： 123456#获取每一层楼的内容,传入页面内容def getContent(self,page): pattern = re.compile('&lt;div id=\"post_content_.*?&gt;(.*?)&lt;/div&gt;',re.S) items = re.findall(pattern,page) for item in items: print item 运行截图如下： 可以看到有很多的换行符和图片符，既然出现这样的情况，那肯定不是我们想要的结果。那我们就必须要将文本进行处理，将各种复杂的标签给剔除，还原帖子的原来面貌。可以使用一个方法或者类将这个处理文本的实现，不过为了更好的代码重用和架构，还是建议使用一个类。 我们将这个类命名为Too（工具类），里面定义一个replace方法，替换各种标签。然后在类中定义几个正则表达式，利用re.sub方法对文本进行匹配后然后替换。 123456789101112131415161718192021222324252627import re#处理页面标签类class Tool: #去除img标签,7位长空格 removeImg = re.compile('&lt;img.*?&gt;| &#123;7&#125;|') #删除超链接标签 removeAddr = re.compile('&lt;a.*?&gt;|&lt;/a&gt;') #把换行的标签换为\\n replaceLine = re.compile('&lt;tr&gt;|&lt;div&gt;|&lt;/div&gt;|&lt;/p&gt;') #将表格制表&lt;td&gt;替换为\\t replaceTD= re.compile('&lt;td&gt;') #把段落开头换为\\n加空两格 replacePara = re.compile('&lt;p.*?&gt;') #将换行符或双换行符替换为\\n replaceBR = re.compile('&lt;br&gt;&lt;br&gt;|&lt;br&gt;') #将其余标签剔除 removeExtraTag = re.compile('&lt;.*?&gt;') def replace(self,x): x = re.sub(self.removeImg,\"\",x) x = re.sub(self.removeAddr,\"\",x) x = re.sub(self.replaceLine,\"\\n\",x) x = re.sub(self.replaceTD,\"\\t\",x) x = re.sub(self.replacePara,\"\\n \",x) x = re.sub(self.replaceBR,\"\\n\",x) x = re.sub(self.removeExtraTag,\"\",x) #strip()将前后多余内容删除 return x.strip() 在使用时，我们只需要初始化一下这个类，然后调用replace方法即可。 现在整体代码是如下这样子的，现在我的代码是写到这样子的: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879#-*-coding:utf8-*-#created by 10412import urllibimport urllib2import re# 处理页面标签类class Tool: # 去除img标签,7位长空格 removeImg = re.compile('&lt;img.*?&gt;| &#123;7&#125;|') # 删除超链接标签 removeAddr = re.compile('&lt;a.*?&gt;|&lt;/a&gt;') # 把换行的标签换为\\n replaceLine = re.compile('&lt;tr&gt;|&lt;div&gt;|&lt;/div&gt;|&lt;/p&gt;') # 将表格制表&lt;td&gt;替换为\\t replaceTD = re.compile('&lt;td&gt;') # 把段落开头换为\\n加空两格 replacePara = re.compile('&lt;p.*?&gt;') # 将换行符或双换行符替换为\\n replaceBR = re.compile('&lt;br&gt;&lt;br&gt;|&lt;br&gt;') # 将其余标签剔除 removeExtraTag = re.compile('&lt;.*?&gt;') def replace(self, x): x = re.sub(self.removeImg, \"\", x) x = re.sub(self.removeAddr, \"\", x) x = re.sub(self.replaceLine, \"\\n\", x) x = re.sub(self.replaceTD, \"\\t\", x) x = re.sub(self.replacePara, \"\\n \", x) x = re.sub(self.replaceBR, \"\\n\", x) x = re.sub(self.removeExtraTag, \"\", x) # strip()将前后多余内容删除 return x.strip()# 百度贴吧爬虫类class BDTB: # 初始化，传入基地址，是否只看楼主的参数 def __init__(self, baseUrl, seeLZ): self.baseURL = baseUrl self.seeLZ = '?see_lz=' + str(seeLZ) self.tool = Tool() # 传入页码，获取该页帖子的代码 def getPage(self, pageNum): try: url = self.baseURL + self.seeLZ + '&amp;pn=' + str(pageNum) request = urllib2.Request(url) response = urllib2.urlopen(request) return response.read().decode('utf-8') except urllib2.URLError, e: if hasattr(e, \"reason\"): print u\"连接百度贴吧失败,错误原因\", e.reason return None # 获取帖子标题 def getTitle(self): page = self.getPage(1) pattern = re.compile('&lt;h1 class=\"core_title_txt.*?&gt;(.*?)&lt;/h1&gt;', re.S) result = re.search(pattern, page) if result: # print result.group(1) #测试输出 return result.group(1).strip() else: return None # 获取帖子一共有多少页 def getPageNum(self): page = self.getPage(1) pattern = re.compile('&lt;li class=\"l_reply_num.*?&lt;/span&gt;.*?&lt;span.*?&gt;(.*?)&lt;/span&gt;', re.S) result = re.search(pattern, page) if result: # print result.group(1) #测试输出 return result.group(1).strip() else: return None # 获取每一层楼的内容,传入页面内容 def getContent(self, page): pattern = re.compile('&lt;div id=\"post_content_.*?&gt;(.*?)&lt;/div&gt;', re.S) items = re.findall(pattern, page) # for item in items: # print item print self.tool.replace(items[1])baseURL = 'http://tieba.baidu.com/p/3138733512'bdtb = BDTB(baseURL, 1)bdtb.getContent(bdtb.getPage(1)) 运行截图如下： 4）替换楼层至于这个问题，我感觉直接提取楼层没什么必要呀，因为只看楼主的话，有些楼层的编号是间隔的，所以我们得到的楼层序号是不连续的，这样我们保存下来也没什么用。 所以可以尝试下面的方法： 1.每打印输出一段楼层，写入一行横线来间隔，或者换行符也好。 2.试着重新编一个楼层，按照顺序，设置一个变量，每打印出一个结果变量加一，打印出这个变量当做楼层。 将getContent方法修改如下： 123456789#获取每一层楼的内容,传入页面内容def getContent(self,page): pattern = re.compile('&lt;div id=\"post_content_.*?&gt;(.*?)&lt;/div&gt;',re.S) items = re.findall(pattern,page) floor = 1 for item in items: print floor,u\"楼------------------------------------------------------------------------------------------------------------------------------------\\n\" print self.tool.replace(item) floor += 1 运行结果截图如下： 4. 写入文件代码： 12file = open(“tb.txt”,”w”)file.writelines(obj) 5. 完善代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134#-*-coding:utf8-*-#created by 10412import urllibimport urllib2import re#处理页面标签类class Tool: #去除img标签,7位长空格 removeImg = re.compile('&lt;img.*?&gt;| &#123;7&#125;|') #删除超链接标签 removeAddr = re.compile('&lt;a.*?&gt;|&lt;/a&gt;') #把换行的标签换为\\n replaceLine = re.compile('&lt;tr&gt;|&lt;div&gt;|&lt;/div&gt;|&lt;/p&gt;') #将表格制表&lt;td&gt;替换为\\t replaceTD= re.compile('&lt;td&gt;') #把段落开头换为\\n加空两格 replacePara = re.compile('&lt;p.*?&gt;') #将换行符或双换行符替换为\\n replaceBR = re.compile('&lt;br&gt;&lt;br&gt;|&lt;br&gt;') #将其余标签剔除 removeExtraTag = re.compile('&lt;.*?&gt;') def replace(self,x): x = re.sub(self.removeImg,\"\",x) x = re.sub(self.removeAddr,\"\",x) x = re.sub(self.replaceLine,\"\\n\",x) x = re.sub(self.replaceTD,\"\\t\",x) x = re.sub(self.replacePara,\"\\n \",x) x = re.sub(self.replaceBR,\"\\n\",x) x = re.sub(self.removeExtraTag,\"\",x) #strip()将前后多余内容删除 return x.strip()#百度贴吧爬虫类class BDTB: #初始化，传入基地址，是否只看楼主的参数 def __init__(self,baseUrl,seeLZ,floorTag): #base链接地址 self.baseURL = baseUrl #是否只看楼主 self.seeLZ = '?see_lz='+str(seeLZ) #HTML标签剔除工具类对象 self.tool = Tool() #全局file变量，文件写入操作对象 self.file = None #楼层标号，初始为1 self.floor = 1 #默认的标题，如果没有成功获取到标题的话则会用这个标题 self.defaultTitle = u\"百度贴吧\" #是否写入楼分隔符的标记 self.floorTag = floorTag #传入页码，获取该页帖子的代码 def getPage(self,pageNum): try: #构建URL url = self.baseURL+ self.seeLZ + '&amp;pn=' + str(pageNum) request = urllib2.Request(url) response = urllib2.urlopen(request) #返回UTF-8格式编码内容 return response.read().decode('utf-8') #无法连接，报错 except urllib2.URLError, e: if hasattr(e,\"reason\"): print u\"连接百度贴吧失败,错误原因\",e.reason return None #获取帖子标题 def getTitle(self,page): #得到标题的正则表达式 pattern = re.compile('&lt;h1 class=\"core_title_txt.*?&gt;(.*?)&lt;/h1&gt;',re.S) result = re.search(pattern,page) if result: #如果存在，则返回标题 return result.group(1).strip() else: return None #获取帖子一共有多少页 def getPageNum(self,page): #获取帖子页数的正则表达式 pattern = re.compile('&lt;li class=\"l_reply_num.*?&lt;/span&gt;.*?&lt;span.*?&gt;(.*?)&lt;/span&gt;',re.S) result = re.search(pattern,page) if result: return result.group(1).strip() else: return None #获取每一层楼的内容,传入页面内容 def getContent(self,page): #匹配所有楼层的内容 pattern = re.compile('&lt;div id=\"post_content_.*?&gt;(.*?)&lt;/div&gt;',re.S) items = re.findall(pattern,page) contents = [] for item in items: #将文本进行去除标签处理，同时在前后加入换行符 content = \"\\n\"+self.tool.replace(item)+\"\\n\" contents.append(content.encode('utf-8')) return contents def setFileTitle(self,title): #如果标题不是为None，即成功获取到标题 if title is not None: self.file = open(title + \".txt\",\"w+\") else: self.file = open(self.defaultTitle + \".txt\",\"w+\") def writeData(self,contents): #向文件写入每一楼的信息 for item in contents: if self.floorTag == '1': #楼之间的分隔符 floorLine = \"\\n\" + str(self.floor) + u\"-----------------------------------------------------------------------------------------\\n\" self.file.write(floorLine) self.file.write(item) self.floor += 1 def start(self): indexPage = self.getPage(1) pageNum = self.getPageNum(indexPage) title = self.getTitle(indexPage) self.setFileTitle(title) if pageNum == None: print \"URL已失效，请重试\" return try: print \"该帖子共有\" + str(pageNum) + \"页\" for i in range(1,int(pageNum)+1): print \"正在写入第\" + str(i) + \"页数据\" page = self.getPage(i) contents = self.getContent(page) self.writeData(contents) #出现写入异常 except IOError,e: print \"写入异常，原因\" + e.message finally: print \"写入任务完成\"print u\"请输入帖子代号\"baseURL = 'http://tieba.baidu.com/p/' + str(raw_input(u'http://tieba.baidu.com/p/'))seeLZ = raw_input(\"是否只获取楼主发言，是输入1，否输入0\\n\")floorTag = raw_input(\"是否写入楼层信息，是输入1，否输入0\\n\")bdtb = BDTB(baseURL,seeLZ,floorTag)bdtb.start() 运行后截图如下： 备注： 运行后注意输入帖子的代号先在网址后空格，再输入帖子代号，输入完再把刚才的空格 删除，只有这样才不会报错。 Traceback (most recent call last):File “E:/python/code/PycharmProject/Python-Projects/baidutieba/BDTB3.py”, line 149,in &lt; module &gt; bdtb.start()File “E:/python/code/PycharmProject/Python-Projects/baidutieba/BDTB3.py”, line 123, in startpageNum = self.getPageNum(indexPage)File “E:/python/code/PycharmProject/Python-Projects/baidutieba/BDTB3.py”, line 86, in getPageNumresult = re.search(pattern,page)File “C:\\Python27\\lib\\re.py”, line 146, in searchreturn _compile(pattern, flags).search(string)TypeError: expected string or buffer","tags":[{"name":"Python","slug":"Python","permalink":"http://www.54tianzhisheng.cn/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"http://www.54tianzhisheng.cn/tags/爬虫/"}]},{"title":"Python爬虫实战之爬取糗事百科段子","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/Python爬虫实战之爬取糗事百科段子/","text":"完整代码地址：Python爬虫实战之爬取糗事百科段子程序代码详解： Spider1-qiushibaike.py：爬取糗事百科的8小时最新页的段子。包含的信息有作者名称，觉得好笑人数，评论人数，发布的内容。如果发布的内容中含有图片的话，则过滤图片，内容依然显示出来。 Spider2-qiushibaike.py：在Spider1-qiushibaike.py基础上，引入类和方法，进行优化和封装，爬取糗事百科的24小时热门页的段子。进一步优化，每按一次回车更新一条内容，当前页的内容抓取完毕后，自动抓取下一页的内容，按‘q’退出。 Spider3-qiushibaike.py：在Spiders-qiushibaike.py基础上，爬取了百科段子的评论。按C查看当前这个糗事的评论，当切换到查看评论时，换回车显示下一个评论,按Q退出回到查看糗事。糗事段子页数是一页一页加载的，如果你已经看完所有的糗事，就会自动退出！ 本爬虫目标： 抓取糗事百科热门段子 过滤带有图片的段子 实现每按一次回车显示一个段子的发布时间，发布人，段子内容，点赞数，评论人数。 糗事百科是不需要登录的，所以也没必要用到Cookie，另外糗事百科有的段子是附图的，我们把图抓下来图片不便于显示，那么我们就尝试过滤掉有图的段子吧。 好，现在我们尝试抓取一下糗事百科的热门段子吧，每按下一次回车我们显示一个段子。 1.确定URL并抓取页面代码首先我们确定好页面的URL是 http://www.qiushibaike.com/hot/page/1，其中最后一个数字1代表页数，我们可以传入不同的值来获得某一页的段子内容。 2.提取某一页的所有段子好，获取了HTML代码之后，我们开始分析怎样获取某一页的所有段子。 首先我们审查元素看一下，按浏览器的F12，截图如下: 我们可以看到，每一个段子都是 &lt;div class=”article block untagged mb15″ id=”…”&gt;…&lt;/div&gt; 包裹的内容。 现在我们想获取发布人，发布日期，段子内容，点赞人数和评论人数。不过另外注意的是，段子有些是带图片的，如果我们想在控制台显示图片是不现实的，所以我们直接把带有图片的段子给它剔除掉，只保存仅含文本的段子。 所以我们加入如下正则表达式来匹配一下，用到的方法是 re.findall 是找寻所有匹配的内容。方法的用法详情可以看前面说的正则表达式的介绍。 好，我们的正则表达式匹配语句书写如下，在原来的基础上追加如下代码： 123456789#正则表达式匹配 pattern = re.compile(&apos;&lt;div.*?author.*?&gt;.*?&lt;img.*?&gt;.*?&lt;h2&gt;(.*?)&lt;/h2&gt;.*?&lt;div.*?&apos;+ &apos;content&quot;&gt;(.*?)&lt;/div&gt;(.*?)&lt;div.*?class=&quot;number&quot;&gt;(.*?)&lt;/i&gt;.*?class=&quot;number&quot;&gt;(.*?)&lt;/i&gt;&apos;,re.S) items = re.findall(pattern,content) for item in items: haveImg = re.search(&quot;img&quot;,item[2]) if not haveImg: print item[0],item[3],item[4],item[1] #item[0]是作者名称 item[3]好笑人数 item[4]评论人数 item[1]内容 item[2]是内容后面的东西，如果含有图片，过滤掉 现在正则表达式在这里稍作说明 1） .*? 是一个固定的搭配， . 和 * 代表可以匹配任意无限多个字符，加上 ？ 表示使用非贪婪模式进行匹配，也就是我们会尽可能短地做匹配，以后我们还会大量用到 .*? 的搭配。 2）(.*?) 代表一个分组，在这个正则表达式中我们匹配了五个分组，在后面的遍历 item 中，item[0] 就代表第一个 (.*?) 所指代的内容，item[1] 就代表第二个 (.*?) 所指代的内容，以此类推。 3）re.S 标志代表在匹配时为点任意匹配模式，点 . 也可以代表换行符。 这样我们就获取了发布人，发布时间，发布内容，附加图片以及点赞数。 在这里注意一下，我们要获取的内容如果是带有图片，直接输出出来比较繁琐，所以这里我们只获取不带图片的段子就好了。 所以，在这里我们就需要对带图片的段子进行过滤。 我们可以发现，带有图片的段子会带有类似下面的代码，而不带图片的则没有，所以，我们的正则表达式的 item[2] 就是获取了下面的内容，如果不带图片，item[2]获取的内容便是空，所以我们只需要判断 item[2]中是否含有 img 标签就可以了。 整体代码如下： 12345678910111213141516171819202122232425262728293031323334#-*-coding:utf8-*-#created by 10412 2016/8/23#爬取糗事百科的8小时最新页的段子。包含的信息有作者名称，觉得好笑人数，评论人数，发布的内容。#如果发布的内容中含有图片的话，则过滤图片，内容依然显示出来。import urllibimport urllib2import re#自定义输入爬取的页数page = raw_input(&quot;please enter the page number:&quot;)url = &apos;http://www.qiushibaike.com/8hr/page/&apos;+ page +&apos;/?s=4880477&apos;user_agent = &apos;Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)&apos;headers = &#123; &apos;User-Agent&apos; : user_agent &#125;try: request = urllib2.Request(url,headers = headers) response = urllib2.urlopen(request) content = response.read().decode(&apos;utf-8&apos;) #正则表达式匹配 pattern = re.compile(&apos;&lt;div.*?author.*?&gt;.*?&lt;img.*?&gt;.*?&lt;h2&gt;(.*?)&lt;/h2&gt;.*?&lt;div.*?&apos;+ &apos;content&quot;&gt;(.*?)&lt;/div&gt;(.*?)&lt;div.*?class=&quot;number&quot;&gt;(.*?)&lt;/i&gt;.*?class=&quot;number&quot;&gt;(.*?)&lt;/i&gt;&apos;,re.S) items = re.findall(pattern,content) for item in items: haveImg = re.search(&quot;img&quot;,item[2]) if not haveImg: print item[0],item[3],item[4],item[1] #item[0]是作者名称 item[3]好笑人数 item[4]评论人数 item[1]内容 item[2]是内容后面的东西，如果含有图片，过滤掉except urllib2.URLError, e: if hasattr(e,&quot;code&quot;): print e.code if hasattr(e,&quot;reason&quot;): print e.reason 运行一下看下效果: 恩，带有图片的段子已经被剔除啦。 3.完善交互，设计面向对象模式好啦，现在最核心的部分我们已经完成啦，剩下的就是修一下边边角角的东西，我们想达到的目的是： 按下回车，读取一个段子，显示出段子的发布人，内容，点赞个数及评论数量。 另外我们需要设计面向对象模式，引入类和方法，将代码做一下优化和封装，最后，我们的代码如下所示 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788#-*-coding:utf8-*-#created by 10412# 在Spider1-qiushibaike.py基础上，引入类和方法，进行优化和封装，爬取糗事百科的24小时热门页的段子。# 进一步优化，每按一次回车更新一条内容，当前页的内容抓取完毕后，自动抓取下一页的内容，按‘q’退出。import urllib2import reclass QSBK: def __init__(self): self.pageIndex = 1 self.user_agent = &apos;Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)&apos; self.headers = &#123;&apos;User-Agent&apos; : self.user_agent&#125; self.stories = [] # 存放程序是否继续运行的变量 self.enable = False # 传入某一页的索引获得页面代码 def getPage(self, pageIndex): try: url = &apos;http://www.qiushibaike.com/hot/page/&apos; + str(pageIndex) request = urllib2.Request(url, headers=self.headers) response = urllib2.urlopen(request) pageCode = response.read().decode(&apos;utf-8&apos;) return pageCode except urllib2.URLError, e: if hasattr(e, &quot;reason&quot;): print u&quot;连接糗事百科失败,错误原因&quot;, e.reason return None # 传入某一页代码，返回本页不带图片的段子列表 def getPageItems(self, pageIndex): pageCode = self.getPage(pageIndex) if not pageCode: print u&quot;出错了&quot; return None pattern = re.compile(&apos;&lt;div class=&quot;author.*?href.*?&lt;img src.*?title=.*?&lt;h2&gt;(.*?)&lt;/h2&gt;.*?&lt;div class=&quot;content&quot;&gt;(.*?)&lt;/div&gt;.*?&lt;i class=&quot;number&quot;&gt;(.*?)&lt;/i&gt;.*?class=&quot;number&quot;&gt;(.*?)&lt;/i&gt;&apos;,re.S) items = re.findall(pattern, pageCode) pageStories = [] for item in items: replaceBR = re.compile(&apos;&lt;br/&gt;&apos;) text = re.sub(replaceBR, &quot;\\n&quot;, item [1] ) pageStories.append([item[0].strip(), text.strip(), item[2].strip(), item[3].strip()]) return pageStories # 加载并提取页面内容，加入到列表中 def loadPage(self): if self.enable == True: if len(self.stories) &lt; 2: # 获取新一页 pageStories = self.getPageItems(self.pageIndex) if pageStories: self.stories.append(pageStories) self.pageIndex += 1 # 调用该方法，回车打印一个段子 def getOneStory(self, pageStories, page): for story in pageStories: input = raw_input() self.loadPage() if input == &quot;Q&quot;: self.enable = False return print u&quot;第%d页\\t发布人:%s\\t赞:%s\\t评论:%s\\n%s&quot; %(page, story[0], story[2], story[2], story [1]) # 开始方法 def start(self): print u&quot;正在读取糗事百科,按回车查看新段子，Q退出&quot; # 使变量为True，程序可以正常运行 self.enable = True # 先加载一页内容 self.loadPage() # 局部变量，控制当前读到了第几页 nowPage = 0 while self.enable: if len(self.stories) &gt; 0: # 从全局list中获取一页的段子 pageStories = self.stories[0] # 当前读到的页数加一 nowPage += 1 # 将全局list中第一个元素删除，因为已经取出 del self.stories[0] # 输出该页的段子 self.getOneStory(pageStories, nowPage)spider = QSBK()spider.start() 好啦，大家来测试一下吧，点一下回车会输出一个段子，包括第几页，发布人，段子内容，点赞数以及评论数量，是不是感觉爽爆了！ 完善更新版爬虫代码在上面爬虫的基础上，还增加爬取了百科段子的评论。按C查看当前这个糗事的评论，当切换到查看评论时，换回车显示下一个评论,按Q退出回到查看糗事。糗事段子页数是一页一页加载的，如果你已经看完所有的糗事，就会自动退出！ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198#-*-coding:utf8-*-#created by 10412#在Spiders-qiushibaike.py基础上，爬取了百科段子的评论。按C查看当前这个糗事的评论，当切换到查看评论时，# 换回车显示下一个评论,按Q退出回到查看糗事。糗事段子页数是一页一页加载的，如果你已经看完所有的糗事，就会自动退出！import urllibimport urllib2import reimport os.pathhtmlCharacterMap = &#123; &apos;&lt;br/&gt;&apos; : &apos;\\n&apos;, &apos;&amp;quot;&apos; : &apos;&quot;&apos;, &apos;&amp;nbsp;&apos; : &apos; &apos;, &apos;&amp;gt;&apos; : &apos;&gt;&apos;, &apos;&amp;lt;&apos; : &apos;&lt;&apos;, &apos;&amp;amp;&apos;: &apos;&amp;&apos;, &apos;&amp;#39&apos;:&quot;&apos;&quot;,&#125;class QSBK(object): &quot;&quot;&quot;糗事百科的爬虫&quot;&quot;&quot; def __init__(self): self.pageIndex = 1 self.pagetotal = 9999 self.user_agent = &apos;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36&apos; self.headers = &#123;&apos;User-Agent&apos; : self.user_agent&#125; self.stories = [] self.comments = [] self.currentStoryId = &apos;&apos; #是否要退出了 self.enable = False #记录当前是否在查看评论 self.viewComment = False def getPageContent(self, pageIndex): try: url = &apos;http://www.qiushibaike.com/8hr/page/%d/&apos; % pageIndex request = urllib2.Request(url, headers=self.headers) print u&apos;开始加载%02d页&apos; % pageIndex response = urllib2.urlopen(request, timeout=5) print u&apos;成功加载%02d页&apos; % pageIndex pageContent = response.read().decode(&apos;utf-8&apos;) return pageContent except urllib2.URLError, e: if hasattr(e, &apos;reason&apos;): print u&quot;连接糗事百科失败，错误原因：&quot;, e.reason return None def getCommentsContent(self, storyId): # 得到段子的评论 try: url = &apos;http://www.qiushibaike.com/article/%s&apos; % storyId request = urllib2.Request(url, headers=self.headers) response = urllib2.urlopen(request, timeout=5) pageContent = response.read().decode(&apos;utf-8&apos;) return pageContent except urllib2.URLError, e: if hasattr(e, &apos;reason&apos;): print u&quot;连接糗事百科失败，错误原因：&quot;, e.reason return None def getPageTotal(self, content): # 得到总页数 if self.pagetotal != 9999: # print u&apos;加载第%d页&apos; % self.pageIndex return pattrenStr = &apos;&lt;span class=&quot;page-numbers&quot;&gt;(?P&lt;pagetotal&gt;.*?)&lt;/span&gt;&apos; pattern = re.compile(pattrenStr, re.S) items = re.findall(pattern, content) if len(items)&gt;0: self.pagetotal = int(items[-1].strip()) print u&apos;总共有%d页&apos; % self.pagetotal def getPageItems(self, pageIndex): pageContent = self.getPageContent(pageIndex) with open(&apos;temp%02d.html&apos; % pageIndex, &apos;w&apos;) as f: f.write(pageContent.encode(&apos;utf-8&apos;)) if not pageContent: print &quot;页面加载失败...&quot; return None self.getPageTotal(pageContent) pattrenStr = r&apos;&lt;h2&gt;(?P&lt;authorname&gt;.*?)&lt;/h2&gt;.*?&apos;\\ r&apos;&lt;div class=&quot;content&quot;&gt;(?P&lt;content&gt;.*?)&lt;/div&gt;&apos;\\ r&apos;(?P&lt;maybehaveimage&gt;.*?)&apos;\\ r&apos;&lt;i class=&quot;number&quot;&gt;(?P&lt;numbervote&gt;.*?)&lt;/i&gt;.*?&apos;\\ r&apos;&lt;span class=&quot;stats-comments&quot;&gt;(?P&lt;comments&gt;.*?)&lt;/div&gt;&apos; pattern = re.compile(pattrenStr, re.S) items = re.findall(pattern, pageContent) return items def getCurrentStoryComments(self, storyId): #切换到查看评论模式 self.viewComment = True content = self.getCommentsContent(storyId) if not content: print &quot;页面加载失败...&quot; return None reStr = r&apos;&lt;div id=&quot;comment-.*?&apos;\\ r&apos;&lt;a href=&quot;/users/.*?/&quot; class=&quot;userlogin&quot; target=&quot;_blank&quot; title=&quot;(?P&lt;username&gt;.*?)&quot;&gt;(?P=username)&lt;/a&gt;.*?&apos;\\ r&apos;&lt;span class=&quot;body&quot;&gt;(?P&lt;comment&gt;.*?)&lt;/span&gt;.*?&apos;\\ r&apos;&lt;div class=&quot;report&quot;&gt;(?P&lt;index&gt;.*?)&lt;/div&gt;&apos; pattern = re.compile(reStr, re.S) items = re.findall(pattern, content) del self.comments[:] for item in items: comentstr = item[0]+&apos;(&apos;+ item[2] + u&apos;楼)&apos; + &apos;\\n&apos; + item[1] + &apos;\\n&apos; for (k,v) in htmlCharacterMap.items(): re.sub(re.compile(k), v, comentstr) self.comments.append(comentstr) if len(self.comments)&gt;0: print &apos;已切换到查看评论，换回车显示下一个评论,按Q退出回到查看糗事&apos; else: print &apos;当前糗事没有评论&apos; self.viewComment = False def getNextPage(self): if self.pageIndex &gt; self.pagetotal: self.enable = False print &quot;你已经看完所有的糗事，现在自动退出！&quot; return items = self.getPageItems(self.pageIndex) self.pageIndex += 1 for item in items: #如果有图片直接跳过，因为图片在终端显示不了 if re.search(&apos;img&apos;, item[2]): continue content = item[1].strip() #转换html的特殊字符 for (k,v) in htmlCharacterMap.items(): content = re.sub(re.compile(k), v, content) authorname = item[0].strip() for (k,v) in htmlCharacterMap.items(): authorname = re.sub(re.compile(k), v, authorname) #找出评论个数，没有为0 pattern = re.compile(r&apos;.*?&lt;a href=&quot;/article/(?P&lt;id&gt;.*?)&quot;.*?&lt;i class=&quot;number&quot;&gt;(?P&lt;number&gt;.*?)&lt;/i&gt;.*?&apos;, re.S) result = re.match(pattern, item[4]) commentnumbers = 0 articleId = &apos;&apos; if result: commentnumbers = result.groupdict().get(&apos;number&apos;, &apos;0&apos;) articleId = result.groupdict().get(&apos;id&apos;, &apos;&apos;) self.stories.append(authorname + &apos;(&apos; + item[3].strip() + u&apos;好笑·&apos; + str(commentnumbers) + u&apos;评论)&apos; + &apos;\\n&apos; + content + &apos;\\n&apos;) self.stories.append(articleId) def getNextComment(self): print self.comments[0] self.comments.pop(0) if len(self.comments)==0: print &apos;你已查看完这个糗事的所有评论,现在自动退出到查看糗事&apos; self.viewComment = False def getOneStory(self): #防止有的页面全是带图片的 while (len(self.stories)==0 and self.enable): self.getNextPage() story = self.stories[0] self.currentStoryId = self.stories[1] print story self.stories.pop(0) self.stories.pop(0) if len(self.stories)==0: self.getNextPage() def start(self): #先删除临时保存的网页 tempfiles = [x for x in os.listdir(&apos;.&apos;) if os.path.isfile(x) and os.path.splitext(x)[1]==&apos;.html&apos; and x.startswith(&apos;temp&apos;)] for file in tempfiles: os.remove(file) print u&quot;正在读取糗事百科，按回车查看下一个糗事，按C查看当前这个糗事的评论，按Q退出或返回&quot; self.enable = True self.getNextPage() while self.enable: input = raw_input() if input.upper() == &quot;Q&quot;: if not self.viewComment: self.enable = False else: self.viewComment = False print &apos;现在退出到查看糗事了&apos; elif input.upper() == &quot;C&quot;: #查看当前看到的糗事的评论 if len(self.currentStoryId)&gt;0: self.getCurrentStoryComments(self.currentStoryId) else: print &apos;这条糗事没有评论&apos; else: if not self.viewComment: self.getOneStory() else: self.getNextComment()if __name__ == &apos;__main__&apos;: spider = QSBK() spider.start()","tags":[{"name":"Python","slug":"Python","permalink":"http://www.54tianzhisheng.cn/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"http://www.54tianzhisheng.cn/tags/爬虫/"}]},{"title":"深入分析 Java Web 中的中文编码问题","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/深入分析 Java Web 中的中文编码问题/","text":"背景： 编码问题一直困扰着程序开发人员，尤其是在 Java 中更加明显，因为 Java 是跨平台的语言，在不同平台的编码之间的切换较多。接下来将介绍 Java 编码问题出现的根本原因；在 Java 中经常遇到的几种编码格式的区别；在 Java 中经常需要编码的场景；出现中文问题的原因分析；在开发 Java Web 中可能存在编码的几个地方；一个 HTTP 请求怎么控制编码格式；如何避免出现中文编码问题等。 1、几种常见的编码格式1.1 为什么要编码 在计算机中存储信息的最小单元是 1 个字节，即 8 个 bit， 所以能表示的字符范围是 0 ~ 255 个。 要表示的符号太多，无法用 1 个字节来完全表示。 1.2 如何翻译计算机中提供多种翻译方式，常见的有 ASCII、ISO-8859-1、GB2312、GBK、UTF-8、UTF-16等。这些都规定了转化的规则，按照这个规则就可以让计算机正确的表示我们的字符。下面介绍这几种编码格式： ASCII 码 总共有 128 个，用 1 个字节的低 7 位表示， 0 ~ 31 是控制字符如换行、回车、删除等，32 ~ 126 是打印字符，可以通过键盘输入并且能够显示出来。 ISO-8859-1 128 个字符显然是不够用的，所以 ISO 组织在 ASCII 的基础上扩展，他们是 ISO-8859-1 至 ISO-8859-15，前者涵盖大多数字符，应用最广。ISO-8859-1 仍是单字节编码，它总归能表示 256 个字符。 GB2312 它是双字节编码，总的编码范围是 A1 ~ F7，其中 A1 ~ A9 是符号区，总共包含 682 个符号；B0 ~ F7 是汉字区，包含 6763 个汉字。 GBk GBK 为《汉字内码扩展规范》，为 GB2312 的扩展，它的编码范围是 8140 ~ FEFE（去掉XX7F），总共有 23940 个码位，能表示 21003 个汉字，和 GB2312的编码兼容，不会有乱码。 UTF-16 它具体定义了 Unicode 字符在计算机中的存取方法。UTF-16 用两个字节来表示 Unicode 的转化格式，它采用定长的表示方法，即不论什么字符用两个字节表示。两个字节是 16 个 bit，所以叫 UTF-16。它表示字符非常方便，没两个字节表示一个字符，这就大大简化了字符串操作。 UTF-8 虽说 UTF-16 统一采用两个字节表示一个字符很简单方便，但是很大一部分字符用一个字节就可以表示，如果用两个字节表示，存储空间放大了一倍，在网络带宽有限的情况下会增加网络传输的流量。UTF-8 采用了一种变长技术，每个编码区域有不同的字码长度不同类型的字符可以由 1 ~ 6 个字节组成。 UTF-8 有以下编码规则： 如果是 1 个字节，最高位（第 8 位）为 0，则表示这是一个 ASCII 字符（00 ~ 7F） 如果是 1 个字节，以 11 开头，则连续的 1 的个数暗示这个字符的字节数 如果是 1 个字节，以 10 开头，表示它不是首字节，则需要向前查找才能得到当前字符的首字节 ​ 2、在 Java 中需要编码的场景2.1 在 I/O 操作中存在的编码 如上图：Reader 类是在 Java 的 I/O 中读取符的父类，而 InputStream 类是读字节的父类， InputStreamReader 类就是关联字节到字符的桥梁，它负责在 I/O 过程中处理读取字节到字符的转换，而对具体字节到字符的解码实现，它又委托 StreamDecoder 去做，在 StreamDecoder 解码过程中必须由用户指定 Charset 编码格式。值得注意的是，如果你没有指定 Charset，则将使用本地环境中默认的字符集，如在中文环境中将使用 GBK 编码。 如下面一段代码，实现了文件的读写功能： 12345678910111213141516171819202122232425String file = \"c:/stream.txt\";String charset = \"UTF-8\";// 写字符换转成字节流FileOutputStream outputStream = new FileOutputStream(file);OutputStreamWriter writer = new OutputStreamWriter(outputStream, charset);try &#123; writer.write(\"这是要保存的中文字符\");&#125; finally &#123; writer.close();&#125;// 读取字节转换成字符FileInputStream inputStream = new FileInputStream(file);InputStreamReader reader = new InputStreamReader(inputStream, charset);StringBuffer buffer = new StringBuffer();char[] buf = new char[64];int count = 0;try &#123; while ((count = reader.read(buf)) != -1) &#123; buffer.append(buffer, 0, count); &#125;&#125; finally &#123; reader.close();&#125; 在我们的应用程序中涉及 I/O 操作时，只要注意指定统一的编解码 Charset 字符集，一般不会出现乱码问题。 2.2 在内存操作中的编码在内存中进行从字符到字节的数据类型转换。 1、String 类提供字符串转换到字节的方法，也支持将字节转换成字符串的构造函数。 123String s = \"字符串\"；byte[] b = s.getBytes(\"UTF-8\");String n = new String(b, \"UTF-8\"); 2、Charset 提供 encode 与 decode，分别对应 char[] 到 byte[] 的编码 和 byte[] 到 char[] 的解码。 123Charset charset = Charset.forName(\"UTF-8\");ByteBuffer byteBuffer = charset.encode(string);CharBuffer charBuffer = charset.decode(byteBuffer); … 3、在 Java 中如何编解码Java 编码类图 首先根据指定的 charsetName 通过 Charset.forName(charsetName) 设置 Charset 类，然后根据 Charset 创建 CharsetEncoder 对象，再调用 CharsetEncoder.encode 对字符串进行编码，不同的编码类型都会对应到一个类中，实际的编码过程是在这些类中完成的。下面是 String. getBytes(charsetName) 编码过程的时序图 Java 编码时序图 从上图可以看出根据 charsetName 找到 Charset 类，然后根据这个字符集编码生成 CharsetEncoder，这个类是所有字符编码的父类，针对不同的字符编码集在其子类中定义了如何实现编码，有了 CharsetEncoder 对象后就可以调用 encode 方法去实现编码了。这个是 String.getBytes 编码方法，其它的如 StreamEncoder 中也是类似的方式。 经常会出现中文变成“？”很可能就是错误的使用了 ISO-8859-1 这个编码导致的。中文字符经过 ISO-8859-1 编码会丢失信息，通常我们称之为“黑洞”，它会把不认识的字符吸收掉。由于现在大部分基础的 Java 框架或系统默认的字符集编码都是 ISO-8859-1，所以很容易出现乱码问题，后面将会分析不同的乱码形式是怎么出现的。 几种编码格式的比较对中文字符后面四种编码格式都能处理，GB2312 与 GBK 编码规则类似，但是 GBK 范围更大，它能处理所有汉字字符，所以 GB2312 与 GBK 比较应该选择 GBK。UTF-16 与 UTF-8 都是处理 Unicode 编码，它们的编码规则不太相同，相对来说 UTF-16 编码效率最高，字符到字节相互转换更简单，进行字符串操作也更好。它适合在本地磁盘和内存之间使用，可以进行字符和字节之间快速切换，如 Java 的内存编码就是采用 UTF-16 编码。但是它不适合在网络之间传输，因为网络传输容易损坏字节流，一旦字节流损坏将很难恢复，想比较而言 UTF-8 更适合网络传输，对 ASCII 字符采用单字节存储，另外单个字符损坏也不会影响后面其它字符，在编码效率上介于 GBK 和 UTF-16 之间，所以 UTF-8 在编码效率上和编码安全性上做了平衡，是理想的中文编码方式。 4、在 Java Web 中涉及的编解码对于使用中文来说，有 I/O 的地方就会涉及到编码，前面已经提到了 I/O 操作会引起编码，而大部分 I/O 引起的乱码都是网络 I/O，因为现在几乎所有的应用程序都涉及到网络操作，而数据经过网络传输都是以字节为单位的，所以所有的数据都必须能够被序列化为字节。在 Java 中数据被序列化必须继承 Serializable 接口。 一段文本它的实际大小应该怎么计算，我曾经碰到过一个问题：就是要想办法压缩 Cookie 大小，减少网络传输量，当时有选择不同的压缩算法，发现压缩后字符数是减少了，但是并没有减少字节数。所谓的压缩只是将多个单字节字符通过编码转变成一个多字节字符。减少的是 String.length()，而并没有减少最终的字节数。例如将“ab”两个字符通过某种编码转变成一个奇怪的字符，虽然字符数从两个变成一个，但是如果采用 UTF-8 编码这个奇怪的字符最后经过编码可能又会变成三个或更多的字节。同样的道理比如整型数字 1234567 如果当成字符来存储，采用 UTF-8 来编码占用 7 个 byte，采用 UTF-16 编码将会占用 14 个 byte，但是把它当成 int 型数字来存储只需要 4 个 byte 来存储。所以看一段文本的大小，看字符本身的长度是没有意义的，即使是一样的字符采用不同的编码最终存储的大小也会不同，所以从字符到字节一定要看编码类型。 我们能够看到的汉字都是以字符形式出现的，例如在 Java 中“淘宝”两个字符，它在计算机中的数值 10 进制是 28120 和 23453，16 进制是 6bd8 和 5d9d，也就是这两个字符是由这两个数字唯一表示的。Java 中一个 char 是 16 个 bit 相当于两个字节，所以两个汉字用 char 表示在内存中占用相当于四个字节的空间。 这两个问题搞清楚后，我们看一下 Java Web 中那些地方可能会存在编码转换？ 用户从浏览器端发起一个 HTTP 请求，需要存在编码的地方是 URL、Cookie、Parameter。服务器端接受到 HTTP 请求后要解析 HTTP 协议，其中 URI、Cookie 和 POST 表单参数需要解码，服务器端可能还需要读取数据库中的数据，本地或网络中其它地方的文本文件，这些数据都可能存在编码问题，当 Servlet 处理完所有请求的数据后，需要将这些数据再编码通过 Socket 发送到用户请求的浏览器里，再经过浏览器解码成为文本。这些过程如下图所示： 一次 HTTP 请求的编码示例 4.1 URL 的编解码用户提交一个 URL，这个 URL 中可能存在中文，因此需要编码，如何对这个 URL 进行编码？根据什么规则来编码？有如何来解码？如下图一个 URL： 上图中以 Tomcat 作为 Servlet Engine 为例，它们分别对应到下面这些配置文件中：Port 对应在 Tomcat 的 中配置，而 Context Path 在 中配置，Servlet Path 在 Web 应用的 web.xml 中的 1234&lt;servlet-mapping&gt; &lt;servlet-name&gt;junshanExample&lt;/servlet-name&gt; &lt;url-pattern&gt;/servlets/servlet/*&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; 中配置，PathInfo 是我们请求的具体的 Servlet，QueryString 是要传递的参数，注意这里是在浏览器里直接输入 URL 所以是通过 Get 方法请求的，如果是 POST 方法请求的话，QueryString 将通过表单方式提交到服务器端。 上图中 PathInfo 和 QueryString 出现了中文，当我们在浏览器中直接输入这个 URL 时，在浏览器端和服务端会如何编码和解析这个 URL 呢？为了验证浏览器是怎么编码 URL 的我选择的是360极速浏览器并通过 Postman 插件观察我们请求的 URL 的实际的内容，以下是 URL： HTTP://localhost:8080/examples/servlets/servlet/君山?author=君山 君山的编码结果是：e5 90 9b e5 b1 b1，和《深入分析 Java Web 技术内幕》中的结果不一样，这是因为我使用的浏览器和插件和原作者是有区别的，那么这些浏览器之间的默认编码是不一样的，原文中的结果是： 君山的编码结果分别是：e5 90 9b e5 b1 b1，be fd c9 bd，查阅上一届的编码可知，PathInfo 是 UTF-8 编码而 QueryString 是经过 GBK 编码，至于为什么会有“%”？查阅 URL 的编码规范 RFC3986 可知浏览器编码 URL 是将非 ASCII 字符按照某种编码格式编码成 16 进制数字然后将每个 16 进制表示的字节前加上“%”，所以最终的 URL 就成了上图的格式了。 从上面测试结果可知浏览器对 PathInfo 和 QueryString 的编码是不一样的，不同浏览器对 PathInfo 也可能不一样，这就对服务器的解码造成很大的困难，下面我们以 Tomcat 为例看一下，Tomcat 接受到这个 URL 是如何解码的。 解析请求的 URL 是在 org.apache.coyote.HTTP11.InternalInputBuffer 的 parseRequestLine 方法中，这个方法把传过来的 URL 的 byte[] 设置到 org.apache.coyote.Request 的相应的属性中。这里的 URL 仍然是 byte 格式，转成 char 是在 org.apache.catalina.connector.CoyoteAdapter 的 convertURI 方法中完成的： 12345678910111213141516171819202122232425262728293031323334protected void convertURI(MessageBytes uri, Request request) throws Exception &#123; ByteChunk bc = uri.getByteChunk(); int length = bc.getLength(); CharChunk cc = uri.getCharChunk(); cc.allocate(length, -1); String enc = connector.getURIEncoding(); if (enc != null) &#123; B2CConverter conv = request.getURIConverter(); try &#123; if (conv == null) &#123; conv = new B2CConverter(enc); request.setURIConverter(conv); &#125; &#125; catch (IOException e) &#123;...&#125; if (conv != null) &#123; try &#123; conv.convert(bc, cc, cc.getBuffer().length - cc.getEnd()); uri.setChars(cc.getBuffer(), cc.getStart(), cc.getLength()); return; &#125; catch (IOException e) &#123;...&#125; &#125; &#125; // Default encoding: fast conversion byte[] bbuf = bc.getBuffer(); char[] cbuf = cc.getBuffer(); int start = bc.getStart(); for (int i = 0; i &lt; length; i++) &#123; cbuf[i] = (char) (bbuf[i + start] &amp; 0xff); &#125; uri.setChars(cbuf, 0, length); &#125; 从上面的代码中可以知道对 URL 的 URI 部分进行解码的字符集是在 connector 的 中定义的，如果没有定义，那么将以默认编码 ISO-8859-1 解析。所以如果有中文 URL 时最好把 URIEncoding 设置成 UTF-8 编码。 QueryString 又如何解析？ GET 方式 HTTP 请求的 QueryString 与 POST 方式 HTTP 请求的表单参数都是作为 Parameters 保存，都是通过 request.getParameter 获取参数值。对它们的解码是在 request.getParameter 方法第一次被调用时进行的。request.getParameter 方法被调用时将会调用 org.apache.catalina.connector.Request 的 parseParameters 方法。这个方法将会对 GET 和 POST 方式传递的参数进行解码，但是它们的解码字符集有可能不一样。POST 表单的解码将在后面介绍，QueryString 的解码字符集是在哪定义的呢？它本身是通过 HTTP 的 Header 传到服务端的，并且也在 URL 中，是否和 URI 的解码字符集一样呢？从前面浏览器对 PathInfo 和 QueryString 的编码采取不同的编码格式不同可以猜测到解码字符集肯定也不会是一致的。的确是这样 QueryString 的解码字符集要么是 Header 中 ContentType 中定义的 Charset 要么就是默认的 ISO-8859-1，要使用 ContentType 中定义的编码就要设置 connector 的 中的 useBodyEncodingForURI 设置为 true。这个配置项的名字有点让人产生混淆，它并不是对整个 URI 都采用 BodyEncoding 进行解码而仅仅是对 QueryString 使用 BodyEncoding 解码，这一点还要特别注意。 从上面的 URL 编码和解码过程来看，比较复杂，而且编码和解码并不是我们在应用程序中能完全控制的，所以在我们的应用程序中应该尽量避免在 URL 中使用非 ASCII 字符，不然很可能会碰到乱码问题，当然在我们的服务器端最好设置 中的 URIEncoding 和 useBodyEncodingForURI 两个参数。 4.2 HTTP Header 的编解码当客户端发起一个 HTTP 请求除了上面的 URL 外还可能会在 Header 中传递其它参数如 Cookie、redirectPath 等，这些用户设置的值很可能也会存在编码问题，Tomcat 对它们又是怎么解码的呢？ 对 Header 中的项进行解码也是在调用 request.getHeader 是进行的，如果请求的 Header 项没有解码则调用 MessageBytes 的 toString 方法，这个方法将从 byte 到 char 的转化使用的默认编码也是 ISO-8859-1，而我们也不能设置 Header 的其它解码格式，所以如果你设置 Header 中有非 ASCII 字符解码肯定会有乱码。 我们在添加 Header 时也是同样的道理，不要在 Header 中传递非 ASCII 字符，如果一定要传递的话，我们可以先将这些字符用 org.apache.catalina.util.URLEncoder 编码然后再添加到 Header 中，这样在浏览器到服务器的传递过程中就不会丢失信息了，如果我们要访问这些项时再按照相应的字符集解码就好了。 4.3 POST 表单的编解码在前面提到了 POST 表单提交的参数的解码是在第一次调用 request.getParameter 发生的，POST 表单参数传递方式与 QueryString 不同，它是通过 HTTP 的 BODY 传递到服务端的。当我们在页面上点击 submit 按钮时浏览器首先将根据 ContentType 的 Charset 编码格式对表单填的参数进行编码然后提交到服务器端，在服务器端同样也是用 ContentType 中字符集进行解码。所以通过 POST 表单提交的参数一般不会出现问题，而且这个字符集编码是我们自己设置的，可以通过 request.setCharacterEncoding(charset) 来设置。 另外针对 multipart/form-data 类型的参数，也就是上传的文件编码同样也是使用 ContentType 定义的字符集编码，值得注意的地方是上传文件是用字节流的方式传输到服务器的本地临时目录，这个过程并没有涉及到字符编码，而真正编码是在将文件内容添加到 parameters 中，如果用这个编码不能编码时将会用默认编码 ISO-8859-1 来编码。 4.4 HTTP BODY 的编解码当用户请求的资源已经成功获取后，这些内容将通过 Response 返回给客户端浏览器，这个过程先要经过编码再到浏览器进行解码。这个过程的编解码字符集可以通过 response.setCharacterEncoding 来设置，它将会覆盖 request.getCharacterEncoding 的值，并且通过 Header 的 Content-Type 返回客户端，浏览器接受到返回的 socket 流时将通过 Content-Type 的 charset 来解码，如果返回的 HTTP Header 中 Content-Type 没有设置 charset，那么浏览器将根据 Html 的 中的 charset 来解码。如果也没有定义的话，那么浏览器将使用默认的编码来解码。 4.5 其它需要编码的地方除了 URL 和参数编码问题外，在服务端还有很多地方可能存在编码，如可能需要读取 xml、velocity 模版引擎、JSP 或者从数据库读取数据等。xml 文件可以通过设置头来制定编码格式 1&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; Velocity 模版设置编码格式： 1services.VelocityService.input.encoding=UTF-8 JSP 设置编码格式： 1&lt;%@page contentType=&quot;text/html; charset=UTF-8&quot;%&gt; 访问数据库都是通过客户端 JDBC 驱动来完成，用 JDBC 来存取数据要和数据的内置编码保持一致，可以通过设置 JDBC URL 来制定如 MySQL：url=”jdbc:mysql://localhost:3306/DB?useUnicode=true&amp;characterEncoding=GBK”。 5、常见问题分析下面看一下，当我们碰到一些乱码时，应该怎么处理这些问题？出现乱码问题唯一的原因都是在 char 到 byte 或 byte 到 char 转换中编码和解码的字符集不一致导致的，由于往往一次操作涉及到多次编解码，所以出现乱码时很难查找到底是哪个环节出现了问题，下面就几种常见的现象进行分析。 5.1 中文变成了看不懂的字符例如，字符串“淘！我喜欢！”变成了“Ì Ô £ ¡Î Ò Ï²»¶ £ ¡”编码过程如下图所示： 字符串在解码时所用的字符集与编码字符集不一致导致汉字变成了看不懂的乱码，而且是一个汉字字符变成两个乱码字符。 5.2 一个汉字变成一个问号例如，字符串“淘！我喜欢！”变成了“？？？？？？”编码过程如下图所示: 将中文和中文符号经过不支持中文的 ISO-8859-1 编码后，所有字符变成了“？”，这是因为用 ISO-8859-1 进行编解码时遇到不在码值范围内的字符时统一用 3f 表示，这也就是通常所说的“黑洞”，所有 ISO-8859-1 不认识的字符都变成了“？”。 5.3 一个汉字变成两个问号例如，字符串“淘！我喜欢！”变成了“？？？？？？？？？？？？”编码过程如下图所示: 这种情况比较复杂，中文经过多次编码，但是其中有一次编码或者解码不对仍然会出现中文字符变成“？”现象，出现这种情况要仔细查看中间的编码环节，找出出现编码错误的地方。 5.4 一种不正常的正确编码还有一种情况是在我们通过 request.getParameter 获取参数值时，当我们直接调用 String value = request.getParameter(name); 会出现乱码，但是如果用下面的方式 String value = String(request.getParameter(name).getBytes(&quot; ISO-8859-1&quot;), &quot;GBK&quot;); 解析时取得的 value 会是正确的汉字字符，这种情况是怎么造成的呢？ 看下如所示： 这种情况是这样的，ISO-8859-1 字符集的编码范围是 0000-00FF，正好和一个字节的编码范围相对应。这种特性保证了使用 ISO-8859-1 进行编码和解码可以保持编码数值“不变”。虽然中文字符在经过网络传输时，被错误地“拆”成了两个欧洲字符，但由于输出时也是用 ISO-8859-1，结果被“拆”开的中文字的两半又被合并在一起，从而又刚好组成了一个正确的汉字。虽然最终能取得正确的汉字，但是还是不建议用这种不正常的方式取得参数值，因为这中间增加了一次额外的编码与解码，这种情况出现乱码时因为 Tomcat 的配置文件中 useBodyEncodingForURI 配置项没有设置为”true”，从而造成第一次解析式用 ISO-8859-1 来解析才造成乱码的。 6、总结本文首先总结了几种常见编码格式的区别，然后介绍了支持中文的几种编码格式，并比较了它们的使用场景。接着介绍了 Java 那些地方会涉及到编码问题，已经 Java 中如何对编码的支持。并以网络 I/O 为例重点介绍了 HTTP 请求中的存在编码的地方，以及 Tomcat 对 HTTP 协议的解析，最后分析了我们平常遇到的乱码问题出现的原因。 综上所述，要解决中文问题，首先要搞清楚哪些地方会引起字符到字节的编码以及字节到字符的解码，最常见的地方就是读取会存储数据到磁盘，或者数据要经过网络传输。然后针对这些地方搞清楚操作这些数据的框架的或系统是如何控制编码的，正确设置编码格式，避免使用软件默认的或者是操作系统平台默认的编码格式。 注明：文章大部分参考书籍《深入 Java Web 技术内幕》第三章，自己有删减，二次转载请也务必注明此出处。","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"编码","slug":"编码","permalink":"http://www.54tianzhisheng.cn/tags/编码/"}]},{"title":"深度探究Java 中 finally 语句块","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/深度探究Java 中 finally 语句块/","text":"乍看这个题目，是不是有人会问，这个谁不知道啊，大凡熟悉 Java 编程的人都知道 finally 语句块的作用和用法。有什么可深度辨析的呢？事实并非如此，我发现即使写了很多年 Java 程序的人，也不一定能够透彻的理解 finally 语句块。本篇将以生动形象的案例来带您由浅入深的来分析一下这个小小的 finally，希望这篇文章能够让您真正的理解 finally 语句块的本质，至少阅读完本篇文章后，没有觉得浪费了时间。 可不能小看这个简单的 finally，看似简单的问题背后，却隐藏了无数的玄机。接下来我就带您一步一步的揭开这个 finally 的神秘面纱。 问题分析首先来问大家一个问题：finally 语句块一定会执行吗？很多人都认为 finally 语句块是肯定要执行的，其中也包括一些很有经验的 Java 程序员。可惜并不像大多人所认为的那样，对于这个问题，答案当然是否定的，我们先来看下面这个例子。 清单 1.12345678910111213141516171819public class Test &#123;public static void main(String[] args) &#123;System.out.println(\"return value of test(): \" + test()); &#125;public static int test() &#123;int i = 1;// if(i == 1)// return 0;System.out.println(\"the previous statement of try block\");i = i / 0;try &#123; System.out.println(\"try block\"); return i; &#125;finally &#123; System.out.println(\"finally block\"); &#125; &#125;&#125; 清单 1 的执行结果如下： 1234the previous statement of try block Exception in thread &quot;main&quot; java.lang.ArithmeticException: / by zero at com.bj.charlie.Test.test(Test.java:15) at com.bj.charlie.Test.main(Test.java:6) 另外，如果去掉上例中被注释的两条语句前的注释符，执行结果则是： 1return value of test(): 0 在以上两种情况下，finally 语句块都没有执行，说明什么问题呢？只有与 finally 相对应的 try 语句块得到执行的情况下，finally 语句块才会执行。以上两种情况，都是在 try 语句块之前返回（return）或者抛出异常，所以 try 对应的 finally 语句块没有执行。 那好，即使与 finally 相对应的 try 语句块得到执行的情况下，finally 语句块一定会执行吗？不好意思，这次可能又让大家失望了，答案仍然是否定的。请看下面这个例子（清单 2）。 清单 2.12345678910111213141516public class Test &#123;public static void main(String[] args) &#123;System.out.println(\"return value of test(): \" + test()); &#125;public static int test() &#123;int i = 1;try &#123;System.out.println(\"try block\");System.exit(0);return i;&#125;finally &#123;System.out.println(\"finally block\"); &#125; &#125;&#125; 清单 2 的执行结果如下： 1try block finally 语句块还是没有执行，为什么呢？因为我们在 try 语句块中执行了 System.exit (0) 语句，终止了 Java 虚拟机的运行。那有人说了，在一般的 Java 应用中基本上是不会调用这个 System.exit(0) 方法的。OK ！没有问题，我们不调用 System.exit(0) 这个方法，那么 finally 语句块就一定会执行吗？ 再一次让大家失望了，答案还是否定的。当一个线程在执行 try 语句块或者 catch 语句块时被打断（interrupted）或者被终止（killed），与其相对应的 finally 语句块可能不会执行。还有更极端的情况，就是在线程运行 try 语句块或者 catch 语句块时，突然死机或者断电，finally 语句块肯定不会执行了。可能有人认为死机、断电这些理由有些强词夺理，没有关系，我们只是为了说明这个问题。 finally 语句剖析说了这么多，还是让我们拿出些有说服力的证据吧！还有什么证据比官方的文档更具说服力呢？让我们来看看官方网站上的《The Java Tutorials》中是怎样来描述 finally 语句块的吧！以下内容原封不动的摘自于《 The Java Tutorials 》文档。 The finally BlockThe finally block always executes when the try block exits. This ensures that the finally block is executed even if an unexpected exception occurs. But finally is useful for more than just exception handling — it allows the programmer to avoid having cleanup code accidentally bypassed by a return,continue, or break. Putting cleanup code in a finally block is always a good practice, even when no exceptions are anticipated.Note: If the JVM exits while the try or catch code is being executed, then the finally block may not execute. Likewise, if the thread executing the try or catch code is interrupted or killed, the finally block may not execute even though the application as a whole continues. 请仔细阅读并认真体会一下以上两段英文，当你真正的理解了这两段英文的确切含义，你就可以非常自信的来回答“finally 语句块是否一定会执行？”这样的问题。看来，大多时候，并不是 Java 语言本身有多么高深，而是我们忽略了对基础知识的深入理解。 接下来，我们看一下 finally 语句块是怎样执行的。在排除了以上 finally 语句块不执行的情况后，finally 语句块就得保证要执行，既然 finally 语句块一定要执行，那么它和 try 语句块与 catch 语句块的执行顺序又是怎样的呢？还有，如果 try 语句块中有 return 语句，那么 finally 语句块是在 return 之前执行，还是在 return 之后执行呢？带着这样一些问题，我们还是以具体的案例来讲解。 关于 try、catch、finally 的执行顺序问题，我们还是来看看权威的论述吧！以下内容摘自 Java 语言规范第四版（《The Java™ Programming Language, Fourth Edition》）中对于 try，catch，和 finally 的描述。 12.4. Try, catch, and finallyYou catch exceptions by enclosing code in Try blocks. The basic syntax for a Try block is:try {statements} catch (exception_type1 identifier1) {statements} catch (exception_type2 identifier2) {statements…} finally {statements} where either at least one catch clause, or the finally clause, must be present. The body of the try statement is executed until either an exception is thrown or the body finishes successfully. If an exception is thrown, each catch clause is examined in turn, from first to last, to see whether the type of the exception object is assignable to the type declared in the catch. When an assignable catch clause is found, its block is executed with its identifier set to reference the exception object. No other catch clause will be executed. Any number of catch clauses, including zero, can be associated with a particular TRy as long as each clause catches a different type of exception. If no appropriate catch is found, the exception percolates out of the try statement into any outer try that might have a catch clause to handle it. If a finally clause is present with a try, its code is executed after all other processing in the try is complete. This happens no matter how completion was achieved, whether normally, through an exception, or through a control flow statement such as return or break. 上面这段文字的大体意思是说，不管 try 语句块正常结束还是异常结束，finally 语句块是保证要执行的。如果 try 语句块正常结束，那么在 try 语句块中的语句都执行完之后，再执行 finally 语句块。如果 try 中有控制转移语句（return、break、continue）呢？那 finally 语句块是在控制转移语句之前执行，还是之后执行呢？似乎从上面的描述中我们还看不出任何端倪，不要着急，后面的讲解中我们会分析这个问题。如果 try 语句块异常结束，应该先去相应的 catch 块做异常处理，然后执行 finally 语句块。同样的问题，如果 catch 语句块中包含控制转移语句呢？ finally 语句块是在这些控制转移语句之前，还是之后执行呢？我们也会在后续讨论中提到。 其实，关于 try，catch，finally 的执行流程远非这么简单，有兴趣的读者可以参考 Java 语言规范第三版（《The Java™ Language Specification, Third Edition》）中对于 Execution of try-catch-finally 的描述，非常复杂的一个流程。限于篇幅的原因，本文不做摘录，请感兴趣的读者自行阅读。 finally 语句示例说明下面，我们先来看一个简单的例子（清单 3）。 清单 3.12345678910public class Test &#123;public static void main(String[] args) &#123;try &#123;System.out.println(\"try block\");return ;&#125; finally &#123;System.out.println(\"finally block\"); &#125; &#125;&#125; 清单 3 的执行结果为：12try blockfinally block 清单 3 说明 finally 语句块在 try 语句块中的 return 语句之前执行。我们再来看另一个例子（清单 4）。 清单 4.123456789101112131415161718public class Test &#123;public static void main(String[] args) &#123;System.out.println(\"reture value of test() : \" + test()); &#125;public static int test()&#123;int i = 1;try &#123; System.out.println(\"try block\"); i = 1 / 0; return 1;&#125;catch (Exception e)&#123;System.out.println(\"exception block\");return 2;&#125;finally &#123;System.out.println(\"finally block\"); &#125; &#125;&#125; 清单 4 的执行结果为：1234try blockexception blockfinally blockreture value of test() : 2 清单 4 说明了 finally 语句块在 catch 语句块中的 return 语句之前执行。 从上面的清单 3 和清单 4，我们可以看出，其实 finally 语句块是在 try 或者 catch 中的 return 语句之前执行的。更加一般的说法是，finally 语句块应该是在控制转移语句之前执行，控制转移语句除了 return 外，还有 break 和 continue。另外，throw 语句也属于控制转移语句。虽然 return、throw、break 和 continue 都是控制转移语句，但是它们之间是有区别的。其中 return 和 throw 把程序控制权转交给它们的调用者（invoker），而 break 和 continue 的控制权是在当前方法内转移。请大家先记住它们的区别，在后续的分析中我们还会谈到。 还是得来点有说服力的证据，下面这段摘自 Java 语言规范第四版（《The Java™ Programming Language, Fourth Edition》），请读者自己体会一下其含义。 Afinallyclause can also be used to clean up forbreak,continue, andreturn, which is one reason you will sometimes see atryclause with nocatchclauses. When any control transfer statement is executed, all relevantfinallyclauses are executed. There is no way to leave atryblock without executing itsfinallyclause. 好了，看到这里，是不是有人认为自己已经掌握了 finally 的用法了？先别忙着下结论，我们再来看两个例子 – 清单 5 和清单 6。 清单 5.123456789101112public class Test &#123; public static void main(String[] args) &#123; System.out.println(\"return value of getValue(): \" + getValue()); &#125; public static int getValue() &#123; try &#123; return 0; &#125; finally &#123; return 1; &#125; &#125; &#125; 清单 5 的执行结果：1return value of getValue(): 1 清单 6.12345678910111213public class Test &#123;public static void main(String[] args) &#123; System.out.println(\"return value of getValue(): \" + getValue()); &#125;public static int getValue() &#123; int i = 1; try &#123; return i; &#125; finally &#123; i++; &#125; &#125;&#125; 清单 6 的执行结果：1return value of getValue(): 1 利用我们上面分析得出的结论：finally 语句块是在 try 或者 catch 中的 return 语句之前执行的。 由此，可以轻松的理解清单 5 的执行结果是 1。因为 finally 中的 return 1；语句要在 try 中的 return 0；语句之前执行，那么 finally 中的 return 1；语句执行后，把程序的控制权转交给了它的调用者 main（）函数，并且返回值为 1。那为什么清单 6 的返回值不是 2，而是 1 呢？按照清单 5 的分析逻辑，finally 中的 i++；语句应该在 try 中的 return i；之前执行啊？ i 的初始值为 1，那么执行 i++；之后为 2，再执行 return i；那不就应该是 2 吗？怎么变成 1 了呢？ 关于 Java 虚拟机是如何编译 finally 语句块的问题，有兴趣的读者可以参考《 The JavaTM Virtual Machine Specification, Second Edition 》中 7.13 节 Compiling finally。那里详细介绍了 Java 虚拟机是如何编译 finally 语句块。实际上，Java 虚拟机会把 finally 语句块作为 subroutine（对于这个 subroutine 不知该如何翻译为好，干脆就不翻译了，免得产生歧义和误解。）直接插入到 try 语句块或者 catch 语句块的控制转移语句之前。但是，还有另外一个不可忽视的因素，那就是在执行 subroutine（也就是 finally 语句块）之前，try 或者 catch 语句块会保留其返回值到本地变量表（Local Variable Table）中。待 subroutine 执行完毕之后，再恢复保留的返回值到操作数栈中，然后通过 return 或者 throw 语句将其返回给该方法的调用者（invoker）。请注意，前文中我们曾经提到过 return、throw 和 break、continue 的区别，对于这条规则（保留返回值），只适用于 return 和 throw 语句，不适用于 break 和 continue 语句，因为它们根本就没有返回值。 是不是不太好理解，那我们就用具体的例子来做形象的说明吧！ 为了能够解释清单 6 的执行结果，我们来分析一下清单 6 的字节码（byte-code）： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647Compiled from &quot;Test.java&quot; public class Test extends java.lang.Object&#123; public Test(); Code: 0: aload_0 1:invokespecial#1; //Method java/lang/Object.&quot;&lt;init&gt;&quot;:()V 4: return LineNumberTable: line 1: 0 public static void main(java.lang.String[]); Code: 0: getstatic #2; //Field java/lang/System.out:Ljava/io/PrintStream; 3: new #3; //class java/lang/StringBuilder 6: dup 7: invokespecial #4; //Method java/lang/StringBuilder.&quot;&lt;init&gt;&quot;:()V 10: ldc #5; //String return value of getValue(): 12: invokevirtual #6; //Method java/lang/StringBuilder.append:( Ljava/lang/String;)Ljava/lang/StringBuilder; 15: invokestatic #7; //Method getValue:()I 18: invokevirtual #8; //Method java/lang/StringBuilder.append:(I)Ljava/lang/StringBuilder; 21: invokevirtual #9; //Method java/lang/StringBuilder.toString:()Ljava/lang/String; 24: invokevirtual #10; //Method java/io/PrintStream.println:(Ljava/lang/String;)V 27: return public static int getValue(); Code: 0: iconst_1 1: istore_0 2: iload_0 3: istore_1 4: iinc 0, 1 7: iload_1 8: ireturn 9: astore_2 10: iinc 0, 1 13: aload_2 14: athrow Exception table: from to target type 2 4 9 any 9 10 9 any &#125; 对于 Test（）构造方法与 main（）方法，在这里，我们不做过多解释。让我们来分析一下 getValue（）方法的执行。在这之前，先让我把 getValue（）中用到的虚拟机指令解释一下，以便读者能够正确的理解该函数的执行。 123456789101112131415161718192021222324252627282930313233343536373839404142434445461. iconst_Description: Push the int constant (-1, 0, 1, 2, 3, 4 or 5) onto the operand stack.Forms: iconst_m1 = 2 (0x2) iconst_0 = 3 (0x3) iconst_1 = 4 (0x4)iconst_2 = 5 (0x5) iconst_3 = 6 (0x6) iconst_4 = 7 (0x7) iconst_5 = 8 (0x8)2. istore_Description: Store int into local variable. The must be an index into thelocal variable array of the current frame.Forms: istore_0 = 59 (0x3b) istore_1 = 60 (0x3c) istore_2 = 61 (0x3d)istore_3 = 62 (0x3e)3. iload_Description: Load int from local variable. The must be an index into thelocal variable array of the current frame.Forms: iload_0 = 26 (0x1a) iload_1 = 27 (0x1b) iload_2 = 28 (0x1c) iload_3 = 29 (0x1d)4. iinc index, constDescription: Increment local variable by constant. The index is an unsigned byte thatmust be an index into the local variable array of the current frame. The const is animmediate signed byte. The local variable at index must contain an int. The valueconst is first sign-extended to an int, and then the local variable at index isincremented by that amount.Forms: iinc = 132 (0x84)Format:iincindexconst5. ireturnDescription: Return int from method.Forms: ireturn = 172 (0xac)6. astore_Description: Store reference into local variable. The must be an index into thelocal variable array of the current frame.Forms: astore_0 = 75 (0x4b) astore_1 = 76 (0x4c) astore_2 =77 (0x4d) astore_3 =78 (0x4e)7. aload_Description: Load reference from local variable. The must be an index into thelocal variable array of the current frame.Forms: aload_0 = 42 (0x2a) aload_1 = 43 (0x2b) aload_2 = 44 (0x2c) aload_3 = 45 (0x2d)8. athrowDescription: Throw exception or error.Forms: athrow = 191 (0xbf) 有了以上的 Java 虚拟机指令，我们来分析一下其执行顺序：分为正常执行（没有 exception）和异常执行（有 exception）两种情况。我们先来看一下正常执行的情况，如图 1 所示： ###图 1. getValue（）函数正常执行的情况 由上图，我们可以清晰的看出，在 finally 语句块（iinc 0, 1）执行之前，getValue（）方法保存了其返回值（1）到本地表量表中 1 的位置，完成这个任务的指令是 istore_1；然后执行 finally 语句块（iinc 0, 1），finally 语句块把位于 0 这个位置的本地变量表中的值加 1，变成 2；待 finally 语句块执行完毕之后，把本地表量表中 1 的位置上值恢复到操作数栈（iload _1），最后执行 ireturn 指令把当前操作数栈中的值（1）返回给其调用者（main）。这就是为什么清单 6 的执行结果是 1，而不是 2 的原因。 再让我们来看看异常执行的情况。是不是有人会问，你的清单 6 中都没有 catch 语句，哪来的异常处理呢？我觉得这是一个好问题，其实，即使没有 catch 语句，Java 编译器编译出的字节码中还是有默认的异常处理的，别忘了，除了需要捕获的异常，还可能有不需捕获的异常（如：RunTimeException 和 Error）。 从 getValue（）方法的字节码中，我们可以看到它的异常处理表（exception table）， 如下：123Exception table:from to target type2 4 9 any 它的意思是说：如果从 2 到 4 这段指令出现异常，则由从 9 开始的指令来处理。 图 2. getValue（）函数异常执行的情况 先说明一点，上图中的 exception 其实应该是 exception 对象的引用，为了方便说明，我直接把它写成 exception 了。 由上图（图 2）可知，当从 2 到 4 这段指令出现异常时，将会产生一个 exception 对象，并且把它压入当前操作数栈的栈顶。接下来是 astore_2 这条指令，它负责把 exception 对象保存到本地变量表中 2 的位置，然后执行 finally 语句块，待 finally 语句块执行完毕后，再由 aload _2 这条指令把预先存储的 exception 对象恢复到操作数栈中，最后由 athrow 指令将其返回给该方法的调用者（main）。 通过以上的分析，大家应该已经清楚 try-catch-finally 语句块的执行流程了吧！为了更具说服力，我们还是来引经据典吧！下面这段仍然摘自 Java 语言规范第四版 《The Java™ Programming Language, Fourth Edition》，请读者自己体会吧！ a finally clause is always entered with a reason. That reason may be that the try code finished normally, that it executed a control flow statement such as return, or that an exception was thrown in code executed in the Try block. The reason is remembered when the finally clause exits by falling out the bottom. However, if the finally block creates its own reason to leave by executing a control flow statement (such as break or return) or by throwing an exception, that reason supersedes the original one, and the original reason is forgotten. For example, consider the following code:try {// … do something …return 1;} finally {return 2;}When the Try block executes its return, the finally block is entered with the “reason” of returning the value 1. However, inside the finally block the value 2 is returned, so the initial intention is forgotten. In fact, if any of the other code in the try block had thrown an exception, the result would still be to return 2. If the finally block did not return a value but simply fell out the bottom, the “return the value 1 ″ reason would be remembered and carried out. 好了，有了以上的知识，让我们再来看以下 3 个例子。 清单 7.123456789101112131415public class Test &#123; public static void main(String[] args) &#123; System.out.println(\"return value of getValue(): \" + getValue()); &#125; @SuppressWarnings(\"finally\") public static int getValue() &#123; int i = 1; try &#123; i = 4; &#125; finally &#123; i++; return i; &#125; &#125; &#125; 清单 7 的执行结果：1return value of getValue(): 5 清单 8.1234567891011121314public class Test &#123;public static void main(String[] args) &#123; System.out.println(\"return value of getValue(): \" + getValue()); &#125;public static int getValue() &#123; int i = 1; try &#123; i = 4; &#125; finally &#123; i++; &#125; return i; &#125;&#125; 清单 8 的执行结果：1return value of getValue(): 5 清单 7 和清单 8 应该还比较简单吧！利用我们上面讲解的知识，很容易分析出其结果。让我们再来看一个稍微复杂一点的例子 – 清单 9。我建议大家最好先不要看执行结果，运用学过的知识来分析一下，看是否能推断出正确的结果。 清单 9.1234567891011121314151617public class Test &#123;public static void main(String[] args) &#123;System.out.println(test()); &#125;public static String test() &#123;try &#123;System.out.println(\"try block\");return test1();&#125; finally &#123;System.out.println(\"finally block\"); &#125; &#125;public static String test1() &#123;System.out.println(\"return statement\");return \"after return\"; &#125;&#125; 清单 9 的结果：1234try blockreturn statementfinally blockafter return 你分析对了吗？其实这个案例也不算很难，return test1();这条语句等同于 : 12String tmp = test1();return tmp; 这样，就应该清楚为什么是上面所示的执行结果了吧！ 如果还是不怎么清楚可以在 IDE 下试用下 Debug，然后查看具体的运行步骤。 好了，就写到这吧！希望大家看完这篇文章能够有所收获！ 总结没想到吧！一个小小的、看似简单的 finally 语句块背后居然隐藏了这么多玄机。看来，我们平时还是应该认真的阅读 Java 相关的基础文档，比如：Java 语言规范、Java 虚拟机规范等，很多棘手的问题都可以从中得到答案。只有真正的吃透了基础知识，才能达到运用自如的境界！ 参考资料参考 The Java Tutorials，查看对 finally 语句块的描述。 参考 《The Java Programming Language, Fourth Edition》，查看 Java 语言规范第四版中对 finally 语句块的解释。 参考 《The Java Language Specification, Third Edition》，查看 Java 语言规范第三版中对 finally 语句块执行流程的具体描述。 参考 《The Java Virtual Machine Specification, Second Edition》，查看 Java 虚拟机是如何来编译 finally 语句块的知识。 查看文章 《Java 的异常处理机制(try … catch … finally)》，了解更多关于 Java 中 finally 语句块的分析。 查看文章 《Java 中 finally 的辨析》，了解更多关于 Java 中 finally 语句块的分析。 查看文章《关于 Java 中 finally 语句块的深度辨析》","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"finally","slug":"finally","permalink":"http://www.54tianzhisheng.cn/tags/finally/"}]},{"title":"通过项目逐步深入了解Mybatis（二）","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/通过项目逐步深入了解Mybatis(二)/","text":"转载请务必注明出处，原创不易！ 相关文章：通过项目逐步深入了解Mybatis&lt;一&gt;本项目全部代码地址：Github-Mybatis Mybatis 解决 jdbc 编程的问题1、 数据库链接创建、释放频繁造成系统资源浪费从而影响系统性能，如果使用数据库链接池可解决此问题。 解决：在SqlMapConfig.xml中配置数据链接池，使用连接池管理数据库链接。 2、 Sql语句写在代码中造成代码不易维护，实际应用sql变化的可能较大，sql变动需要改变java代码。 解决：将Sql语句配置在XXXXmapper.xml文件中与java代码分离。 3、 向sql语句传参数麻烦，因为sql语句的where条件不一定，可能多也可能少，占位符需要和参数一一对应。 解决：Mybatis自动将java对象映射至sql语句，通过statement中的parameterType定义输入参数的类型。 4、 对结果集解析麻烦，sql变化导致解析代码变化，且解析前需要遍历，如果能将数据库记录封装成pojo对象解析比较方便。 解决：Mybatis自动将sql执行结果映射至java对象，通过statement中的resultType定义输出结果的类型。 Mybatis 与 Hibernate 不同Mybatis和hibernate不同，它不完全是一个ORM框架，因为MyBatis需要程序员自己编写Sql语句，不过mybatis可以通过XML或注解方式灵活配置要运行的sql语句，并将java对象和sql语句映射生成最终执行的sql，最后将sql执行的结果再映射生成java对象。 Mybatis学习门槛低，简单易学，程序员直接编写原生态sql，可严格控制sql执行性能，灵活度高，非常适合对关系数据模型要求不高的软件开发，例如互联网软件、企业运营类软件等，因为这类软件需求变化频繁，一但需求变化要求成果输出迅速。但是灵活的前提是mybatis无法做到数据库无关性，如果需要实现支持多种数据库的软件则需要自定义多套sql映射文件，工作量大。 Hibernate对象/关系映射能力强，数据库无关性好，对于关系模型要求高的软件（例如需求固定的定制化软件）如果用hibernate开发可以节省很多代码，提高效率。但是Hibernate的学习门槛高，要精通门槛更高，而且怎么设计O/R映射，在性能和对象模型之间如何权衡，以及怎样用好Hibernate需要具有很强的经验和能力才行。 总之，按照用户的需求在有限的资源环境下只要能做出维护性、扩展性良好的软件架构都是好架构，所以框架只有适合才是最好。 Mybatis 开发 dao两种方法 原始 dao 开发方法（程序需要编写 dao 接口和 dao 实现类）（掌握） Mybatis 的 mapper 接口（相当于 dao 接口）代理开发方法（掌握） 需求将下边的功能实现Dao： 根据用户id查询一个用户信息 根据用户名称模糊查询用户信息列表 添加用户信息 Mybatis 配置文件 SqlMapConfig.xml Sqlsession 的使用范围SqlSession 中封装了对数据库的操作，如：查询、插入、更新、删除等。 通过 SqlSessionFactory 创建 SqlSession，而 SqlSessionFactory 是通过 SqlSessionFactoryBuilder 进行创建。 1、SqlSessionFactoryBuilderSqlSessionFactoryBuilder 用于创建 SqlSessionFacoty，SqlSessionFacoty 一旦创建完成就不需要SqlSessionFactoryBuilder 了，因为 SqlSession 是通过 SqlSessionFactory 生产，所以可以将SqlSessionFactoryBuilder 当成一个工具类使用，最佳使用范围是方法范围即方法体内局部变量。 2、SqlSessionFactorySqlSessionFactory 是一个接口，接口中定义了 openSession 的不同重载方法，SqlSessionFactory 的最佳使用范围是整个应用运行期间，一旦创建后可以重复使用，通常以单例模式管理 SqlSessionFactory。 3、SqlSessionSqlSession 是一个面向用户的接口， sqlSession 中定义了数据库操作，默认使用 DefaultSqlSession 实现类。 执行过程如下： 1）、 加载数据源等配置信息 Environment environment = configuration.getEnvironment(); 2）、 创建数据库链接 3）、 创建事务对象 4）、 创建Executor，SqlSession 所有操作都是通过 Executor 完成，mybatis 源码如下： 12345678910if (ExecutorType.BATCH == executorType) &#123; executor = newBatchExecutor(this, transaction); &#125; elseif (ExecutorType.REUSE == executorType) &#123; executor = new ReuseExecutor(this, transaction); &#125; else &#123; executor = new SimpleExecutor(this, transaction); &#125;if (cacheEnabled) &#123; executor = new CachingExecutor(executor, autoCommit); &#125; 5）、 SqlSession的实现类即 DefaultSqlSession，此对象中对操作数据库实质上用的是 Executor 结论：每个线程都应该有它自己的SqlSession实例。SqlSession的实例不能共享使用，它也是线程不安全的。因此最佳的范围是请求或方法范围(定义局部变量使用)。绝对不能将SqlSession实例的引用放在一个类的静态字段或实例字段中。 打开一个SqlSession；使用完毕就要关闭它。通常把这个关闭操作放到 finally 块中以确保每次都能执行关闭。如下： 123456SqlSession session = sqlSessionFactory.openSession(); try &#123; // do work &#125; finally &#123; session.close();&#125; 原始 Dao 开发方法思路：需要程序员编写 Dao 接口和 Dao 实现类； 需要在 Dao 实现类中注入 SqlsessionFactory ，在方法体内通过 SqlsessionFactory 创建 Sqlsession。 Dao接口1234567891011public interface UserDao //dao接口，用户管理&#123; //根据id查询用户信息 public User findUserById(int id) throws Exception; //添加用户信息 public void addUser(User user) throws Exception; //删除用户信息 public void deleteUser(int id) throws Exception;&#125; Dao 实现类1234567891011121314151617181920212223242526272829303132333435363738394041public class UserDaoImpl implements UserDao //dao接口实现类&#123; //需要在 Dao 实现类中注入 SqlsessionFactory //这里通过构造方法注入 private SqlSessionFactory sqlSessionFactory; public UserDaoImpl(SqlSessionFactory sqlSessionFactory) &#123; this.sqlSessionFactory = sqlSessionFactory; &#125; @Override public User findUserById(int id) throws Exception &#123; //在方法体内通过 SqlsessionFactory 创建 Sqlsession SqlSession sqlSession = sqlSessionFactory.openSession(); User user = sqlSession.selectOne(\"test.findUserById\", id); sqlSession.close(); return user; &#125; @Override public void insertUser(User user) throws Exception &#123; //在方法体内通过 SqlsessionFactory 创建 Sqlsession SqlSession sqlSession = sqlSessionFactory.openSession(); //执行插入的操作 sqlSession.insert(\"test.insetrUser\", user); //提交事务 sqlSession.commit(); //释放资源 sqlSession.close(); &#125; @Override public void deleteUser(int id) throws Exception &#123; //在方法体内通过 SqlsessionFactory 创建 Sqlsession SqlSession sqlSession = sqlSessionFactory.openSession(); sqlSession.delete(\"test.deleteUserById\", id); //提交事务 sqlSession.commit(); sqlSession.close(); &#125;&#125; 测试12345678910111213141516171819202122232425public class UserDaoImplTest&#123; private SqlSessionFactory sqlSessionFactory; //此方法是在 testFindUserById 方法之前执行的 @Before public void setup() throws Exception &#123; //创建sqlSessionFactory //Mybatis 配置文件 String resource = \"SqlMapConfig.xml\"; //得到配置文件流 InputStream inputStream = Resources.getResourceAsStream(resource); //创建会话工厂,传入Mybatis的配置文件信息 sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); &#125; @Test public void testFindUserById() throws Exception &#123; //创建UserDao的对象 UserDao userDao = new UserDaoImpl(sqlSessionFactory); //调用UserDao方法 User user = userDao.findUserById(1); System.out.println(user); &#125;&#125; 通过id查询用户信息测试结果如下：（其他的可以自己在写测试代码，原理类似） 问题原始Dao开发中存在以下问题： Dao方法体存在重复代码：通过 SqlSessionFactory 创建 SqlSession，调用 SqlSession 的数据库操作方法 调用 sqlSession 的数据库操作方法需要指定 statement 的i d，这里存在硬编码，不得于开发维护。 调用 sqlSession 的数据库操作方法时传入的变量，由于 sqlsession 方法使用泛型，即使变量类型传入错误，在编译阶段也不报错，不利于程序员开发。 Mybatis 的 mapper 接口思路程序员需要编写 mapper.xml 映射文件 只需要程序员编写Mapper接口（相当于Dao接口），需遵循一些开发规范，mybatis 可以自动生成 mapper 接口类代理对象。 开发规范： 在 mapper.xml 中 namespace 等于 mapper 接口地址 1&lt;mapper namespace=\"cn.zhisheng.mybatis.mapper.UserMapper\"&gt;&lt;/mapper&gt; 在 xxxmapper.java 接口中的方法名要与 xxxMapper.xml 中 statement 的 id 一致。 在 xxxmapper.java 接口中的输入参数类型要与 xxxMapper.xml 中 statement 的 parameterType 指定的参数类型一致。 在 xxxmapper.java 接口中的返回值类型要与 xxxMapper.xml 中 statement 的 resultType 指定的类型一致。 UserMapper.java 12//根据id查询用户信息 public User findUserById(int id) throws Exception; UserMapper.xml 123&lt;select id=\"findUserById\" parameterType=\"int\" resultType=\"cn.zhisheng.mybatis.po.User\"&gt; select * from user where id = #&#123;1&#125;&lt;/select&gt; 总结：以上的开发规范主要是对下边的代码进行统一的生成： 1234User user = sqlSession.selectOne(\"test.findUserById\", id);sqlSession.insert(\"test.insetrUser\", user);sqlSession.delete(\"test.deleteUserById\", id);List&lt;User&gt; list = sqlSession.selectList(\"test.findUserByName\", username); 测试测试之前记得在 SqlMapConfig.xml 文件中添加加载映射文件 UserMapper.xml： 1&lt;mapper resource=\"mapper/UserMapper.xml\"/&gt; 测试代码： 1234567891011121314151617181920212223242526public class UserMapperTest&#123; private SqlSessionFactory sqlSessionFactory; //此方法是在 testFindUserById 方法之前执行的 @Before public void setup() throws Exception &#123; //创建sqlSessionFactory //Mybatis 配置文件 String resource = \"SqlMapConfig.xml\"; //得到配置文件流 InputStream inputStream = Resources.getResourceAsStream(resource); //创建会话工厂,传入Mybatis的配置文件信息 sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); &#125; @Test public void testFindUserById() throws Exception &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); //创建usermapper对象,mybatis自动生成代理对象 UserMapper userMapper = sqlSession.getMapper(UserMapper.class); //调用UserMapper的方法 User user = userMapper.findUserById(1); System.out.println(user); &#125;&#125; 通过id查询用户信息测试结果如下：（其他的请自己根据上下文写测试代码，或者去看我 Github-Mybatis学习笔记 上看我这个项目的全部代码） 通过姓名查询用户信息： 代理对象内部调用 selectOne 或者 selectList 如果 mapper 方法返回单个 pojo 对象（非集合对象），代理对象内部通过 selectOne 查询数据库 如果 mapper 方法返回集合对象，代理对象内部通过 selectList 查询数据库 mapper接口方法参数只能有一个是否影响系统开发 mapper 接口方法参数只能有一个，系统是否不利于维护？ 系统框架中，dao层的代码是被业务层公用的。 即使 mapper 接口只有一个参数，可以使用包装类型的 pojo 满足不同的业务方法的需求。 注意：持久层方法的参数可以包装类型、map…. ，service方法中不建议使用包装类型。（不利于业务层的可扩展性） SqlMapConfig.xml 文件Mybatis 的全局配置变量，配置内容和顺序如下： properties（属性） settings（全局配置参数） typeAliases（类型别名） typeHandlers（类型处理器） objectFactory（对象工厂） plugins（插件） environments（环境集合属性对象） ​ environment（环境子属性对象） ​ transactionManager（事务管理） ​ dataSource（数据源） mappers（映射器） properties 属性需求：将数据库连接参数单独配置在 db.properties 中，只需要在 SqlMapConfig.xml 中加载该配置文件 db.properties 的属性值。在 SqlMapConfig.xml 中就不需要直接对数据库的连接参数进行硬编码了。方便以后对参数进行统一的管理，其他的xml文件可以引用该 db.properties 。 db.properties 1234jdbc.driver=com.mysql.jdbc.Driverjdbc.url=jdbc:mysql://localhost:3306/mybatis_test?characterEncoding=utf-8jdbc.username=rootjdbc.password=root 那么 SqlMapConfig.xml 中的配置变成如下： 12345678910111213141516&lt;!--加载配置文件--&gt; &lt;properties resource=\"db.properties\"&gt;&lt;/properties&gt; &lt;!-- 和spring整合后 environments配置将废除--&gt; &lt;environments default=\"development\"&gt; &lt;environment id=\"development\"&gt; &lt;!-- 使用jdbc事务管理,事务由 Mybatis 控制--&gt; &lt;transactionManager type=\"JDBC\" /&gt; &lt;!-- 数据库连接池--&gt; &lt;dataSource type=\"POOLED\"&gt; &lt;property name=\"driver\" value=\"$&#123;jdbc.driver&#125;\" /&gt; &lt;property name=\"url\" value=\"$&#123;jdbc.url&#125;\" /&gt; &lt;property name=\"username\" value=\"$&#123;jdbc.username&#125;\" /&gt; &lt;property name=\"password\" value=\"$&#123;jdbc.password&#125;\" /&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; 配置完成后我们测试一下是否能够和刚才一样的能够成功呢？那么我就先在db.properties中把数据库密码故意改错，看是否是正确的？不出意外的话是会报错的。 注意： MyBatis 将按照下面的顺序来加载属性： 在 properties 元素体内定义的属性首先被读取。 然后会读取 properties 元素中 resource 或 url 加载的属性，它会覆盖已读取的同名属性。 最后读取 parameterType 传递的属性，它会覆盖已读取的同名属性。 因此，通过parameterType传递的属性具有最高优先级，resource或 url 加载的属性次之，最低优先级的是 properties 元素体内定义的属性。 建议： 不要在 properties 元素体内添加任何属性值，只将属性值定义在 db.properties 文件之中。 在 db.properties 文件之中定义的属性名要有一定的特殊性。如 xxx.xxx.xxx settings（全局配置参数）Mybatis 框架在运行时可以调整一些运行参数 比如：开启二级缓存、开启延迟加载。。。 typeAliases（类型别名）需求： 在mapper.xml中，定义很多的statement，statement需要parameterType指定输入参数的类型、需要resultType指定输出结果的映射类型。 如果在指定类型时输入类型全路径，不方便进行开发，可以针对parameterType或resultType指定的类型定义一些别名，在mapper.xml中通过别名定义，方便开发。 Mybatis支持的别名： 别名 映射的类型 _byte byte _long long _short short _int int _integer int _double double _float float _boolean boolean string String byte Byte long Long short Short int Integer integer Integer double Double float Float boolean Boolean date Date decimal BigDecimal bigdecimal BigDecimal 自定义别名： 在 SqlMapConfig.xml 中配置：(设置别名) 1234567&lt;typeAliases&gt; &lt;!-- 单个别名定义 --&gt; &lt;typeAlias alias=\"user\" type=\"cn.zhisheng.mybatis.po.User\"/&gt; &lt;!-- 批量别名定义，扫描整个包下的类，别名为类名（首字母大写或小写都可以） --&gt; &lt;package name=\"cn.zhisheng.mybatis.po\"/&gt; &lt;package name=\"其它包\"/&gt;&lt;/typeAliases&gt; 在 UserMapper.xml 中引用别名：( resultType 为 user ) 123&lt;select id=\"findUserById\" parameterType=\"int\" resultType=\"user\"&gt; select * from user where id = #&#123;id&#125;&lt;/select&gt; 测试结果： typeHandlers（类型处理器）mybatis中通过typeHandlers完成jdbc类型和java类型的转换。 通常情况下，mybatis提供的类型处理器满足日常需要，不需要自定义. mybatis支持类型处理器： 类型处理器 Java类型 JDBC类型 BooleanTypeHandler Boolean，boolean 任何兼容的布尔值 ByteTypeHandler Byte，byte 任何兼容的数字或字节类型 ShortTypeHandler Short，short 任何兼容的数字或短整型 IntegerTypeHandler Integer，int 任何兼容的数字和整型 LongTypeHandler Long，long 任何兼容的数字或长整型 FloatTypeHandler Float，float 任何兼容的数字或单精度浮点型 DoubleTypeHandler Double，double 任何兼容的数字或双精度浮点型 BigDecimalTypeHandler BigDecimal 任何兼容的数字或十进制小数类型 StringTypeHandler String CHAR和VARCHAR类型 ClobTypeHandler String CLOB和LONGVARCHAR类型 NStringTypeHandler String NVARCHAR和NCHAR类型 NClobTypeHandler String NCLOB类型 ByteArrayTypeHandler byte[] 任何兼容的字节流类型 BlobTypeHandler byte[] BLOB和LONGVARBINARY类型 DateTypeHandler Date（java.util） TIMESTAMP类型 DateOnlyTypeHandler Date（java.util） DATE类型 TimeOnlyTypeHandler Date（java.util） TIME类型 SqlTimestampTypeHandler Timestamp（java.sql） TIMESTAMP类型 SqlDateTypeHandler Date（java.sql） DATE类型 SqlTimeTypeHandler Time（java.sql） TIME类型 ObjectTypeHandler 任意 其他或未指定类型 EnumTypeHandler Enumeration类型 VARCHAR-任何兼容的字符串类型，作为代码存储（而不是索引）。 mappers（映射器） 使用相对于类路径的资源，如： 使用完全限定路径如： 使用 mapper 接口类路径 如： 注意：此种方法要求 mapper 接口名称和 mapper 映射文件名称相同，且放在同一个目录中。 注册指定包下的所有mapper接口如：注意：此种方法要求 mapper 接口名称和 mapper 映射文件名称相同，且放在同一个目录中。 Mapper.xml 映射文件Mapper.xml映射文件中定义了操作数据库的sql，每个sql是一个statement，映射文件是mybatis的核心。 输入映射通过 parameterType 指定输入参数的类型，类型可以是简单类型、hashmap、pojo的包装类型。 传递 pojo 包装对象 （重点） 开发中通过pojo传递查询条件 ，查询条件是综合的查询条件，不仅包括用户查询条件还包括其它的查询条件（比如将用户购买商品信息也作为查询条件），这时可以使用包装对象传递输入参数。 定义包装对象 定义包装对象将查询条件(pojo)以类组合的方式包装起来。 UserQueryVo.java 123456789101112131415public class UserQueryVo //用户包装类型&#123; //在这里包装所需要的查询条件 //用户查询条件 private UserCustom userCustom; public UserCustom getUserCustom() &#123; return userCustom; &#125; public void setUserCustom(UserCustom userCustom) &#123; this.userCustom = userCustom; &#125; //还可以包装其他的查询条件，比如订单、商品&#125; UserCustomer.java 1234public class UserCustom extends User //用户的扩展类&#123; //可以扩展用户的信息&#125; UserMapper.xml 文件 1234567&lt;!--用户信息综合查询 #&#123;userCustom.sex&#125; :取出pojo包装对象中的性别值 #&#123;userCustom.username&#125; :取出pojo包装对象中的用户名称 --&gt; &lt;select id=\"findUserList\" parameterType=\"cn.zhisheng.mybatis.po.UserQueryVo\" resultType=\"cn.zhisheng.mybatis.po.UserCustom\"&gt; select * from user where user.sex = #&#123;userCustom.sex&#125; and user.username like '%$&#123;userCustom.username&#125;%' &lt;/select&gt; UserMapper.java 12//用户信息综合查询public List&lt;UserCustom&gt; findUserList(UserQueryVo userQueryVo) throws Exception; 测试代码 1234567891011121314151617//测试用户信息综合查询 @Test public void testFindUserList() throws Exception &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); //创建usermapper对象,mybatis自动生成代理对象 UserMapper userMapper = sqlSession.getMapper(UserMapper.class); //创建包装对象，设置查询条件 UserQueryVo userQueryVo = new UserQueryVo(); UserCustom userCustom = new UserCustom(); userCustom.setSex(\"男\"); userCustom.setUsername(\"张小明\"); userQueryVo.setUserCustom(userCustom); //调用UserMapper的方法 List&lt;UserCustom&gt; list = userMapper.findUserList(userQueryVo); System.out.println(list); &#125; 测试结果 输出映射 resultType 使用 resultType 进行输出映射，只有查询出来的列名和 pojo 中的属性名一致，该列才可以映射成功。 如果查询出来的列名和 pojo 中的属性名全部不一致，没有创建 pojo 对象。 只要查询出来的列名和 pojo 中的属性有一个一致，就会创建 pojo 对象。 输出简单类型需求：用户信息综合查询列表总数，通过查询总数和上边用户综合查询列表才可以实现分页 实现： 1234567&lt;!--用户信息综合查询总数 parameterType:指定输入的类型和findUserList一样 resultType:输出结果类型为 int --&gt; &lt;select id=\"findUserCount\" parameterType=\"cn.zhisheng.mybatis.po.UserQueryVo\" resultType=\"int\"&gt; select count(*) from user where user.sex = #&#123;userCustom.sex&#125; and user.username like '%$&#123;userCustom.username&#125;%' &lt;/select&gt; 12//用户信息综合查询总数 public int findUserCount(UserQueryVo userQueryVo) throws Exception; 12345678910111213141516//测试用户信息综合查询总数 @Test public void testFindUserCount() throws Exception &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); //创建usermapper对象,mybatis自动生成代理对象 UserMapper userMapper = sqlSession.getMapper(UserMapper.class); //创建包装对象，设置查询条件 UserQueryVo userQueryVo = new UserQueryVo(); UserCustom userCustom = new UserCustom(); userCustom.setSex(\"男\"); userCustom.setUsername(\"张小明\"); userQueryVo.setUserCustom(userCustom); //调用UserMapper的方法 System.out.println(userMapper.findUserCount(userQueryVo)); &#125; 注意：查询出来的结果集只有一行且一列，可以使用简单类型进行输出映射。 输出pojo对象和pojo列表 不管是输出的pojo单个对象还是一个列表（list中包括pojo），在mapper.xml中resultType指定的类型是一样的。 在mapper.java指定的方法返回值类型不一样： 1、输出单个pojo对象，方法返回值是单个对象类型 12//根据id查询用户信息 public User findUserById(int id) throws Exception; 2、输出pojo对象list，方法返回值是List 12//根据用户名查询用户信息 public List&lt;User&gt; findUserByUsername(String userName) throws Exception; resultType总结： 输出pojo对象和输出pojo列表在sql中定义的resultType是一样的。 返回单个pojo对象要保证sql查询出来的结果集为单条，内部使用session.selectOne方法调用，mapper接口使用pojo对象作为方法返回值。 返回pojo列表表示查询出来的结果集可能为多条，内部使用session.selectList方法，mapper接口使用List对象作为方法返回值。 resultMap resultType 可以指定 pojo 将查询结果映射为 pojo，但需要 pojo 的属性名和 sql 查询的列名一致方可映射成功。 如果sql查询字段名和pojo的属性名不一致，可以通过resultMap将字段名和属性名作一个对应关系 ，resultMap实质上还需要将查询结果映射到pojo对象中。 resultMap可以实现将查询结果映射为复杂类型的pojo，比如在查询结果映射对象中包括pojo和list实现一对一查询和一对多查询。 使用方法： 1、定义 resultMap 2、使用 resultMap 作为 statement 的输出映射类型 将下面的 sql 使用 User 完成映射 1select id id_, username username_ from user where id = #&#123;value&#125; User 类中属性名和上边查询的列名不一致。 所以需要： 1、定义 resultMap 1234567891011121314151617181920&lt;!--定义 resultMap 将select id id_, username username_ from user where id = #&#123;value&#125; 和User类中的属性做一个映射关系 type: resultMap最终映射的java对象类型 id:对resultMap的唯一标识 --&gt; &lt;resultMap id=\"userResultMap\" type=\"user\"&gt; &lt;!--id表示查询结果中的唯一标识 column：查询出来的列名 property：type指定pojo的属性名 最终resultMap对column和property做一个映射关系（对应关系） --&gt; &lt;id column=\"id_\" property=\"id\"/&gt; &lt;!--result: 对普通结果映射定义 column：查询出来的列名 property：type指定pojo的属性名 最终resultMap对column和property做一个映射关系（对应关系） --&gt; &lt;result column=\"username_\" property=\"username\"/&gt; &lt;/resultMap&gt; 2、使用 resultMap 作为 statement 的输出映射类型 12345&lt;!--使用 resultMap 作为输出映射类型 resultMap=\"userResultMap\":其中的userResultMap就是我们刚才定义的 resultMap 的id值,如果这个resultMap在其他的mapper文件中，前边须加上namespace --&gt; &lt;select id=\"findUserByIdResultMap\" parameterType=\"int\" resultMap=\"userResultMap\"&gt; select id id_, username username_ from user where id = #&#123;value&#125; &lt;/select&gt; 3、UserMapper.java 12//根据id查询用户信息，使用 resultMap 输出public User findUserByIdResultMap(int id) throws Exception; 4、测试 1234567891011//测试根据id查询用户信息，使用 resultMap 输出 @Test public void testFindUserByIdResultMap() throws Exception &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); //创建usermapper对象,mybatis自动生成代理对象 UserMapper userMapper = sqlSession.getMapper(UserMapper.class); //调用UserMapper的方法 User user = userMapper.findUserByIdResultMap(1); System.out.println(user); &#125; 5、测试结果 动态 SQL通过mybatis提供的各种标签方法实现动态拼接sql。 需求： 用户信息综合查询列表和用户信息查询列表总数这两个 statement的定义使用动态sql。 对查询条件进行判断，如果输入的参数不为空才进行查询条件拼接。 UserMapper.xml (findUserList的配置如下，那么findUserCount的也是一样的，这里就不全部写出来了) 1234567891011121314&lt;select id=\"findUserList\" parameterType=\"cn.zhisheng.mybatis.po.UserQueryVo\" resultType=\"cn.zhisheng.mybatis.po.UserCustom\"&gt; select * from user &lt;!--where可以自动的去掉条件中的第一个and--&gt; &lt;where&gt; &lt;if test=\"userCustom != null\"&gt; &lt;if test=\"userCustom.sex != null and userCustom.sex != ''\"&gt; and user.sex = #&#123;userCustom.sex&#125; &lt;/if&gt; &lt;if test=\"userCustom.username != null\"&gt; and user.username like '%$&#123;userCustom.username&#125;%' &lt;/if&gt; &lt;/if&gt; &lt;/where&gt; &lt;/select&gt; 测试代码：因为设置了动态的sql，如果不设置某个值，那么条件就不会拼接在sql上 所以我们就注释掉设置username的语句 1//userCustom.setUsername(\"张小明\"); 测试结果： Sql 片段通过上面的其实看到在 where sql语句中有很多重复代码，我们可以将其抽取出来，组成一个sql片段，其他的statement就可以引用这个sql片段，利于系统的开发。 这里我们就拿上边sql 中的where定义一个sq片段如下： 123456789101112131415&lt;!--sql片段 id:唯一标识 经验：是基于单表来定义sql片段，这样的话sql片段的可重用性才高 一般不包含where --&gt; &lt;sql id=\"query_user_where\"&gt; &lt;if test=\"userCustom != null\"&gt; &lt;if test=\"userCustom.sex != null and userCustom.sex != ''\"&gt; and user.sex = #&#123;userCustom.sex&#125; &lt;/if&gt; &lt;if test=\"userCustom.username != null\"&gt; and user.username like '%$&#123;userCustom.username&#125;%' &lt;/if&gt; &lt;/if&gt; &lt;/sql&gt; 那么我们该怎样引用这个sql片段呢？如下： 12345select * from user &lt;where&gt; &lt;!--refid: 指定sql片段的id，如果是写在其他的mapper文件中，则需要在前面加上namespace--&gt; &lt;include refid=\"query_user_where\"/&gt; &lt;/where&gt; 测试的话还是那样了，就不继续说了，前面已经说了很多了。 foreach向sql传递数组或List，mybatis使用foreach解析 需求： 在用户查询列表和查询总数的statement中增加多个id输入查询。 sql语句如下： 123SELECT * FROM USER WHERE id=1 OR id=10 ORid=16或者SELECT * FROM USER WHERE id IN(1,10,16) 在输入参数类型中添加 List ids 传入多个 id 12345public class UserQueryVo //用户包装类型&#123; //传入多个id private List&lt;Integer&gt; ids;&#125; 修改 UserMapper.xml文件 WHERE id=1 OR id=10 OR id=16 在查询条件中，查询条件定义成一个sql片段，需要修改sql片段。 12345678910111213141516171819202122&lt;if test=\"ids!=null\"&gt; &lt;!-- 使用 foreach遍历传入ids collection：指定输入 对象中集合属性 item：每个遍历生成对象中 open：开始遍历时拼接的串 close：结束遍历时拼接的串 separator：遍历的两个对象中需要拼接的串 --&gt; &lt;!-- 使用实现下边的sql拼接： AND (id=1 OR id=10 OR id=16) --&gt; &lt;foreach collection=\"ids\" item=\"user_id\" open=\"AND (\" close=\")\" separator=\"or\"&gt; &lt;!-- 每个遍历需要拼接的串 --&gt; id=#&#123;user_id&#125; &lt;/foreach&gt; &lt;!-- 实现 “ and id IN(1,10,16)”拼接 --&gt; &lt;!-- &lt;foreach collection=\"ids\" item=\"user_id\" open=\"and id IN(\" close=\")\" separator=\",\"&gt; 每个遍历需要拼接的串 #&#123;user_id&#125; &lt;/foreach&gt; --&gt; &lt;/if&gt; 测试代码： 1234567 //传入多个idList&lt;Integer&gt; ids = new ArrayList&lt;&gt;();ids.add(1);ids.add(10);ids.add(16);//将ids传入statement中userQueryVo.setIds(ids); 期待后续的文章吧！","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"http://www.54tianzhisheng.cn/tags/Mybatis/"},{"name":"SpringMVC","slug":"SpringMVC","permalink":"http://www.54tianzhisheng.cn/tags/SpringMVC/"}]},{"title":"《Java 多线程编程核心技术》学习笔记及总结","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/Java-Thread/","text":"第一章 —— Java 多线程技能线程技术点： 线程的启动 如何使线程暂停 如何使线程停止 线程的优先级 线程安全相关问题 进程和线程的概念及多线程的优点进程：比如我们电脑运行的 QQ.exe 程序，是操作系统管理的基本运行单元 线程：在进程中独立运行的子任务，比如 QQ.exe 进程中就有很多线程在运行，下载文件线程、发送消息线程、语音线程、视频线程等。 多线程优点：我们电脑可以同时操作不同的软件，边听着歌，敲着代码，查看 pdf 文档，浏览网页等，CPU 在这些任务之间不停的切换，切换非常快，所以我们就觉得他们是在同时运行的。 使用多线程继承 Thread 类JDK 源码注释（Thread.java）如下： 12345678910111213141516171819One is to declare a class to be a subclass(子类) of &lt;code&gt;Thread&lt;/code&gt;. This subclass should override the &lt;code&gt;run&lt;/code&gt; method of class &lt;code&gt;Thread&lt;/code&gt;. An instance of the subclass can then be allocated and started. For example, a thread that computes primeslarger than a stated value could be written as follows://继承 Thread 类class PrimeThread extends Thread &#123; long minPrime; PrimeThread(long minPrime) &#123; this.minPrime = minPrime; &#125; public void run() &#123; // compute primes larger than minPrime 重写 Thread 类的 run 方法 &#125; &#125;The following code would then create a thread and start it running://开启线程 PrimeThread p = new PrimeThread(143); p.start(); 实现 Runnable 接口JDK 源码注释（Thread.java）如下： 12345678910111213141516171819The other way to create a thread is to declare a class that implements the &lt;code&gt;Runnable&lt;/code&gt; interface. That class then implements the &lt;code&gt;run&lt;/code&gt; method. An instance of the class can then be allocated, passed as an argument when creating&lt;code&gt;Thread&lt;/code&gt;, and started. The same example in this other style looks like the following://实现 Runnable 接口 class PrimeRun implements Runnable &#123; long minPrime; PrimeRun(long minPrime) &#123; this.minPrime = minPrime; &#125; public void run() &#123; // compute primes larger than minPrime //重写 run 方法 &#125; &#125;The following code would then create a thread and start it running://开启线程 PrimeRun p = new PrimeRun(143); new Thread(p).start(); currentThread() 方法该方法返回代码段正在被哪个线程调用的信息。 isAlive() 方法判断当前线程是否处于活动状态（已经启动但未终止） sleep() 方法在指定的毫秒数内让当前“正在执行的线程（this.currentThread() 返回的线程）”休眠（暂停执行）。 getId() 方法获取线程的唯一标识 停止线程可以使用 Thread.stop() 方法，但最好不要用，因为这个方法是不安全的，已经弃用作废了。 大多数停止一个线程是使用 Thread.interrupt() 方法 判断线程是否是停止状态 interrupted() 12345//测试当前线程是否已经中断了，这个线程的中断状态会被这个方法清除。//换句话说，如果连续两次调用了这个方法，第二次调用的时候将会返回 false ，public static boolean interrupted() &#123; return currentThread().isInterrupted(true);&#125; isInterrupted() 1234567891011 //测试线程是否已经中断了，线程的状态不会受这个方法的影响 //线程中断被忽略，因为线程处于中断下不处于活动状态的线程由此返回false的方法反映出来 public boolean isInterrupted() &#123; return isInterrupted(false); &#125; /*** Tests if some Thread has been interrupted. The interrupted state* is reset or not based on the value of ClearInterrupted that is* passed.*/private native boolean isInterrupted(boolean ClearInterrupted); 在沉睡中停止1234567891011121314151617181920212223242526public class MyThread2 extends Thread&#123; @Override public void run() &#123; try &#123; System.out.println(\"run start\"); Thread.sleep(20000); System.out.println(\"run end\"); &#125; catch (InterruptedException e) &#123; System.out.println(\"run catch \"+this.isInterrupted()); e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; try &#123; MyThread2 t2 = new MyThread2(); t2.start(); Thread.sleep(200); t2.interrupt(); &#125; catch (InterruptedException e) &#123; System.out.println(\"main catch\"); e.printStackTrace(); &#125; System.out.println(\"main end\"); &#125;&#125; 运行结果： 123456run startmain endrun catch falsejava.lang.InterruptedException: sleep interrupted at java.lang.Thread.sleep(Native Method) at com.zhisheng.thread.thread1.MyThread2.run(MyThread2.java:12) 从运行结果来看，如果在 sleep 状态下停止某一线程，会进入 catch 语句，并清除停止状态值，使之变成 false。 在停止中沉睡12345678910111213141516171819public class MyThread3 extends Thread&#123; @Override public void run() &#123; try &#123; System.out.println(\"run start\"); Thread.sleep(20000); System.out.println(\"run end\"); &#125; catch (InterruptedException e) &#123; System.out.println(\"run catch \"+this.isInterrupted()); e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; MyThread3 t3 = new MyThread3(); t3.start(); t3.interrupt(); &#125;&#125; 运行结果： 12345run startrun catch falsejava.lang.InterruptedException: sleep interrupted at java.lang.Thread.sleep(Native Method) at com.zhisheng.thread.thread1.MyThread3.run(MyThread3.java:12) 能停止的线程 —— 暴力停止使用 stop() 方法停止线程 暂停线程可使用 suspend 方法暂停线程，使用 resume() 方法恢复线程的执行。 suspend 和 resume 方法的使用1234567891011121314151617181920212223242526272829public class MyThread4 extends Thread&#123; private int i; public int getI() &#123; return i; &#125; public void setI(int i) &#123; this.i = i; &#125; @Override public void run() &#123; while (true) &#123; i++; &#125; &#125; public static void main(String[] args) throws InterruptedException &#123; MyThread4 t4 = new MyThread4(); t4.start(); System.out.println(\"A----- \" + System.currentTimeMillis() + \" ---- \" + t4.getI()); Thread.sleep(2000); System.out.println(\"A----- \" + System.currentTimeMillis() + \" ---- \" + t4.getI()); t4.suspend(); Thread.sleep(2000); t4.resume(); System.out.println(\"B----- \" + System.currentTimeMillis() + \" ---- \" + t4.getI()); Thread.sleep(2000); System.out.println(\"B----- \" + System.currentTimeMillis() + \" ---- \" + t4.getI()); &#125;&#125; 从运行结果来看，线程的确能够暂停和恢复。 但是 suspend 和 resume 方法的缺点就是：不同步，因为线程的暂停导致数据的不同步。 yield 方法123456789101112131415161718/** * A hint to the scheduler that the current thread is willing to yield * its current use of a processor. The scheduler is free to ignore this * hint. * * &lt;p&gt; Yield is a heuristic attempt to improve relative progression * between threads that would otherwise over-utilise a CPU. Its use * should be combined with detailed profiling and benchmarking to * ensure that it actually has the desired effect. * * &lt;p&gt; It is rarely appropriate to use this method. It may be useful * for debugging or testing purposes, where it may help to reproduce * bugs due to race conditions. It may also be useful when designing * concurrency control constructs such as the ones in the * &#123;@link java.util.concurrent.locks&#125; package. */ //暂停当前正在执行的线程对象，并执行其他线程。暂停的时间不确定。 public static native void yield(); 1234567891011121314151617public class MyThread5 extends Thread&#123; @Override public void run() &#123; double start = System.currentTimeMillis(); for (int i = 0; i &lt; 200000; i++) &#123; //yield();//暂停的时间不确定 i++; &#125; double end = System.currentTimeMillis(); System.out.println(\"time is \"+(end - start)); &#125; public static void main(String[] args) &#123; MyThread5 t5 = new MyThread5(); t5.start(); &#125;&#125; 线程的优先级设置优先级的方法：setPriority() 方法 12345678910111213public final void setPriority(int newPriority) &#123; ThreadGroup g; checkAccess(); if (newPriority &gt; MAX_PRIORITY || newPriority &lt; MIN_PRIORITY) &#123; throw new IllegalArgumentException(); &#125; if((g = getThreadGroup()) != null) &#123; if (newPriority &gt; g.getMaxPriority()) &#123; newPriority = g.getMaxPriority(); &#125; setPriority0(priority = newPriority); &#125; &#125; 不一定优先级高的线程就先执行。 守护线程当进程中不存在非守护线程了，则守护线程自动销毁。垃圾回收线程就是典型的守护线程，当进程中没有非守护线程了，则垃圾回收线程也就没有存在的必要了，自动销毁。 12345678910111213141516171819202122/** * Marks this thread as either a &#123;@linkplain #isDaemon daemon&#125; thread * or a user thread. The Java Virtual Machine exits when the only * threads running are all daemon threads. * * &lt;p&gt; This method must be invoked before the thread is started. * * @param on * if &#123;@code true&#125;, marks this thread as a daemon thread * @throws IllegalThreadStateException * if this thread is &#123;@linkplain #isAlive alive&#125; * @throws SecurityException * if &#123;@link #checkAccess&#125; determines that the current * thread cannot modify this thread */ public final void setDaemon(boolean on) &#123; checkAccess(); if (isAlive()) &#123; throw new IllegalThreadStateException(); &#125; daemon = on; &#125; 第二章 —— 对象及变量的并发访问技术点： synchronized 对象监视器为 Object 时的使用 synchronized 对象监视器为 Class 时的使用 非线程安全是如何出现的 关键字 volatile 的主要作用 关键字 volatile 与 synchronized 的区别及使用情况 synchronized 同步方法方法内的变量为线程安全“非线程安全”问题存在于“实例变量”中，如果是方法内部的私有变量，则不存在“非线程安全”问题，所得结果也就是“线程安全”了。 实例变量非线程安全如果多线程共同访问一个对象中的实例变量，则有可能出现“非线程安全”问题。 在两个线程访问同一个对象中的同步方法时一定是线程安全的。 脏读发生脏读的情况是在读取实例变量时，此值已经被其他线程更改过了。 如下例子就可以说明，如果不加 synchronized 关键字在 setValue 和 getValue 方法上，就会出现数据脏读。 123456789101112131415161718192021222324252627282930313233343536373839404142class VarName&#123; private String userName = \"A\"; private String password = \"AA\"; synchronized public void setValue(String userName, String password) &#123; try &#123; this.userName = userName; Thread.sleep(500); this.password = password; System.out.println(\"setValue method Thread name is : \" + Thread.currentThread().getName() + \" userName = \" + userName + \" password = \" + password); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; //synchronized public void getValue() &#123; System.out.println(\"getValue method Thread name is : \" + Thread.currentThread().getName() + \" userName = \" + userName + \" password = \" + password); &#125;&#125;class Thread1 extends Thread&#123; private VarName varName; public Thread1(VarName varName) &#123; this.varName = varName; &#125; @Override public void run() &#123; varName.setValue(\"B\", \"BB\"); &#125;&#125;public class Test&#123; public static void main(String[] args) throws InterruptedException &#123; VarName v = new VarName(); Thread1 thread1 = new Thread1(v); thread1.start(); Thread.sleep(200);//打印结果受睡眠时间的影响 v.getValue(); &#125;&#125; synchronized 锁重入关键字 synchronized 拥有锁重入的功能，也就是在使用 synchronized 时，当一个线程得到一个对象锁后，再次请求此对象锁是可以再次得到该对象的锁的。这也证明了在一个 synchronized 方法/块的内部调用本类的其他 synchronized 方法/块时，是永远可以得到锁的。 12345678910111213141516171819202122232425262728293031class Service&#123; synchronized public void service1() &#123; System.out.println(\"service 1\"); service2(); &#125; synchronized public void service2() &#123; System.out.println(\"service 2\"); service3(); &#125; synchronized public void service3() &#123; System.out.println(\"service 3\"); &#125;&#125;class Thread2 extends Thread&#123; @Override public void run() &#123; Service s = new Service(); s.service1(); &#125;&#125;public class Test2&#123; public static void main(String[] args) &#123; Thread2 t2 = new Thread2(); t2.start(); &#125;&#125; 运行结果： 123service 1service 2service 3 同步不具有继承性同步不可以继承。 synchronized 同步语句块synchronized 代码块间的同步性当一个线程访问 object 的一个 synchronized(this) 同步代码块时，其他线程对同一个 object 中所有其他 synchronized(this) 同步代码块的访问将被阻塞，这说明 synchronized 使用的 “对象监视器” 是一个。 将任意对象作为对象监视器多个线程调用同一个对象中的不同名称的 synchronized 同步方法或者 synchronized(this) 同步代码块时，调用的效果就是按顺序执行，也就是同步的，阻塞的。 静态同步 synchronized 方法与 synchronized(class) 代码块关键字 synchronized 还可以应用在 static 静态方法上，如果这样写就是对当前的 *.java 文件对应的 Class 类进行加锁。而 synchronized 关键字加到非 static 静态方法上就是给对象加锁。 多线程的死锁volatile 关键字作用：使变量在多个线程间可见。 通过使用 volatile 关键字，强制的从公共内存中读取变量的值。使用 volatile 关键字增加了实例变量在多个线程之间的可见性，但 volatile 关键字最致命的缺点就是不支持原子性。 关键字 synchronized 和 volatile 比较： 关键字 volatile 是线程同步的轻量实现，所以 volatile 性能肯定要比 synchronized 要好，并且 volatile 只能修饰于变量，而 synchronized 可以修饰方法，以及代码块。 多线程访问 volatile 不会发生阻塞，而 synchronized 会出现阻塞。 volatile 能保证数据的可见性，但不能保证原子性；而 synchronized 可以保证原子性，也可以间接保证可见性，因为它会将私有内存和公有内存中的数据做同步。 关键字 volatile 解决的是变量在多个线程之间的可见性；而 synchronized 关键字解决的是多个线程之间访问资源的同步性。 ​ 第三章 —— 线程间通信技术点： 使用 wait/notify 实现线程间的通信 生产者/消费者模式的实现 方法 join 的使用 ThreadLocal 类的使用 等待/通知机制wait 使线程停止运行，notify 使停止的线程继续运行。 关键字 synchronized 可以将任何一个 Object 对象作为同步对象看待，而 Java 为每个 Object 都实现了 wait() 和 notify() 方法，他们必须用在被 synchronized 同步的 Object 的临界区内。通过调用 wait 方法可以使处于临界区内的线程进入等待状态，同时释放被同步对象的锁。而 notify 操作可以唤醒一个因调用了 wait 方法而处于阻塞状态的线程，使其进入就绪状态。被重新唤醒的线程会试图重新获得临界区的控制权，继续执行临界区内 wait 之后的代码。 wait 方法可以使调用该方法的线程释放共享资源的锁，从运行状态退出，进入等待状态，直到再次被唤醒。 notify() 方法可以随机唤醒等待对列中等待同一共享资源的一个线程，并使该线程退出等待状态，进入可运行状态。 notifyAll() 方法可以随机唤醒等待对列中等待同一共享资源的所有线程，并使这些线程退出等待状态，进入可运行状态。 线程状态示意图： 新创建一个线程对象后，在调用它的 start() 方法，系统会为此线程分配 CPU 资源，使其处于 Runnable（可运行）状态，如果线程抢占到 CPU 资源，此线程就会处于 Running （运行）状态 Runnable 和 Running 状态之间可以相互切换，因为线程有可能运行一段时间后，有其他优先级高的线程抢占了 CPU 资源，此时线程就从 Running 状态变成了 Runnable 状态。 线程进入 Runnable 状态有如下几种情况： 调用 sleep() 方法后经过的时间超过了指定的休眠时间 线程调用的阻塞 IO 已经返回，阻塞方法执行完毕 线程成功的获得了试图同步的监视器 线程正在等待某个通知，其他线程发出了通知 处于挂状态的线程调用了 resume 恢复方法 Blocked 是阻塞的意思，例如线程遇到一个 IO 操作，此时 CPU 处于空闲状态，可能会转而把 CPU 时间片分配给其他线程，这时也可以称为 “暂停”状态。Blocked 状态结束之后，进入 Runnable 状态，等待系统重新分配资源。 出现阻塞状态的有如下几种情况： 线程调用 sleep 方法，主动放弃占用的处理器资源 线程调用了阻塞式 IO 方法，在该方法返回之前，该线程被阻塞 线程试图获得一个同步监视器，但该同步监视器正在被其他线程所持有 线程等待某个通知 程序调用了 suspend 方法将该线程挂起 run 方法运行结束后进入销毁阶段，整个线程执行完毕。 生产者/消费者模式实现一个生产者，一个消费者 存储值对象： 12345678910package com.zhisheng.thread.thread5;/** * Created by 10412 on 2017/6/3. * 存储值对象 */public class ValueObject&#123; public static String value = \"\";&#125; 生产者： 123456789101112131415161718192021222324252627282930package com.zhisheng.thread.thread5;/** * Created by 10412 on 2017/6/3. * 生产者 */public class Product&#123; private String lock; public Product(String lock) &#123; this.lock = lock; &#125; public void setValue() &#123; synchronized (lock) &#123; if (!ValueObject.value.equals(\"\")) &#123; try &#123; lock.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; String value = System.currentTimeMillis() + \"_\" + System.nanoTime(); System.out.println(\"生产者 set 的值是：\" + value); ValueObject.value = value; lock.notify(); &#125; &#125;&#125; 消费者： 1234567891011121314151617181920212223242526272829package com.zhisheng.thread.thread5;/** * Created by 10412 on 2017/6/3. * 消费者 */public class Resume&#123; private String lock; public Resume(String lock) &#123; this.lock = lock; &#125; public void getValue() &#123; synchronized (lock) &#123; if (ValueObject.value.equals(\"\")) &#123; try &#123; lock.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(\"消费者 get 的值：\" + ValueObject.value); ValueObject.value = \"\"; lock.notify(); &#125; &#125;&#125; 生产者线程： 123456789101112131415161718192021package com.zhisheng.thread.thread5;/** * Created by 10412 on 2017/6/3. * 生产者线程 */public class ProductThread extends Thread&#123; private Product p; public ProductThread(Product p) &#123; this.p = p; &#125; @Override public void run() &#123; while (true) &#123; p.setValue(); &#125; &#125;&#125; 消费者线程： 123456789101112131415161718192021package com.zhisheng.thread.thread5;/** * Created by 10412 on 2017/6/3. * 消费者线程 */public class ResumeThread extends Thread&#123; private Resume r; public ResumeThread(Resume r) &#123; this.r = r; &#125; @Override public void run() &#123; while (true) &#123; r.getValue(); &#125; &#125;&#125; 主函数： 123456789101112131415161718package com.zhisheng.thread.thread5;/** * Created by 10412 on 2017/6/3. * 一个生产者一个消费者测试 */public class Test&#123; public static void main(String[] args) &#123; String str = new String(\"\"); Product p = new Product(str); Resume r = new Resume(str);; ProductThread pt = new ProductThread(p); ResumeThread rt = new ResumeThread(r); pt.start(); rt.start(); &#125;&#125; 题目：创建20个线程，其中10个线程是将数据备份到数据库A，另外10个线程将数据备份到数据库B中去，并且备份数据库A和备份数据库B是交叉进行的。 工具类： 123456789101112131415161718192021222324252627282930313233343536373839404142package com.zhisheng.thread.thread6;/** * Created by 10412 on 2017/6/3. * 创建20个线程，其中10个线程是将数据备份到数据库A，另外10个线程将数据备份到数据库B中去，并且 * 备份数据库A和备份数据库B是交叉进行的 */public class DBTools&#123; volatile private boolean prevIsA = false; //确保A备份先进行 synchronized public void backA() &#123; while (prevIsA == true) &#123; try &#123; wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; for (int i = 0; i &lt; 5; i++) &#123; System.out.println(\"AAAAA\"); &#125; prevIsA = true; notifyAll(); &#125; synchronized public void backB() &#123; while (prevIsA == false) &#123; try &#123; wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; for (int i = 0; i &lt; 5; i++) &#123; System.out.println(\"BBBBB\"); &#125; prevIsA = false; notifyAll(); &#125;&#125; 备份A先线程： 123456789101112131415161718package com.zhisheng.thread.thread6;/** * Created by 10412 on 2017/6/3. */public class ThreadA extends Thread&#123; private DBTools dbTools; public ThreadA(DBTools dbTools) &#123; this.dbTools = dbTools; &#125; @Override public void run() &#123; dbTools.backA(); &#125;&#125; 备份B线程： 123456789101112131415161718package com.zhisheng.thread.thread6;/** * Created by 10412 on 2017/6/3. */public class ThreadB extends Thread&#123; private DBTools dbTools; public ThreadB(DBTools dbTools) &#123; this.dbTools = dbTools; &#125; @Override public void run() &#123; dbTools.backB(); &#125;&#125; 测试： 1234567891011121314151617package com.zhisheng.thread.thread6;/** * Created by 10412 on 2017/6/3. */public class Test&#123; public static void main(String[] args) &#123; DBTools dbTools = new DBTools(); for (int i = 0; i &lt; 20; i++) &#123; ThreadB tb = new ThreadB(dbTools); tb.start(); ThreadA ta = new ThreadA(dbTools); ta.start(); &#125; &#125;&#125; Join 方法的使用作用：等待线程对象销毁 join 方法具有使线程排队运行的作用，有些类似同步的运行效果。join 与 synchronized 的区别是：join 在内部使用 wait() 方法进行等待，而 synchronized 关键字使用的是 “对象监视器” 原理做为同步。 在 join 过程中，如果当前线程对象被中断，则当前线程出现异常。 方法 join(long) 中的参数是设定等待的时间。 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * 等待该线程终止的时间最长为 millis 毫秒。超时为 0 意味着要一直等下去。 * Waits at most &#123;@code millis&#125; milliseconds for this thread to * die. A timeout of &#123;@code 0&#125; means to wait forever. * * &lt;p&gt; This implementation uses a loop of &#123;@code this.wait&#125; calls * conditioned on &#123;@code this.isAlive&#125;. As a thread terminates the * &#123;@code this.notifyAll&#125; method is invoked. It is recommended that * applications not use &#123;@code wait&#125;, &#123;@code notify&#125;, or * &#123;@code notifyAll&#125; on &#123;@code Thread&#125; instances. * * @param millis * the time to wait in milliseconds * * @throws IllegalArgumentException * if the value of &#123;@code millis&#125; is negative * * @throws InterruptedException * if any thread has interrupted the current thread. The * &lt;i&gt;interrupted status&lt;/i&gt; of the current thread is * cleared when this exception is thrown. */ public final synchronized void join(long millis) throws InterruptedException &#123; long base = System.currentTimeMillis(); long now = 0; if (millis &lt; 0) &#123; throw new IllegalArgumentException(\"timeout value is negative\"); if (millis == 0) &#123; while (isAlive()) &#123; wait(0); &#125; &#125; else &#123; while (isAlive()) &#123; long delay = millis - now; if (delay &lt;= 0) &#123; break; &#125; wait(delay); now = System.currentTimeMillis() - base; &#125; &#125; &#125; 类 ThreadLocal 的使用该类提供了线程局部 (thread-local) 变量。这些变量不同于它们的普通对应物，因为访问某个变量（通过其 get 或set 方法）的每个线程都有自己的局部变量，它独立于变量的初始化副本。ThreadLocal 实例通常是类中的private static 字段，它们希望将状态与某一个线程（例如，用户 ID 或事务 ID）相关联。 get() 方法12345678910111213public T get() &#123; Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) &#123; ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings(\"unchecked\") T result = (T)e.value; return result; &#125; &#125; return setInitialValue(); &#125; 返回此线程局部变量的当前线程副本中的值。如果变量没有用于当前线程的值，则先将其初始化为调用 initialValue() 方法返回的值。 InheritableThreadLocal 类的使用该类扩展了 ThreadLocal，为子线程提供从父线程那里继承的值：在创建子线程时，子线程会接收所有可继承的线程局部变量的初始值，以获得父线程所具有的值。通常，子线程的值与父线程的值是一致的；但是，通过重写这个类中的 childValue 方法，子线程的值可以作为父线程值的一个任意函数。 当必须将变量（如用户 ID 和 事务 ID）中维护的每线程属性（per-thread-attribute）自动传送给创建的所有子线程时，应尽可能地采用可继承的线程局部变量，而不是采用普通的线程局部变量。 第四章 —— Lock 的使用使用 ReentrantLock 类一个可重入的互斥锁 Lock，它具有与使用 synchronized 方法和语句所访问的隐式监视器锁相同的一些基本行为和语义，但功能更强大。 ReentrantLock 将由最近成功获得锁，并且还没有释放该锁的线程所拥有。当锁没有被另一个线程所拥有时，调用 lock 的线程将成功获取该锁并返回。如果当前线程已经拥有该锁，此方法将立即返回。可以使用 isHeldByCurrentThread()和 getHoldCount()方法来检查此情况是否发生。 此类的构造方法接受一个可选的公平 参数。当设置为 true 时，在多个线程的争用下，这些锁倾向于将访问权授予等待时间最长的线程。否则此锁将无法保证任何特定访问顺序。与采用默认设置（使用不公平锁）相比，使用公平锁的程序在许多线程访问时表现为很低的总体吞吐量（即速度很慢，常常极其慢），但是在获得锁和保证锁分配的均衡性时差异较小。不过要注意的是，公平锁不能保证线程调度的公平性。因此，使用公平锁的众多线程中的一员可能获得多倍的成功机会，这种情况发生在其他活动线程没有被处理并且目前并未持有锁时。还要注意的是，未定时的 tryLock方法并没有使用公平设置。因为即使其他线程正在等待，只要该锁是可用的，此方法就可以获得成功。 建议总是 立即实践，使用 lock 块来调用 try，在之前/之后的构造中，最典型的代码如下： 12345678910111213class X &#123; private final ReentrantLock lock = new ReentrantLock(); // ... public void m() &#123; lock.lock(); // block until condition holds try &#123; // ... method body &#125; finally &#123; lock.unlock() &#125; &#125; &#125; ConditionCondition 将 Object 监视器方法（wait、notify 和 notifyAll）分解成截然不同的对象，以便通过将这些对象与任意 Lock 实现组合使用，为每个对象提供多个等待 set（wait-set）。其中，Lock 替代了 synchronized 方法和语句的使用，Condition 替代了 Object 监视器方法的使用。 假定有一个绑定的缓冲区，它支持 put 和 take 方法。如果试图在空的缓冲区上执行 take 操作，则在某一个项变得可用之前，线程将一直阻塞；如果试图在满的缓冲区上执行 put 操作，则在有空间变得可用之前，线程将一直阻塞。我们喜欢在单独的等待 set 中保存 put 线程和 take 线程，这样就可以在缓冲区中的项或空间变得可用时利用最佳规划，一次只通知一个线程。可以使用两个 Condition 实例来做到这一点。 12345678910111213141516171819202122232425262728293031323334353637class BoundedBuffer &#123; final Lock lock = new ReentrantLock(); final Condition notFull = lock.newCondition(); final Condition notEmpty = lock.newCondition(); final Object[] items = new Object[100]; int putptr, takeptr, count; public void put(Object x) throws InterruptedException &#123; lock.lock(); try &#123; while (count == items.length) notFull.await(); items[putptr] = x; if (++putptr == items.length) putptr = 0; ++count; notEmpty.signal(); &#125; finally &#123; lock.unlock(); &#125; &#125; public Object take() throws InterruptedException &#123; lock.lock(); try &#123; while (count == 0) notEmpty.await(); Object x = items[takeptr]; if (++takeptr == items.length) takeptr = 0; --count; notFull.signal(); return x; &#125; finally &#123; lock.unlock(); &#125; &#125; &#125; 正确使用 Condition 实现等待/通知MyService.java 123456789101112131415161718192021222324252627282930313233343536package com.zhisheng.thread.Thread9;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;/** * Created by 10412 on 2017/6/4. */public class MyService&#123; private Lock lock = new ReentrantLock(); private Condition condition = lock.newCondition(); public void await() &#123; lock.lock(); try &#123; System.out.println(\"await A\"); condition.await();//使当前执行的线程处于等待状态 waiting System.out.println(\"await B\"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;finally &#123; lock.unlock(); System.out.println(\"释放锁\"); &#125; &#125; public void signal() &#123; lock.lock(); System.out.println(\"signal A\"); condition.signal(); System.out.println(\"signal B\"); lock.unlock(); &#125;&#125; ThreadA.java 123456789101112131415161718package com.zhisheng.thread.Thread9;/** * Created by 10412 on 2017/6/4. */public class ThreadA extends Thread&#123; private MyService service; public ThreadA(MyService service) &#123; this.service = service; &#125; @Override public void run() &#123; service.await(); &#125;&#125; Test.java 123456789101112131415package com.zhisheng.thread.Thread9;/** * Created by 10412 on 2017/6/4. */public class Test&#123; public static void main(String[] args) throws InterruptedException &#123; MyService service = new MyService(); ThreadA ta = new ThreadA(service); ta.start(); Thread.sleep(5000); service.signal(); &#125;&#125; 运行结果： 12345await Asignal Asignal Bawait B释放锁 Object 类中的 wait() 方法相当于 Condition 类中 await() 方法 Object 类中的 wait(long time) 方法相当于 Condition 类中 await(long time, TimeUnit unit) 方法 Object 类中的 notify() 方法相当于 Condition 类中 signal() 方法 Object 类中的 notifyAll() 方法相当于 Condition 类中 signalAll() 方法 题目：实现生产者与消费者 一对一交替打印 MyService.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.zhisheng.thread.thread10;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;/** * Created by 10412 on 2017/6/4. * 实现生产者与消费者 一对一·交替打印 */public class MyService&#123; private Lock lock = new ReentrantLock(); private Condition condition = lock.newCondition(); private boolean flag = false; public void setValue() &#123; lock.lock(); while (flag == true) &#123; try &#123; condition.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(\"SetValue AAAAAA\"); flag = true; condition.signal(); lock.unlock(); &#125; public void getValue() &#123; lock.lock(); while (flag == false) &#123; try &#123; condition.await(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; System.out.println(\"GetValue BBBB\"); flag = false; condition.signal(); lock.unlock(); &#125;&#125; ThreadA.java 1234567891011121314151617181920package com.zhisheng.thread.thread10;/** * Created by 10412 on 2017/6/4. */public class ThreadA extends Thread&#123; private MyService service; public ThreadA(MyService service) &#123; this.service = service; &#125; @Override public void run() &#123; for (int i = 0; i &lt; Integer.MAX_VALUE; i++) &#123; service.setValue(); &#125; &#125;&#125; ThreadB.java 1234567891011121314151617181920package com.zhisheng.thread.thread10;/** * Created by 10412 on 2017/6/4. */public class ThreadB extends Thread&#123; private MyService service; public ThreadB(MyService service) &#123; this.service = service; &#125; @Override public void run() &#123; for (int i = 0; i &lt; Integer.MAX_VALUE; i++) &#123; service.getValue(); &#125; &#125;&#125; Test.java 123456789101112131415package com.zhisheng.thread.thread10;/** * Created by 10412 on 2017/6/4. */public class Test&#123; public static void main(String[] args) &#123; MyService service = new MyService(); ThreadA ta = new ThreadA(service); ThreadB tb = new ThreadB(service); ta.start(); tb.start(); &#125;&#125; getHoldCount() 查询当前线程保持此锁定的个数，也就是调用 lock() 的方法 getQueueLength() 返回正等待获取此锁定的线程估计数 getWaitQueueLength() 返回等待与此锁定相关的给定条件 Condition 的线程估计数 hasQueuedThread() 查询指定的线程是否正在等待获取此锁定 hasQueuedThreads() 查询是否有线程正在等待获取此锁定 hasWaiters() 查询是否有线程正在等待与此锁定有关的 condition 条件 isFair() 判断是否是公平锁（默认下 ReentrantLock类使用的是非公平锁） isHeldByCurrentThread() 查询当前线程是否保持此锁定 isLocked() 查询此锁定是否由任意线程保持 lockInterruptibly() 如果当前线程未被中断，则获取锁定，如果已经被中断则出现异常 tryLock() 仅在调用时锁定未被另一个线程保持的情况下，才获取该锁定 tryLock(long time, TimeUtil util) 如果锁定在给定的等待时间内没有被另一个线程保持，且当前线程未被中断，则获取该锁定。 使用 ReentrantReadWriteLock 类读写互斥： MyService.java 123456789101112131415161718192021222324252627282930313233package com.zhisheng.thread.Thread11;import java.util.concurrent.locks.ReentrantReadWriteLock;/** * Created by 10412 on 2017/6/4. */public class MyService&#123; private ReentrantReadWriteLock lock = new ReentrantReadWriteLock(); public void read() &#123; lock.readLock().lock(); System.out.println(Thread.currentThread().getName() + \" Read AAA \" + System.currentTimeMillis()); try &#123; Thread.sleep(10000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; lock.readLock().unlock(); &#125; public void write() &#123; lock.writeLock().lock(); System.out.println(Thread.currentThread().getName() + \" write BBB \" + System.currentTimeMillis()); try &#123; Thread.sleep(10000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; lock.writeLock().unlock(); &#125;&#125; ThreadA.java 123456789101112131415161718package com.zhisheng.thread.Thread11;/** * Created by 10412 on 2017/6/4. */public class ThreadA extends Thread&#123; private MyService service; public ThreadA(MyService service) &#123; this.service = service; &#125; @Override public void run() &#123; service.read(); &#125;&#125; ThreadB.java 123456789101112131415161718package com.zhisheng.thread.Thread11;/** * Created by 10412 on 2017/6/4. */public class ThreadB extends Thread&#123; private MyService service; public ThreadB(MyService service) &#123; this.service = service; &#125; @Override public void run() &#123; service.write(); &#125;&#125; Test.java 123456789101112131415161718package com.zhisheng.thread.Thread11;/** * Created by 10412 on 2017/6/4. */public class Test&#123; public static void main(String[] args) throws InterruptedException &#123; MyService service = new MyService(); ThreadA ta = new ThreadA(service); ta.setName(\"A\"); ta.start(); Thread.sleep(1000); ThreadB tb = new ThreadB(service); tb.setName(\"B\"); tb.start(); &#125;&#125; 运行结果： 12A Read AAA 1496556770402B write BBB 1496556780402 第六章 —— 单例模式与多线程推荐文章 《深入浅出单实例Singleton设计模式》 立即加载模式 / “饿汉模式”立即加载：使用类的时候已经将对象创建完毕，new 实例化 123456789public class MyObject&#123; private static MyObject object = new MyObject(); private MyObject() &#123; &#125; public static MyObject getInstance() &#123; return object; &#125;&#125; 延迟加载 / “ 懒汉模式 ”就是在调用 get 的时候实例才被创建。在 get() 方法中进行 new 实例化。 12345678910111213public class MyObject&#123; private static MyObject object; private MyObject() &#123; &#125; public static MyObject getInstance() &#123; if (object != null) &#123; &#125; else &#123; object = new MyObject(); &#125; return object; &#125;&#125; 使用 DCL 双重检查锁，解决“懒汉模式”遇到的多线程问题 123456789101112131415161718public class MyObject&#123; private volatile static MyObject object; private MyObject() &#123; &#125; //synchronized public static MyObject getInstance() &#123; if (object != null) &#123; &#125; else &#123; synchronized (MyObject.class) &#123; if (object == null) &#123; object = new MyObject(); &#125; &#125; &#125; return object; &#125;&#125; 使用静态内部类实现单例模式123456789101112public class MyObject&#123; private static class MyObjectHandler &#123; private static MyObject object = new MyObject(); &#125; private MyObject() &#123; &#125; public static MyObject getInstance() &#123; return MyObjectHandler.object; &#125;&#125; 序列化与反序列化的单例模式实现MyObject.java 12345678910111213141516171819202122232425package com.zhisheng.thread.thread15;import java.io.ObjectStreamException;import java.io.Serializable;/** * Created by 10412 on 2017/6/4. */public class MyObject implements Serializable&#123; private static final long serialVersionUID = 888L; private static class MyObjectHandler &#123; private static final MyObject object = new MyObject(); &#125; private MyObject() &#123; &#125; public static MyObject getInstance() &#123; return MyObjectHandler.object; &#125; protected Object readResolve() throws ObjectStreamException &#123; System.out.println(\"调用了readResolve方法！\"); return MyObjectHandler.object; &#125;&#125; SaveAndRead.java 12345678910111213141516171819202122232425262728293031323334353637383940package com.zhisheng.thread.thread15;import java.io.*;/** * Created by 10412 on 2017/6/4. */public class SaveAndRead&#123; public static void main(String[] args) &#123; try &#123; MyObject object = MyObject.getInstance(); FileOutputStream fos = new FileOutputStream(new File(\"fos.txt\")); ObjectOutputStream oos = new ObjectOutputStream(fos); oos.writeObject(object); oos.close(); fos.close(); System.out.println(object.hashCode()); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; try &#123; FileInputStream fis = new FileInputStream(new File(\"fos.txt\")); ObjectInputStream ois = new ObjectInputStream(fis); MyObject o = (MyObject) ois.readObject(); ois.close(); fis.close(); System.out.println(o.hashCode()); &#125; catch (FileNotFoundException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 这里主要要指出 MyObject.java 中 readResolve 方法 1234protected Object readResolve() throws ObjectStreamException &#123; System.out.println(\"调用了readResolve方法！\"); return MyObjectHandler.object; &#125; 方法 readResolve 允许 class 在反序列化返回对象前替换、解析在流中读出来的对象。实现 readResolve 方法，一个 class 可以直接控制反序化返回的类型和对象引用。 方法 readResolve 会在 ObjectInputStream 已经读取一个对象并在准备返回前调用。ObjectInputStream 会检查对象的 class 是否定义了 readResolve 方法。如果定义了，将由 readResolve 方法指定返回的对象。返回对象的类型一定要是兼容的，否则会抛出 ClassCastException 。 使用 static 代码块实现单例模式1234567891011121314151617package com.zhisheng.thread.thread16;/** * Created by 10412 on 2017/6/4. */public class MyObject&#123; private static MyObject instance = null; private MyObject() &#123; &#125; static &#123; instance = new MyObject(); &#125; public static MyObject getInstance() &#123; return instance; &#125;&#125; ThreadA.java 1234567891011121314package com.zhisheng.thread.thread16;/** * Created by 10412 on 2017/6/4. */public class ThreadA extends Thread&#123; @Override public void run() &#123; for (int i = 0; i &lt; 5; i++) &#123; System.out.println(MyObject.getInstance().hashCode()); &#125; &#125;&#125; Test.java 12345678910111213141516package com.zhisheng.thread.thread16;/** * Created by 10412 on 2017/6/4. */public class Test&#123; public static void main(String[] args) &#123; ThreadA ta1 = new ThreadA(); ThreadA ta2 = new ThreadA(); ThreadA ta3 = new ThreadA(); ta1.start(); ta2.start(); ta3.start(); &#125;&#125; 使用枚举数据类型实现单例模式在使用枚举类时，构造方法会被自动调用，也可以应用这个特性实现单例模式。 123456789101112131415public class MyObject &#123; private enum MyEnumSingleton&#123; INSTANCE; private Resource resource; private MyEnumSingleton()&#123; resource = new Resource(); &#125; public Resource getResource()&#123; return resource; &#125; &#125; public static Resource getResource()&#123; return MyEnumSingleton.INSTANCE.getResource(); &#125;&#125; 测试： 123456789101112131415161718192021import test.MyObject;public class Run &#123; class MyThread extends Thread &#123; @Override public void run() &#123; for (int i = 0; i &lt; 5; i++) &#123; System.out.println(MyObject.getResource().hashCode()); &#125; &#125; &#125; public static void main(String[] args) &#123; Run.MyThread t1 = new Run().new MyThread(); Run.MyThread t2 = new Run().new MyThread(); Run.MyThread t3 = new Run().new MyThread(); t1.start(); t2.start(); t3.start(); &#125;&#125; 这里再推荐一篇 stackoverflow 上的一个问题回答： What is an efficient way to implement a singleton pattern in Java? 总结本篇文章是我读 《Java多线程编程核心技术》 的笔记及自己的一些总结，觉得不错，欢迎点赞和转发。","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"多线程","slug":"多线程","permalink":"http://www.54tianzhisheng.cn/tags/多线程/"}]},{"title":"Java NIO 系列教程","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/Java NIO 系列教程/","text":"Java NIO（New IO）是从Java 1.4版本开始引入的一个新的IO API，可以替代标准的Java IO API。 Java NIO提供了与标准IO不同的IO工作方式： Channels and Buffers（通道和缓冲区）：标准的IO基于字节流和字符流进行操作的，而NIO是基于通道（Channel）和缓冲区（Buffer）进行操作，数据总是从通道读取到缓冲区中，或者从缓冲区写入到通道中。 Asynchronous IO（异步IO）：Java NIO可以让你异步的使用IO，例如：当线程从通道读取数据到缓冲区时，线程还是可以进行其他事情。当数据被写入到缓冲区时，线程可以继续处理它。从缓冲区写入通道也类似。 Selectors（选择器）：Java NIO引入了选择器的概念，选择器用于监听多个通道的事件（比如：连接打开，数据到达）。因此，单个的线程可以监听多个数据通道。 下面就来详细介绍Java NIO的相关知识。 1、Java NIO 概述Java NIO 由以下几个核心部分组成： Channels Buffers Selectors 虽然 Java NIO 中除此之外还有很多类和组件，但在我看来，Channel，Buffer 和 Selector 构成了核心的 API。其它组件，如 Pipe 和 FileLock，只不过是与三个核心组件共同使用的工具类。因此，在概述中我将集中在这三个组件上。其它组件会在单独的章节中讲到。 Channel 和 Buffer基本上，所有的 IO 在NIO 中都从一个 Channel 开始。Channel 有点象流。 数据可以从 Channel 读到 Buffer 中，也可以从 Buffer 写到 Channel 中。这里有个图示： Channel 和 Buffer 有好几种类型。下面是 JAVA NIO 中的一些主要 Channel 的实现： FileChannel DatagramChannel SocketChannel ServerSocketChannel 正如你所看到的，这些通道涵盖了 UDP 和 TCP 网络 IO，以及文件 IO。 与这些类一起的有一些有趣的接口，但为简单起见，我尽量在概述中不提到它们。本教程其它章节与它们相关的地方我会进行解释。 以下是 Java NIO 里关键的 Buffer 实现： ByteBuffer CharBuffer DoubleBuffer FloatBuffer IntBuffer LongBuffer ShortBuffer 这些 Buffer 覆盖了你能通过 IO 发送的基本数据类型：byte, short, int, long, float, double 和 char。 Java NIO 还有个 MappedByteBuffer，用于表示内存映射文件， 我也不打算在概述中说明。 SelectorSelector 允许单线程处理多个 Channel。如果你的应用打开了多个连接（通道），但每个连接的流量都很低，使用 Selector 就会很方便。例如，在一个聊天服务器中。 这是在一个单线程中使用一个 Selector 处理3个 Channel 的图示： 要使用 Selector，得向 Selector 注册 Channel，然后调用它的 select() 方法。这个方法会一直阻塞到某个注册的通道有事件就绪。一旦这个方法返回，线程就可以处理这些事件，事件的例子有如新连接进来，数据接收等。 2、ChannelJava NIO 的通道类似流，但又有些不同： 既可以从通道中读取数据，又可以写数据到通道。但流的读写通常是单向的。 通道可以异步地读写。 通道中的数据总是要先读到一个 Buffer，或者总是要从一个 Buffer 中写入。 正如上面所说，从通道读取数据到缓冲区，从缓冲区写入数据到通道。如下图所示： Channel 的实现这些是 Java NIO 中最重要的通道的实现： FileChannel DatagramChannel SocketChannel ServerSocketChannel FileChannel 从文件中读写数据。 DatagramChannel 能通过 UDP 读写网络中的数据。 SocketChannel 能通过 TCP 读写网络中的数据。 ServerSocketChannel 可以监听新进来的 TCP 连接，像 Web 服务器那样。对每一个新进来的连接都会创建一个 SocketChannel。 基本的 Channel 示例下面是一个使用 FileChannel 读取数据到 Buffer 中的示例： 12345678910111213141516171819RandomAccessFile aFile = new RandomAccessFile(&quot;data/nio-data.txt&quot;, &quot;rw&quot;);FileChannel inChannel = aFile.getChannel();ByteBuffer buf = ByteBuffer.allocate(48);int bytesRead = inChannel.read(buf);while (bytesRead != -1) &#123;System.out.println(&quot;Read &quot; + bytesRead);buf.flip();while(buf.hasRemaining())&#123;System.out.print((char) buf.get());&#125;buf.clear();bytesRead = inChannel.read(buf);&#125;aFile.close(); 注意 buf.flip() 的调用，首先读取数据到Buffer，然后反转Buffer,接着再从Buffer中读取数据。下一节会深入讲解Buffer的更多细节。 3、BufferJava NIO 中的 Buffer 用于和 NIO 通道进行交互。如你所知，数据是从通道读入缓冲区，从缓冲区写入到通道中的。 缓冲区本质上是一块可以写入数据，然后可以从中读取数据的内存。这块内存被包装成 NIO Buffer 对象，并提供了一组方法，用来方便的访问该块内存。 Buffer 的基本用法使用 Buffer 读写数据一般遵循以下四个步骤： 写入数据到 Buffer 调用 flip() 方法 从 Buffer 中读取数据 调用 clear() 方法或者 compact() 方法 当向 buffer 写入数据时，buffer 会记录下写了多少数据。一旦要读取数据，需要先通过 flip() 方法将 Buffer 从写模式切换到读模式。在读模式下，可以读取之前写入到 buffer 的所有数据。 一旦读完了所有的数据，就需要清空缓冲区，让它可以再次被写入。有两种方式能清空缓冲区：调用 clear() 或 compact() 方法。clear() 方法会清空整个缓冲区。compact() 方法只会清除已经读过的数据，任何未读的数据都被移到缓冲区的起始处，新写入的数据将放到缓冲区未读数据的后面。 下面是一个使用Buffer的例子： 12345678910111213141516171819RandomAccessFile aFile = new RandomAccessFile(&quot;data/nio-data.txt&quot;, &quot;rw&quot;);FileChannel inChannel = aFile.getChannel();//create buffer with capacity of 48 bytesByteBuffer buf = ByteBuffer.allocate(48);int bytesRead = inChannel.read(buf); //read into buffer.while (bytesRead != -1) &#123; buf.flip(); //make buffer ready for read while(buf.hasRemaining())&#123; System.out.print((char) buf.get()); // read 1 byte at a time &#125; buf.clear(); //make buffer ready for writing bytesRead = inChannel.read(buf);&#125;aFile.close(); Buffer 的 capacity、 position 和 limit缓冲区本质上是一块可以写入数据，然后可以从中读取数据的内存。这块内存被包装成 NIO Buffer 对象，并提供了一组方法，用来方便的访问该块内存。 为了理解 Buffer 的工作原理，需要熟悉它的三个属性： capacity position limit position 和 limit 的含义取决于 Buffer 处在读模式还是写模式。不管 Buffer 处在什么模式，capacity 的含义总是一样的。 这里有一个关于 capacity，position 和 limit 在读写模式中的说明，详细的解释在插图后面。 capacity 作为一个内存块，Buffer 有一个固定的大小值，也叫“capacity”.你只能往里写 capacity 个byte、long，char 等类型。一旦 Buffer 满了，需要将其清空（通过读数据或者清除数据）才能继续写数据往里写数据。 position 当你写数据到 Buffer 中时，position 表示当前的位置。初始的 position 值为 0。当一个 byte、long 等数据写到 Buffer 后， position 会向前移动到下一个可插入数据的 Buffer 单元。position 最大可为 capacity – 1. 当读取数据时，也是从某个特定位置读。当将 Buffer 从写模式切换到读模式，position 会被重置为 0. 当从 Buffer 的 position 处读取数据时，position 向前移动到下一个可读的位置。 limit 在写模式下，Buffer 的 limit 表示你最多能往 Buffer 里写多少数据。 写模式下，limit 等于Buffer 的 capacity。 当切换 Buffer 到读模式时， limit 表示你最多能读到多少数据。因此，当切换 Buffer 到读模式时，limit 会被设置成写模式下的 position 值。换句话说，你能读到之前写入的所有数据（limit被设置成已写数据的数量，这个值在写模式下就是 position） Buffer的类型Java NIO 有以下Buffer类型 ByteBuffer MappedByteBuffer CharBuffer DoubleBuffer FloatBuffer IntBuffer LongBuffer ShortBuffer 如你所见，这些Buffer类型代表了不同的数据类型。换句话说，就是可以通过char，short，int，long，float 或 double类型来操作缓冲区中的字节。 MappedByteBuffer 有些特别，在涉及它的专门章节中再讲。 Buffer 的分配要想获得一个 Buffer 对象首先要进行分配。 每一个 Buffer 类都有一个 allocate 方法。下面是一个分配48字节 capacity 的 ByteBuffer 的例子。 1ByteBuffer buf = ByteBuffer.allocate(48); 这是分配一个可存储1024个字符的 CharBuffer： 1CharBuffer buf = CharBuffer.allocate(1024); 向 Buffer 中写数据写数据到 Buffer 有两种方式： 从 Channel 写到 Buffer。 通过 Buffer 的 put() 方法写到 Buffer 里。 从 Channel 写到 Buffer 的例子： 1int bytesRead = inChannel.read(buf); //read into buffer. 通过put方法写Buffer的例子： 1buf.put(127); put 方法有很多版本，允许你以不同的方式把数据写入到 Buffer 中。例如， 写到一个指定的位置，或者把一个字节数组写入到 Buffer。 更多Buffer实现的细节参考JavaDoc。 flip() 方法 flip() 方法将 Buffer 从写模式切换到读模式。调用 flip() 方法会将 position 设回 0，并将 limit 设置成之前 position 的值。 换句话说，position 现在用于标记读的位置，limit 表示之前写进了多少个 byte、char等 —— 现在能读取多少个byte、char等。 从Buffer中读取数据从Buffer中读取数据有两种方式： 从Buffer读取数据到Channel。 使用get()方法从Buffer中读取数据。 从Buffer读取数据到Channel的例子： 12//read from buffer into channel.int bytesWritten = inChannel.write(buf); 使用get()方法从Buffer中读取数据的例子 1byte aByte = buf.get(); get方法有很多版本，允许你以不同的方式从Buffer中读取数据。例如，从指定position读取，或者从Buffer中读取数据到字节数组。更多Buffer实现的细节参考JavaDoc。 rewind()方法Buffer.rewind()将position设回0，所以你可以重读Buffer中的所有数据。limit保持不变，仍然表示能从Buffer中读取多少个元素（byte、char等）。 clear()与compact()方法一旦读完Buffer中的数据，需要让Buffer准备好再次被写入。可以通过clear()或compact()方法来完成。 如果调用的是clear()方法，position将被设回0，limit被设置成 capacity的值。换句话说，Buffer 被清空了。Buffer中的数据并未清除，只是这些标记告诉我们可以从哪里开始往Buffer里写数据。 如果Buffer中有一些未读的数据，调用clear()方法，数据将“被遗忘”，意味着不再有任何标记会告诉你哪些数据被读过，哪些还没有。 如果Buffer中仍有未读的数据，且后续还需要这些数据，但是此时想要先先写些数据，那么使用compact()方法。 compact()方法将所有未读的数据拷贝到Buffer起始处。然后将position设到最后一个未读元素正后面。limit属性依然像clear()方法一样，设置成capacity。现在Buffer准备好写数据了，但是不会覆盖未读的数据。 mark()与reset()方法通过调用Buffer.mark()方法，可以标记Buffer中的一个特定position。之后可以通过调用Buffer.reset()方法恢复到这个position。例如： 123buffer.mark();//call buffer.get() a couple of times, e.g. during parsing.buffer.reset(); //set position back to mark. equals()与compareTo()方法可以使用equals()和compareTo()方法两个Buffer。 equals() 当满足下列条件时，表示两个Buffer相等： 有相同的类型（byte、char、int等）。 Buffer中剩余的byte、char等的个数相等。 Buffer中所有剩余的byte、char等都相同。 如你所见，equals只是比较Buffer的一部分，不是每一个在它里面的元素都比较。实际上，它只比较Buffer中的剩余元素。 compareTo()方法 compareTo()方法比较两个Buffer的剩余元素(byte、char等)， 如果满足下列条件，则认为一个Buffer“小于”另一个Buffer： 第一个不相等的元素小于另一个Buffer中对应的元素 。 所有元素都相等，但第一个Buffer比另一个先耗尽(第一个Buffer的元素个数比另一个少)。 （译注：剩余元素是从 position到limit之间的元素） 4、Scatter/GatherJava NIO 开始支持 scatter/gather，scatter/gather 用于描述从 Channel（译者注：Channel 在中文经常翻译为通道）中读取或者写入到 Channel 的操作。 分散（scatter）从 Channel 中读取是指在读操作时将读取的数据写入多个 buffer 中。因此，Channel 将从 Channel 中读取的数据“分散（scatter）”到多个Buffer中。 聚集（gather）写入Channel是指在写操作时将多个buffer的数据写入同一个Channel，因此，Channel 将多个Buffer中的数据“聚集（gather）”后发送到Channel。 scatter / gather经常用于需要将传输的数据分开处理的场合，例如传输一个由消息头和消息体组成的消息，你可能会将消息体和消息头分散到不同的buffer中，这样你可以方便的处理消息头和消息体。 Scattering ReadsScattering Reads是指数据从一个channel读取到多个buffer中。如下图描述： 代码示例如下： 123456ByteBuffer header = ByteBuffer.allocate(128);ByteBuffer body = ByteBuffer.allocate(1024);ByteBuffer[] bufferArray = &#123; header, body &#125;;channel.read(bufferArray); 注意buffer首先被插入到数组，然后再将数组作为channel.read() 的输入参数。read()方法按照buffer在数组中的顺序将从channel中读取的数据写入到buffer，当一个buffer被写满后，channel紧接着向另一个buffer中写。 Scattering Reads在移动下一个buffer前，必须填满当前的buffer，这也意味着它不适用于动态消息(译者注：消息大小不固定)。换句话说，如果存在消息头和消息体，消息头必须完成填充（例如 128byte），Scattering Reads才能正常工作。 Gathering WritesGathering Writes是指数据从多个buffer写入到同一个channel。如下图描述： 代码示例如下： 12345678ByteBuffer header = ByteBuffer.allocate(128);ByteBuffer body = ByteBuffer.allocate(1024);//write data into buffersByteBuffer[] bufferArray = &#123; header, body &#125;;channel.write(bufferArray); buffers数组是write()方法的入参，write()方法会按照buffer在数组中的顺序，将数据写入到channel，注意只有position和limit之间的数据才会被写入。因此，如果一个buffer的容量为128byte，但是仅仅包含58byte的数据，那么这58byte的数据将被写入到channel中。因此与Scattering Reads相反，Gathering Writes能较好的处理动态消息。 5、通道之间的数据传输在Java NIO中，如果两个通道中有一个是FileChannel，那你可以直接将数据从一个channel（译者注：channel中文常译作通道）传输到另外一个channel。 transferFrom()FileChannel的transferFrom()方法可以将数据从源通道传输到FileChannel中（译者注：这个方法在JDK文档中的解释为将字节从给定的可读取字节通道传输到此通道的文件中）。 下面是一个简单的例子： 12345678910RandomAccessFile fromFile = new RandomAccessFile(&quot;fromFile.txt&quot;, &quot;rw&quot;);FileChannel fromChannel = fromFile.getChannel();RandomAccessFile toFile = new RandomAccessFile(&quot;toFile.txt&quot;, &quot;rw&quot;);FileChannel toChannel = toFile.getChannel();long position = 0;long count = fromChannel.size();toChannel.transferFrom(position, count, fromChannel); 方法的输入参数position表示从position处开始向目标文件写入数据，count表示最多传输的字节数。如果源通道的剩余空间小于 count 个字节，则所传输的字节数要小于请求的字节数。 此外要注意，在SoketChannel的实现中，SocketChannel只会传输此刻准备好的数据（可能不足count字节）。因此，SocketChannel可能不会将请求的所有数据(count个字节)全部传输到FileChannel中。 transferTo()transferTo()方法将数据从FileChannel传输到其他的channel中。 下面是一个简单的例子： 12345678910RandomAccessFile fromFile = new RandomAccessFile(&quot;fromFile.txt&quot;, &quot;rw&quot;);FileChannel fromChannel = fromFile.getChannel();RandomAccessFile toFile = new RandomAccessFile(&quot;toFile.txt&quot;, &quot;rw&quot;);FileChannel toChannel = toFile.getChannel();long position = 0;long count = fromChannel.size();fromChannel.transferTo(position, count, toChannel); 是不是发现这个例子和前面那个例子特别相似？除了调用方法的FileChannel对象不一样外，其他的都一样。 上面所说的关于SocketChannel的问题在transferTo()方法中同样存在。SocketChannel会一直传输数据直到目标buffer被填满。 6、SelectorSelector（选择器）是Java NIO中能够检测一到多个NIO通道，并能够知晓通道是否为诸如读写事件做好准备的组件。这样，一个单独的线程可以管理多个channel，从而管理多个网络连接。 为什么使用Selector?仅用单个线程来处理多个Channels的好处是，只需要更少的线程来处理通道。事实上，可以只用一个线程处理所有的通道。对于操作系统来说，线程之间上下文切换的开销很大，而且每个线程都要占用系统的一些资源（如内存）。因此，使用的线程越少越好。 但是，需要记住，现代的操作系统和CPU在多任务方面表现的越来越好，所以多线程的开销随着时间的推移，变得越来越小了。实际上，如果一个CPU有多个内核，不使用多任务可能是在浪费CPU能力。不管怎么说，关于那种设计的讨论应该放在另一篇不同的文章中。在这里，只要知道使用Selector能够处理多个通道就足够了。 下面是单线程使用一个Selector处理3个channel的示例图： Selector的创建通过调用Selector.open()方法创建一个Selector，如下： 1Selector selector = Selector.open(); 向Selector注册通道为了将Channel和Selector配合使用，必须将channel注册到selector上。通过SelectableChannel.register()方法来实现，如下： 123channel.configureBlocking(false);SelectionKey key = channel.register(selector, Selectionkey.OP_READ); 与Selector一起使用时，Channel必须处于非阻塞模式下。这意味着不能将FileChannel与Selector一起使用，因为FileChannel不能切换到非阻塞模式。而套接字通道都可以。 注意register()方法的第二个参数。这是一个“interest集合”，意思是在通过Selector监听Channel时对什么事件感兴趣。可以监听四种不同类型的事件： Connect Accept Read Write 通道触发了一个事件意思是该事件已经就绪。所以，某个channel成功连接到另一个服务器称为“连接就绪”。一个server socket channel准备好接收新进入的连接称为“接收就绪”。一个有数据可读的通道可以说是“读就绪”。等待写数据的通道可以说是“写就绪”。 这四种事件用SelectionKey的四个常量来表示： SelectionKey.OP_CONNECT SelectionKey.OP_ACCEPT SelectionKey.OP_READ SelectionKey.OP_WRITE 如果你对不止一种事件感兴趣，那么可以用“位或”操作符将常量连接起来，如下： 1int interestSet = SelectionKey.OP_READ | SelectionKey.OP_WRITE; 在下面还会继续提到interest集合。 SelectionKey在上一小节中，当向Selector注册Channel时，register()方法会返回一个SelectionKey对象。这个对象包含了一些你感兴趣的属性： interest集合 ready集合 Channel Selector 附加的对象（可选）下面我会描述这些属性。 interest集合就像向Selector注册通道一节中所描述的，interest集合是你所选择的感兴趣的事件集合。可以通过SelectionKey读写interest集合，像这样： 123456int interestSet = selectionKey.interestOps();boolean isInterestedInAccept = (interestSet &amp; SelectionKey.OP_ACCEPT) == SelectionKey.OP_ACCEPT；boolean isInterestedInConnect = interestSet &amp; SelectionKey.OP_CONNECT;boolean isInterestedInRead = interestSet &amp; SelectionKey.OP_READ;boolean isInterestedInWrite = interestSet &amp; SelectionKey.OP_WRITE; 可以看到，用“位与”操作interest 集合和给定的SelectionKey常量，可以确定某个确定的事件是否在interest 集合中。 ready集合ready 集合是通道已经准备就绪的操作的集合。在一次选择(Selection)之后，你会首先访问这个ready set。Selection将在下一小节进行解释。可以这样访问ready集合： 1int readySet = selectionKey.readyOps(); 可以用像检测interest集合那样的方法，来检测channel中什么事件或操作已经就绪。但是，也可以使用以下四个方法，它们都会返回一个布尔类型： 1234selectionKey.isAcceptable();selectionKey.isConnectable();selectionKey.isReadable();selectionKey.isWritable(); Channel + Selector从SelectionKey访问Channel和Selector很简单。如下： 12Channel channel = selectionKey.channel();Selector selector = selectionKey.selector(); 附加的对象可以将一个对象或者更多信息附着到SelectionKey上，这样就能方便的识别某个给定的通道。例如，可以附加 与通道一起使用的Buffer，或是包含聚集数据的某个对象。使用方法如下： 12selectionKey.attach(theObject);Object attachedObj = selectionKey.attachment(); 还可以在用register()方法向Selector注册Channel的时候附加对象。如： 1SelectionKey key = channel.register(selector, SelectionKey.OP_READ, theObject); 通过Selector选择通道一旦向Selector注册了一或多个通道，就可以调用几个重载的select()方法。这些方法返回你所感兴趣的事件（如连接、接受、读或写）已经准备就绪的那些通道。换句话说，如果你对“读就绪”的通道感兴趣，select()方法会返回读事件已经就绪的那些通道。 下面是select()方法： int select() int select(long timeout) int selectNow() select()阻塞到至少有一个通道在你注册的事件上就绪了。 select(long timeout)和select()一样，除了最长会阻塞timeout毫秒(参数)。 selectNow()不会阻塞，不管什么通道就绪都立刻返回（译者注：此方法执行非阻塞的选择操作。如果自从前一次选择操作后，没有通道变成可选择的，则此方法直接返回零。）。 select()方法返回的int值表示有多少通道已经就绪。亦即，自上次调用select()方法后有多少通道变成就绪状态。如果调用select()方法，因为有一个通道变成就绪状态，返回了1，若再次调用select()方法，如果另一个通道就绪了，它会再次返回1。如果对第一个就绪的channel没有做任何操作，现在就有两个就绪的通道，但在每次select()方法调用之间，只有一个通道就绪了。 selectedKeys()一旦调用了select()方法，并且返回值表明有一个或更多个通道就绪了，然后可以通过调用selector的selectedKeys()方法，访问“已选择键集（selected key set）”中的就绪通道。如下所示： 当像Selector注册Channel时，Channel.register()方法会返回一个SelectionKey 对象。这个对象代表了注册到该Selector的通道。可以通过SelectionKey的selectedKeySet()方法访问这些对象。 可以遍历这个已选择的键集合来访问就绪的通道。如下： 123456789101112131415Set selectedKeys = selector.selectedKeys();Iterator keyIterator = selectedKeys.iterator();while(keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if(key.isAcceptable()) &#123; // a connection was accepted by a ServerSocketChannel. &#125; else if (key.isConnectable()) &#123; // a connection was established with a remote server. &#125; else if (key.isReadable()) &#123; // a channel is ready for reading &#125; else if (key.isWritable()) &#123; // a channel is ready for writing &#125; keyIterator.remove();&#125; 这个循环遍历已选择键集中的每个键，并检测各个键所对应的通道的就绪事件。 注意每次迭代末尾的keyIterator.remove()调用。Selector不会自己从已选择键集中移除SelectionKey实例。必须在处理完通道时自己移除。下次该通道变成就绪时，Selector会再次将其放入已选择键集中。 SelectionKey.channel()方法返回的通道需要转型成你要处理的类型，如ServerSocketChannel或SocketChannel等。 wakeUp()某个线程调用select()方法后阻塞了，即使没有通道已经就绪，也有办法让其从select()方法返回。只要让其它线程在第一个线程调用select()方法的那个对象上调用Selector.wakeup()方法即可。阻塞在select()方法上的线程会立马返回。 如果有其它线程调用了wakeup()方法，但当前没有线程阻塞在select()方法上，下个调用select()方法的线程会立即“醒来（wake up）”。 close()用完Selector后调用其close()方法会关闭该Selector，且使注册到该Selector上的所有SelectionKey实例无效。通道本身并不会关闭。 完整的示例这里有一个完整的示例，打开一个Selector，注册一个通道注册到这个Selector上(通道的初始化过程略去),然后持续监控这个Selector的四种事件（接受，连接，读，写）是否就绪。 12345678910111213141516171819202122Selector selector = Selector.open();channel.configureBlocking(false);SelectionKey key = channel.register(selector, SelectionKey.OP_READ);while(true) &#123; int readyChannels = selector.select(); if(readyChannels == 0) continue; Set selectedKeys = selector.selectedKeys(); Iterator keyIterator = selectedKeys.iterator(); while(keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if(key.isAcceptable()) &#123; // a connection was accepted by a ServerSocketChannel. &#125; else if (key.isConnectable()) &#123; // a connection was established with a remote server. &#125; else if (key.isReadable()) &#123; // a channel is ready for reading &#125; else if (key.isWritable()) &#123; // a channel is ready for writing &#125; keyIterator.remove(); &#125;&#125; 7、FileChannelJava NIO中的FileChannel是一个连接到文件的通道。可以通过文件通道读写文件。 FileChannel无法设置为非阻塞模式，它总是运行在阻塞模式下。 打开FileChannel在使用FileChannel之前，必须先打开它。但是，我们无法直接打开一个FileChannel，需要通过使用一个InputStream、OutputStream或RandomAccessFile来获取一个FileChannel实例。下面是通过RandomAccessFile打开FileChannel的示例： 12RandomAccessFile aFile = new RandomAccessFile(&quot;data/nio-data.txt&quot;, &quot;rw&quot;);FileChannel inChannel = aFile.getChannel(); 从FileChannel读取数据调用多个read()方法之一从FileChannel中读取数据。如： 12ByteBuffer buf = ByteBuffer.allocate(48);int bytesRead = inChannel.read(buf); 首先，分配一个Buffer。从FileChannel中读取的数据将被读到Buffer中。 然后，调用FileChannel.read()方法。该方法将数据从FileChannel读取到Buffer中。read()方法返回的int值表示了有多少字节被读到了Buffer中。如果返回-1，表示到了文件末尾。 向FileChannel写数据使用FileChannel.write()方法向FileChannel写数据，该方法的参数是一个Buffer。如： 1234567891011String newData = &quot;New String to write to file...&quot; + System.currentTimeMillis();ByteBuffer buf = ByteBuffer.allocate(48);buf.clear();buf.put(newData.getBytes());buf.flip();while(buf.hasRemaining()) &#123; channel.write(buf);&#125; 注意FileChannel.write()是在while循环中调用的。因为无法保证write()方法一次能向FileChannel写入多少字节，因此需要重复调用write()方法，直到Buffer中已经没有尚未写入通道的字节。 关闭FileChannel用完FileChannel后必须将其关闭。如：1channel.close(); FileChannel的position方法有时可能需要在FileChannel的某个特定位置进行数据的读/写操作。可以通过调用position()方法获取FileChannel的当前位置。 也可以通过调用position(long pos)方法设置FileChannel的当前位置。 这里有两个例子:12long pos = channel.position();channel.position(pos +123); 如果将位置设置在文件结束符之后，然后试图从文件通道中读取数据，读方法将返回-1 —— 文件结束标志。 如果将位置设置在文件结束符之后，然后向通道中写数据，文件将撑大到当前位置并写入数据。这可能导致“文件空洞”，磁盘上物理文件中写入的数据间有空隙。 FileChannel的size方法FileChannel实例的size()方法将返回该实例所关联文件的大小。如:1long fileSize = channel.size(); FileChannel的truncate方法可以使用FileChannel.truncate()方法截取一个文件。截取文件时，文件将中指定长度后面的部分将被删除。如：1channel.truncate(1024); 这个例子截取文件的前1024个字节。 FileChannel的force方法FileChannel.force()方法将通道里尚未写入磁盘的数据强制写到磁盘上。出于性能方面的考虑，操作系统会将数据缓存在内存中，所以无法保证写入到FileChannel里的数据一定会即时写到磁盘上。要保证这一点，需要调用force()方法。 force()方法有一个boolean类型的参数，指明是否同时将文件元数据（权限信息等）写到磁盘上。 下面的例子同时将文件数据和元数据强制写到磁盘上： 查看源代码打印帮助1channel.force(true); 8、SocketChannelJava NIO中的SocketChannel是一个连接到TCP网络套接字的通道。可以通过以下2种方式创建SocketChannel： 打开一个SocketChannel并连接到互联网上的某台服务器。一个新连接到达ServerSocketChannel时，会创建一个SocketChannel。 打开 SocketChannel下面是SocketChannel的打开方式： 12SocketChannel socketChannel = SocketChannel.open();socketChannel.connect(new InetSocketAddress(&quot;http://jenkov.com&quot;, 80)); 关闭 SocketChannel 当用完SocketChannel之后调用SocketChannel.close()关闭SocketChannel： 1socketChannel.close(); 从 SocketChannel 读取数据要从SocketChannel中读取数据，调用一个read()的方法之一。以下是例子： 12ByteBuffer buf = ByteBuffer.allocate(48);int bytesRead = socketChannel.read(buf); 首先，分配一个Buffer。从SocketChannel读取到的数据将会放到这个Buffer中。 然后，调用SocketChannel.read()。该方法将数据从SocketChannel 读到Buffer中。read()方法返回的int值表示读了多少字节进Buffer里。如果返回的是-1，表示已经读到了流的末尾（连接关闭了）。 写入 SocketChannel写数据到SocketChannel用的是SocketChannel.write()方法，该方法以一个Buffer作为参数。示例如下： 1234567891011String newData = &quot;New String to write to file...&quot; + System.currentTimeMillis();ByteBuffer buf = ByteBuffer.allocate(48);buf.clear();buf.put(newData.getBytes());buf.flip();while(buf.hasRemaining()) &#123; channel.write(buf);&#125; 注意SocketChannel.write()方法的调用是在一个while循环中的。Write()方法无法保证能写多少字节到SocketChannel。所以，我们重复调用write()直到Buffer没有要写的字节为止。 非阻塞模式可以设置 SocketChannel 为非阻塞模式（non-blocking mode）.设置之后，就可以在异步模式下调用connect(), read() 和write()了。 connect()如果SocketChannel在非阻塞模式下，此时调用connect()，该方法可能在连接建立之前就返回了。为了确定连接是否建立，可以调用finishConnect()的方法。像这样： 123456socketChannel.configureBlocking(false);socketChannel.connect(new InetSocketAddress(&quot;http://jenkov.com&quot;, 80));while(! socketChannel.finishConnect() )&#123; //wait, or do something else...&#125; write()非阻塞模式下，write()方法在尚未写出任何内容时可能就返回了。所以需要在循环中调用write()。前面已经有例子了，这里就不赘述了。 read()非阻塞模式下,read()方法在尚未读取到任何数据时可能就返回了。所以需要关注它的int返回值，它会告诉你读取了多少字节。 非阻塞模式与选择器非阻塞模式与选择器搭配会工作的更好，通过将一或多个SocketChannel注册到Selector，可以询问选择器哪个通道已经准备好了读取，写入等。Selector与SocketChannel的搭配使用会在后面详讲。 9、ServerSocketChannelJava NIO中的 ServerSocketChannel 是一个可以监听新进来的TCP连接的通道, 就像标准IO中的ServerSocket一样。ServerSocketChannel类在 java.nio.channels包中。 这里有个例子： 12345678910ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();serverSocketChannel.socket().bind(new InetSocketAddress(9999));while(true)&#123; SocketChannel socketChannel = serverSocketChannel.accept(); //do something with socketChannel...&#125; 打开 ServerSocketChannel通过调用 ServerSocketChannel.open() 方法来打开ServerSocketChannel.如： 1ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); 关闭 ServerSocketChannel通过调用ServerSocketChannel.close() 方法来关闭ServerSocketChannel. 如： 1serverSocketChannel.close(); 监听新进来的连接通过 ServerSocketChannel.accept() 方法监听新进来的连接。当 accept()方法返回的时候,它返回一个包含新进来的连接的 SocketChannel。因此, accept()方法会一直阻塞到有新连接到达。 通常不会仅仅只监听一个连接,在while循环中调用 accept()方法. 如下面的例子： 123456while(true)&#123; SocketChannel socketChannel = serverSocketChannel.accept(); //do something with socketChannel...&#125; 当然,也可以在while循环中使用除了true以外的其它退出准则。 非阻塞模式ServerSocketChannel可以设置成非阻塞模式。在非阻塞模式下，accept() 方法会立刻返回，如果还没有新进来的连接,返回的将是null。 因此，需要检查返回的SocketChannel是否是null.如： 12345678910111213ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();serverSocketChannel.socket().bind(new InetSocketAddress(9999));serverSocketChannel.configureBlocking(false);while(true)&#123; SocketChannel socketChannel = serverSocketChannel.accept(); if(socketChannel != null)&#123; //do something with socketChannel... &#125;&#125; 10、Java NIO DatagramChannelJava NIO中的DatagramChannel是一个能收发UDP包的通道。因为UDP是无连接的网络协议，所以不能像其它通道那样读取和写入。它发送和接收的是数据包。 打开 DatagramChannel下面是 DatagramChannel 的打开方式： 12DatagramChannel channel = DatagramChannel.open();channel.socket().bind(new InetSocketAddress(9999)); 这个例子打开的 DatagramChannel可以在UDP端口9999上接收数据包。 接收数据通过receive()方法从DatagramChannel接收数据，如： 123ByteBuffer buf = ByteBuffer.allocate(48);buf.clear();channel.receive(buf); receive()方法会将接收到的数据包内容复制到指定的Buffer. 如果Buffer容不下收到的数据，多出的数据将被丢弃。 发送数据通过send()方法从DatagramChannel发送数据，如: 12345678String newData = &quot;New String to write to file...&quot; + System.currentTimeMillis();ByteBuffer buf = ByteBuffer.allocate(48);buf.clear();buf.put(newData.getBytes());buf.flip();int bytesSent = channel.send(buf, new InetSocketAddress(&quot;jenkov.com&quot;, 80)); 这个例子发送一串字符到”jenkov.com”服务器的UDP端口80。 因为服务端并没有监控这个端口，所以什么也不会发生。也不会通知你发出的数据包是否已收到，因为UDP在数据传送方面没有任何保证。 连接到特定的地址可以将DatagramChannel“连接”到网络中的特定地址的。由于UDP是无连接的，连接到特定地址并不会像TCP通道那样创建一个真正的连接。而是锁住DatagramChannel ，让其只能从特定地址收发数据。 这里有个例子: 1channel.connect(new InetSocketAddress(&quot;jenkov.com&quot;, 80)); 当连接后，也可以使用read()和write()方法，就像在用传统的通道一样。只是在数据传送方面没有任何保证。这里有几个例子： 12int bytesRead = channel.read(buf);int bytesWritten = channel.write(but); 11、PipeJava NIO 管道是2个线程之间的单向数据连接。Pipe有一个source通道和一个sink通道。数据会被写到sink通道，从source通道读取。 这里是Pipe原理的图示： 创建管道通过Pipe.open()方法打开管道。例如： 1Pipe pipe = Pipe.open(); 向管道写数据要向管道写数据，需要访问sink通道。像这样： 1Pipe.SinkChannel sinkChannel = pipe.sink(); 通过调用SinkChannel的write()方法，将数据写入SinkChannel,像这样： 12345678910String newData = &quot;New String to write to file...&quot; + System.currentTimeMillis();ByteBuffer buf = ByteBuffer.allocate(48);buf.clear();buf.put(newData.getBytes());buf.flip();while(buf.hasRemaining()) &#123; sinkChannel.write(buf);&#125; 从管道读取数据从读取管道的数据，需要访问source通道，像这样： 1Pipe.SourceChannel sourceChannel = pipe.source(); 调用source通道的read()方法来读取数据，像这样： 123ByteBuffer buf = ByteBuffer.allocate(48);int bytesRead = sourceChannel.read(buf); read()方法返回的int值会告诉我们多少字节被读进了缓冲区。 12、Java NIO与IO的对比当学习了Java NIO和IO的API后，一个问题马上涌入脑海： 我应该何时使用IO，何时使用NIO呢？在本文中，我会尽量清晰地解析Java NIO和IO的差异、它们的使用场景，以及它们如何影响您的代码设计。 Java NIO和IO的主要区别下表总结了Java NIO和IO之间的主要差别，我会更详细地描述表中每部分的差异。 IO NIO Stream oriented Buffer oriented Blocking IO Non blocking IO 无 Selectors 面向流与面向缓冲Java NIO和IO之间第一个最大的区别是，IO是面向流的，NIO是面向缓冲区的。 Java IO面向流意味着每次从流中读一个或多个字节，直至读取所有字节，它们没有被缓存在任何地方。此外，它不能前后移动流中的数据。如果需要前后移动从流中读取的数据，需要先将它缓存到一个缓冲区。 Java NIO的缓冲导向方法略有不同。数据读取到一个它稍后处理的缓冲区，需要时可在缓冲区中前后移动。这就增加了处理过程中的灵活性。但是，还需要检查是否该缓冲区中包含所有您需要处理的数据。而且，需确保当更多的数据读入缓冲区时，不要覆盖缓冲区里尚未处理的数据。 阻塞与非阻塞IOJava IO的各种流是阻塞的。这意味着，当一个线程调用read() 或 write()时，该线程被阻塞，直到有一些数据被读取，或数据完全写入。该线程在此期间不能再干任何事情了。 Java NIO的非阻塞模式，使一个线程从某通道发送请求读取数据，但是它仅能得到目前可用的数据，如果目前没有数据可用时，就什么都不会获取。而不是保持线程阻塞，所以直至数据变的可以读取之前，该线程可以继续做其他的事情。 非阻塞写也是如此。一个线程请求写入一些数据到某通道，但不需要等待它完全写入，这个线程同时可以去做别的事情。 线程通常将非阻塞IO的空闲时间用于在其它通道上执行IO操作，所以一个单独的线程现在可以管理多个输入和输出通道（channel）。 选择器（Selectors）Java NIO的选择器允许一个单独的线程来监视多个输入通道，你可以注册多个通道使用一个选择器，然后使用一个单独的线程来“选择”通道：这些通道里已经有可以处理的输入，或者选择已准备写入的通道。这种选择机制，使得一个单独的线程很容易来管理多个通道。 NIO和IO如何影响应用程序的设计无论您选择IO或NIO工具箱，可能会影响您应用程序设计的以下几个方面： 对NIO或IO类的API调用。 数据处理。 用来处理数据的线程数。 API调用当然，使用NIO的API调用时看起来与使用IO时有所不同，但这并不意外，因为并不是仅从一个InputStream逐字节读取，而是数据必须先读入缓冲区再处理。 数据处理使用纯粹的NIO设计相较IO设计，数据处理也受到影响。 在IO设计中，我们从InputStream或 Reader逐字节读取数据。假设你正在处理一基于行的文本数据流，例如： 1234Name: AnnaAge: 25Email: anna@mailserver.comPhone: 1234567890 该文本行的流可以这样处理： 1234567InputStream input = … ; // get the InputStream from the client socketBufferedReader reader = new BufferedReader(new InputStreamReader(input));String nameLine = reader.readLine();String ageLine = reader.readLine();String emailLine = reader.readLine();String phoneLine = reader.readLine(); 请注意处理状态由程序执行多久决定。换句话说，一旦reader.readLine()方法返回，你就知道肯定文本行就已读完， readline()阻塞直到整行读完，这就是原因。你也知道此行包含名称；同样，第二个readline()调用返回的时候，你知道这行包含年龄等。 正如你可以看到，该处理程序仅在有新数据读入时运行，并知道每步的数据是什么。一旦正在运行的线程已处理过读入的某些数据，该线程不会再回退数据（大多如此）。下图也说明了这条原则： 上图：从一个阻塞的流中读数据 而一个NIO的实现会有所不同，下面是一个简单的例子： 123ByteBuffer buffer = ByteBuffer.allocate(48);int bytesRead = inChannel.read(buffer); 注意第二行，从通道读取字节到ByteBuffer。当这个方法调用返回时，你不知道你所需的所有数据是否在缓冲区内。你所知道的是，该缓冲区包含一些字节，这使得处理有点困难。假设第一次 read(buffer)调用后，读入缓冲区的数据只有半行，例如，“Name:An”，你能处理数据吗？显然不能，需要等待，直到整行数据读入缓存，在此之前，对数据的任何处理毫无意义。 所以，你怎么知道是否该缓冲区包含足够的数据可以处理呢？好了，你不知道。发现的方法只能查看缓冲区中的数据。其结果是，在你知道所有数据都在缓冲区里之前，你必须检查几次缓冲区的数据。这不仅效率低下，而且可以使程序设计方案杂乱不堪。例如： 12345ByteBuffer buffer = ByteBuffer.allocate(48);int bytesRead = inChannel.read(buffer);while(! bufferFull(bytesRead) ) &#123;bytesRead = inChannel.read(buffer);&#125; bufferFull()方法必须跟踪有多少数据读入缓冲区，并返回真或假，这取决于缓冲区是否已满。换句话说，如果缓冲区准备好被处理，那么表示缓冲区满了。 bufferFull()方法扫描缓冲区，但必须保持在bufferFull（）方法被调用之前状态相同。如果没有，下一个读入缓冲区的数据可能无法读到正确的位置。这是不可能的，但却是需要注意的又一问题。 如果缓冲区已满，它可以被处理。如果它不满，并且在你的实际案例中有意义，你或许能处理其中的部分数据。但是许多情况下并非如此。下图展示了“缓冲区数据循环就绪”： 上图：从一个通道里读数据，直到所有的数据都读到缓冲区里 总结NIO可让您只使用一个（或几个）单线程管理多个通道（网络连接或文件），但付出的代价是解析数据可能会比从一个阻塞流中读取数据更复杂。 如果需要管理同时打开的成千上万个连接，这些连接每次只是发送少量的数据，例如聊天服务器，实现NIO的服务器可能是一个优势。同样，如果你需要维持许多打开的连接到其他计算机上，如P2P网络中，使用一个单独的线程来管理你所有出站连接，可能是一个优势。一个线程多个连接的设计方案如下图所示： 上图：单线程管理多个连接 如果你有少量的连接使用非常高的带宽，一次发送大量的数据，也许典型的IO服务器实现可能非常契合。下图说明了一个典型的IO服务器设计： 上图：一个典型的IO服务器设计：一个连接通过一个线程处理 注明：文章转载自 NIO|并发编程网，二次转载请务必注明原出处！","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"},{"name":"NIO","slug":"NIO","permalink":"http://www.54tianzhisheng.cn/tags/NIO/"}]},{"title":"《疯狂 Java 突破程序员基本功的 16 课》读书笔记","date":"2017-06-12T16:00:00.000Z","path":"2017/06/13/Java-16-lession/","text":"第 1 课 —— 数组与内存控制数组初始化数组初始化之后，该数组的长度是不可变的（可通过数组的 length 属性访问数组的长度）。Java 中的数组必须经过初始化（为数组对象的元素分配内存空间，并为每个数组元素指定初始值）才可使用。 数组初始化的形式： 静态初始化：初始化时由程序员显示的指定每个数组的初始值，系统决定数组长度。 动态初始化：初始化时程序员只指定数组的长度，系统为数组元素分配初始值。 使用数组数组元素就是变量：例如 int[] 数组元素相当于 int 类型的变量 当通过索引来使用数组元素时（访问数组元素的值、为数组元素赋值），将该数组元素当成普通变量使用即可。 第 2 课 —— 对象与内存的控制Java 内存管理分为：内存分配和内存回收。 内存分配：创建 Java 对象时 JVM 为该对象在堆内存中所分配的内存空间。 内存回收：当 Java 对象失去引用，变成垃圾，JVM 的垃圾回收机制自动清理该对象，并回收内存 实例变量 和 类变量局部变量特点：作用时间短，存储在方法的栈内存中 种类： 形参：方法签名中定义的局部变量，由方法调用者负责为其赋值，随方法结束而消亡 方法内的局部变量：方法内定义的局部变量，必须在方法内对其进行显示初始化，从初始化后开始生效，随方法结束而消亡 代码块内的局部变量：在代码块中定义的局部变量，必须在代码块中进行显示初始化，从初始化后开始生效，随代码块结束而消亡 成员变量类体内定义的变量，如果该成员变量没有使用 static 修饰，那该成员变量又被称为非静态变量或实例变量，如果使用 static 修饰，则该成员变量又可被称为静态变量或类变量。 实例变量和类变量的属性使用 static 修饰的成员变量是类变量，属于该类本身，没有使用 static 修饰的成员变量是实例变量，属于该类的实例，在同一个类中，每一个类只对应一个 Class 对象，但每个类可以创建多个对象。 由于同一个 JVM 内的每个类只对应一个 CLass 对象，因此同一个 JVM 内的一个类的类变量只需要一块内存空间；但对于实例变量而言，该类每创建一次实例，就需要为该实例变量分配一块内存空间。也就是说，程序中创建了几个实例，实例变量就需要几块内存空间。 这里我想到一道面试题目： 123456789101112public class A&#123; &#123; System.out.println(\"我是代码块\"); &#125; static&#123; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); &#125;&#125; 结果： 123我是静态代码块我是代码块我是代码块 静态代码块只执行一次，而代码块每创建一个实例，就会打印一次。 实例变量的初始化时机程序可在3个地方对实例变量执行初始化： 定义实例变量时指定初始值 非静态初始化块中对实例变量指定初始值 构造器中对实例变量指定初始值 上面第一种和第二种方式比第三种方式更早执行，但第一、二种方式的执行顺序与他们在源程序中的排列顺序相同。 同样在上面那个代码上加上一个变量 weight 的成员变量，我们来验证下上面的初始化顺序： 1、定义实例变量指定初始值 在 非静态初始化块对实例变量指定初始值 之后: 123456789101112131415public class A&#123; &#123; weight = 2.1; System.out.println(\"我是代码块\"); &#125; double weight = 2.0; static&#123; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); &#125;&#125; 结果是： 1234我是静态代码块我是代码块我是代码块2.0 2、定义实例变量指定初始值 在 非静态初始化块对实例变量指定初始值 之前: 123456789101112131415public class A&#123; double weight = 2.0; &#123; weight = 2.1; System.out.println(\"我是代码块\"); &#125; static&#123; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); &#125;&#125; 结果为： 1234我是静态代码块我是代码块我是代码块2.1 大家有没有觉得很奇怪？ 我来好好说清楚下： 定义实例变量时指定的初始值、初始代码块中为实例变量指定初始值的语句的地位是平等的，当经过编译器处理后，他们都将会被提取到构造器中。也就是说，这条语句 double weight = 2.0; 实际上会被分成如下 2 次执行： double weight; : 创建 Java 对象时系统根据该语句为该对象分配内存。 weight = 2.1; : 这条语句将会被提取到 Java 类的构造器中执行。 只说原理，大家肯定不怎么信，那么还有拿出源码来，这样才有信服能力的吗？是不？ 这里我直接使用软件将代码的字节码文件反编译过来，看看里面是怎样的组成？ 第一个代码的反编译源码如下： 1234567891011121314151617181920public class A&#123; double weight; public A() &#123; this.weight = 2.1D; System.out.println(\"我是代码块\"); this.weight = 2.0D; &#125; static &#123; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); &#125;&#125; 第二个代码反编译源码如下： 1234567891011121314151617181920public class A&#123; double weight; public A() &#123; this.weight = 2.0D; this.weight = 2.1D; System.out.println(\"我是代码块\"); &#125; static &#123; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); &#125;&#125; 这下子满意了吧！ 通过反编译的源码可以看到该类定义的 weight 实例变量时不再有初始值，为 weight 指定初始值的代码也被提到了构造器中去了，但是我们也可以发现之前规则也是满足的。 他们的赋值语句都被合并到构造器中，在合并过程中，定义的变量语句转换得到的赋值语句，初始代码块中的语句都转换得到的赋值语句，总是位于构造器的所有语句之前，合并后，两种赋值语句的顺序也保持了它们在 Java 源代码中的顺序。 大致过程应该了解了吧？如果还不怎么清楚的，建议还是自己将怎个过程在自己的电脑上操作一遍，毕竟光看不练假把式。 类变量的初始化时机JVM 对每一个 Java 类只初始化一次，因此 Java 程序每运行一次，系统只为类变量分配一次内存空间，执行一次初始化。程序可在两个地方对类变量执行初始化： 定义类变量时指定初始值 静态初始化代码块中对类变量指定初始值 这两种方式的执行顺序与它们在源代码中的排列顺序相同。 还是用上面那个示例，我们在其基础上加个被 static 修饰的变量 height： 1、定义类变量时指定初始值 在 静态初始化代码块中对类变量指定初始值 之后： 123456789101112131415161718public class A&#123; double weight = 2.0; &#123; weight = 2.1; System.out.println(\"我是代码块\"); &#125; static&#123; height = 10.1; System.out.println(\"我是静态代码块\"); &#125; static double height = 10.0; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); System.out.println(height); &#125;&#125; 运行结果： 12345我是静态代码块我是代码块我是代码块2.110.0 2、定义类变量时指定初始值 在 静态初始化代码块中对类变量指定初始值 之前： 123456789101112131415161718public class A&#123; static double height = 10.0; double weight = 2.0; &#123; weight = 2.1; System.out.println(\"我是代码块\"); &#125; static&#123; height = 10.1; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); System.out.println(height); &#125;&#125; 运行结果： 12345我是静态代码块我是代码块我是代码块2.110.1 其运行结果正如我们预料，但是我们还是看看反编译后的代码吧！ 第一种情况下反编译的代码： 1234567891011121314151617181920212223public class A&#123; double weight; public A() &#123; this.weight = 2.0D; this.weight = 2.1D; System.out.println(\"我是代码块\"); &#125; static &#123; System.out.println(\"我是静态代码块\"); &#125; static double height = 10.0D; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); System.out.println(height); &#125;&#125; 第二种情况下反编译的代码： 123456789101112131415161718192021222324public class A&#123; static double height = 10.0D; double weight; public A() &#123; this.weight = 2.0D; this.weight = 2.1D; System.out.println(\"我是代码块\"); &#125; static &#123; height = 10.1D; System.out.println(\"我是静态代码块\"); &#125; public static void main(String[] args) &#123; A a = new A(); A a1 = new A(); System.out.println(a.weight); System.out.println(height); &#125;&#125; 通过反编译源码，可以看到第一种情况下(定义类变量时指定初始值 在 静态初始化代码块中对类变量指定初始值 之后): 我们在 静态初始化代码块中对类变量指定初始值 已经不存在了，只有一个类变量指定的初始值 static double height = 10.0D; , 而在第二种情况下（定义类变量时指定初始值 在 静态初始化代码块中对类变量指定初始值 之前）和之前的源代码顺序是一样的，没啥区别。 上面的代码中充分的展示了类变量的两种初始化方式 ：每次运行该程序时，系统会为 A 类执行初始化，先为所有类变量分配内存空间，再按照源代码中的排列顺序执行静态初始代码块中所指定的初始值和定义类变量时所指定的初始值。 父类构造器当创建任何 Java 对象时，程序总会先依次调用每个父类非静态初始化代码块、父类构造器（总是从 Object 开始）执行初始化，最后才调用本类的非静态初始化代码块、构造器执行初始化。 隐式调用和显示调用当调用某个类的构造器来创建 Java 对象时，系统总会先调用父类的非静态初始化代码块进行初始化。这个调用是隐式执行的，而且父类的静态初始化代码块总是会被执行。接着会调用父类的一个或多个构造器执行初始化，这个调用既可以是通过 super 进行显示调用，也可以是隐式调用。 当所有父类的非静态初始代码块、构造器依次调用完成后，系统调用本类的非静态代码块、构造器执行初始化，最后返回本类的实例。至于调用父类的哪个构造器执行初始化，分以下几种情况： 子类构造器执行体的第一行代码使用 super 显式调用父类构造器，系统将根据 super 调用里传入的实参列表来确定调用父类的哪个构造器； 子类构造器执行体的第一行代码使用 this 显式调用本类中的重载构造器，系统将根据 this 调用里传入的实参列表来确定奔雷的另一个构造器（执行本类中另一个构造器时即进入第一种情况）； 子类构造器中既没有 super 调用，也没有 this 调用，系统将会在执行子类构造器之前，隐式调用父类无参构造器。 注：super 和 this 必须在构造器的第一行，且不能同时存在。 推荐一篇博客：Java初始化顺序 文章从无继承和继承两种情况下分析了 Java 初始化的顺序。 Java初始化顺序如图： 访问子类对象的实例变量调用被子类重写的方法父子实例的内存控制继承成员变量和继承方法的区别方法的行为总是表现出它们实际类型的行为；实例变量的值总是表现出声明这些变量所用类型的行为。 内存中的子类实例父、子类的类变量final 修饰符final 可以修饰变量、方法、类。 修饰变量，变量被赋初始值之后，不能够对他在进行修改 修饰方法，不能够被重写 修饰类，不能够被继承 final 修饰的实例变量只能在如下位置指定初始值： 定义 final 实例变量时指定初始值 在非静态代码块中为 final 实例变量指定初始值 在构造器中为 final 实例变量指定初始值 final 修饰的类变量只能在如下位置指定初始值： 定义 final 类变量时指定初始值 在静态代码块中为 final 类变量指定初始值 第 3 课 —— 常见 Java 集合的实现细节Java 集合框架类图： Set 和 MapSet 代表一种集合元素无序、集合元素不可重复的集合，Map 则代表一种由多个 key-value 对组合的集合，Map 集合类似于传统的关联数组。 Set 和 Map 的关系1、Map 集合中的 key 不能重复且没有顺序。将这些 key 组合起来就是一个 Set 集合。所以有一个 Set&lt;k&gt; keySet() 方法来返回所有 key 组成的 Set 集合。 2、Set 也可以转换成 Map。（在 Set 中将 每一对 key 和 value 存放在一起） HashMap 和 HashSetHashSet：系统采用 Hash 算法决定集合元素的存储位置。（基于 HashMap 实现的） HashMap：系统将 value 当成 key 的附属，系统根据 Hash 算法决定 key 的存储位置。 HashSet 的绝大部分方法都是通过调用 HashMap 的方法实现的，因此 HashSet 和 HashMap 两个集合在实现本质上是相同的。 TreeMap 和 TreeSetTreeSet 底层使用 TreeMap 来包含 Set 集合中的所有元素。 TreeMap 采用的是一种“红黑树”的排序二叉树来保存 Map 中每个 Entry —— 每个 Entry 都被当成 “红黑树” 的一个节点对待。 Map 和 ListMap 的 values() 方法不管是 HashMap 还是 TreeMap ，它们的 values() 方法都可以返回其所有 value 组成的 Collection 集合，其实是一个不存储元素的 Collection 集合，当程序遍历 Collection 集合时，实际上就是遍历 Map 对象的 value。 HashMap 和 TreeMap 的 values() 方法并未把 Map 中的 values 重新组合成一个包含元素的集合对象，这样就可以降低系统内存开销。 Map 和 List 的关系底层实现很相似；用法上很相似。 Map 接口提供 get(K key) 方法允许 Map 对象根据 key 来取得 value； List 接口提供了 get(int index) 方法允许 List 对象根据元素索引来取得 value； ArrayList 和 LinkedListList 集合的实现类，主要有 ArrayList 、Vector 和 LinkedList。 ArrayList 是一个可改变大小的数组.当更多的元素加入到 ArrayList 中时, 其大小将会动态地增长. 内部的元素可以直接通过 get 与 set 方法进行访问, 因为 ArrayList 本质上就是一个数组. LinkedList 是一个双链表, 在添加和删除元素时具有比 ArrayList 更好的性能. 但在 get 与 set 方面弱于ArrayList. 当然, 这些对比都是指数据量很大或者操作很频繁的情况下的对比, 如果数据和运算量很小,那么对比将失去意义. Vector 和 ArrayList 类似, 但属于强同步类。如果你的程序本身是线程安全的(thread-safe,没有在多个线程之间共享同一个集合/对象),那么使用 ArrayList 是更好的选择。 Vector 和 ArrayList 在更多元素添加进来时会请求更大的空间。Vector 每次请求其大小的双倍空间，而 ArrayList每次对 size 增长 50%. 而 LinkedList 还实现了 Queue 接口, 该接口比 List 提供了更多的方法,包括 offer(), peek(), poll()等. 注意: 默认情况下 ArrayList 的初始容量非常小, 所以如果可以预估数据量的话, 分配一个较大的初始值属于最佳实践, 这样可以减少调整大小的开销。 ArrayList与LinkedList性能对比 时间复杂度对比如下: LinkedList 更适用于: 没有大规模的随机读取 大量的增加/删除操作 Iterator 迭代器是一个迭代器接口，专门用于迭代各种 Collection 集合，包括 Set 集合和 List 集合。 第 4 课 —— Java 的内存回收Java 引用的种类对象在内存中的状态JVM 垃圾回收机制，是否回收一个对象的标准在于：是否还有引用变量引用该对象？只要有引用变量引用该对象，垃圾回收机制就不会回收它。 Java 语言对对象的引用有： 强引用 软引用 弱引用 虚引用 强引用程序创建一个对象，并把这个对象赋给一个引用变量，这个引用变量就是强引用。当一个对象被一个或者一个以上的强引用变量所引用时，它处于可达状态，它是不会被系统的垃圾回收机制回收。 软引用软引用需要通过 SoftReference 类来实现，当一个对象只具有软引用时，它有可能会被垃圾回收机制回收。对于只有软引用的对象而言，当系统内存空间足够时，它不会被系统回收，程序也可使用该对象；当系统内存空间不足时，系统将会回收它。 弱引用弱引用和软引用有点相似，区别在于弱引用所引用对象的生存期更短。 虚引用虚引用主要用于跟踪对象被垃圾回收的状态，虚引用不能单独使用，虚引用必须和引用队列联合使用。 Java 的内存泄漏ArrayList.java 中的 remove 方法 1234567891011public E remove(int index) &#123; rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work return oldValue; &#125; 其中 elementData[--size] = null; // clear to let GC do its work 语句是清除数组元素的引用，避免内存的泄漏，如果没有这句的话，那么就是只有两个作用： 修饰 Stack 的属性，也就是将值减 1； 返回索引为 size -1 的值。 垃圾回收机制 跟踪并监控每个 Java 对象，当某个对象处于不可达状态时，回收该对象所占用的内存。 清理内存分配，回收过程中产生的内存碎片。 垃圾回收的基本算法对于一个垃圾回收器的设计算法来说，大概有如下几个设计： 串行回收 和 并行回收 串行回收：不管系统有多少个 CPU，始终使用一个 CPU 来执行垃圾回收操作 并行回收：把整个回收工作拆分成多部分，每个部分由一个 CPU 负责，从而让多个 CPU 并行回收 并发执行 和 应用程序停止 压缩 和 不压缩 和 复制 复制：将堆内分成两个相同的空间，从根开始访问每一个关联的可达对象，将空间A的可达对象全部复制到空间B，然后一次性回收整个空间A。 标记清除：也就是 不压缩 的回收方式。垃圾回收器先从根开始访问所有可达对象，将它们标记为可达状态，然后再遍历一次整个内存区域，把所有没有标记为可达的对象进行回收处理。 标记压缩：这是压缩方式，这种方式充分利用上述两种算法的优点，垃圾回收器先从根开始访问所有可达对象，将他们标记为可达状态，接下来垃圾回收器会将这些活动对象搬迁在一起，这个过程叫做内存压缩，然后垃圾回收机制再次回收那些不可达对象所占用的内存空间，这样就避免了回收产生的内存碎片。 堆内存的分代回收1、Young 代 2、Old 代 3、Permanent 代 内存管理小技巧 尽量使用直接量 使用 StringBuilder 和 StringBuffer 进行字符串拼接 尽早释放无用对象的引用 尽量少用静态变量 避免在经常调用的方法、循环中创建 Java 对象 缓存经常使用的对象 尽量不要使用 finalize 方法 考虑使用 SoftReference 第 5 课 —— 表达式中的陷阱关于字符串的陷阱JVM 对字符串的处理String java = new String(&quot;Java&quot;) 这句创建了两个字符串对象，一个是 “Java” 这个直接量对应的字符串对象，另外一个是 new String() 构造器返回的字符串对象。 Java 程序中创建对象的方法： 通过 new 调用构造器创建 Java 对象 通过 Class 对象的 newInstance() 方法调用构造器创建 Java 对象 通过 Java 的反序列化机制从 IO 流中恢复 Java 对象 通过 Java 对象提供的 clone() 方法复制一个新的 Java 对象 对于字符串以及 Byte、Short、Int、Long、Character、Float、Double 和 Boolean 这些基本类型的包装类 直接量的方式来创建 Java 对象 Integer in = 5； 通过简单的算法表达式，连接运算来创建 Java 对象 String str = “a” + “b”; （如果这个字符串表达式的值在编译时确定下来，那么 JVM 会在编译时计算该字符串变量的值，并让它指向字符串池中对应的字符串。如果这些算法表达式都是字符串直接量、整数直接量，没有变量和方法参与，那么就可以在编译期就可以确定字符串的值；如果使用了变量、调用了方法，那么只有等到运行时才能确定字符串表达式的值；如果字符串连接运算所有的变量都可执行 “宏替换”（使用 final 修饰的变量），那在编译时期也能确定字符串连接表达式的值） 对于 Java 程序的字符直接量，JVM 会使用一个字符串池来保护它们；当第一次使用某个字符串直接量时，JVM 会将它放入字符串池进行缓存。在一般的情况下，字符串池中的字符串对象不会被垃圾回收器回收，当程序再次需要使用该字符串时，无需重新创建一个新的字符串，而是直接让引用变量指向字符串池中已有的字符串。 不可变的字符串String 类是一个不可变类，当一个 String 对象创建完成后，该 String 类里包含的字符序列就被固定下来，以后永远不能修改。 如果程序需要一个字符序列会发生改变的字符串，那么建议使用 StringBuilder （效率比 StringBuffer 高） 字符串比较如果要比较两个字符串是否相同，用 == 进行判断就行，但如果要判断两个字符串所包含的字符序列是否相同，则应该用 String 重写过的 equals() 方法进行比较。 123456789101112131415161718192021222324252627public boolean equals(Object anObject) &#123; //如果两个字符串相同 if (this == anObject) &#123; return true; &#125; //如果anObject是String类型 if (anObject instanceof String) &#123; String anotherString = (String)anObject; //n代表字符串的长度 int n = value.length; //如果两个字符串长度相等 if (n == anotherString.value.length) &#123; //获取当前字符串、anotherString底层封装的字符数组 char v1[] = value; char v2[] = anotherString.value; int i = 0; //逐一比较v1 和 v2数组中的每个字符 while (n-- != 0) &#123; if (v1[i] != v2[i]) return false; i++; &#125; return true; &#125; &#125; return false; &#125; 还可以使用 String 提供的 compareTo() 方法返回两个字符串的大小 123456789101112131415161718public int compareTo(String anotherString) &#123; int len1 = value.length; int len2 = anotherString.value.length; int lim = Math.min(len1, len2); char v1[] = value; char v2[] = anotherString.value; int k = 0; while (k &lt; lim) &#123; char c1 = v1[k]; char c2 = v2[k]; if (c1 != c2) &#123; return c1 - c2; &#125; k++; &#125; return len1 - len2; &#125; 表达式类型的陷阱表达式类型的自动提升 所有 byte、short、char类型将被提升到 int 类型参与运算 整个算术表达式的数据类型自动提升到与表达式中最高等级操作数同样的类型，操作数的等级排列如下：char -&gt; int -&gt; long -&gt;float -&gt; double byte -&gt; short -&gt; int -&gt; long -&gt;float -&gt; double 复合赋值运算符的陷阱Java 语言允许所有的双目运算符和 = 一起结合组成复合赋值运算符，如 +=、-=、*=、/=、%= 、&amp;= 等，复合赋值运算符包含了一个隐式的类型转换。 123//下面这两条语句不等价a = a + 5; //a += 5; //实际上等价于 a = (a的类型) (a + 5); 复合赋值运算符会自动的将它计算的结果值强制转换为其左侧变量的类型。 输入法导致的陷阱注释的字符必须合法转义字符的陷阱 慎用字符的 Unicode 转义形式 中止行注释的转义字符 泛型可能引起的错误原始类型变量的赋值 当程序把一个原始类型的变量赋给一个带有泛型信息的变量时，总是可以通过编译（只是会提示警告信息） 当程序试图访问带泛型声明的集合的集合元素时，编译器总是把集合元素当成泛型类型处理（它并不关心集合里集合元素的实际类型） 当程序试图访问带泛型声明的集合的集合元素时，JVM会遍历每个集合元素自动执行强制转型，如果集合元素的实际类型与集合所带的泛型信息不匹配，运行时将引发 ClassCastException 原始类型带来的擦除当把一个具有泛型信息的对象赋给另一个没有泛型信息的变量时，所有在尖括号之间的类型信息都会丢弃。 创建泛型数组的陷阱Java 中不允许创建泛型数组 正则表达式的陷阱有些符号本身就是正则表达式，我们需要对符号做转义运算。 多线程的陷阱不要调用 run 方法开启线程是用 start() 方法，而不是 run() 方法。 静态的同步方法对于同步代码块而言，程序必须显式为它指定同步监视器；对于同步非静态方法而言，该方法的同步监视器是 this —— 即调用该方法的 Java 对象；对于静态的同步方法而言，该方法的同步监视器不是 this，而是该类本身。 第 6 课 —— 流程控制的陷阱switch 语句陷阱break 语句不要忘记写 switch 的表达式类型： byte short int char enum String （Jdk 1.7 以后有 String） 标签引起的陷阱Java 中的标签通常是和循环中的 break 和 continue 结合使用，让 break 直接终止标签所标识的循环，让 continue 语句忽略标签所标识的循环的剩下语句。 。。 第 7 课 —— 面向对象的陷阱instanceof 运算符的陷阱instanceof 它用于判断前面的对象是否是后面的类或其子类、实现类的实例。如果是返回 true，否则返回 false。 instanceof 运算符前面操作数的编译时类型必须是： 要么与后面的类相同 要么是后面类的父类 要么是后面类型的子类 构造器陷阱构造器是 Java 中每个类都会提供的一个“特殊方法”。构造器负责对 Java 对象执行初始化操作，不管是定义实例变量时指定的初始值，还是在非静态初始化代码块中所做的操作，实际上都会被提取到构造器中执行。 构造器不能声明返回值类型，也不能使用void声明构造器没有返回值。 构造器创建对象吗构造器并不会创建 Java 对象，构造器只是负责执行初始化，在构造器执行之前，Java 对象所需要的内存空间，是由 new 关键字申请出来的。绝大部分时候，程序使用 new 关键字为一个 Java 对象申请空间之后，都需要使用构造器为这个对象执行初始化，但在某些时候，程序创建 Java 对象无需调用构造器，如下： 使用反序列化的方式恢复 Java 对象 使用 clone 方法复制 Java 对象 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778package com.zhisheng.test;import java.io.*;/** * Created by 10412 on 2017/5/31. */class Wolf implements Serializable&#123; private String name; public Wolf(String name) &#123; System.out.println(\"调用了有参构造方法\"); this.name = name; &#125; @Override public boolean equals(Object o) &#123; if (this == o) return true; if (o == null || getClass() != o.getClass()) return false; Wolf wolf = (Wolf) o; return name != null ? name.equals(wolf.name) : wolf.name == null; &#125; @Override public int hashCode() &#123; return name != null ? name.hashCode() : 0; &#125;&#125;public class SerializableTest&#123; public static void main(String[] args) &#123; Wolf w = new Wolf(\"灰太狼\"); System.out.println(\"对象创建完成\"); Wolf w2 = null; ObjectInputStream ois = null; ObjectOutputStream oos = null; try &#123; //创建输出对象流 oos = new ObjectOutputStream(new FileOutputStream(\"a.bin\")); //创建输入对象流 ois = new ObjectInputStream(new FileInputStream(\"a.bin\")); //序列输出java 对象 oos.writeObject(w); oos.flush(); //反序列化恢复java对象 w2 = (Wolf) ois.readObject(); System.out.println(w); System.out.println(w2); //两个对象的实例变量值完全相等，输出true System.out.println(w.equals(w2)); //两个对象不同，输出false System.out.println(w == w2); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125;finally &#123; if (ois!=null) try &#123; ois.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; if (oos!=null) try &#123; oos.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 程序运行结果： 123456调用了有参构造方法对象创建完成com.zhisheng.test.Wolf@1b15382com.zhisheng.test.Wolf@1b15382truefalse 正如结果所示：创建 Wolf 对象时，程序调用了相应的构造器来对该对象执行初始化；当程序通过反序列化机制恢复 Java 对象时，系统无需在调用构造器来进行初始化。通过反序列化恢复出来的 Wolf 对象和原来的 Wolf 对象具有完全相同的实例变量值，但系统会产生两个对象。 无限递归构造器12345678910111213public class ConstrutionTest&#123; ConstrutionTest ct; &#123; ct = new ConstrutionTest(); &#125; public ConstrutionTest() &#123; System.out.println(\"无参构造器\"); &#125; public static void main(String[] args) &#123; ConstrutionTest ct = new ConstrutionTest(); &#125;&#125; 运行结果抛出异常 java.lang.StackOverflowError 因为不管定义实例变量时指定的初始值，还是在非静态初始化代码块中执行的初始化操作，最终都将提取到构造器中执行，因为代码中递归调用了类的构造器，最终导致出现 java.lang.StackOverflowError 异常。 到底调用哪个重载方法1、第一阶段 JVM 将会选取所有可获得并匹配调用的方法或者构造器 2、第二个阶段决定到底要调用哪个方法，此时 JVM 会在第一阶段所选取的方法或者构造器中再次选取最精确匹配的那一个。 1234567891011121314151617public class OverrideTest&#123; public void info(Object obj, int a) &#123; System.out.println(\"obj 参数\" + obj); System.out.println(\"整型参数 \" + a); &#125; public void info(Object[] obj, double a) &#123; System.out.println(\"obj 参数\" + obj); System.out.println(\"整型参数 \" + a); &#125; public static void main(String[] args) &#123; OverrideTest o = new OverrideTest(); o.info(null, 5); &#125;&#125; 报错如下： 12Error:(20, 10) java: 对info的引用不明确 com.zhisheng.test.OverrideTest 中的方法 info(java.lang.Object,int) 和 com.zhisheng.test.OverrideTest 中的方法 info(java.lang.Object[],double) 都匹配 在这种复杂的条件下，JVM 无法判断哪个方法更匹配实际调用，将会导致程序编译错误。 方法重写的陷阱无法重写父类 private 方法。如果子类有一个与父类 private 方法具有相同方法名、相同形参列表、相同返回值类型的方法，依然不是重写，只是子类定义了一个与父类相同的方法。 static 关键字static 可以修饰类中定义的成员：field、方法、内部类、初始化代码块、内部枚举类 静态方法属于类被 static 修饰的成员（field、方法、内部类、初始化块、内部枚举类）属于类本身，而不是单个的 Java 对象。静态方法也是属于类。 第 8 课 —— 异常捕捉的陷阱正确关闭资源的方式 使用 finally 块来保证回收，保证关闭操作总是会被执行 关闭每个资源之前首先保证引用该资源的引用变量不为 null 为每个物理资源单独使用 try .. catch 块关闭资源，保证关闭资源时引发的异常不会影响其他资源的关闭。 finally 块陷阱finally 执行顺序，看我以前写的一篇文章《深度探究Java 中 finally 语句块》。 catch 块用法在 try 块后使用 catch 块来捕获多个异常时，程序应该小心多个 catch 块之间的顺序：捕获父类异常的 catch 块都应该排在捕获子类异常的 catch 块之后（先处理小异常，再处理大异常），否则出现编译错误。 继承得到的异常子类重写父类方法时，不能声明抛出比父类方法类型更多、范围更大的异常。 二叉树性质： 二叉树第 i 层上的节点数目至多为 2 ^(i - 1) (i &gt;= 1) 深度为 k 的二叉树至多有 2 ^ k - 1 个节点 在任何一颗二叉树中，如果其叶子结点的数量为 n0，度为 2 的子节点数量为 n2，则 n0 = n2 + 1 具有 n 个节点的完全二叉树的深度为 log n + 1 (log 的底为 2) 对于一棵有 n 个节点的完全二叉树的节点按层自左向右编号，则对任一编号为 i 的节点有如下性质： 当 i == 1 时，节点 i 是二叉树的根；若 i &gt; 1 时，则节点的父节点是 i/2 当 2i &lt;= n，则节点 i 有左孩子，左孩子的编号是 2i，否则，节点无左孩子，并且是叶子结点 若 2i + 1 &lt;= n ，则节点 i 有右孩子，右孩子的编号是 2i + 1；否则，节点无右孩子。 对于一颗 n 个节点的完全二叉树的节点按层自左向右编号，1 ~ n/2 范围的节点都是有孩子节点的非叶子结点，其余的节点全部都是叶子结点。编号为 n/2 的节点有可能只有左节点，也可能既有左节点，又有右节点。 选择排序直接选择排序需要经过 n - 1 趟比较 第一趟比较：程序将记录定位在第一个数据上，拿第一个数据依次和它后面的每个数据进行比较，如果第一个数据大于后面某个数据，交换它们。。依此类推，经过第一趟比较，这组数据中最小的数据被选出来，它被排在第一位。 第二趟比较：程序将记录定位在第二个数据上，拿第二个数据依次和它后面每个数据进行比较，如果第二个数据大于后面某个数据，交换它们。。依次类推，经过第二趟比较，这组数据中第二小的数据被选出，它排在第二位 。。 按此规则一共进行 n-1 趟比较，这组数据中第 n - 1小（第二大）的数据被选出，被排在第 n -1 位（倒数第一位）；剩下的就是最大的数据，它排在最后。 直接选择排序的优点就是算法简单，容易实现，缺点就是每趟只能确定一个元素，n个数组需要进行 n-1 趟比较。 堆排序 建堆 拿堆的根节点和最后一个节点交换 交换排序冒泡排序第一趟：依次比较0和1，1和2，2和3 … n-2 和 n - 1 索引的元素，如果发现第一个数据大于后一个数据，交换它们，经过第一趟，最大的元素排到了最后。 第二趟：依次比较0和1，1和2，2和3 … n-3 和 n - 2 索引的元素，如果发现第一个数据大于后一个数据，交换它们，经过第二趟，第二大的元素排到了倒数第二位 。。 第 n -1 趟：依次比较0和1元素，如果发现第一个数据大于后一个数据，交换它们，经过第 n - 1 趟，第二小的元素排到了第二位。 快速排序从待排的数据序列中任取一个数据作为分界值，所有比它小的数据元素一律放在左边，所有比他大的元素一律放在右边，这样一趟下来，该序列就分成了两个子序列，接下来对两个子序列进行递归，直到每个子序列只剩一个，排序完成。 插入排序直接插入排序依次将待排序的数据元素按其关键字值的大小插入前面的有序序列。 折半插入排序当第 i - 1 趟需要将第 i 个元素插入前面的 0 ~ i -1 个元素序列中时： 计算 0 ~ i - 1 索引的中间点，也就是用 i 索引处的元素和 （0 + i - 1）/2 索引处的元素进行比较，如果 i 索引处的元素大，就直接在 （（0 + i - 1）/2 ） ~ （i - 1）后半个范围内进行搜索，反之在前半个范围搜索。 重复上面步骤 确定第 i 个元素的插入位置，就将该位置的后面所有元素整体后移一位，然后将第 i 个元素放入该位置。","tags":[{"name":"Java","slug":"Java","permalink":"http://www.54tianzhisheng.cn/tags/Java/"}]},{"title":"通过项目逐步深入了解Mybatis（一）","date":"2017-06-11T16:00:00.000Z","path":"2017/06/12/通过项目逐步深入了解Mybatis(一)/","text":"Mybatis 和 SpringMVC 通过订单商品案例驱动 官方中文地址：http://www.mybatis.org/mybatis-3/zh/ 官方托管地址：https://github.com/mybatis/mybatis-3 本项目全部代码地址：https://github.com/zhisheng17/mybatis 如果觉得不错的话，欢迎给个 star ， 如果你想完善这个项目的话，你也可以 fork 后修改然后推送给我。 基础知识：对原生态 jdbc 程序（单独使用 jdbc 开发）问题总结1、环境​ java 环境 ：jdk1.8.0_77 ​ 开发工具 ： IDEA 2016.1 ​ 数据库 ： MySQL 5.7 2、创建数据库​ mybatis_test.sql ​ Tables ：items、orderdetail、orders、user 3、JDBC 程序​ 使用 JDBC 查询 MySQL 数据库中用户表的记录 ​ 代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980package cn.zhisheng.mybatis.jdbc;/** * Created by 10412 on 2016/11/27. */import java.sql.*;/** *通过单独的jdbc程序来总结问题 */public class JdbcTest&#123; public static void main(String[] args) &#123; //数据库连接 Connection connection = null; //预编译的Statement，使用预编译的Statement可以提高数据库性能 PreparedStatement preparedStatement = null; //结果集 ResultSet resultSet = null; try &#123; //加载数据库驱动 Class.forName(\"com.mysql.jdbc.Driver\"); //通过驱动管理类获取数据库链接 connection = DriverManager.getConnection(\"jdbc:mysql://localhost:3306/mybatis_test?characterEncoding=utf-8\", \"root\", \"root\"); //定义sql语句 ?表示占位符（在这里表示username） String sql = \"select * from user where username = ?\"; //获取预处理statement preparedStatement = connection.prepareStatement(sql); //设置参数，第一个参数为sql语句中参数的序号（从1开始），第二个参数为设置的参数值 preparedStatement.setString(1, \"王五\"); //向数据库发出sql执行查询，查询出结果集 resultSet = preparedStatement.executeQuery(); //遍历查询结果集 while(resultSet.next()) &#123; System.out.println(resultSet.getString(\"id\")+\" \"+resultSet.getString(\"username\")); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally&#123; //释放资源 if(resultSet!=null) &#123; try &#123; resultSet.close(); &#125; catch (SQLException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; if(preparedStatement!=null) &#123; try &#123; preparedStatement.close(); &#125; catch (SQLException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; if(connection!=null) &#123; try &#123; connection.close(); &#125; catch (SQLException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125; &#125;&#125; 4、问题总结 数据库连接，使用时就创建，不使用立即释放，对数据库频繁连接开启和关闭，造成数据库资源的浪费，影响数据库性能。 解决方法：使用数据库连接池管理数据库连接。 将 sql 语句硬编码到 java 代码中，如果 sql 语句需要修改，那么就需要重新编译 java 代码，不利于系统的维护。 设想：将 sql 语句配置在 xml 配置文件中，即使 sql 语句发生变化，也不需要重新编译 java 代码。 向 preparedStatement 中设置参数，对占位符号位置和设置参数值，硬编码在 java 代码中，同样也不利于系统的维护。 设想：将 sql 语句、占位符、参数值配置在 xml 配置文件中。 从 resultSet 中遍历结果集数据时，存在硬编码，将获取表的字段进行硬编码，不利于系统维护。 设想：将查询的结果集自动映射成 java 对象。 Mybatis框架原理（掌握）1、Mybatis 是什么？​ Mybatis 是一个持久层的架构，是 appach 下的顶级项目。 ​ Mybatis 原先是托管在 googlecode 下，再后来是托管在 Github 上。 ​ Mybatis 让程序员将主要的精力放在 sql 上，通过 Mybatis 提供的映射方式，自由灵活生成（半自动，大部分需要程序员编写 sql ）满足需要 sql 语句。 ​ Mybatis 可以将向 preparedStatement 中的输入参数自动进行输入映射，将查询结果集灵活的映射成 java 对象。（输出映射） 2、Mybatis 框架 注解： SqlMapConfig.xml （Mybatis的全局配置文件，名称不定）配置了数据源、事务等 Mybatis 运行环境 Mapper.xml 映射文件（配置 sql 语句） SqlSessionFactory （会话工厂）根据配置文件配置工厂、创建 SqlSession SqlSession （会话）面向用户的接口、操作数据库（发出 sql 增删改查） Executor （执行器）是一个接口（基本执行器、缓存执行器）、SqlSession 内部通过执行器操作数据库 Mapped Statement （底层封装对象）对操作数据库存储封装，包括 sql 语句、输入参数、输出结果类型 ​ Mybatis入门程序1、需求实现以下功能： 根据用户id查询一个用户信息 根据用户名称模糊查询用户信息列表 添加用户 更新用户 删除用户 2、环境java 环境 ：jdk1.8.0_77 开发工具 ： IDEA 2016.1 数据库 ： MySQL 5.7 Mybatis 运行环境（ jar 包） MySQL 驱动包 其他依赖包 3、 log4j.properties在classpath下创建log4j.properties如下： 1234567# Global logging configuration#在开发环境日志级别要设置为DEBUG、生产环境要设置为INFO或者ERRORlog4j.rootLogger=DEBUG, stdout# Console output...log4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%5p [%t] - %m%n Mybatis默认使用log4j作为输出日志信息。 4、工程结构 5、SqlMapConfig.xml配置 Mybatis 的运行环境、数据源、事务等 1234567891011121314151617181920&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\"&gt;&lt;configuration&gt; &lt;!-- 和spring整合后 environments配置将废除--&gt; &lt;environments default=\"development\"&gt; &lt;environment id=\"development\"&gt; &lt;!-- 使用jdbc事务管理,事务由 Mybatis 控制--&gt; &lt;transactionManager type=\"JDBC\" /&gt; &lt;!-- 数据库连接池,由Mybatis管理，数据库名是mybatis_test，Mysql用户名root，密码root --&gt; &lt;dataSource type=\"POOLED\"&gt; &lt;property name=\"driver\" value=\"com.mysql.jdbc.Driver\" /&gt; &lt;property name=\"url\" value=\"jdbc:mysql://localhost:3306/mybatis_test?characterEncoding=utf-8\" /&gt; &lt;property name=\"username\" value=\"root\" /&gt; &lt;property name=\"password\" value=\"root\" /&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt;&lt;/configuration&gt; 6、创建 po 类Po 类作为 mybatis 进行 sql 映射使用，po 类通常与数据库表对应，User.java 如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package cn.zhisheng.mybatis.po;import java.util.Date;/** * Created by 10412 on 2016/11/28. */public class User&#123; private int id; private String username; // 用户姓名 private String sex; // 性别 private Date birthday; // 生日 private String address; // 地址 //getter and setter public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getUsername() &#123; return username; &#125; public void setUsername(String username) &#123; this.username = username; &#125; public Date getBirthday() &#123; return birthday; &#125; public void setBirthday(Date birthday) &#123; this.birthday = birthday; &#125; public String getSex() &#123; return sex; &#125; public void setSex(String sex) &#123; this.sex = sex; &#125; public String getAddress() &#123; return address; &#125; public void setAddress(String address) &#123; this.address = address; &#125;&#125; 7、根据用户 id（主键）查询用户信息 映射文件 User.xml（原在 Ibatis 中命名）在 Mybatis 中命名规则为 xxxmapper.xml 在映射文件中配置 sql 语句 User.xml 123456&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;!DOCTYPE mapperPUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\"\"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"test\"&gt;&lt;/mapper&gt; namespace ：命名空间，对 sql 进行分类化管理，用于隔离 sql 语句，后面会讲另一层非常重要的作用。 ​ 在 User.xml 中加入 123456789101112&lt;!--通过select执行数据库查询 id:标识映射文件中的sql 将sql语句封装到mappedStatement对象中，所以id称为Statement的id #&#123;&#125;：表示占位符 #&#123;id&#125;：其中的id表示接收输入的参数，参数名称就是id，如果输入参数是简单类型，那么#&#123;&#125;中的参数名可以任意，可以是value或者其他名称 parameterType：表示指定输入参数的类型 resultType：表示指定sql输出结果的所映射的java对象类型 --&gt;&lt;!-- 根据id获取用户信息 --&gt; &lt;select id=\"findUserById\" parameterType=\"int\" resultType=\"cn.zhisheng.mybatis.po.User\"&gt; select * from user where id = #&#123;id&#125; &lt;/select&gt; User.xml 映射文件已经完全写好了，那接下来就需要在 SqlMapConfig.xml中加载映射文件 User.xml 1234&lt;!--加载映射文件--&gt; &lt;mappers&gt; &lt;mapper resource=\"sqlmap/User.xml\"/&gt; &lt;/mappers&gt; ​ 编写程序 `MybatisFirst.java` ​ 123456789101112131415161718192021222324252627282930313233343536373839404142434445package cn.zhisheng.mybatis.first;import cn.zhisheng.mybatis.po.User;import org.apache.ibatis.io.Resources;import org.apache.ibatis.session.SqlSession;import org.apache.ibatis.session.SqlSessionFactory;import org.apache.ibatis.session.SqlSessionFactoryBuilder;import org.junit.Test;import java.io.IOException;import java.io.InputStream; /*** Created by 10412 on 2016/11/28.*/public class MybatisFirst&#123; //根据id查询用户信息，得到用户的一条记录 @Test public void findUserByIdTest() throws IOException &#123; //Mybatis 配置文件 String resource = \"SqlMapConfig.xml\"; //得到配置文件流 InputStream inputStream = Resources.getResourceAsStream(resource); //创建会话工厂,传入Mybatis的配置文件信息 SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); //通过工厂得到SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //通过SqlSession操作数据库 //第一个参数：映射文件中Statement的id，等于 = namespace + \".\" + Statement的id //第二个参数：指定和映射文件中所匹配的parameterType类型的参数 //sqlSession.selectOne 结果与映射文件中所匹配的resultType类型的对象 User user = sqlSession.selectOne(\"test.findUserById\", 1); System.out.println(user); //释放资源 sqlSession.close(); &#125;&#125; 然后运行一下这个测试，发现结果如下就代表可以了： 8、根据用户名称模糊查询用户信息列表 映射文件 依旧使用 User.xml 文件，只不过要在原来的文件中加入 123456789&lt;!-- 自定义条件查询用户列表 resultType：指定就是单条记录所映射的java对象类型 $&#123;&#125;:表示拼接sql串，将接收到的参数内容不加修饰的拼接在sql中 使用$&#123;&#125;拼接sql，会引起sql注入 $&#123;value&#125;：接收输入参数的内容，如果传入类型是简单类型，$&#123;&#125;中只能够使用value--&gt; &lt;select id=\"findUserByUsername\" parameterType=\"java.lang.String\" resultType=\"cn.zhisheng.mybatis.po.User\"&gt; select * from user where username like '%$&#123;value&#125;%' &lt;/select&gt; 编写程序 依旧直接在刚才那个 MybatisFirst.java 中加入测试代码： 12345678910111213141516171819202122232425262728//根据用户名称模糊查询用户信息列表 @Test public void findUserByUsernameTest() throws IOException &#123; //Mybatis 配置文件 String resource = \"SqlMapConfig.xml\"; //得到配置文件流 InputStream inputStream = Resources.getResourceAsStream(resource); //创建会话工厂,传入Mybatis的配置文件信息 SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); //通过工厂得到SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //通过SqlSession操作数据库 //第一个参数：映射文件中Statement的id，等于 = namespace + \".\" + Statement的id //第二个参数：指定和映射文件中所匹配的parameterType类型的参数 //selectList 查询结果可能多条 //list中的user和映射文件中resultType所指定的类型一致 List&lt;User&gt; list = sqlSession.selectList(\"test.findUserByUsername\", \"小明\"); System.out.println(list); //释放资源 sqlSession.close(); &#125; 同样测试一下findUserByUsernameTest ，如果运行结果如下就代表没问题： 提示：通过这个代码可以发现，其中有一部分代码是冗余的，我们可以将其封装成一个函数。 1234567public void createSqlSessionFactory() throws IOException &#123; // 配置文件 String resource = \"SqlMapConfig.xml\"; InputStream inputStream = Resources.getResourceAsStream(resource); // 使用SqlSessionFactoryBuilder从xml配置文件中创建SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); &#125; 注意：1、#{ } 和 ${ } 的区别 #{ }表示一个占位符号，通过#{ }可以实现 preparedStatement 向占位符中设置值，自动进行java 类型和 jdbc 类型转换，#{ } 可以有效防止sql注入。#{ } 可以接收简单类型值或 pojo 属性值（通过 OGNL 读取对象中的值，属性.属性.属性..方式获取对象属性值）。 如果 parameterType 传输单个简单类型值，#{ }括号中可以是 value 或其它名称。 ${ } 表示拼接 sql 串，通过${ }可以将 parameterType 传入的内容拼接在 sql 中且不进行 jdbc 类型转换， ${ }可以接收简单类型值或 pojo 属性值（（通过 OGNL 读取对象中的值，属性.属性.属性..方式获取对象属性值）），如果 parameterType 传输单个简单类型值，${}括号中只能是 value。 2、parameterType 和 resultType 区别 parameterType：指定输入参数类型，mybatis 通过 ognl 从输入对象中获取参数值拼接在 sql 中。 resultType：指定输出结果类型，mybatis 将 sql 查询结果的一行记录数据映射为 resultType 指定类型的对象。 3、selectOne 和 selectList 区别 selectOne 查询一条记录来进行映射，如果使用selectOne查询多条记录则抛出异常： org.apache.ibatis.exceptions.TooManyResultsException: Expected one result (or null) to bereturned by selectOne(), but found: 3 at selectList 可以查询一条或多条记录来进行映射。 9、添加用户 映射文件 在 User.xml 中加入： 12345678&lt;!-- 添加用户 --&gt; &lt;insert id=\"insetrUser\" parameterType=\"cn.zhisheng.mybatis.po.User\" &gt; &lt;selectKey keyProperty=\"id\" order=\"AFTER\" resultType=\"java.lang.Integer\"&gt; select LAST_INSERT_ID() &lt;/selectKey&gt; insert into user(username, birthday, sex, address) values(#&#123;username&#125;, #&#123;birthday&#125;, #&#123;sex&#125;, #&#123;address&#125;) &lt;/insert&gt; 注意: selectKey将主键返回，需要再返回 添加selectKey实现将主键返回 keyProperty:返回的主键存储在pojo中的哪个属性 order：selectKey的执行顺序，是相对与insert语句来说，由于mysql的自增原理执行完insert语句之后才将主键生成，所以这里selectKey的执行顺序为after resultType:返回的主键是什么类型 LAST_INSERT_ID():是mysql的函数，返回auto_increment自增列新记录id值。 然后在 MybatisFirst.java 中写一个测试函数，代码如下 123456789101112131415161718192021@Test public void insetrUser() throws IOException, ParseException &#123; //Mybatis 配置文件 String resource = \"SqlMapConfig.xml\"; //得到配置文件流 InputStream inputStream = Resources.getResourceAsStream(resource); //创建会话工厂,传入Mybatis的配置文件信息 SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); //通过工厂得到SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); User user = new User(); SimpleDateFormat sdf = new SimpleDateFormat (\"yyyy-MM-dd\"); user.setUsername(\"田志声\"); user.setSex(\"男\"); user.setBirthday(sdf.parse(\"2016-11-29\")); user.setAddress(\"江西南昌\"); sqlSession.insert(\"test.insetrUser\", user); sqlSession.commit(); //释放资源 sqlSession.close(); &#125; 然后 run 一下，如果出现的结果如下，那么就是成功了。 同时数据库也能查询到刚插入的用户信息： 10、自增主键返回 与 非自增主键返回 MySQL 自增主键：执行 insert 提交之前自动生成一个自增主键，通过 MySQL 函数获取到刚插入记录的自增主键： LAST_INSERT_ID() ，是在 insert 函数之后调用。 非自增主键返回：使用 MySQL 的 uuid() 函数生成主键，需要修改表中 id 字段类型为 String ，长度设置为 35 位，执行思路：先通过 uuid() 查询到主键，将主键输入到 sql 语句中；执行 uuid() 语句顺序相对于 insert 语句之前执行。 刚才那个插入用户的地方，其实也可以通过 uuid() 来生成主键，如果是这样的话，那么我们就需要在 User.xml 中加入如下代码： 123456789&lt;!--使用 MySQL 的 uuid()生成主键 执行过程： 首先通过uuid()得到主键，将主键设置到user对象的id属性中 其次执行insert时，从user对象中取出id属性值 --&gt;&lt;selectKey keyProperty=\"id\" order=\"BEFORE\" resultType=\"java.lang.String\"&gt; select uuid()&lt;/selectKey&gt;insert into user(id, username, birthday, sex, address) values(#&#123;id&#125;, #&#123;username&#125;, #&#123;birthday&#125;, #&#123;sex&#125;, #&#123;address&#125;) Oracle 使用序列生成主键 首先自定义一个序列且用于生成主键，selectKey使用如下： 12345678&lt;insert id=\"insertUser\" parameterType=\"cn.itcast.mybatis.po.User\"&gt; &lt;selectKey resultType=\"java.lang.Integer\" order=\"BEFORE\" keyProperty=\"id\"&gt; SELECT 自定义序列.NEXTVAL FROM DUAL &lt;/selectKey&gt;insert into user(id,username,birthday,sex,address) values(#&#123;id&#125;,#&#123;username&#125;,#&#123;birthday&#125;,#&#123;sex&#125;,#&#123;address&#125;)&lt;/insert&gt; ​ 11、删除用户前面说了这么多了，这里就简单来说明下： 在 User.xml 文件中加入如下代码： 1234&lt;!--删除用户--&gt; &lt;delete id=\"deleteUserById\" parameterType=\"int\"&gt; delete from user where user.id = #&#123;id&#125; &lt;/delete&gt; 在 MybatisFirst.java 文件中加入如下代码： 1234567891011121314151617181920212223242526272829//删除用户 @Test public void deleteUserByIdTest() throws IOException &#123; //Mybatis 配置文件 String resource = \"SqlMapConfig.xml\"; //得到配置文件流 InputStream inputStream = Resources.getResourceAsStream(resource); //创建会话工厂,传入Mybatis的配置文件信息 SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); //通过工厂得到SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //通过SqlSession操作数据库 //第一个参数：映射文件中Statement的id，等于 = namespace + \".\" + Statement的id //第二个参数：指定和映射文件中所匹配的parameterType类型的参数 sqlSession.delete(\"test.deleteUserById\", 26); //提交事务 sqlSession.commit(); //释放资源 sqlSession.close(); &#125; 测试结果如下： 之前的数据库 user 表查询结果： 执行完测试代码后，结果如下： 12、更新用户信息在 User.xml 中加入如下代码： 123456789&lt;!--根据id更新用户 需要输入用户的id 传入用户要更新的信息 parameterType指定user对象，包括id和更新信息，id必须存在 #&#123;id&#125;：从输入对象中获取id属性值--&gt;&lt;update id=\"updateUserById\" parameterType=\"cn.zhisheng.mybatis.po.User\"&gt; update user set username = #&#123;username&#125;, birthday = #&#123;birthday&#125;, sex = #&#123;sex&#125;, address = #&#123;address&#125; where user.id = #&#123;id&#125; &lt;/update&gt; 然后在 MybatisFirst.java 中加入 12345678910111213141516171819202122232425262728293031323334353637//根据id更新用户信息 @Test public void updateUserByIdTest() throws IOException, ParseException &#123; //Mybatis 配置文件 String resource = \"SqlMapConfig.xml\"; //得到配置文件流 InputStream inputStream = Resources.getResourceAsStream(resource); //创建会话工厂,传入Mybatis的配置文件信息 SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); //通过工厂得到SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //为了设置生日的日期输入 SimpleDateFormat sdf = new SimpleDateFormat (\"yyyy-MM-dd\"); User user = new User(); //根据id更新用户信息 user.setId(24); user.setUsername(\"张四风\"); user.setBirthday(sdf.parse(\"2015-01-12\")); user.setSex(\"女\"); user.setAddress(\"上海黄埔\"); //通过SqlSession操作数据库 //第一个参数：映射文件中Statement的id，等于 = namespace + \".\" + Statement的id //第二个参数：指定和映射文件中所匹配的parameterType类型的参数 sqlSession.update(\"test.updateUserById\", user); //提交事务 sqlSession.commit(); //释放资源 sqlSession.close(); &#125; 测试结果如下： 查看数据库，id 为 24 的用户信息是否更新了： 啊，是不是很爽，所有的需求都完成了。 没错，这只是 Mybatis 的一个简单的入门程序，简单的实现了对数据库的增删改查功能，通过这个我们大概可以了解这个编程方式了。 期待接下来的 Mybatis高级知识文章吧！ 更多文章请见 微信公众号：猿blog","tags":[{"name":"Mybatis","slug":"Mybatis","permalink":"http://www.54tianzhisheng.cn/tags/Mybatis/"},{"name":"SpringMVC","slug":"SpringMVC","permalink":"http://www.54tianzhisheng.cn/tags/SpringMVC/"}]},{"title":" 六月 —— 愿你做最美好的自己！","date":"2017-06-01T16:00:00.000Z","path":"2017/06/02/poetry2/","text":"美妙的六月已经到来，送你八封信，愿时光轻缓，愿微风正好，愿你做最美好的自己。 第一封 —— 关于压力 鸡蛋，从外打破是食物，从内打破是生命。人生亦是，从外打破是压力，从内打破是成长。如果你等待别人打破你，那么你注定成为别人的食物；如果能让自己从内打破，那么你会发现自己的成长相当于一种重生。 第二封 —— 关于读书 读书是一种充实人生的艺术，没有书的人生就像空心的竹子一样，空洞无物。犹太人让孩子们亲吻涂有蜂蜜的书本，是为了让他们记住：书本是甜的，要让甜蜜充满人生就要读书。读书是一本人生最难得的存折，一点一滴地积累，最后你会发现：自己是世界上最富有的人。第三封 —— 关于人际关系 你可以要求自己守信，但不能要求别人守信；你可以要求自己对人好，但不能期待别人对你好。你怎样对人，并不代表人家就会怎么对你。如果看不透这一点，你只会徒添不必要的烦恼。 第四封 —— 关于孤独 每个人都要经历一段孤独的日子，每段路都有一段独孤的时光。父母不可能一直帮着你，朋友也不可能一直围着你转。孤独不是孤僻，更不是寂寞。经历过孤独的人，内心更坚强，不管处于什么样的环境都能让自己安静，更好地调整状态，面对环境。 第五封 —— 关于修养 看别人不顺眼，是自己修养不够。人愤怒的那一瞬间，智商是零，过一分钟后恢复正常。人的优雅关键在于控制自己的惰绪，用嘴伤害人，是最愚蠢的一种行为。 第六封 —— 关于现实 现实有太多的不如意，就算生活给你的是垃圾，你同样能把垃圾踩在脚底下登上世界之巅。你要把自己逼出最大的潜能，没有人会为你的未来买单，你要么努力向上爬，要么烂在社会最底层的泥里，这就是生活。 第七封 —— 关于自己 一个人经过不同程度的鍛炼，就获得不同程度的修养。好比香料，捣得愈碎，磨得愈细，香得愈浓烈。我们曾如此渴望命运的波澜，到最后才发现：人生最曼妙的风景，竟是内心的淡定与从容。我们曾如此期盼外界的认可，到最后才知道：世界是自己的，与他人毫无关系。 第八封 —— 关于幸福 常有人说，我现在不幸福，等我结了婚或买了房……就幸福了。事实是，幸福的人在哪儿都幸福，不幸福的人在哪儿都不幸福。所以要先培养自己的幸福力，不论发生什么，别人都动不了你的自在开心。这才是真正强大的气场和自信。 幸福的人生，需要三种姿态：对过去，要淡；对现在，要惜；对未来，要信。愿你拥有幸福的能力，做最美好的自己。","tags":[{"name":"随笔","slug":"随笔","permalink":"http://www.54tianzhisheng.cn/tags/随笔/"}]},{"title":"最近很火的鸡汤，分享给大家","date":"2017-05-11T16:00:00.000Z","path":"2017/05/12/poetry/","text":"&lt;最近很火的一首小诗，分享给大家&gt; 纽约时间比加州时间早三个小时， New York is 3 hours ahead of California, 但加州时间并没有变慢。 but it does not make California slow. 有人22岁就毕业了， Someone graduated at the age of 22, 但等了五年才找到好的工作！ but waited 5 years before securing a good job! 有人25岁就当上CEO， Someone became a CEO at 25, 却在50岁去世。 and died at 50. 也有人迟到50岁才当上CEO， While another became a CEO at 50, 然后活到90岁。 and lived to 90 years. 有人依然单身， Someone is still single, 同时也有人已婚。 while someone else got married. 奥巴马55岁就退休， Obama retires at 55, 川普70岁才开始当总统。 but Trump starts at 70. 世上每个人本来就有自己的发展时区。 Absolutely everyone in this world works based on their Time Zone. 身边有些人看似走在你前面， People around you might seem to go ahead of you, 也有人看似走在你后面。 some might seem to be behind you. 但其实每个人在自己的时区有自己的步程。 But everyone is running their own RACE, in their own TIME. 不用嫉妒或嘲笑他们。 Don’t envy them or mock them. 他们都在自己的时区里，你也是！ They are in their TIME ZONE, and you are in yours! 生命就是等待正确的行动时机。 Life is about waiting for the right moment to act. 所以，放轻松。 So, RELAX. 你没有落后。 You’re not LATE. 你没有领先。 You’re not EARLY. 在命运为你安排的属于自己的时区里，一切都准时。 You are very much ON TIME, and in your TIME ZONE Destiny set up for you.","tags":[{"name":"随笔","slug":"随笔","permalink":"http://www.54tianzhisheng.cn/tags/随笔/"}]},{"title":"Github pages + Hexo 博客 yilia 主题使用畅言评论系统","date":"2017-04-12T16:00:00.000Z","path":"2017/04/13/Hexo-yilia-changyan/","text":"前言Hexo的Yilia主题由于原来使用的是多说的留言板，近期多说公告要停止提供服务了，所以我就把多说换成搜狐的畅言了，下面写一个简单的小教程。 注册畅言进入畅言官网 , 点击右上角 “免费注册”，并填写注册信息。（注意域名需要备案信息） 登录并进入畅言后台注册完后，登录进入畅言官网，获取你的畅言 app id 和 app key。 使用畅言系统下面说下修改评论为畅言的方法，其实方法和多说是差不多的。 1、修改 themes\\yilia\\layout\\_partial\\article.ejs 模板，把如下代码 1234567&lt;% if (!index &amp;&amp; post.comments &amp;&amp; config.disqus_shortname)&#123; %&gt; &lt;section id=\"comments\"&gt; &lt;div id=\"disqus_thread\"&gt; 这里还有很多代码 &lt;/div&gt; &lt;/section&gt; &lt;% &#125; %&gt; 修改为： 1234567891011121314151617181920 &lt;% if (!index &amp;&amp; post.comments)&#123; %&gt; &lt;section id=&quot;comments&quot;&gt;&lt;!--高速版，加载速度快，使用前需测试页面的兼容性--&gt;&lt;div id=&quot;SOHUCS&quot; sid=&quot;&lt;%= page.title %&gt;&quot;&gt;&lt;/div&gt;&lt;script&gt; (function()&#123; var appid = &apos;你的APP ID&apos;, conf = &apos;你的APP KEY&apos;; var doc = document, s = doc.createElement(&apos;script&apos;), h = doc.getElementsByTagName(&apos;head&apos;)[0] || doc.head || doc.documentElement; s.type = &apos;text/javascript&apos;; s.charset = &apos;utf-8&apos;; s.src = &apos;http://assets.changyan.sohu.com/upload/changyan.js?conf=&apos;+ conf +&apos;&amp;appid=&apos; + appid; h.insertBefore(s,h.firstChild); window.SCS_NO_IFRAME = true; &#125;)()&lt;/script&gt; &lt;/section&gt; &lt;% &#125; %&gt; 上面的APP ID和APP KEY是在畅言设置中得到。 这里需要注意一点的是：sid=&quot;&lt;%= page.title %&gt;&quot;&gt; 这样的话畅言就可以直接根据对应的文章来识别，使得文章有对应的评论，不会都乱在一起。 2、在每篇文章开头的 front-matter 中添加一句comments: true，然后回到博客根目录执行命令 hexo d -g ，重新生成博客并部署博客，然后刷新，任选一篇文章进入下拉，会发现评论功能可以使用了。 修改 BUG但是，这是你会发现一个 Bug，表情按钮点击不了，原因是被左侧的 div 层覆盖了，回到我们刚才改过的代码，找到 &lt;div id=&quot;SOHUCS&quot; 开头的一串代码。并做如下更改 1&lt;div id=\"SOHUCS\" sid=\"&lt;%=title %&gt;\" style=\"padding: 0px 30px 0px 46px;\"&gt;&lt;/div&gt; 加上上面这一段样式代码，即可修复。 参考文章： 1、Hexo博客yilia主题更换畅言评论系统 2、在Hexo中使用畅言评论系统 新增由于问题太多了，所以新写了篇文章：Github page + Hexo + yilia 搭建博客可能会遇到的所有疑问","tags":[{"name":"hexo","slug":"hexo","permalink":"http://www.54tianzhisheng.cn/tags/hexo/"},{"name":"yilia","slug":"yilia","permalink":"http://www.54tianzhisheng.cn/tags/yilia/"}]}]